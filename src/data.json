[{"id": "0033890579", "domain": "Glaucoma (unspecified)", "model_name": "Tan et al.", "publication_date": "2021/05/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33890579/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33890579", "task": null, "abstract": "By 2050, almost 5 billion people globally are projected to have myopia, of whom 20% are likely to have high myopia with clinically significant risk of sight-threatening complications such as myopic macular degeneration. These are diagnoses that typically require specialist assessment or measurement with multiple unconnected pieces of equipment. Artificial intelligence (AI) approaches might be effective for risk stratification and to identify individuals at highest risk of visual loss. However, unresolved challenges for AI medical studies remain, including paucity of transparency, auditability, and traceability. In this retrospective multicohort study, we developed and tested retinal photograph-based deep learning algorithms for detection of myopic macular degeneration and high myopia, using a total of 226\u2008686 retinal images. First we trained and internally validated the algorithms on datasets from Singapore, and then externally tested them on datasets from China, Taiwan, India, Russia, and the UK. We also compared the performance of the deep learning algorithms against six human experts in the grading of a randomly selected dataset of 400 images from the external datasets. As proof of concept, we used a blockchain-based AI platform to demonstrate the real-world application of secure data transfer, model transfer, and model testing across three sites in Singapore and China. The deep learning algorithms showed robust diagnostic performance with areas under the receiver operating characteristic curves [AUC] of 0\u00b7969 (95% CI 0\u00b7959-0\u00b7977) or higher for myopic macular degeneration and 0\u00b7913 (0\u00b7906-0\u00b7920) or higher for high myopia across the external testing datasets with available data. In the randomly selected dataset, the deep learning algorithms outperformed all six expert graders in detection of each condition (AUC of 0\u00b7978 [0\u00b7957-0\u00b7994] for myopic macular degeneration and 0\u00b7973 [0\u00b7941-0\u00b7995] for high myopia). We also successfully used blockchain technology for data transfer, model transfer, and model testing between sites and across two countries. Deep learning algorithms can be effective tools for risk stratification and screening of myopic macular degeneration and high myopia among the large global population with myopia. The blockchain platform developed here could potentially serve as a trusted platform for performance testing of future AI models in medicine. None.", "keywords": ["billion people globally", "myopic macular degeneration", "deep learning algorithms", "clinically significant risk", "macular degeneration", "deep learning", "myopic macular", "learning algorithms", "high myopia", "billion people", "people globally", "globally are projected", "clinically significant", "sight-threatening complications", "myopia", "macular", "learning", "algorithms", "degeneration", "deep"], "paper_title": "Retinal photograph-based deep learning algorithms for myopia and a blockchain platform to facilitate artificial intelligence medical research: a retrospective multicohort study.", "last_updated": "2023/02/04"}, {"id": "0033785808", "domain": "Macular degeneration", "model_name": "Chou et al.", "publication_date": "2021/03/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33785808/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33785808", "task": null, "abstract": "Polypoidal choroidal vasculopathy (PCV) and neovascular age-related macular degeneration (nAMD) share some similarity in clinical imaging manifestations. However, their disease entity and treatment strategy as well as visual outcomes are very different. To distinguish these two vision-threatening diseases is somewhat challenging but necessary. In this study, we propose a new artificial intelligence model using an ensemble stacking technique, which combines a color fundus photograph-based deep learning (DL) model and optical coherence tomography-based biomarkers, for differentiation of PCV from nAMD. Furthermore, we introduced multiple correspondence analysis, a method of transforming categorical data into principal components, to handle the dichotomous data for combining with another image DL system. This model achieved a robust performance with an accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve of 83.67%, 80.76%, 84.72%, and 88.57%, respectively, by training nearly 700 active cases with suitable imaging quality and transfer learning architecture. This work could offer an alternative method of developing a multimodal DL model, improve its efficiency for distinguishing different diseases, and facilitate the broad application of medical engineering in a DL model design.", "keywords": ["Polypoidal choroidal vasculopathy", "age-related macular degeneration", "neovascular age-related macular", "clinical imaging manifestations", "Polypoidal choroidal", "choroidal vasculopathy", "macular degeneration", "share some similarity", "neovascular age-related", "age-related macular", "similarity in clinical", "imaging manifestations", "clinical imaging", "model", "PCV", "Polypoidal", "vasculopathy", "degeneration", "share", "manifestations"], "paper_title": "Deep learning and ensemble stacking technique for differentiating polypoidal choroidal vasculopathy from neovascular age-related macular degeneration.", "last_updated": "2023/02/04"}, {"id": "0035220077", "domain": "Macular degeneration", "model_name": "Ma et al.", "publication_date": "2022/02/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35220077/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35220077", "task": "IWqQC1koJA", "abstract": "This study aims to achieve an automatic differential diagnosis between two types of retinal pathologies with similar pathological features - Polypoidal choroidal vasculopathy (PCV) and wet age-related macular degeneration (AMD) from volumetric optical coherence tomography (OCT) images, and identify clinically-relevant pathological features, using an explainable deep-learning-based framework. This is a retrospective study with data from a cross-sectional cohort. The OCT volume of 73 eyes from 59 patients was included in this study. Disease differentiation was achieved through single-B-scan-based classification followed by a volumetric probability prediction aggregation step. We compared different labeling strategies with and without identifying pathological B-scans within each OCT volume. Clinical interpretability was achieved through normalized aggregation of B-scan-based saliency maps followed by maximum-intensity-projection onto the en face plane. We derived the PCV score from the proposed differential diagnosis framework with different labeling strategies. The en face projection of saliency map was validated with the pathologies identified in Indocyanine green angiography (ICGA). Model trained with both labeling strategies achieved similar level differentiation power (>90%), with good correspondence between pathological features detected from the projected en face saliency map and ICGA. This study demonstrated the potential clinical application of non-invasive differential diagnosis using AI-driven OCT-based analysis, with minimal requirement of labeling efforts, along with clinical explainability achieved through automatically detected disease-related pathologies.", "keywords": ["Polypoidal choroidal vasculopathy", "age-related macular degeneration", "optical coherence tomography", "wet age-related macular", "Polypoidal choroidal", "volumetric optical coherence", "identify clinically-relevant pathological", "choroidal vasculopathy", "macular degeneration", "coherence tomography", "aims to achieve", "achieve an automatic", "types of retinal", "wet age-related", "age-related macular", "optical coherence", "identify clinically-relevant", "AMD", "OCT volume", "Polypoidal"], "paper_title": "Clinical explainable differential diagnosis of polypoidal choroidal vasculopathy and age-related macular degeneration using deep learning.", "last_updated": "2023/02/04"}, {"id": "0033516917", "domain": "Diabetic retinopathy", "model_name": "Fu et al.", "publication_date": "2021/01/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33516917/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33516917", "task": null, "abstract": "To evaluate the predictive usefulness of quantitative imaging biomarkers, acquired automatically from OCT scans, of cross-sectional and future visual outcomes of patients with neovascular age-related macular degeneration (AMD) starting anti-vascular endothelial growth factor (VEGF) therapy. Retrospective cohort study. Treatment-naive, first-treated eyes of patients with neovascular AMD between 2007 and 2017 at Moorfields Eye Hospital (a large, United Kingdom single center) undergoing anti-VEGF therapy. Automatic segmentation was carried out by applying a deep learning segmentation algorithm to 137 379 OCT scans from 6467 eyes of 3261 patients with neovascular AMD. After applying selection criteria, 926 eyes of 926 patients were analyzed. Correlation coefficients (R<sup>2</sup> values) and mean absolute error (MAE) between quantitative OCT (qOCT) parameters and cross-sectional visual function, as well as the predictive value of these parameters for short-term visual change, that is, incremental visual acuity (VA) resulting from an individual injection, as well as VA at distant time points (up to 12 months after baseline). Visual acuity at distant time points could be predicted: R<sup>2</sup>\u00a0= 0.80 (MAE, 5.0 Early Treatment Diabetic Retinopathy Study [ETDRS] letters) and R<sup>2</sup>\u00a0= 0.7 (MAE, 7.2 ETDRS letters) after injection at 3 and at 12 months after baseline (P < 0.001 for both), respectively. Best performing models included both baseline qOCT parameters and treatment response. Furthermore, we present proof-of-principle evidence that the incremental change in VA from an injection can be predicted: R<sup>2</sup>\u00a0= 0.14 (MAE, 5.6 ETDRS letters) for injection 2 and R<sup>2</sup>\u00a0= 0.11 (MAE, 5.0 ETDRS letters) for injection 3 (P < 0.001 for both). Automatic segmentation enables rapid acquisition of quantitative and reproducible OCT biomarkers with potential to inform treatment decisions in the care of neovascular AMD. This furthers development of point-of-care decision-aid systems for personalized medicine.", "keywords": ["age-related macular degeneration", "starting anti-vascular endothelial", "endothelial growth factor", "anti-vascular endothelial growth", "neovascular age-related macular", "neovascular AMD", "ETDRS letters", "MAE", "future visual outcomes", "Moorfields Eye Hospital", "acquired automatically", "macular degeneration", "starting anti-vascular", "growth factor", "age-related macular", "anti-vascular endothelial", "endothelial growth", "AMD", "ETDRS", "VEGF"], "paper_title": "Predicting Incremental and Future Visual Change in Neovascular Age-Related Macular Degeneration Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0034211137", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2021/07/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34211137/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34211137", "task": "IWqQC1koJA", "abstract": "To present and validate a deep ensemble algorithm to detect diabetic retinopathy (DR) and diabetic macular oedema (DMO) using retinal fundus images. A total of 8739 retinal fundus images were collected from a retrospective cohort of 3285 patients. For detecting DR and DMO, a multiple improved Inception-v4 ensembling approach was developed. We measured the algorithm's performance and made a comparison with that of human experts on our primary dataset, while its generalization was assessed on the publicly available Messidor-2 dataset. Also, we investigated systematically the impact of the size and number of input images used in training on model's performance, respectively. Further, the time budget of training/inference versus model performance was analyzed. On our primary test dataset, the model achieved an 0.992 (95% CI, 0.989-0.995) AUC corresponding to 0.925 (95% CI, 0.916-0.936) sensitivity and 0.961 (95% CI, 0.950-0.972) specificity for referable DR, while the sensitivity and specificity for ophthalmologists ranged from 0.845 to 0.936, and from 0.912 to 0.971, respectively. For referable DMO, our model generated an AUC of 0.994 (95% CI, 0.992-0.996) with a 0.930 (95% CI, 0.919-0.941) sensitivity and 0.971 (95% CI, 0.965-0.978) specificity, whereas ophthalmologists obtained sensitivities ranging between 0.852 and 0.946, and specificities ranging between 0.926 and 0.985. This study showed that the deep ensemble model exhibited excellent performance in detecting DR and DMO, and had good robustness and generalization, which could potentially help support and expand DR/DMO screening programs.", "keywords": ["detect diabetic retinopathy", "diabetic macular oedema", "retinal fundus images", "diabetic retinopathy", "detect diabetic", "diabetic macular", "retinal fundus", "fundus images", "macular oedema", "present and validate", "DMO", "diabetic", "model", "images", "model performance", "performance", "fundus", "retinal", "deep ensemble algorithm", "referable DMO"], "paper_title": "Deep learning-based automated detection for diabetic retinopathy and diabetic macular oedema in retinal fundus photographs.", "last_updated": "2023/02/04"}, {"id": "0033422464", "domain": "Macular degeneration", "model_name": "Liefers et al.", "publication_date": "2021/01/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33422464/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33422464", "task": null, "abstract": "We sought to develop and validate a deep learning model for segmentation of 13 features associated with neovascular and atrophic age-related macular degeneration (AMD). Development and validation of a deep-learning model for feature segmentation. Data for model development were obtained from 307 optical coherence tomography volumes. Eight experienced graders manually delineated all abnormalities in 2712 B-scans. A deep neural network was trained with these data to perform voxel-level segmentation of the 13 most common abnormalities (features). For evaluation, 112 B-scans from 112 patients with a diagnosis of neovascular AMD were annotated by 4 independent observers. The main outcome measures were Dice score, intraclass correlation coefficient, and free-response receiver operating characteristic curve. On 11 of 13 features, the model obtained a mean Dice score of 0.63 \u00b1 0.15, compared with 0.61 \u00b1 0.17 for the observers. The mean intraclass correlation coefficient for the model was 0.66 \u00b1 0.22, compared with 0.62 \u00b1 0.21 for the observers. Two features were not evaluated quantitatively because of a lack of data. Free-response receiver operating characteristic analysis demonstrated that the model scored similar or higher sensitivity per false positives compared with the observers. The quality of the automatic segmentation matches that of experienced graders for most features, exceeding human performance for some features. The quantified parameters provided by the model can be used in the current clinical routine and open possibilities for further research into treatment response outside clinical trials.", "keywords": ["age-related macular degeneration", "atrophic age-related macular", "macular degeneration", "sought to develop", "develop and validate", "atrophic age-related", "age-related macular", "deep learning model", "model", "features", "observers", "learning model", "validate a deep", "deep learning", "segmentation", "Dice score", "AMD", "B-scans", "neovascular AMD", "Data"], "paper_title": "Quantification of Key Retinal Features in Early and Late Age-Related Macular Degeneration Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0033792724", "domain": "Macular degeneration", "model_name": "M3", "publication_date": "2021/08/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33792724/", "code_link": "https://github.com/raghakot/keras-vis", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "33792724", "task": "IWqQC1koJA", "abstract": "Reticular pseudodrusen (RPD), a key feature of age-related macular degeneration (AMD), are poorly detected by human experts on standard color fundus photography (CFP) and typically require advanced imaging modalities such as fundus autofluorescence (FAF). The objective was to develop and evaluate the performance of a novel multimodal, multitask, multiattention (M3) deep learning framework on RPD detection. A deep learning framework (M3) was developed to detect RPD presence accurately using CFP alone, FAF alone, or both, employing >8000 CFP-FAF image pairs obtained prospectively (Age-Related Eye Disease Study 2). The M3 framework includes multimodal (detection from single or multiple image modalities), multitask (training different tasks simultaneously to improve generalizability), and multiattention (improving ensembled feature representation) operation. Performance on RPD detection was compared with state-of-the-art deep learning models and 13 ophthalmologists; performance on detection of 2 other AMD features (geographic atrophy and pigmentary abnormalities) was also evaluated. For RPD detection, M3 achieved an area under the receiver-operating characteristic curve (AUROC) of 0.832, 0.931, and 0.933 for CFP alone, FAF alone, and both, respectively. M3 performance on CFP was very substantially superior to human retinal specialists (median F1 score = 0.644 vs 0.350). External validation (the Rotterdam Study) demonstrated high accuracy on CFP alone (AUROC, 0.965). The M3 framework also accurately detected geographic atrophy and pigmentary abnormalities (AUROC, 0.909 and 0.912, respectively), demonstrating its generalizability. This study demonstrates the successful development, robust evaluation, and external validation of a novel deep learning framework that enables accessible, accurate, and automated AMD diagnosis and prognosis.", "keywords": ["color fundus photography", "standard color fundus", "typically require advanced", "require advanced imaging", "age-related macular degeneration", "deep learning framework", "advanced imaging modalities", "RPD detection", "fundus photography", "fundus autofluorescence", "deep learning", "color fundus", "Reticular pseudodrusen", "RPD", "Age-Related Eye Disease", "macular degeneration", "Eye Disease Study", "learning framework", "experts on standard", "standard color"], "paper_title": "Multimodal, multitask, multiattention (M3) deep learning detection of reticular pseudodrusen: Toward automated and accessible classification of age-related macular degeneration.", "last_updated": "2023/02/04"}, {"id": "0036458946", "domain": "Macular degeneration", "model_name": "Schwartz et al.", "publication_date": "2022/12/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36458946/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36458946", "task": "IWqQC1koJA", "abstract": "The purpose of this study was to develop and validate a deep learning (DL) framework for the detection and quantification of reticular pseudodrusen (RPD) and drusen on optical coherence tomography (OCT) scans. A DL framework was developed consisting of a classification model and an out-of-distribution (OOD) detection model for the identification of ungradable scans; a classification model to identify scans with drusen or RPD; and an image segmentation model to independently segment lesions as RPD or drusen. Data were obtained from 1284 participants in the UK Biobank (UKBB) with a self-reported diagnosis of age-related macular degeneration (AMD) and 250 UKBB controls. Drusen and RPD were manually delineated by five retina specialists. The main outcome measures were sensitivity, specificity, area under the receiver operating characteristic (ROC) curve (AUC), kappa, accuracy, intraclass correlation coefficient (ICC), and free-response receiver operating characteristic (FROC) curves. The classification models performed strongly at their respective tasks (0.95, 0.93, and 0.99 AUC, respectively, for the ungradable scans classifier, the OOD model, and the drusen and RPD classification models). The mean ICC for the drusen and RPD area versus graders was 0.74 and 0.61, respectively, compared with 0.69 and 0.68 for intergrader agreement. FROC curves showed that the model's sensitivity was close to human performance. The models achieved high classification and segmentation performance, similar to human performance. Application of this robust framework will further our understanding of RPD as a separate entity from drusen in both research and clinical settings.", "keywords": ["optical coherence tomography", "RPD", "deep learning", "reticular pseudodrusen", "coherence tomography", "develop and validate", "validate a deep", "quantification of reticular", "optical coherence", "RPD classification models", "drusen", "OCT", "scans", "model", "ungradable scans", "classification", "Drusen and RPD", "RPD classification", "classification model", "framework"], "paper_title": "A Deep Learning Framework for the Detection and Quantification of Reticular Pseudodrusen and Drusen on Optical Coherence Tomography.", "last_updated": "2023/02/04"}, {"id": "0032285025", "domain": "Macular degeneration", "model_name": "amdprogresscnn", "publication_date": "2020/02/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32285025/", "code_link": "https://github.com/qiyanpitt/amdprogresscnn", "model_type": "CNN", "verified": false, "model_task": "Forecasting", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "32285025", "task": "0GPcU42tzV", "abstract": "Both genetic and environmental factors influence the etiology of age-related macular degeneration (AMD), a leading cause of blindness. AMD severity is primarily measured by fundus images and recently developed machine learning methods can successfully predict AMD progression using image data. However, none of these methods have utilized both genetic and image data for predicting AMD progression. Here we jointly used genotypes and fundus images to predict an eye as having progressed to late AMD with a modified deep convolutional neural network (CNN). In total, we used 31,262 fundus images and 52 AMD-associated genetic variants from 1,351 subjects from the Age-Related Eye Disease Study (AREDS) with disease severity phenotypes and fundus images available at baseline and follow-up visits over a period of 12 years. Our results showed that fundus images coupled with genotypes could predict late AMD progression with an averaged area under the curve (AUC) value of 0.85 (95%CI: 0.83-0.86). The results using fundus images alone showed an averaged AUC of 0.81 (95%CI: 0.80-0.83). We implemented our model in a cloud-based application for individual risk assessment.", "keywords": ["environmental factors influence", "age-related macular degeneration", "AMD progression", "AMD", "fundus images", "macular degeneration", "environmental factors", "factors influence", "influence the etiology", "late AMD", "fundus", "images", "predict AMD progression", "late AMD progression", "predicting AMD progression", "successfully predict AMD", "Eye Disease Study", "predict late AMD", "image data", "predict AMD"], "paper_title": "Deep-learning-based Prediction of Late Age-Related Macular Degeneration Progression.", "last_updated": "2023/02/04"}, {"id": "0033785508", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2021/03/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33785508/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33785508", "task": "IWqQC1koJA", "abstract": "To explore and evaluate an appropriate deep learning system (DLS) for the detection of 12 major fundus diseases using colour fundus photography. Diagnostic performance of a DLS was tested on the detection of normal fundus and 12 major fundus diseases including referable diabetic retinopathy, pathologic myopic retinal degeneration, retinal vein occlusion, retinitis pigmentosa, retinal detachment, wet and dry age-related macular degeneration, epiretinal membrane, macula hole, possible glaucomatous optic neuropathy, papilledema and optic nerve atrophy. The DLS was developed with 56\u2009738 images and tested with 8176 images from one internal test set and two external test sets. The comparison with human doctors was also conducted. The area under the receiver operating characteristic curves of the DLS on the internal test set and the two external test sets were 0.950 (95% CI 0.942 to 0.957) to 0.996 (95% CI 0.994 to 0.998), 0.931 (95% CI 0.923 to 0.939) to 1.000 (95% CI 0.999 to 1.000) and 0.934 (95% CI 0.929 to 0.938) to 1.000 (95% CI 0.999 to 1.000), with sensitivities of 80.4% (95% CI 79.1% to 81.6%) to 97.3% (95% CI 96.7% to 97.8%), 64.6% (95% CI 63.0% to 66.1%) to 100% (95% CI 100% to 100%) and 68.0% (95% CI 67.1% to 68.9%) to 100% (95% CI 100% to 100%), respectively, and specificities of 89.7% (95% CI 88.8% to 90.7%) to 98.1% (95%CI 97.7% to 98.6%), 78.7% (95% CI 77.4% to 80.0%) to 99.6% (95% CI 99.4% to 99.8%) and 88.1% (95% CI 87.4% to 88.7%) to 98.7% (95% CI 98.5% to 99.0%), respectively. When compared with human doctors, the DLS obtained a higher diagnostic sensitivity but lower specificity. The proposed DLS is effective in diagnosing normal fundus and 12 major fundus diseases, and thus has much potential for fundus diseases screening in the real world.", "keywords": ["major fundus diseases", "colour fundus photography", "deep learning system", "fundus diseases", "major fundus", "fundus diseases including", "external test sets", "fundus diseases screening", "internal test set", "fundus", "fundus photography", "colour fundus", "DLS", "learning system", "explore and evaluate", "deep learning", "normal fundus", "myopic retinal degeneration", "diseases including referable", "test set"], "paper_title": "Development and evaluation of a deep learning model for the detection of multiple fundus diseases based on colour fundus photography.", "last_updated": "2023/02/04"}, {"id": "0032354098", "domain": "Macular degeneration", "model_name": "Heo et al.", "publication_date": "2020/04/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32354098/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32354098", "task": "IWqQC1koJA", "abstract": "The use of deep-learning-based artificial intelligence (AI) is emerging in ophthalmology, with AI-mediated differential diagnosis of neovascular age-related macular degeneration (AMD) and dry AMD a promising methodology for precise treatment strategies and prognosis. Here, we developed deep learning algorithms and predicted diseases using 399 images of fundus. Based on feature extraction and classification with fully connected layers, we applied the Visual Geometry Group with 16 layers (VGG16) model of convolutional neural networks to classify new images. Image-data augmentation in our model was performed using Keras ImageDataGenerator, and the leave-one-out procedure was used for model cross-validation. The prediction and validation results obtained using the AI AMD diagnosis model showed relevant performance and suitability as well as better diagnostic accuracy than manual review by first-year residents. These results suggest the efficacy of this tool for early differential diagnosis of AMD in situations involving shortages of ophthalmology specialists and other medical devices.", "keywords": ["age-related macular degeneration", "neovascular age-related macular", "precise treatment strategies", "artificial intelligence", "macular degeneration", "strategies and prognosis", "Visual Geometry Group", "neovascular age-related", "age-related macular", "promising methodology", "methodology for precise", "precise treatment", "treatment strategies", "AI-mediated differential diagnosis", "dry AMD", "AI-mediated differential", "AMD", "Visual Geometry", "Geometry Group", "AMD diagnosis model"], "paper_title": "Development of a Deep-Learning-Based Artificial Intelligence Tool for Differential Diagnosis between Dry and Neovascular Age-Related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0030349958", "domain": "Macular degeneration", "model_name": "Yoo et al.", "publication_date": "2018/10/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30349958/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "30349958", "task": "IWqQC1koJA", "abstract": "Recently, researchers have built new deep learning (DL) models using a single image modality to diagnose age-related macular degeneration (AMD). Retinal fundus and optical coherence tomography (OCT) images in clinical settings are the most important modalities investigating AMD. Whether concomitant use of fundus and OCT data in DL technique is beneficial has not been so clearly identified. This experimental analysis used OCT and fundus image data of postmortems from the Project Macula. The DL based on OCT, fundus, and combination of OCT and fundus were invented to diagnose AMD. These models consisted of pre-trained VGG-19 and transfer learning using random forest. Following the data augmentation and training process, the DL using OCT alone showed diagnostic efficiency with area under the curve (AUC) of 0.906 (95% confidence interval, 0.891-0.921) and 82.6% (81.0-84.3%) accuracy rate. The DL using fundus alone exhibited AUC of 0.914 (0.900-0.928) and 83.5% (81.8-85.0%) accuracy rate. Combined usage of the fundus with OCT increased the diagnostic power with AUC of 0.969 (0.956-0.979) and 90.5% (89.2-91.8%) accuracy rate. The Delong test showed that the DL using both OCT and fundus data outperformed the DL using OCT alone (P value <\u20090.001) and fundus image alone (P value <\u20090.001). This multimodal random forest model showed even better performance than a restricted Boltzmann machine (P value\u2009=\u20090.002) and deep belief network algorithms (P value\u2009=\u20090.042). According to Duncan's multiple range test, the multimodal methods significantly improved the performance obtained by the single-modal methods. In this preliminary study, a multimodal DL algorithm based on the combination of OCT and fundus image raised the diagnostic accuracy compared to this data alone. Future diagnostic DL needs to adopt the multimodal process to combine various types of imaging for a more precise AMD diagnosis. Graphical abstract The basic architectural structure of the tested multimodal deep learning model based on pre-trained deep convolutional neural network and random forest using the combination of OCT and fundus image.", "keywords": ["age-related macular degeneration", "OCT", "single image modality", "diagnose age-related macular", "fundus", "fundus image", "OCT and fundus", "researchers have built", "macular degeneration", "AMD", "age-related macular", "image", "modalities investigating AMD", "combination of OCT", "accuracy rate", "OCT data", "data", "AUC", "single image", "image modality"], "paper_title": "The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment.", "last_updated": "2023/02/04"}, {"id": "0033610832", "domain": "Macular degeneration", "model_name": "Du et al.", "publication_date": "2021/02/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33610832/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33610832", "task": "IWqQC1koJA", "abstract": "To determine whether eyes with pathologic myopia can be identified and whether each type of myopic maculopathy lesion on fundus photographs can be diagnosed by deep learning (DL) algorithms. A DL algorithm was developed to recognize myopic maculopathy features and to categorize the myopic maculopathy automatically. We examined 7020 fundus images from 4432 highly myopic eyes obtained from the Advanced Clinical Center for Myopia. Deep learning (DL) algorithms were developed to recognize the key features of myopic maculopathy with 5176 fundus images. These algorithms were also used to develop a Meta-analysis for Pathologic Myopia (META-PM) study categorizing system (CS) by adding a specific processing layer. Models and the system were evaluated by 1844 fundus image. The area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were used to determine the performance of each DL algorithm. The rate of correct predictions was used to determine the performance of the META-PM study CS. Four trained DL models were able to recognize the lesions of myopic maculopathy accurately with high sensitivity and specificity. The META-PM study CS also showed a high accuracy and was qualified to be used in a semiautomated way during screening for myopic maculopathy in highly myopic eyes. The sensitivity of the DL models was 84.44% for diffuse atrophy, 87.22% for patchy atrophy, 85.10% for macular atrophy, and 37.07% for choroidal neovascularization, and the AUC values were 0.970, 0.978, 0.982, and 0.881, respectively. The rate of total correct predictions from the META-PM study CS was 87.53%, with rates of 90.18%, 95.28%, 97.50%, and 91.14%, respectively, for each type of lesion. The META-PM study CS showed an overall rate of 92.08% in detecting pathologic myopia correctly, which was defined as having myopic maculopathy equal to or more serious than diffuse atrophy. The novel DL models and system can achieve high sensitivity and specificity in identifying the different types of lesions of myopic maculopathy. These results will assist in the screening for pathologic myopia and subsequent protection of patients against low vision and blindness caused by myopic maculopathy.", "keywords": ["myopic maculopathy", "Advanced Clinical Center", "myopic", "pathologic myopia", "maculopathy", "META-PM study", "myopia", "highly myopic eyes", "recognize myopic maculopathy", "myopic eyes", "META-PM", "study", "Advanced Clinical", "Clinical Center", "myopic maculopathy features", "myopic maculopathy automatically", "fundus", "myopic maculopathy lesion", "pathologic", "deep learning"], "paper_title": "Deep Learning Approach for Automated Detection of Myopic Maculopathy and Pathologic Myopia in Fundus Images.", "last_updated": "2023/02/04"}, {"id": "0035172022", "domain": "Macular degeneration", "model_name": "Chen et al.", "publication_date": "2022/03/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35172022/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35172022", "task": "IWqQC1koJA", "abstract": "To develop a computer-aided diagnostic (CADx) system of age-related macular degeneration (AMD) through feature fusion between infrared reflectance (IR) and optical coherence tomography (OCT) modalities in order to explore the superiority of multi-modality CADx system and the optimal feature fusion patterns between multi-modality inputs. This is a dual center retrospective study. We retrospectively collected 2006 pairs of IR and OCT images to develop the algorithms. Two single-modality models and three multi-modality models were constructed for the comparison of the diagnostic efficacy. The multi-modality models were designed utilizing a novel feature fusion method, namely, vertical plane feature fusion (VPFF). The results were validated using an independent external validation dataset and compared by three ophthalmologists. In the test set of the ZJU dataset, our best model named OCT_MAIN demonstrated diagnostic efficiency with an overall accuracy of 0.9608 and area under the curve of 0.9944 for the normal category, 0.9659 for the dry AMD category, and 0.9930 for the wet AMD category. The external validation exhibited an overall accuracy of 0.9159. Its diagnostic efficiency was comparable to that of the senior ophthalmologist. The VPFF method was successfully employed to develop a multi-modal intelligent diagnostic system for the AMD classification. This is a valuable complement and optimization to the existing CADx system, which reveals a wide application prospect and research potential.", "keywords": ["age-related macular degeneration", "optical coherence tomography", "optimal feature fusion", "feature fusion patterns", "feature fusion", "macular degeneration", "infrared reflectance", "coherence tomography", "modalities in order", "age-related macular", "optical coherence", "order to explore", "explore the superiority", "optimal feature", "fusion patterns", "AMD category", "AMD", "multi-modality inputs", "OCT", "feature fusion method"], "paper_title": "Automated diagnosis of age-related macular degeneration using multi-modal vertical plane feature fusion via deep learning.", "last_updated": "2023/02/04"}, {"id": "0032504911", "domain": "Macular degeneration", "model_name": "Alsaih et al.", "publication_date": "2020/05/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32504911/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32504911", "task": "aipbNdPTIt", "abstract": "Aged people usually are more to be diagnosed with retinal diseases in developed countries. Retinal capillaries leakage into the retina swells and causes an acute vision loss, which is called age-related macular degeneration (AMD). The disease can not be adequately diagnosed solely using fundus images as depth information is not available. The variations in retina volume assist in monitoring ophthalmological abnormalities. Therefore, high-fidelity AMD segmentation in optical coherence tomography (OCT) imaging modality has raised the attention of researchers as well as those of the medical doctors. Many methods across the years encompassing machine learning approaches and convolutional neural networks (CNN) strategies have been proposed for object detection and image segmentation. In this paper, we analyze four wide-spread deep learning models designed for the segmentation of three retinal fluids outputting dense predictions in the RETOUCH challenge data. We aim to demonstrate how a patch-based approach could push the performance for each method. Besides, we also evaluate the methods using the OPTIMA challenge dataset for generalizing network performance. The analysis is driven into two sections: the comparison between the four approaches and the significance of patching the images. The performance of networks trained on the RETOUCH dataset is higher than human performance. The analysis further generalized the performance of the best network obtained by fine-tuning it and achieved a mean Dice similarity coefficient (DSC) of 0.85. Out of the three types of fluids, intraretinal fluid (IRF) is more recognized, and the highest DSC value of 0.922 is achieved using Spectralis dataset. Additionally, the highest average DSC score is 0.84, which is achieved by PaDeeplabv3+ model using Cirrus dataset. The proposed method segments the three fluids in the retina with high DSC value. Fine-tuning the networks trained on the RETOUCH dataset makes the network perform better and faster than training from scratch. Enriching the networks with inputting a variety of shapes by extracting patches helped to segment the fluids better than using a full image.", "keywords": ["Aged people", "developed countries", "RETOUCH dataset", "Retinal capillaries leakage", "DSC", "performance", "retinal diseases", "retinal", "dataset", "networks", "high-fidelity AMD segmentation", "RETOUCH", "AMD segmentation", "fluids", "AMD", "retinal fluids outputting", "network", "networks trained", "retinal fluids", "retina"], "paper_title": "Deep learning architectures analysis for age-related macular degeneration segmentation on optical coherence tomography scans.", "last_updated": "2023/02/04"}, {"id": "0031407214", "domain": "Macular degeneration", "model_name": "Motozawa et al.", "publication_date": "2019/08/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31407214/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31407214", "task": "IWqQC1koJA", "abstract": "The use of optical coherence tomography (OCT) images is increasing in the medical treatment of age-related macular degeneration (AMD), and thus, the amount of data requiring analysis is increasing. Advances in machine-learning techniques may facilitate processing of large amounts of medical image data. Among deep-learning methods, convolution neural networks (CNNs) show superior image recognition ability. This study aimed to build deep-learning models that could distinguish AMD from healthy OCT scans and to distinguish AMD with and without exudative changes without using a segmentation algorithm. This was a cross-sectional observational clinical study. A total of 1621 spectral domain (SD)-OCT images of patients with AMD and a healthy control group were studied. The first CNN model was trained and validated using 1382 AMD images and 239 normal images. The second transfer-learning model was trained and validated with 721 AMD images with exudative changes and 661 AMD images without any exudate. The attention area of the CNN was described as a heat map by class activation mapping (CAM). In the second model, which classified images into AMD with or without exudative changes, we compared the learning stabilization of models using or not using transfer learning. Using the first CNN model, we could classify AMD and normal OCT images with 100% sensitivity, 91.8% specificity, and 99.0% accuracy. In the second, transfer-learning model, we could classify AMD as having or not having exudative changes, with 98.4% sensitivity, 88.3% specificity, and 93.9% accuracy. CAM successfully described the heat-map area on the OCT images. Including the transfer-learning model in the second model resulted in faster stabilization than when the transfer-learning model was not included. Two computational deep-learning models were developed and evaluated here; both models showed good performance. Automation of the interpretation process by using deep-learning models can save time and improve efficiency. No15073.", "keywords": ["optical coherence tomography", "age-related macular degeneration", "AMD", "data requiring analysis", "AMD images", "OCT images", "analysis is increasing", "images", "OCT", "model", "coherence tomography", "macular degeneration", "optical coherence", "treatment of age-related", "age-related macular", "requiring analysis", "distinguish AMD", "medical image data", "transfer-learning model", "classify AMD"], "paper_title": "Optical Coherence Tomography-Based Deep-Learning Models for Classifying Normal and Age-Related Macular Degeneration and Exudative and Non-Exudative Age-Related Macular Degeneration Changes.", "last_updated": "2023/02/04"}, {"id": "0030821810", "domain": "Diabetic retinopathy", "model_name": "Arcadu et al.", "publication_date": "2019/08/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30821810/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30821810", "task": null, "abstract": "To develop deep learning (DL) models for the automatic detection of optical coherence tomography (OCT) measures of diabetic macular thickening (MT) from color fundus photographs (CFPs). Retrospective analysis on 17,997 CFPs and their associated OCT measurements from the phase 3 RIDE/RISE diabetic macular edema (DME) studies. DL with transfer-learning cascade was applied on CFPs to predict time-domain OCT (TD-OCT)-equivalent measures of MT, including central subfield thickness (CST) and central foveal thickness (CFT). MT was defined by using two OCT cutoff points: 250 \u03bcm and 400 \u03bcm. A DL regression model was developed to directly quantify the actual CFT and CST from CFPs. The best DL model was able to predict CST \u2265 250 \u03bcm and CFT \u2265 250 \u03bcm with an area under the curve (AUC) of 0.97 (95% confidence interval [CI], 0.89-1.00) and 0.91 (95% CI, 0.76-0.99), respectively. To predict CST \u2265 400 \u03bcm and CFT \u2265 400 \u03bcm, the best DL model had an AUC of 0.94 (95% CI, 0.82-1.00) and 0.96 (95% CI, 0.88-1.00), respectively. The best deep convolutional neural network regression model to quantify CST and CFT had an R2 of 0.74 (95% CI, 0.49-0.91) and 0.54 (95% CI, 0.20-0.87), respectively. The performance of the DL models declined when the CFPs were of poor quality or contained laser scars. DL is capable of predicting key quantitative TD-OCT measurements related to MT from CFPs. The DL models presented here could enhance the efficiency of DME diagnosis in tele-ophthalmology programs, promoting better visual outcomes. Future research is needed to validate DL algorithms for MT in the real-world.", "keywords": ["optical coherence tomography", "color fundus photographs", "diabetic macular thickening", "RISE diabetic macular", "develop deep learning", "diabetic macular", "diabetic macular edema", "coherence tomography", "fundus photographs", "automatic detection", "detection of optical", "optical coherence", "color fundus", "OCT", "macular thickening", "CFT", "RISE diabetic", "CST", "predict CST", "CFPs"], "paper_title": "Deep Learning Predicts OCT Measures of Diabetic Macular Thickening From Color Fundus Photographs.", "last_updated": "2023/02/04"}, {"id": "0030091055", "domain": "Macular degeneration", "model_name": "Treder et al.", "publication_date": "2018/08/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30091055/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "30091055", "task": "IWqQC1koJA", "abstract": "To automatically detect and classify geographic atrophy (GA) in fundus autofluorescence (FAF) images using a deep learning algorithm. In this study, FAF images of patients with GA, a healthy comparable group and a comparable group with other retinal diseases (ORDs) were used to train a multi-layer deep convolutional neural network (DCNN) (1) to detect GA and (2) to differentiate in GA between a diffuse-trickling pattern (dt-GA) and other GA FAF patterns (ndt-GA) in FAF images. 1. For the automated detection of GA in FAF images, two classifiers were built (GA vs. healthy/GA vs. ORD). The DCNN was trained and validated with 400 FAF images in each case (GA 200, healthy 200, or ORD 200). For the subsequent testing, the built classifiers were then tested with 60 untrained FAF images in each case (AMD 30, healthy 30, or ORD 30). Hereby, both classifiers automatically determined a GA probability score and a normal FAF probability score or an ORD probability score. 2. To automatically differentiate between dt-GA and ndt-GA, the DCNN was trained and validated with 200 FAF images (dt-GA 72; ndt-GA 138). Afterwards, the built classifier was tested with 20 untrained FAF images (dt-GA 10; ndt-GA 10) and a dt-GA probability score and an ndt-GA probability score was calculated. For both classifiers, the performance of the training and validation procedure after 500 training steps was measured by determining training accuracy, validation accuracy, and cross entropy. For the GA classifiers (GA vs. healthy/GA vs. ORD), the achieved training accuracy was 99/98%, the validation accuracy 96/91%, and the cross entropy 0.062/0.100. For the dt-GA classifier, the training accuracy was 99%, the validation accuracy 77%, and the cross entropy 0.166. The mean GA probability score was 0.981\u2009\u00b1\u20090.048 (GA vs. healthy)/0.972\u2009\u00b1\u20090.439 (GA vs. ORD) in the GA image group and 0.01\u2009\u00b1\u20090.016 (healthy)/0.061\u2009\u00b1\u20090.072 (ORD) in the comparison groups (p\u2009<\u20090.001). The mean dt-GA probability score was 0.807\u2009\u00b1\u20090.116 in the dt-GA image group and 0.180\u2009\u00b1\u20090.100 in the ndt-GA image group (p\u2009<\u20090.001). For the first time, this study describes the use of a deep learning-based algorithm to automatically detect and classify GA in FAF. Hereby, the created classifiers showed excellent results. With further developments, this model may be a tool to predict the individual progression risk of GA and give relevant information for future therapeutic approaches.", "keywords": ["FAF images", "FAF", "untrained FAF images", "ORD", "probability score", "FAF probability score", "images", "untrained FAF", "ORD probability score", "healthy", "image group", "score", "dt-GA", "probability", "FAF patterns", "classify geographic atrophy", "FAF probability", "dt-GA probability score", "classifiers", "normal FAF probability"], "paper_title": "Deep learning-based detection and classification of geographic atrophy using a deep convolutional neural network classifier.", "last_updated": "2023/02/04"}, {"id": "0032499330", "domain": "Macular degeneration", "model_name": "Xu et al.", "publication_date": "2020/06/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32499330/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32499330", "task": "IWqQC1koJA", "abstract": "To investigate the efficacy of a bi-modality deep convolutional neural network (DCNN) framework to categorise age-related macular degeneration (AMD) and polypoidal choroidal vasculopathy (PCV) from colour fundus images and optical coherence tomography (OCT) images. A retrospective cross-sectional study was proposed of patients with AMD or PCV who came to Peking Union Medical College Hospital. Diagnoses of all patients were confirmed by two retinal experts based on diagnostic gold standard for AMD and PCV. Patients with concurrent retinal vascular diseases were excluded. Colour fundus images and spectral domain OCT images were taken from dilated eyes of patients and healthy controls, and anonymised. All images were pre-labelled into normal, dry or wet AMD or PCV. ResNet-50 models were used as the backbone and alternate machine learning models including random forest classifiers were constructed for further comparison. For human-machine comparison, the same testing data set was diagnosed by three retinal experts independently. All images from the same participant were presented only within a single partition subset. On a test set of 143 fundus and OCT image pairs from 80 eyes (20 eyes per-group), the bi-modal DCNN demonstrated the best performance, with accuracy 87.4%, sensitivity 88.8% and specificity 95.6%, and a perfect agreement with diagnostic gold standard (Cohen's \u03ba 0.828), exceeds slightly over the best expert (Human1, Cohen's \u03ba 0.810). For recognising PCV, the model outperformed the best expert as well. A bi-modal DCNN for automated classification of AMD and PCV is accurate and promising in the realm of public health.", "keywords": ["convolutional neural network", "age-related macular degeneration", "polypoidal choroidal vasculopathy", "optical coherence tomography", "bi-modality deep convolutional", "deep convolutional neural", "categorise age-related macular", "Medical College Hospital", "Peking Union Medical", "Union Medical College", "AMD", "neural network", "framework to categorise", "macular degeneration", "choroidal vasculopathy", "coherence tomography", "PCV", "College Hospital", "investigate the efficacy", "bi-modality deep"], "paper_title": "Automated diagnoses of age-related macular degeneration and polypoidal choroidal vasculopathy using bi-modal deep convolutional neural networks.", "last_updated": "2023/02/04"}, {"id": "0036061600", "domain": "Macular degeneration", "model_name": "Song et al.", "publication_date": "2022/08/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36061600/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36061600", "task": null, "abstract": "Using deep learning (DL)-based technique, we identify risk factors and create a prediction model for refractory neovascular age-related macular degeneration (nAMD) characterized by persistent disease activity (PDA) in spectral domain optical coherence tomography (SD-OCT) images. A total of 671 typical B-scans were collected from 186 eyes of 186 patients with nAMD. Spectral domain optical coherence tomography images were analyzed using a classification convolutional neural network (CNN) and a fully convolutional network (FCN) algorithm to extract six features involved in nAMD, including ellipsoid zone (EZ), external limiting membrane (ELM), intraretinal fluid (IRF), subretinal fluid (SRF), pigment epithelium detachment (PED), and subretinal hyperreflective material (SHRM). Random forest models were probed to predict 1-year disease activity (stable, PDA, and cured) based on the quantitative features computed from automated segmentation and evaluated with cross-validation. The algorithm to segment six SD-OCT features achieved the mean accuracy of 0.930 (95% CI: 0.916-0.943), dice coefficients of 0.873 (95% CI: 0.847-0.899), a sensitivity of 0.873 (95% CI: 0.844-0.910), and a specificity of 0.922 (95% CI: 0.905-0.940). The six-metric model including EZ and ELM achieved the optimal performance to predict 1-year disease activity, with an area under the receiver operating characteristic (ROC) curve (AUC) of 0.980, the accuracy of 0.930, the sensitivity of 0.920, and the specificity of 0.962. The integrity of EZ and ELM significantly improved the performance of the six-metric model than that of the four-metric model. The prediction model reveals the potential to predict PDA in nAMD eyes. The integrity of EZ and ELM constituted the strongest predictive factor for PDA in nAMD eyes in real-world clinical practice. The results of this study are a significant step toward image-guided prediction of long-term disease activity in the management of nAMD and highlight the importance of the automatic identification of photoreceptor layers.", "keywords": ["age-related macular degeneration", "refractory neovascular age-related", "neovascular age-related macular", "domain optical coherence", "spectral domain optical", "optical coherence tomography", "identify risk factors", "persistent disease activity", "coherence tomography images", "deep learning", "macular degeneration", "characterized by persistent", "disease activity", "identify risk", "refractory neovascular", "neovascular age-related", "age-related macular", "domain optical", "optical coherence", "spectral domain"], "paper_title": "Automatic quantification of retinal photoreceptor integrity to predict persistent disease activity in neovascular age-related macular degeneration using deep learning.", "last_updated": "2023/02/04"}, {"id": "0032197912", "domain": "Macular degeneration", "model_name": "Liefers et al.", "publication_date": "2020/02/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32197912/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32197912", "task": "aipbNdPTIt", "abstract": "To develop and validate a deep learning model for the automatic segmentation of geographic atrophy (GA) using color fundus images (CFIs) and its application to study the growth rate of GA. Prospective, multicenter, natural history study with up to 15 years of follow-up. Four hundred nine CFIs of 238 eyes with GA from the Rotterdam Study (RS) and Blue Mountain Eye Study (BMES) for model development, and 3589 CFIs of 376 eyes from the Age-Related Eye Disease Study (AREDS) for analysis of GA growth rate. A deep learning model based on an ensemble of encoder-decoder architectures was implemented and optimized for the segmentation of GA in CFIs. Four experienced graders delineated, in consensus, GA in CFIs from the RS and BMES. These manual delineations were used to evaluate the segmentation model using 5-fold cross-validation. The model was applied further to CFIs from the AREDS to study the growth rate of GA. Linear regression analysis was used to study associations between structural biomarkers at baseline and the GA growth rate. A general estimate of the progression of GA area over time was made by combining growth rates of all eyes with GA from the AREDS set. Automatically segmented GA and GA growth rate. The model obtained an average Dice coefficient of 0.72\u00b10.26 on the BMES and RS set while comparing the automatically segmented GA area with the graders' manual delineations. An intraclass correlation coefficient of 0.83 was reached between the automatically estimated GA area and the graders' consensus measures. Nine automatically calculated structural biomarkers (area, filled area, convex area, convex solidity, eccentricity, roundness, foveal involvement, perimeter, and circularity) were significantly associated with growth rate. Combining all growth rates indicated that GA area grows quadratically up to an area of approximately 12 mm<sup>2</sup>, after which growth rate stabilizes or decreases. The deep learning model allowed for fully automatic and robust segmentation of GA on CFIs. These segmentations can be used to extract structural characteristics of GA that predict its growth rate.", "keywords": ["growth rate", "color fundus images", "Mountain Eye Study", "Eye Disease Study", "study", "growth", "deep learning model", "Blue Mountain Eye", "rate", "CFIs", "model", "geographic atrophy", "fundus images", "area", "develop and validate", "color fundus", "Rotterdam Study", "Disease Study", "learning model", "deep learning"], "paper_title": "A Deep Learning Model for Segmentation of Geographic Atrophy to Study Its Long-Term Natural History.", "last_updated": "2023/02/04"}, {"id": "0036786498", "domain": "Macular degeneration", "model_name": "Pan et al.", "publication_date": "2023/02/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36786498/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36786498", "task": null, "abstract": "The purpose of this study was to build an automated age-related macular degeneration (AMD) colour fundus photography (CFP) recognition method that incorporates confounders (other ocular diseases) and normal age-related changes by using drusen masks for spatial feature supervision. A range of clinical sources were used to acquire 7588 CFPs. Contrast limited adaptive histogram equalisation was used for pre-processing. ResNet50 was used as the backbone network, and a spatial attention block was added to integrate prior knowledge of drusen features into the backbone. The evaluation metrics used were sensitivity, specificity and F1 score, which is the harmonic mean of precision and recall (sensitivity) and area under the receiver-operating characteristic (AUC). Fivefold cross-validation was performed, and the results compared with four other methods. Excellent discrimination results were obtained with the algorithm. On the public dataset (n\u00a0=\u00a06565), the proposed method achieved a mean (SD) sensitivity of 0.54 (0.09), specificity of 0.99 (0.00), F1 score of 0.62 (0.06) and AUC of 0.92 (0.02). On the private dataset (n\u00a0=\u00a01023), the proposed method achieved a sensitivity of 0.92 (0.02), specificity of 0.98 (0.01), F1 score of 0.92 (0.01) and AUC of 0.98 (0.01). The proposed drusen-aware model outperformed baseline and other vessel feature-based methods in F1 and AUC on the AMD/normal CFP classification task and achieved comparable results on datasets that included other diseases that often confound classification. The method also improved results when a five-category grading protocol was used, thereby reflecting discriminative ability of the algorithm within a real-life clinical setting.", "keywords": ["age-related macular degeneration", "automated age-related macular", "colour fundus photography", "automated age-related", "age-related macular", "spatial feature supervision", "macular degeneration", "colour fundus", "fundus photography", "incorporates confounders", "build an automated", "AUC", "age-related", "feature supervision", "proposed method achieved", "normal age-related", "recognition method", "drusen masks", "CFP", "normal CFP classification"], "paper_title": "Drusen-aware model for age-related macular degeneration recognition.", "last_updated": "2023/02/04"}, {"id": "0031946304", "domain": "Macular degeneration", "model_name": "An et al.", "publication_date": "2020/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31946304/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31946304", "task": "IWqQC1koJA", "abstract": "The objective of this study was to build deep learning models with optical coherence tomography (OCT) images to classify normal and age related macular degeneration (AMD), AMD with fluid, and AMD without any fluid. In this study, 185 normal OCT images from 49 normal subjects, 535 OCT images of AMD with fluid, and 514 OCT mages of AMD without fluid from 120 AMD eyes as training data, while 49 normal images from 25 normal eyes, 188 AMD OCT images with fluid and 154 AMD images without any fluid from 77 AMD eyes as test data, were enrolled. Data augmentation was applied to increase the number of images to build deep learning models. Totally, two classification models were built in two steps. In the first step, a VGG16 model pre-trained on ImageNet dataset was transfer learned to classify normal and AMD, including AMD with fluid and/or without any fluid. Then, in the second step, the fine-tuned model in the first step was transfer learned again to distinguish the images of AMD with fluid from the ones without any fluid. With the first model, normal and AMD OCT images were classified with 0.999 area under receiver operating characteristic curve (AUC), and 99.2% accuracy. With the second model, AMD with the presence of any fluid, and AMD without fluid were classified with 0.992 AUC, and 95.1% accuracy. Compared with a transfer learned VGG16 model pre-trained on ImageNet dataset, to classify the three categories directly, higher classification performance was achieved with our notable approach. Conclusively, two classification models for AMD clinical practice were built with high classification performance, and these models should help improve the early diagnosis and treatment for AMD.", "keywords": ["AMD OCT images", "AMD", "AMD OCT", "OCT images", "OCT", "normal OCT images", "optical coherence tomography", "related macular degeneration", "age related macular", "fluid", "AMD eyes", "images", "build deep learning", "normal OCT", "normal", "AMD with fluid", "deep learning models", "AMD images", "OCT mages", "coherence tomography"], "paper_title": "Deep Learning Classification Models Built with Two-step Transfer Learning for Age Related Macular Degeneration Diagnosis.", "last_updated": "2023/02/04"}, {"id": "0034057419", "domain": "Macular degeneration", "model_name": "Kang et al.", "publication_date": "2021/05/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34057419/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34057419", "task": "IWqQC1koJA", "abstract": "Retinal vascular diseases, including diabetic macular edema (DME), neovascular age-related macular degeneration (nAMD), myopic choroidal neovascularization (mCNV), and branch and central retinal vein occlusion (BRVO/CRVO), are considered vision-threatening eye diseases. However, accurate diagnosis depends on multimodal imaging and the expertise of retinal ophthalmologists. The aim of this study was to develop a deep learning model to detect treatment-requiring retinal vascular diseases using multimodal imaging. This retrospective study enrolled participants with multimodal ophthalmic imaging data from 3 hospitals in Taiwan from 2013 to 2019. Eye-related images were used, including those obtained through retinal fundus photography, optical coherence tomography (OCT), and fluorescein angiography with or without indocyanine green angiography (FA/ICGA). A deep learning model was constructed for detecting DME, nAMD, mCNV, BRVO, and CRVO and identifying treatment-requiring diseases. Model performance was evaluated and is presented as the area under the curve (AUC) for each receiver operating characteristic curve. A total of 2992 eyes of 2185 patients were studied, with 239, 1209, 1008, 211, 189, and 136 eyes in the control, DME, nAMD, mCNV, BRVO, and CRVO groups, respectively. Among them, 1898 eyes required treatment. The eyes were divided into training, validation, and testing groups in a 5:1:1 ratio. In total, 5117 retinal fundus photos, 9316 OCT images, and 20,922 FA/ICGA images were used. The AUCs for detecting mCNV, DME, nAMD, BRVO, and CRVO were 0.996, 0.995, 0.990, 0.959, and 0.988, respectively. The AUC for detecting treatment-requiring diseases was 0.969. From the heat maps, we observed that the model could identify retinal vascular diseases. Our study developed a deep learning model to detect retinal diseases using multimodal ophthalmic imaging. Furthermore, the model demonstrated good performance in detecting treatment-requiring retinal diseases.", "keywords": ["diabetic macular edema", "neovascular age-related macular", "age-related macular degeneration", "myopic choroidal neovascularization", "including diabetic macular", "retinal vein occlusion", "central retinal vein", "macular edema", "macular degeneration", "diabetic macular", "age-related macular", "Retinal vascular diseases", "considered vision-threatening eye", "BRVO", "Retinal", "neovascular age-related", "myopic choroidal", "choroidal neovascularization", "vein occlusion", "diseases"], "paper_title": "A Multimodal Imaging-Based Deep Learning Model for Detecting Treatment-Requiring Retinal Vascular Diseases: Model Development and Validation Study.", "last_updated": "2023/02/04"}, {"id": "0034183684", "domain": "Macular degeneration", "model_name": "Wang et al.", "publication_date": "2021/06/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34183684/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34183684", "task": "aipbNdPTIt", "abstract": "Macular edema is considered as a major cause of visual loss and blindness in patients with ocular fundus diseases. Optical coherence tomography (OCT) is a non-invasive imaging technique, which has been widely applied for diagnosing macular edema due to its non-invasive and high resolution properties. However, the practical applications remain challenges due to the distorted retinal morphology and blurred boundaries near macular edema. Herein, we developed a novel deep learning model for the segmentation of macular edema in OCT images based on DeepLab framework (OCT-DeepLab). In this model, we used atrous spatial pyramid pooling (ASPP) to detect macular edema at multiple features and used the fully connected conditional random field (CRF) to refine the boundary of macular edema. OCT-DeepLab model was compared against the traditional hand-crafted methods (C-V and SBG) and the end-to-end methods (FCN, PSPnet, and U-net) to estimate the segmentation performance. OCT-DeepLab showed great advantage over the hand-crafted methods (C-V and SBG) and end-to-end methods (FCN, PSPnet, and U-net) as shown by higher precision, sensitivity, specificity, and F1-score. The segmentation performance of OCT-DeepLab was comparable to that of manual label, with an average area under the curve (AUC) of 0.963, which was superior to other end-to-end methods (FCN, PSPnet, and U-net). Collectively, OCT-DeepLab model is suitable for the segmentation of macular edema and assist ophthalmologists in the management of ocular disease.", "keywords": ["Macular edema", "Macular", "macular edema due", "edema", "visual loss", "loss and blindness", "blindness in patients", "diagnosing macular edema", "methods", "detect macular edema", "FCN", "U-net", "ocular fundus diseases", "segmentation", "OCT-DeepLab", "fundus diseases", "hand-crafted methods", "model", "PSPnet", "ocular fundus"], "paper_title": "Automated segmentation of macular edema for the diagnosis of ocular disease using deep learning method.", "last_updated": "2023/02/04"}, {"id": "0036277849", "domain": "Glaucoma (unspecified)", "model_name": "Wang et al.", "publication_date": "2022/07/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36277849/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36277849", "task": null, "abstract": "Genome-wide association studies (GWASs) of eye disorders have identified hundreds of genetic variants associated with ocular disease. However, the vast majority of these variants are noncoding, making it challenging to interpret their function. Here we present a joint single-cell atlas of gene expression and chromatin accessibility of the adult human retina with more than 50,000 cells, which we used to analyze single-nucleotide polymorphisms (SNPs) implicated by GWASs of age-related macular degeneration, glaucoma, diabetic retinopathy, myopia, and type 2 macular telangiectasia. We integrate this atlas with a HiChIP enhancer connectome, expression quantitative trait loci (eQTL) data, and base-resolution deep learning models to predict noncoding SNPs with causal roles in eye disease, assess SNP impact on transcription factor binding, and define their known and novel target genes. Our efforts nominate pathogenic SNP-target gene interactions for multiple vision disorders and provide a potentially powerful resource for interpreting noncoding variation in the eye.", "keywords": ["Genome-wide association studies", "Genome-wide association", "association studies", "identified hundreds", "hundreds of genetic", "genetic variants", "ocular disease", "Genome-wide", "studies", "variants", "disease", "eye", "noncoding", "disorders have identified", "association", "identified", "hundreds", "genetic", "ocular", "making it challenging"], "paper_title": "Single-cell multiome of the human retina and deep learning nominate causal variants in complex eye diseases.", "last_updated": "2023/02/04"}, {"id": "0032907811", "domain": "Macular degeneration", "model_name": "Rim et al.", "publication_date": "2020/09/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32907811/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32907811", "task": "IWqQC1koJA", "abstract": "The ability of deep learning (DL) algorithms to identify eyes with neovascular age-related macular degeneration (nAMD) from optical coherence tomography (OCT) scans has been previously established. We herewith evaluate the ability of a DL model, showing excellent performance on a Korean data set, to generalse onto an American data set despite ethnic differences. In addition, expert graders were surveyed to verify if the DL model was appropriately identifying lesions indicative of nAMD on the OCT scans. Model development data set-12\u00a0247 OCT scans from South Korea; external validation data set-91\u00a0509 OCT scans from Washington, USA. In both data sets, normal eyes or eyes with nAMD were included. After internal testing, the algorithm was sent to the University of Washington, USA, for external validation. Area under the receiver operating characteristic curve (AUC) and precision-recall curve (AUPRC) were calculated. For model explanation, saliency maps were generated using Guided GradCAM. On external validation, AUC and AUPRC remained high at 0.952 (95% CI 0.942 to 0.962) and 0.891 (95% CI 0.875 to 0.908) at the individual level. Saliency maps showed that in normal OCT scans, the fovea was the main area of interest; in nAMD OCT scans, the appropriate pathological features were areas of model interest. Survey of 10 retina specialists confirmed this. Our DL algorithm exhibited high performance for nAMD identification in a Korean population, and generalised well to an ethnically distinct, American population. The model correctly focused on the differences within the macular area to extract features associated with nAMD.", "keywords": ["optical coherence tomography", "OCT scans", "OCT", "nAMD OCT scans", "age-related macular degeneration", "neovascular age-related macular", "normal OCT scans", "deep learning", "coherence tomography", "previously established", "scans", "neovascular age-related", "optical coherence", "ability of deep", "Korean data set", "data", "model", "American data set", "nAMD", "data set"], "paper_title": "Detection of features associated with neovascular age-related macular degeneration in ethnically distinct data sets by an optical coherence tomography: trained deep learning algorithm.", "last_updated": "2023/02/04"}, {"id": "0036584638", "domain": "Macular degeneration", "model_name": "KFWC", "publication_date": "2022/12/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36584638/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36584638", "task": "IWqQC1koJA", "abstract": "Automated diagnosis using deep neural networks can help ophthalmologists detect the blinding eye disease wet Age-related Macular Degeneration (AMD). Wet-AMD has two similar subtypes, Neovascular AMD and Polypoidal Choroidal Vasculopathy (PCV). However, due to the difficulty in data collection and the similarity between images, most studies have only achieved the coarse-grained classification of wet-AMD rather than a fine-grained one of wet-AMD subtypes. Therefore, designing and building a deep learning model to diagnose neovascular AMD and PCV is a great challenge. To solve this issue, in this paper, we propose a Knowledge-driven Fine-grained Wet-AMD Classification Model (KFWC) to enhance the model's accuracy in the fine-grained disease classification with insufficient data. We innovatively introduced a two-stage method. In the first stage, we present prior knowledge of 10 lesion signs through pre-training; in the second stage, the model implements the classification task with the help of human knowledge. With the pre-training of priori knowledge of 10 lesion signs from input images, KFWC locates the powerful image features in the fine-grained disease classification task and therefore achieves better classification. To demonstrate the effectiveness of KFWC, we conduct a series of experiments on a clinical dataset collected in cooperation with a Grade III Level A ophthalmology hospital in China. The AUC score of KFWC reaches 99.71%, with 6.69% over the best baseline and 4.14% over ophthalmologists. KFWC can also provide good interpretability and effectively alleviate the pressure of data collection and annotation in the field of fine-grained disease classification for wet-AMD. The model proposed in this paper effectively solves the difficulties of small data volume and high image similarity in the wet-AMD fine-grained classification task through a knowledge-driven approach. Besides, this method effectively relieves the pressure of data collection and annotation in the field of fine-grained classification. In the diagnosis of wet-AMD, KFWC is superior to previous work and human ophthalmologists.", "keywords": ["Age-related Macular Degeneration", "wet Age-related Macular", "Macular Degeneration", "Age-related Macular", "disease wet Age-related", "Polypoidal Choroidal Vasculopathy", "wet Age-related", "Neovascular AMD", "fine-grained disease classification", "eye disease wet", "blinding eye disease", "deep neural networks", "diagnose neovascular AMD", "classification", "KFWC", "Choroidal Vasculopathy", "AMD", "Polypoidal Choroidal", "AMD and Polypoidal", "disease classification"], "paper_title": "KFWC: A Knowledge-Driven Deep Learning Model for Fine-grained Classification of Wet-AMD.", "last_updated": "2023/02/04"}, {"id": "0034509423", "domain": "Macular degeneration", "model_name": "Zhang et al.", "publication_date": "2021/09/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34509423/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34509423", "task": "IWqQC1koJA", "abstract": "Geographic atrophy is a major vision-threatening manifestation of age-related macular degeneration, one of the leading causes of blindness globally. Geographic atrophy has no proven treatment or method for easy detection. Rapid, reliable, and objective detection and quantification of geographic atrophy from optical coherence tomography (OCT) retinal scans is necessary for disease monitoring, prognostic research, and to serve as clinical endpoints for therapy development. To this end, we aimed to develop and validate a fully automated method to detect and quantify geographic atrophy from OCT. We did a deep-learning model development and external validation study on OCT retinal scans at Moorfields Eye Hospital Reading Centre and Clinical AI Hub (London, UK). A modified U-Net architecture was used to develop four distinct deep-learning models for segmentation of geographic atrophy and its constituent retinal features from OCT scans acquired with Heidelberg Spectralis. A manually segmented clinical dataset for model development comprised 5049 B-scans from 984 OCT volumes selected randomly from 399 eyes of 200 patients with geographic atrophy secondary to age-related macular degeneration, enrolled in a prospective, multicentre, phase 2 clinical trial for the treatment of geographic atrophy (FILLY study). Performance was externally validated on an independently recruited dataset from patients receiving routine care at Moorfields Eye Hospital (London, UK). The primary outcome was segmentation and classification agreement between deep-learning model geographic atrophy prediction and consensus of two independent expert graders on the external validation dataset. The external validation cohort included 884 B-scans from 192 OCT volumes taken from 192 eyes of 110 patients as part of real-life clinical care at Moorfields Eye Hospital between Jan 1, 2016, and Dec, 31, 2019 (mean age 78\u00b73 years [SD 11\u00b71], 58 [53%] women). The resultant geographic atrophy deep-learning model produced predictions similar to consensus human specialist grading on the external validation dataset (median Dice similarity coefficient [DSC] 0\u00b796 [IQR 0\u00b710]; intraclass correlation coefficient [ICC] 0\u00b793) and outperformed agreement between human graders (DSC 0\u00b780 [0\u00b728]; ICC 0\u00b779). Similarly, the three independent feature-specific deep-learning models could accurately segment each of the three constituent features of geographic atrophy: retinal pigment epithelium loss (median DSC 0\u00b795 [IQR 0\u00b715]), overlying photoreceptor degeneration (0\u00b796 [0\u00b712]), and hypertransmission (0\u00b797 [0\u00b707]) in the external validation dataset versus consensus grading. We present a fully developed and validated deep-learning composite model for segmentation of geographic atrophy and its subtypes that achieves performance at a similar level to manual specialist assessment. Fully automated analysis of retinal OCT from routine clinical practice could provide a promising horizon for diagnosis and prognosis in both research and real-life patient care, following further clinical validation FUNDING: Apellis Pharmaceuticals.", "keywords": ["Geographic atrophy", "Moorfields Eye Hospital", "major vision-threatening manifestation", "Geographic", "Moorfields Eye", "atrophy", "Eye Hospital", "OCT", "Eye Hospital Reading", "external validation", "model geographic atrophy", "external validation dataset", "Hospital Reading Centre", "blindness globally", "geographic atrophy deep-learning", "major vision-threatening", "vision-threatening manifestation", "clinical", "validation", "deep-learning"], "paper_title": "Clinically relevant deep learning for detection and quantification of geographic atrophy from optical coherence tomography: a model development and external validation study.", "last_updated": "2023/02/04"}, {"id": "0032750929", "domain": "Macular degeneration", "model_name": "Romo-Bucheli et al.", "publication_date": "2020/12/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32750929/", "code_link": null, "model_type": "RNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32750929", "task": null, "abstract": "Neovascular age-related macular degeneration (nAMD) is nowadays successfully treated with anti-VEGF substances, but inter-individual treatment requirements are vastly heterogeneous and currently poorly plannable resulting in suboptimal treatment frequency. Optical coherence tomography (OCT) with its 3D high-resolution imaging serves as a companion diagnostic to anti-VEGF therapy. This creates a need for building predictive models using automated image analysis of OCT scans acquired during the treatment initiation phase. We propose such a model based on deep learning (DL) architecture, comprised of a densely connected neural network (DenseNet) and a recurrent neural network (RNN), trainable end-to-end. The method starts by sampling several 2D-images from an OCT volume to obtain a lower-dimensional OCT representation. At the core of the predictive model, the DenseNet learns useful retinal spatial features while the RNN integrates information from different time points. The introduced model was evaluated on the prediction of anti-VEGF treatment requirements in nAMD patients treated under a pro-re-nata (PRN) regimen. The DL model was trained on 281 patients and evaluated on a hold-out test set of 69 patient. The predictive model achieved a concordance index of 0.7 in regressing the number of received treatments, while in a classification task it obtained an 0.85 (0.81) AUC in detecting the patients with low (high) treatment requirements. The proposed model outperformed previous machine learning strategies that relied on a set of spatio-temporal image features, showing that the proposed DL architecture successfully learned to extract the relevant spatio-temporal patterns directly from raw longitudinal OCT images.", "keywords": ["Neovascular age-related macular", "age-related macular degeneration", "poorly plannable resulting", "suboptimal treatment frequency", "Neovascular age-related", "macular degeneration", "age-related macular", "vastly heterogeneous", "poorly plannable", "plannable resulting", "resulting in suboptimal", "inter-individual treatment requirements", "OCT", "anti-VEGF substances", "model", "treatment requirements", "nowadays successfully treated", "treatment frequency", "anti-VEGF treatment requirements", "inter-individual treatment"], "paper_title": "End-to-End Deep Learning Model for Predicting Treatment Requirements in Neovascular AMD From Longitudinal Retinal OCT Imaging.", "last_updated": "2023/02/04"}, {"id": "0035781941", "domain": "Macular degeneration", "model_name": "OCT", "publication_date": "2022/05/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35781941/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35781941", "task": "aipbNdPTIt", "abstract": "<b>Introduction -</b> Retinal layer segmentation in optical coherence tomography (OCT) images is an important approach for detecting and prognosing disease. Automating segmentation using robust machine learning techniques lead to computationally efficient solutions and significantly reduces the cost of labor-intensive labeling, which is traditionally performed by trained graders at a reading center, sometimes aided by semi-automated algorithms. Although several algorithms have been proposed since the revival of deep learning, eyes with severe pathological conditions continue to challenge fully automated segmentation approaches. There remains an opportunity to leverage the underlying spatial correlations between the retinal surfaces in the segmentation approach. <b>Methods -</b> Some of these proposed traditional methods can be expanded to utilize the three-dimensional spatial context governing the retinal image volumes by replacing the use of 2D filters with 3D filters. Towards this purpose, we propose a spatial-context, continuity and anatomical relationship preserving semantic segmentation algorithm, which utilizes the 3D spatial context from the image volumes with the use of 3D filters. We propose a 3D deep neural network capable of learning the surface positions of the layers in the retinal volumes. <b>Results -</b> We utilize a dataset of OCT images from patients with Age-related Macular Degeneration (AMD) to assess performance of our model and provide both qualitative (including segmentation maps and thickness maps) and quantitative (including error metric comparisons and volumetric comparisons) results, which demonstrate that our proposed method performs favorably even for eyes with pathological changes caused by severe retinal diseases. The Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for patients with a wide range of AMD severity scores (0-11) were within 0.84\u00b10.41 and 1.33\u00b10.73 pixels, respectively, which are significantly better than some of the other state-of-the-art algorithms. <b>Conclusion -</b> The results demonstrate the utility of extracting features from the entire OCT volume by treating the volume as a correlated entity and show the benefit of utilizing 3D autoencoder based regression networks for smoothing the approximated retinal layers by inducing shape based regularization constraints.", "keywords": ["optical coherence tomography", "coherence tomography", "optical coherence", "detecting and prognosing", "Retinal layer segmentation", "Retinal", "segmentation", "retinal image volumes", "Introduction", "important approach", "image volumes", "Age-related Macular Degeneration", "prognosing disease", "severe retinal diseases", "robust machine learning", "machine learning techniques", "learning techniques lead", "OCT", "semantic segmentation algorithm", "spatial context"], "paper_title": "Retinal layer segmentation in optical coherence tomography (OCT) using a 3D deep-convolutional regression network for patients with age-related macular degeneration.", "last_updated": "2023/02/04"}, {"id": "0036038116", "domain": "Macular degeneration", "model_name": "Anegondi et al.", "publication_date": "2022/08/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36038116/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36038116", "task": null, "abstract": "To develop deep learning models for annualized geographic atrophy (GA) growth rate prediction using fundus autofluorescence (FAF) images and spectral-domain OCT volumes from baseline visits, which can be used for prognostic covariate adjustment to increase power of clinical trials. This retrospective analysis estimated GA growth rate as the slope of a linear fit on all available measurements of lesion area over a 2-year period. Three multitask deep learning models-FAF-only, OCT-only, and multimodal (FAF and OCT)-were developed to predict concurrent GA area and annualized growth rate. Patients were from prospective and observational lampalizumab clinical trials. The 3 models were trained on the development data set, tested on the holdout set, and further evaluated on the independent test sets. Baseline FAF images and OCT volumes from study eyes of patients with bilateral GA (NCT02247479; NCT02247531; and NCT02479386) were split into development (1279 patients/eyes) and holdout (443 patients/eyes) sets. Baseline FAF images from study eyes of NCT01229215 (106 patients/eyes) and NCT02399072 (169 patients/eyes) were used as independent test sets. Model performance was evaluated using squared Pearson correlation coefficient (r<sup>2</sup>) between observed and predicted lesion areas/growth rates. Confidence intervals were calculated by bootstrap resampling (B\u00a0= 10 000). On the holdout data set, r<sup>2</sup> (95% confidence interval) of the FAF-only, OCT-only, and multimodal models for GA lesion area prediction was 0.96 (0.95-0.97), 0.91 (0.87-0.95), and 0.94 (0.92-0.96), respectively, and for GA growth rate prediction was 0.48 (0.41-0.55), 0.36 (0.29-0.43), and 0.47 (0.40-0.54), respectively. On the 2 independent test sets, r<sup>2</sup> of the FAF-only model for GA lesion area was 0.98 (0.97-0.99) and 0.95 (0.93-0.96), and for GA growth rate was 0.65 (0.52-0.75) and 0.47 (0.34-0.60). We show the feasibility of using baseline FAF images and OCT volumes to predict individual GA area and growth rates using a multitask deep learning approach. The deep learning-based growth rate predictions could be used for covariate adjustment to increase power of clinical trials. Proprietary or commercial disclosure may be found after the references.", "keywords": ["Baseline FAF images", "growth rate", "annualized geographic atrophy", "spectral-domain OCT volumes", "FAF images", "OCT volumes", "Baseline FAF", "independent test sets", "FAF", "growth", "growth rate prediction", "spectral-domain OCT", "OCT", "geographic atrophy", "fundus autofluorescence", "test sets", "rate", "develop deep learning", "clinical trials", "annualized growth rate"], "paper_title": "Deep Learning to Predict Geographic Atrophy Area and Growth Rate from Multimodal Imaging.", "last_updated": "2023/02/04"}, {"id": "0033344065", "domain": "Macular degeneration", "model_name": "Lee et al.", "publication_date": "2020/12/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33344065/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33344065", "task": null, "abstract": "Delayed rod-mediated dark adaptation (RMDA) is a functional biomarker for incipient age-related macular degeneration (AMD). We used anatomically restricted spectral domain optical coherence tomography (SD-OCT) imaging data to localize de novo imaging features associated with and to test hypotheses about delayed RMDA. Rod intercept time (RIT) was measured in participants with and without AMD at 5 degrees from the fovea, and macular SD-OCT images were obtained. A deep learning model was trained with anatomically restricted information using a single representative B-scan through the fovea of each eye. Mean-occlusion masking was utilized to isolate the relevant imaging features. The model identified hyporeflective outer retinal bands on macular SD-OCT associated with delayed RMDA. The validation mean standard error (MSE) registered to the foveal B-scan localized the lowest error to 0.5 mm temporal to the fovea center, within an overall low-error region across the rod-free zone and adjoining parafovea. Mean absolute error (MAE) on the test set was 4.71 minutes (8.8% of the dynamic range). We report a novel framework for imaging biomarker discovery using deep learning and demonstrate its ability to identify and localize a previously undescribed biomarker in retinal imaging. The hyporeflective outer retinal bands in central macula on SD-OCT demonstrate a structural basis for dysfunctional rod vision that correlates to published histopathologic findings. This agnostic approach to anatomic biomarker discovery strengthens the rationale for RMDA as an outcome measure in early AMD clinical trials, and also expands the utility of deep learning beyond automated diagnosis to fundamental discovery.", "keywords": ["rod-mediated dark adaptation", "Delayed rod-mediated dark", "age-related macular degeneration", "incipient age-related macular", "delayed RMDA", "dark adaptation", "rod-mediated dark", "incipient age-related", "Delayed rod-mediated", "RMDA", "macular degeneration", "age-related macular", "AMD", "macular SD-OCT", "imaging", "Delayed", "anatomically restricted", "deep learning", "SD-OCT", "functional biomarker"], "paper_title": "Exploring a Structural Basis for Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration Via Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0034450729", "domain": "Diabetic retinopathy", "model_name": "Nazir et al.", "publication_date": "2021/08/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34450729/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34450729", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is an eye disease that alters the blood vessels of a person suffering from diabetes. Diabetic macular edema (DME) occurs when DR affects the macula, which causes fluid accumulation in the macula. Efficient screening systems require experts to manually analyze images to recognize diseases. However, due to the challenging nature of the screening method and lack of trained human resources, devising effective screening-oriented treatment is an expensive task. Automated systems are trying to cope with these challenges; however, these methods do not generalize well to multiple diseases and real-world scenarios. To solve the aforementioned issues, we propose a new method comprising two main steps. The first involves dataset preparation and feature extraction and the other relates to improving a custom deep learning based CenterNet model trained for eye disease classification. Initially, we generate annotations for suspected samples to locate the precise region of interest, while the other part of the proposed solution trains the Center Net model over annotated images. Specifically, we use DenseNet-100 as a feature extraction method on which the one-stage detector, CenterNet, is employed to localize and classify the disease lesions. We evaluated our method over challenging datasets, namely, APTOS-2019 and IDRiD, and attained average accuracy of 97.93% and 98.10%, respectively. We also performed cross-dataset validation with benchmark EYEPACS and Diaretdb1 datasets. Both qualitative and quantitative results demonstrate that our proposed approach outperforms state-of-the-art methods due to more effective localization power of CenterNet, as it can easily recognize small lesions and deal with over-fitted training data. Our proposed framework is proficient in correctly locating and classifying disease lesions. In comparison to existing DR and DME classification approaches, our method can extract representative key points from low-intensity and noisy images and accurately classify them. Hence our approach can play an important role in automated detection and recognition of DR and DME lesions.", "keywords": ["suffering from diabetes", "alters the blood", "blood vessels", "person suffering", "Diabetic retinopathy", "Diabetic macular edema", "method", "disease", "eye disease", "Diabetic macular", "disease lesions", "Diabetic", "DME", "lesions", "affects the macula", "macula", "Center Net model", "eye disease classification", "images", "retinopathy"], "paper_title": "Detection of Diabetic Eye Disease from Retinal Images Using a Deep Learning Based CenterNet Model.", "last_updated": "2023/02/04"}, {"id": "0035970318", "domain": "Macular degeneration", "model_name": "Pramil et al.", "publication_date": "2022/08/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35970318/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35970318", "task": "aipbNdPTIt", "abstract": "To present a deep learning algorithm for segmentation of geographic atrophy (GA) using en face swept-source OCT (SS-OCT) images that is accurate and reproducible for the assessment of GA growth over time. Retrospective review of images obtained as part of a prospective natural history study. Patients with GA (n\u00a0= 90), patients with early or intermediate age-related macular degeneration (n\u00a0= 32), and healthy controls (n\u00a0= 16). An automated algorithm using scan volume data to generate 3 image inputs characterizing the main OCT features of GA-hypertransmission in subretinal pigment epithelium (sub-RPE) slab, regions of RPE loss, and loss of retinal thickness-was trained using 126 images (93 with GA and 33 without GA, from the same number of eyes) using a fivefold cross-validation method and data augmentation techniques. It was tested in an independent set of one hundred eighty 6\u00a0\u00d7 6-mm<sup>2</sup> macular SS-OCT scans consisting of 3 repeated scans of 30 eyes with GA at baseline and follow-up as well as 45 images obtained from 42 eyes without GA. The GA area, enlargement rate of GA area, square root of GA area, and square root of the enlargement rate of GA area measurements were calculated using the automated algorithm and compared with ground truth calculations performed by 2 manual graders. The repeatability of these measurements was determined using intraclass coefficients (ICCs). There were no significant differences in the GA areas, enlargement rates of GA area, square roots of GA area, and square roots of the enlargement rates of GA area between the graders and the automated algorithm. The algorithm showed high repeatability, with ICCs of 0.99 and 0.94 for the GA area measurements and the enlargement rates of GA area, respectively. The repeatability limit for the GA area measurements made by grader 1, grader 2, and the automated algorithm was 0.28, 0.33, and 0.92 mm<sup>2</sup>, respectively. When compared with manual methods, this proposed deep learning-based automated algorithm for GA segmentation using en face SS-OCT images was able to accurately delineate GA and produce reproducible measurements of the enlargement rates of GA.", "keywords": ["area", "enlargement rates", "automated algorithm", "face swept-source OCT", "area measurements", "geographic atrophy", "growth over time", "swept-source OCT", "algorithm", "enlargement", "deep learning algorithm", "images", "automated", "rates", "learning algorithm", "measurements", "images obtained", "square root", "OCT", "square"], "paper_title": "A Deep Learning Model for Automated Segmentation of Geographic Atrophy Imaged Using Swept-Source OCT.", "last_updated": "2023/02/04"}, {"id": "0033031250", "domain": "Diabetic retinopathy", "model_name": "Wu et al.", "publication_date": "2021/12/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33031250/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33031250", "task": "IWqQC1koJA", "abstract": "To develop a deep learning (DL) model to detect morphologic patterns of diabetic macular edema (DME) based on optical coherence tomography (OCT) images. In the training set, 12,365 OCT images were extracted from a public data set and an ophthalmic center. A total of 656 OCT images were extracted from another ophthalmic center for external validation. The presence or absence of three OCT patterns of DME, including diffused retinal thickening, cystoid macular edema, and serous retinal detachment, was labeled with 1 or 0, respectively. A DL model was trained to detect three OCT patterns of DME. The occlusion test was applied for the visualization of the DL model. Applying 5-fold cross-validation method in internal validation, the area under the receiver operating characteristic curve for the detection of three OCT patterns (i.e., diffused retinal thickening, cystoid macular edema, and serous retinal detachment) was 0.971, 0.974, and 0.994, respectively, with an accuracy of 93.0%, 95.1%, and 98.8%, respectively, a sensitivity of 93.5%, 94.5%, and 96.7%, respectively, and a specificity of 92.3%, 95.6%, and 99.3%, respectively. In external validation, the area under the receiver operating characteristic curve was 0.970, 0.997, and 0.997, respectively, with an accuracy of 90.2%, 95.4%, and 95.9%, respectively, a sensitivity of 80.1%, 93.4%, and 94.9%, respectively, and a specificity of 97.6%, 97.2%, and 96.5%, respectively. The occlusion test showed that the DL model could successfully identify the pathologic regions most critical for detection. Our DL model demonstrated high accuracy and transparency in the detection of OCT patterns of DME. These results emphasized the potential of artificial intelligence in assisting clinical decision-making processes in patients with DME.", "keywords": ["optical coherence tomography", "OCT patterns", "OCT images", "OCT", "diabetic macular edema", "detect morphologic patterns", "DME", "patterns of DME", "deep learning", "based on optical", "coherence tomography", "cystoid macular edema", "develop a deep", "optical coherence", "macular edema", "patterns", "morphologic patterns", "serous retinal detachment", "model", "images were extracted"], "paper_title": "DETECTION OF MORPHOLOGIC PATTERNS OF DIABETIC MACULAR EDEMA USING A DEEP LEARNING APPROACH BASED ON OPTICAL COHERENCE TOMOGRAPHY IMAGES.", "last_updated": "2023/02/04"}, {"id": "0029159541", "domain": "Macular degeneration", "model_name": "Treder et al.", "publication_date": "2017/11/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29159541/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "29159541", "task": "IWqQC1koJA", "abstract": "Our purpose was to use deep learning for the automated detection of age-related macular degeneration (AMD) in spectral domain optical coherence tomography (SD-OCT). A total of 1112 cross-section SD-OCT images of patients with exudative AMD and a healthy control group were used for this study. In the first step, an open-source multi-layer deep convolutional neural network (DCNN), which was pretrained with 1.2 million images from ImageNet, was trained and validated with 1012 cross-section SD-OCT scans (AMD: 701; healthy: 311). During this procedure training accuracy, validation accuracy and cross-entropy were computed. The open-source deep learning framework TensorFlow\u2122 (Google Inc., Mountain View, CA, USA) was used to accelerate the deep learning process. In the last step, a created DCNN classifier, using the information of the above mentioned deep learning process, was tested in detecting 100 untrained cross-section SD-OCT images (AMD: 50; healthy: 50). Therefore, an AMD testing score was computed: 0.98 or higher was presumed for AMD. After an iteration of 500 training steps, the training accuracy and validation accuracies were 100%, and the cross-entropy was 0.005. The average AMD scores were 0.997\u2009\u00b1\u20090.003 in the AMD testing group and 0.9203\u2009\u00b1\u20090.085 in the healthy comparison group. The difference between the two groups was highly significant (p\u2009<\u20090.001). With a deep learning-based approach using TensorFlow\u2122, it is possible to detect AMD in SD-OCT with high sensitivity and specificity. With more image data, an expansion of this classifier for other macular diseases or further details in AMD is possible, suggesting an application for this model as a support in clinical decisions. Another possible future application would involve the individual prediction of the progress and success of therapy for different diseases by automatically detecting hidden image information.", "keywords": ["optical coherence tomography", "spectral domain optical", "domain optical coherence", "AMD", "age-related macular degeneration", "cross-section SD-OCT images", "cross-section SD-OCT", "coherence tomography", "deep learning", "automated detection", "detection of age-related", "spectral domain", "domain optical", "optical coherence", "deep learning process", "SD-OCT", "deep", "SD-OCT images", "AMD testing", "cross-section SD-OCT scans"], "paper_title": "Automated detection of exudative age-related macular degeneration in spectral domain optical coherence tomography using deep learning.", "last_updated": "2023/02/04"}, {"id": "0031981283", "domain": "Macular degeneration", "model_name": "Shah et al.", "publication_date": "2020/01/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31981283/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31981283", "task": "IWqQC1koJA", "abstract": "Recent advances in deep learning have seen an increase in its application to automated image analysis in ophthalmology for conditions with a high prevalence. We wanted to identify whether deep learning could be used for the automated classification of optical coherence tomography (OCT) images from patients with Stargardt disease (STGD) using a smaller dataset than traditionally used. Sixty participants with STGD and 33 participants with a normal retinal OCT were selected, and a single OCT scan containing the centre of the fovea was selected as the input data. Two approaches were used: Model 1 - a pretrained convolutional neural network (CNN); Model 2 - a new CNN architecture. Both models were evaluated on their accuracy, sensitivity, specificity and Jaccard similarity score (JSS). About 102 OCT scans from participants with a normal retinal OCT and 647 OCT scans from participants with STGD were selected. The highest results were achieved when both models were implemented as a binary classifier: Model 1 - accuracy 99.6%, sensitivity 99.8%, specificity 98.0% and JSS 0.990; Model 2 - accuracy 97.9%, sensitivity 97.9%, specificity 98.0% and JSS 0.976. The deep learning classification models used in this study were able to achieve high accuracy despite using a smaller dataset than traditionally used and are effective in differentiating between normal OCT scans and those from patients with STGD. This preliminary study provides promising results for the application of deep learning to classify OCT images from patients with inherited retinal diseases.", "keywords": ["OCT", "automated image analysis", "OCT scans", "Recent advances", "analysis in ophthalmology", "ophthalmology for conditions", "STGD", "normal retinal OCT", "deep learning", "JSS", "Model", "retinal OCT", "participants with STGD", "image analysis", "participants", "models", "deep", "learning", "OCT images", "patients with Stargardt"], "paper_title": "Automated classification of normal and Stargardt disease optical coherence tomography images using deep learning.", "last_updated": "2023/02/04"}, {"id": "0035865175", "domain": "Macular degeneration", "model_name": "OCT", "publication_date": "2022/07/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35865175/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35865175", "task": "0GPcU42tzV", "abstract": "Previously, we have shown the capability of a hybrid deep learning (DL) model that combines a U-Net and a sliding-window (SW) convolutional neural network (CNN) for automatic segmentation of retinal layers from OCT scan images in retinitis pigmentosa (RP). We found that one of the shortcomings of the hybrid model is that it tends to underestimate ellipsoid zone (EZ) width or area, especially when EZ extends toward or beyond the edge of the macula. In this study, we trained the model with additional data which included more OCT scans having extended EZ. We evaluated its performance in automatic measurement of EZ area on SD-OCT volume scans obtained from the participants of the RUSH2A natural history study by comparing the model's performance to the reading center's manual grading. De-identified Spectralis high-resolution 9-mm 121-line macular volume scans as well as their EZ area measurements by a reading center were transferred from the management center of the RUSH2A study under the data transfer and processing agreement. A total of 86 baseline volume scans from 86 participants of the RUSH2A study were included to evaluate two hybrid models: the original RP240 model trained on 480 mid-line B-scans from 220 patients with retinitis pigmentosa (RP) and 20 participants with normal vision from a single site, and the new RP340 model trained on a revised RP340 dataset which included RP240 dataset plus an additional 200 mid-line B-scans from another 100 patients with RP. There was no overlap of patients between training and evaluation datasets. EZ and apical RPE in each B-scan image were automatically segmented by the hybrid model. EZ areas were determined by interpolating the discrete 2-dimensional B-scan EZ-RPE layer over the scan area. Dice similarity, correlation, linear regression, and Bland-Altman analyses were conducted to assess the agreement between the EZ areas measured by the hybrid model and by the reading center. For EZ area > 1 mm<sup>2</sup>, average dice coefficients \u00b1 SD between the EZ band segmentations determined by the DL model and the manual grading were 0.835 \u00b1 0.132 and 0.867 \u00b1 0.105 for RP240 and RP340 hybrid models, respectively (<i>p</i> < 0.0005; <i>n</i> = 51). When compared to the manual grading, correlation coefficients (95% CI) were 0.991 (0.987-0.994) and 0.994 (0.991-0.996) for RP240 and RP340 hybrid models, respectively. Linear regression slopes (95% CI) were 0.918 (0.896-0.940) and 0.995 (0.975-1.014), respectively. Bland-Altman analysis revealed a mean difference \u00b1 SD of -0.137 \u00b1 1.131 mm<sup>2</sup> and 0.082 \u00b1 0.825 mm<sup>2</sup>, respectively. Additional training data improved the hybrid model's performance, especially reducing the bias and narrowing the range of the 95% limit of agreement when compared to manual grading. The close agreement of DL models to manual grading suggests that DL may provide effective tools to significantly reduce the burden of reading centers to analyze OCT scan images. In addition to EZ area, our DL models can also provide the measurements of photoreceptor outer segment volume and thickness to further help assess disease progression and to facilitate the study of structure and function relationship in RP.", "keywords": ["convolutional neural network", "hybrid deep learning", "OCT scan images", "hybrid model", "model", "OCT scan", "hybrid", "manual grading", "volume scans", "models", "deep learning", "convolutional neural", "neural network", "shown the capability", "combines a U-Net", "hybrid model performance", "OCT scans", "area", "OCT", "model trained"], "paper_title": "Performance of Deep Learning Models in Automatic Measurement of Ellipsoid Zone Area on Baseline Optical Coherence Tomography (OCT) Images From the Rate of Progression of USH2A-Related Retinal Degeneration (RUSH2A) Study.", "last_updated": "2023/02/04"}, {"id": "0036528999", "domain": "Macular degeneration", "model_name": "Dom\u00ednguez et al.", "publication_date": "2022/12/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36528999/", "code_link": null, "model_type": "transformer", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36528999", "task": "IWqQC1koJA", "abstract": "Age-related macular degeneration (AMD) is an eye disease that happens when ageing causes damage to the macula, and it is the leading cause of blindness in developed countries. Screening retinal fundus images allows ophthalmologists to early detect, diagnose and treat this disease; however, the manual interpretation of images is a time-consuming task. In this paper, we aim to study different deep learning methods to diagnose AMD. We have conducted a thorough study of two families of deep learning models based on convolutional neural networks (CNN) and transformer architectures to automatically diagnose referable/non-referable AMD, and grade AMD severity scales (no AMD, early AMD, intermediate AMD, and advanced AMD). In addition, we have analysed several progressive resizing strategies and ensemble methods for convolutional-based architectures to further improve the performance of the models. As a first result, we have shown that transformer-based architectures obtain considerably worse results than convolutional-based architectures for diagnosing AMD. Moreover, we have built a model for diagnosing referable AMD that yielded a mean F1-score (SD) of 92.60% (0.47), a mean AUROC (SD) of 97.53% (0.40), and a mean weighted kappa coefficient (SD) of 85.28% (0.91); and an ensemble of models for grading AMD severity scales with a mean accuracy (SD) of 82.55% (2.92), and a mean weighted kappa coefficient (SD) of 84.76% (2.45). This work shows that working with convolutional based architectures is more suitable than using transformer based models for classifying and grading AMD from retinal fundus images. Furthermore, convolutional models can be improved by means of progressive resizing strategies and ensemble methods.", "keywords": ["Age-related macular degeneration", "AMD", "Age-related macular", "macular degeneration", "developed countries", "ageing causes damage", "blindness in developed", "AMD severity scales", "eye disease", "AMD severity", "grading AMD", "architectures", "models", "diagnose AMD", "grading AMD severity", "early AMD", "diagnose", "retinal fundus images", "deep learning", "grade AMD severity"], "paper_title": "Binary and multi-class automated detection of age-related macular degeneration using convolutional- and transformer-based architectures.", "last_updated": "2023/02/04"}, {"id": "0036716039", "domain": "Macular degeneration", "model_name": "Chen et al.", "publication_date": "2023/02/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36716039/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36716039", "task": "IWqQC1koJA", "abstract": "This study was designed to apply deep learning models in retinal disease screening and lesion detection based on optical coherence tomography (OCT) images. We collected 37,138 OCT images from 775 patients and labelled by ophthalmologists. Multiple deep learning models including ResNet50 and YOLOv3 were developed to identify the types and locations of diseases or lesions based on the images. The model were evaluated using patient-based independent holdout set. For binary classification of OCT images with or without lesions, the performance accuracy was 98.5%, sensitivity was 98.7%, specificity was 98.4%, and the F1 score was 97.7%. For multiclass multilabel disease classification, the models was able to detect vitreomacular traction syndrome and age-related macular degeneration both with an accuracy of more than 99%, sensitivity of more than 98%, specificity of more than 98%, and an F1 score of more than 97%. For lesion location detection, the recalls for different lesion types ranged from 87.0% (epiretinal membrane) to 98.2% (macular pucker). Deep learning-based models have potentials to aid retinal disease screening, classification and diagnosis with excellent performance, which may serve as useful references for ophthalmologists. The deep learning-based models are capable of identifying and predicting different eye diseases and lesions from OCT images and may have potential clinical application to assist the ophthalmologists for fast and accuracy retinal disease screening.", "keywords": ["optical coherence tomography", "OCT images", "apply deep learning", "coherence tomography", "deep learning models", "study was designed", "designed to apply", "optical coherence", "OCT", "images", "retinal disease screening", "learning models", "based on optical", "models", "deep learning", "retinal disease", "disease screening", "Deep learning-based models", "disease", "apply deep"], "paper_title": "Deep Learning-Based System for Disease Screening and Pathologic Region Detection From Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0032221349", "domain": "Diabetic retinopathy", "model_name": "photoreceptor-segmentation.git", "publication_date": "2020/03/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32221349/", "code_link": "https://github.com/ignaciorlando/photoreceptor-segmentation.git", "model_type": "CNN", "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "32221349", "task": null, "abstract": "Diabetic macular edema (DME) and retina vein occlusion (RVO) are macular diseases in which central photoreceptors are affected due to pathological accumulation of fluid. Optical coherence tomography allows to visually assess and evaluate photoreceptor integrity, whose alteration has been observed as an important biomarker of both diseases. However, the manual quantification of this layered structure is challenging, tedious and time-consuming. In this paper we introduce a deep learning approach for automatically segmenting and characterising photoreceptor alteration. The photoreceptor layer is segmented using an ensemble of four different convolutional neural networks. En-face representations of the layer thickness are produced to characterize the photoreceptors. The pixel-wise standard deviation of the score maps produced by the individual models is also taken to indicate areas of photoreceptor abnormality or ambiguous results. Experimental results showed that our ensemble is able to produce results in pair with a human expert, outperforming each of its constitutive models. No statistically significant differences were observed between mean thickness estimates obtained from automated and manually generated annotations. Therefore, our model is able to reliable quantify photoreceptors, which can be used to improve prognosis and managment of macular diseases.", "keywords": ["retina vein occlusion", "Diabetic macular edema", "vein occlusion", "accumulation of fluid", "retina vein", "affected due", "due to pathological", "pathological accumulation", "DME", "RVO", "Diabetic macular", "macular edema", "macular diseases", "diseases", "central photoreceptors", "photoreceptor", "Optical coherence tomography", "evaluate photoreceptor integrity", "Diabetic", "edema"], "paper_title": "Automated Quantification of Photoreceptor alteration in macular disease using Optical Coherence Tomography and Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0030629091", "domain": "Diabetic retinopathy", "model_name": "Burlina et al.", "publication_date": "2019/11/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30629091/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30629091", "task": null, "abstract": "Deep learning (DL) used for discriminative tasks in ophthalmology, such as diagnosing diabetic retinopathy or age-related macular degeneration (AMD), requires large image data sets graded by human experts to train deep convolutional neural networks (DCNNs). In contrast, generative DL techniques could synthesize large new data sets of artificial retina images with different stages of AMD. Such images could enhance existing data sets of common and rare ophthalmic diseases without concern for personally identifying information to assist medical education of students, residents, and retinal specialists, as well as for training new DL diagnostic models for which extensive data sets from large clinical trials of expertly graded images may not exist. To develop DL techniques for synthesizing high-resolution realistic fundus images serving as proxy data sets for use by retinal specialists and DL machines. Generative adversarial networks were trained on 133 821 color fundus images from 4613 study participants from the Age-Related Eye Disease Study (AREDS), generating synthetic fundus images with and without AMD. We compared retinal specialists' ability to diagnose AMD on both real and synthetic images, asking them to assess image gradability and testing their ability to discern real from synthetic images. The performance of AMD diagnostic DCNNs (referable vs not referable AMD) trained on either all-real vs all-synthetic data sets was compared. Accuracy of 2 retinal specialists (T.Y.A.L. and K.D.P.) for diagnosing and distinguishing AMD on real vs synthetic images and diagnostic performance (area under the curve) of DL algorithms trained on synthetic vs real images. The diagnostic accuracy of 2 retinal specialists on real vs synthetic images was similar. The accuracy of diagnosis as referable vs nonreferable AMD compared with certified human graders for retinal specialist 1 was 84.54% (error margin, 4.06%) on real images vs 84.12% (error margin, 4.16%) on synthetic images and for retinal specialist 2 was 89.47% (error margin, 3.45%) on real images vs 89.19% (error margin, 3.54%) on synthetic images. Retinal specialists could not distinguish real from synthetic images, with an accuracy of 59.50% (error margin, 3.93%) for retinal specialist 1 and 53.67% (error margin, 3.99%) for retinal specialist 2. The DCNNs trained on real data showed an area under the curve of 0.9706 (error margin, 0.0029), and those trained on synthetic data showed an area under the curve of 0.9235 (error margin, 0.0045). Deep learning-synthesized images appeared to be realistic to retinal specialists, and DCNNs achieved diagnostic performance on synthetic data close to that for real images, suggesting that DL generative techniques hold promise for training humans and machines.", "keywords": ["error margin", "data sets", "images", "synthetic images", "AMD", "retinal specialists", "retinal", "synthetic", "data", "error", "margin", "real", "sets", "age-related macular degeneration", "real images", "requires large image", "convolutional neural networks", "image data sets", "diagnosing diabetic retinopathy", "specialists"], "paper_title": "Assessment of Deep Generative Models for High-Resolution Synthetic Retinal Image Generation of Age-Related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0035808169", "domain": "Diabetic retinopathy", "model_name": "Ara et al.", "publication_date": "2022/06/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35808169/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35808169", "task": "IWqQC1koJA", "abstract": "The use of optical coherence tomography (OCT) in medical diagnostics is now common. The growing amount of data leads us to propose an automated support system for medical staff. The key part of the system is a classification algorithm developed with modern machine learning techniques. The main contribution is to present a new approach for the classification of eye diseases using the convolutional neural network model. The research concerns the classification of patients on the basis of OCT B-scans into one of four categories: Diabetic Macular Edema (DME), Choroidal Neovascularization (CNV), Drusen, and Normal. Those categories are available in a publicly available dataset of above 84,000 images utilized for the research. After several tested architectures, our 5-layer neural network gives us a promising result. We compared them to the other available solutions which proves the high quality of our algorithm. Equally important for the application of the algorithm is the computational time, which is reduced by the limited size of the model. In addition, the article presents a detailed method of image data augmentation and its impact on the classification results. The results of the experiments were also presented for several derived models of convolutional network architectures that were tested during the research. Improving processes in medical treatment is important. The algorithm cannot replace a doctor but, for example, can be a valuable tool for speeding up the process of diagnosis during screening tests.", "keywords": ["optical coherence tomography", "coherence tomography", "optical coherence", "medical diagnostics", "Diabetic Macular Edema", "OCT B-scans", "classification", "medical", "automated support system", "classification algorithm developed", "medical staff", "algorithm", "neural network", "OCT", "Choroidal Neovascularization", "Diabetic Macular", "Macular Edema", "classification algorithm", "neural network model", "research"], "paper_title": "Fast and Efficient Method for Optical Coherence Tomography Images Classification Using Deep Learning Approach.", "last_updated": "2023/02/04"}, {"id": "0031093370", "domain": "Diabetic retinopathy", "model_name": "Kuwayama et al.", "publication_date": "2019/04/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31093370/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31093370", "task": "IWqQC1koJA", "abstract": "Although optical coherence tomography (OCT) is essential for ophthalmologists, reading of findings requires expertise. The purpose of this study is to test deep learning with image augmentation for automated detection of chorioretinal diseases. A retina specialist diagnosed 1,200 OCT images. The diagnoses involved normal eyes (<i>n</i>=570) and those with wet age-related macular degeneration (AMD) (<i>n</i>=136), diabetic retinopathy (DR) (<i>n</i>=104), epiretinal membranes (ERMs) (<i>n</i>=90), and another 19 diseases. Among them, 1,100 images were used for deep learning training, augmented to 59,400 by horizontal flipping, rotation, and translation. The remaining 100 images were used to evaluate the trained convolutional neural network (CNN) model. Automated disease detection showed that the first candidate disease corresponded to the doctor's decision in 83 (83%) images and the second candidate disease in seven (7%) images. The precision and recall of the CNN model were 0.85 and 0.97 for normal eyes, 1.00 and 0.77 for wet AMD, 0.78 and 1.00 for DR, and 0.75 and 0.75 for ERMs, respectively. Some of rare diseases such as Vogt-Koyanagi-Harada disease were correctly detected by image augmentation in the CNN training. Automated detection of macular diseases from OCT images might be feasible using the CNN model. Image augmentation might be effective to compensate for a small image number for training.", "keywords": ["optical coherence tomography", "findings requires expertise", "coherence tomography", "essential for ophthalmologists", "reading of findings", "requires expertise", "optical coherence", "findings requires", "OCT images", "CNN", "OCT", "images", "CNN model", "image", "image augmentation", "diseases", "disease", "AMD", "deep learning", "CNN training"], "paper_title": "Automated Detection of Macular Diseases by Optical Coherence Tomography and Artificial Intelligence Machine Learning of Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0036315120", "domain": "Macular degeneration", "model_name": "De Silva et al.", "publication_date": "2022/11/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36315120/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36315120", "task": null, "abstract": "This study investigates deep-learning (DL) sequence modeling techniques to reliably fit dark adaptation (DA) curves and estimate their key parameters in patients with age-related macular degeneration (AMD) to improve robustness and curve predictions. A long-short-term memory autoencoder was used as the DL method to model the DA curve. The performance was compared against the classical nonlinear regression method using goodness-of-fit and repeatability metrics. Experiments were performed to predict the latter portion of the curve using data from early measurements. The prediction accuracy was quantified as the rod intercept time (RIT) prediction error between predicted and actual curves. The two models had comparable goodness-of-fit measures, with root mean squared error (RMSE; SD) = 0.11 (0.04) log-units (LU) for the classical model and RMSE = 0.13 (0.06) LU for the DL model. Repeatability of the curve fits evaluated after introduction of random perturbations, and after performing repeated testing, demonstrated superiority of the DL method, especially among parameters related to cone decay. The DL method exhibited superior ability to predict the curve and RIT using points prior to -2 LU, with 3.1 \u00b1 3.1 minutes RIT prediction error, compared to 19.1 \u00b1 18.6 minutes RIT error for the classical method. The parameters obtained from the DL method demonstrated superior robustness as well as predictability of the curve. These could provide important advances in using multiple DA curve parameters to characterize AMD severity. Dark adaptation is an important functional measure in studies of AMD and curve modeling using DL methods can lead to improved clinical trial end points.", "keywords": ["study investigates deep-learning", "age-related macular degeneration", "sequence modeling techniques", "investigates deep-learning", "macular degeneration", "curve", "study investigates", "techniques to reliably", "estimate their key", "patients with age-related", "age-related macular", "method", "RIT", "RIT prediction error", "reliably fit dark", "minutes RIT", "minutes RIT prediction", "minutes RIT error", "AMD", "key parameters"], "paper_title": "Deep Learning-Based Modeling of the Dark Adaptation Curve for Robust Parameter Estimation.", "last_updated": "2023/02/04"}, {"id": "0033383315", "domain": "Macular degeneration", "model_name": "Miere et al.", "publication_date": "2020/12/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33383315/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33383315", "task": "IWqQC1koJA", "abstract": "To automatically classify retinal atrophy according to its etiology, using fundus autofluorescence (FAF) images, using a deep learning model. In this study, FAF images of patients with advanced dry age-related macular degeneration (AMD), also called geographic atrophy (GA), and genetically confirmed inherited retinal diseases (IRDs) in late atrophic stages [Stargardt disease (STGD1) and Pseudo-Stargardt Pattern Dystrophy (PSPD)] were included. The FAF images were used to train a multi-layer deep convolutional neural network (CNN) to differentiate on FAF between atrophy in the context of AMD (GA) and atrophy secondary to IRDs. Three-hundred fourteen FAF images were included, of which 110 images were of GA eyes and 204 were eyes with genetically confirmed STGD1 or PSPD. In the first approach, the CNN was trained and validated with 251 FAF images. Established augmentation techniques were used and an Adam optimizer was used for training. For the subsequent testing, the built classifiers were then tested with 63 untrained FAF images. The visualization method was integrated gradient visualization. In the second approach, 10-fold cross-validation was used to determine the model's performance. In the first approach, the best performance of the model was obtained using 10 epochs, with an accuracy of 0.92 and an area under the curve for Receiver Operating Characteristic (AUC-ROC) of 0.981. Mean accuracy was 87.30\u00a0\u00b1\u00a02.96. In the second approach, a mean accuracy of 0.79\u00a0\u00b1\u00a00.06 was obtained. This study describes the use of a deep learning-based algorithm to automatically classify atrophy on FAF imaging according to its etiology. Accurate differential diagnosis between GA and late-onset IRDs masquerading as GA on FAF can be performed with good accuracy and AUC-ROC values.", "keywords": ["FAF images", "FAF", "Pseudo-Stargardt Pattern Dystrophy", "fourteen FAF images", "images", "untrained FAF images", "fundus autofluorescence", "Pattern Dystrophy", "deep learning model", "inherited retinal diseases", "atrophy", "Stargardt disease", "classify retinal atrophy", "deep learning", "confirmed inherited retinal", "AMD", "approach", "PSPD", "retinal diseases", "retinal atrophy"], "paper_title": "Deep learning-based classification of retinal atrophy using fundus autofluorescence imaging.", "last_updated": "2023/02/04"}, {"id": "0036701964", "domain": "Macular degeneration", "model_name": "Moradi et al.", "publication_date": "2023/01/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36701964/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36701964", "task": "aipbNdPTIt", "abstract": "Accurate retinal layer segmentation in optical coherence tomography (OCT) images is crucial for quantitatively analyzing age-related macular degeneration (AMD) and monitoring its progression. However, previous retinal segmentation models depend on experienced experts and manually annotating retinal layers is time-consuming. On the other hand, accuracy of AMD diagnosis is directly related to the segmentation model's performance. To address these issues, we aimed to improve AMD detection using optimized retinal layer segmentation and deep ensemble learning. We integrated a graph-cut algorithm with a cubic spline to automatically annotate 11 retinal boundaries. The refined images were fed into a deep ensemble mechanism that combined a Bagged Tree and end-to-end deep learning classifiers. We tested the developed deep ensemble model on internal and external datasets. The total error rates for our segmentation model using the boundary refinement approach was significantly lower than OCT Explorer segmentations (1.7% vs. 7.8%, p-value\u00a0=\u00a00.03). We utilized the refinement approach to quantify 169 imaging features using Zeiss SD-OCT volume scans. The presence of drusen and thickness of total retina, neurosensory retina, and ellipsoid zone to inner-outer segment (EZ-ISOS) thickness had higher contributions to AMD classification compared to other features. The developed ensemble learning model obtained a higher diagnostic accuracy in a shorter time compared with two human graders. The area under the curve (AUC) for normal vs. early AMD was 99.4%. Testing results showed that the developed framework is repeatable and effective as a potentially valuable tool in retinal imaging research.", "keywords": ["optical coherence tomography", "age-related macular degeneration", "quantitatively analyzing age-related", "analyzing age-related macular", "retinal layer segmentation", "Accurate retinal layer", "coherence tomography", "macular degeneration", "monitoring its progression", "optical coherence", "crucial for quantitatively", "quantitatively analyzing", "analyzing age-related", "age-related macular", "layer segmentation", "AMD", "OCT Explorer segmentations", "retinal layer", "segmentation", "retinal"], "paper_title": "Deep ensemble learning for automated non-advanced AMD classification using optimized retinal layer segmentation and SD-OCT scans.", "last_updated": "2023/02/04"}, {"id": "0032818086", "domain": "Macular degeneration", "model_name": "AMD", "publication_date": "2020/04/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32818086/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32818086", "task": "0GPcU42tzV", "abstract": "To build and validate artificial intelligence (AI)-based models for AMD screening and for predicting late dry and wet AMD progression within 1 and 2 years. The dataset of the Age-related Eye Disease Study (AREDS) was used to train and validate our prediction model. External validation was performed on the Nutritional AMD Treatment-2 (NAT-2) study. An ensemble of deep learning screening methods was trained and validated on 116,875 color fundus photos from 4139 participants in the AREDS study to classify them as no, early, intermediate, or advanced AMD and further stratified them along the AREDS 12 level severity scale. Second step: the resulting AMD scores were combined with sociodemographic clinical data and other automatically extracted imaging data by a logistic model tree machine learning technique to predict risk for progression to late AMD within 1 or 2 years, with training and validation performed on 923 AREDS participants who progressed within 2 years, 901 who progressed within 1 year, and 2840 who did not progress within 2 years. For those found at risk of progression to late AMD, we further predicted the type (dry or wet) of the progression of late AMD. For identification of early/none vs. intermediate/late (i.e., referral level) AMD, we achieved 99.2% accuracy. The prediction model for a 2-year incident late AMD (any) achieved 86.36% accuracy, with 66.88% for late dry and 67.15% for late wet AMD. For the NAT-2 dataset, the 2-year late AMD prediction accuracy was 84%. Validated color fundus photo-based models for AMD screening and risk prediction for late AMD are now ready for clinical testing and potential telemedical deployment. Noninvasive, highly accurate, and fast AI methods to screen for referral level AMD and to predict late AMD progression offer significant potential improvements in our care of this prevalent blinding disease.", "keywords": ["late AMD", "AMD", "validate artificial intelligence", "Age-related Eye Disease", "late", "Eye Disease Study", "Age-related Eye", "AMD screening", "late AMD progression", "AMD progression", "late wet AMD", "AREDS", "wet AMD", "years", "artificial intelligence", "late AMD prediction", "Nutritional AMD", "predicting late dry", "predict late AMD", "wet AMD progression"], "paper_title": "Artificial Intelligence to Stratify Severity of Age-Related Macular Degeneration (AMD) and Predict Risk of Progression to Late AMD.", "last_updated": "2023/02/04"}, {"id": "0035507621", "domain": "Disc hemorrhage", "model_name": "Fang et al.", "publication_date": "2022/09/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35507621/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "35507621", "task": "IWqQC1koJA", "abstract": "Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance, as the vision loss caused by this disease is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. Cutting edge deep learning based algorithms have been recently developed for automatically detecting AMD from fundus images. However, there are still lack of a comprehensive annotated dataset and standard evaluation benchmarks. To deal with this issue, we set up the Automatic Detection challenge on Age-related Macular degeneration (ADAM), which was held as a satellite event of the ISBI 2020 conference. The ADAM challenge consisted of four tasks which cover the main aspects of detecting and characterizing AMD from fundus images, including detection of AMD, detection and segmentation of optic disc, localization of fovea, and detection and segmentation of lesions. As part of the ADAM challenge, we have released a comprehensive dataset of 1200 fundus images with AMD diagnostic labels, pixel-wise segmentation masks for both optic disc and AMD-related lesions (drusen, exudates, hemorrhages and scars, among others), as well as the coordinates corresponding to the location of the macular fovea. A uniform evaluation framework has been built to make a fair comparison of different models using this dataset. During the ADAM challenge, 610 results were submitted for online evaluation, with 11 teams finally participating in the onsite challenge. This paper introduces the challenge, the dataset and the evaluation methods, as well as summarizes the participating methods and analyzes their results for each task. In particular, we observed that the ensembling strategy and the incorporation of clinical domain knowledge were the key to improve the performance of the deep learning models.", "keywords": ["Age-related macular degeneration", "AMD", "ADAM challenge", "visual impairment", "impairment among elderly", "ADAM", "Age-related macular", "fundus images", "Automatic Detection challenge", "challenge", "detection", "macular degeneration", "automatically detecting AMD", "ADAM challenge consisted", "fundus", "detecting AMD", "AMD from fundus", "dataset", "detection of AMD", "macular"], "paper_title": "ADAM Challenge: Detecting Age-Related Macular Degeneration From Fundus Images.", "last_updated": "2023/02/04"}, {"id": "0036672999", "domain": "Macular degeneration", "model_name": "Kaothanthong et al.", "publication_date": "2023/01/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36672999/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36672999", "task": "aipbNdPTIt", "abstract": "We compared the performance of deep learning (DL) in the classification of optical coherence tomography (OCT) images of macular diseases between automated classification alone and in combination with automated segmentation. OCT images were collected from patients with neovascular age-related macular degeneration, polypoidal choroidal vasculopathy, diabetic macular edema, retinal vein occlusion, cystoid macular edema in Irvine-Gass syndrome, and other macular diseases, along with the normal fellow eyes. A total of 14,327 OCT images were used to train DL models. Three experiments were conducted: classification alone (CA), use of automated segmentation of the OCT images by RelayNet, and the graph-cut technique before the classification (combination method 1 (CM1) and 2 (CM2), respectively). For validation of classification of the macular diseases, the sensitivity, specificity, and accuracy of CA were found at 62.55%, 95.16%, and 93.14%, respectively, whereas the sensitivity, specificity, and accuracy of CM1 were found at 72.90%, 96.20%, and 93.92%, respectively, and of CM2 at 71.36%, 96.42%, and 94.80%, respectively. The accuracy of CM2 was statistically higher than that of CA (<i>p</i> = 0.05878). All three methods achieved AUC at 97%. Applying DL for segmentation of OCT images prior to classification of the images by another DL model may improve the performance of the classification.", "keywords": ["optical coherence tomography", "OCT images", "macular diseases", "OCT", "OCT images prior", "deep learning", "coherence tomography", "macular", "optical coherence", "images", "classification", "macular edema", "diabetic macular edema", "cystoid macular edema", "age-related macular degeneration", "automated segmentation", "neovascular age-related macular", "diseases", "compared the performance", "performance of deep"], "paper_title": "The Classification of Common Macular Diseases Using Deep Learning on Optical Coherence Tomography Images with and without Prior Automated Segmentation.", "last_updated": "2023/02/04"}, {"id": "0031913272", "domain": "Diabetic retinopathy", "model_name": "inception_v3.py", "publication_date": "2020/01/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31913272/", "code_link": "https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "31913272", "task": null, "abstract": "Center-involved diabetic macular edema (ci-DME) is a major cause of vision loss. Although the gold standard for diagnosis involves 3D imaging, 2D imaging by fundus photography is usually used in screening settings, resulting in high false-positive and false-negative calls. To address this, we train a deep learning model to predict ci-DME from fundus photographs, with an ROC-AUC of 0.89 (95% CI: 0.87-0.91), corresponding\u00a0to 85% sensitivity at 80% specificity. In comparison, retinal specialists have similar sensitivities (82-85%), but only half the specificity (45-50%, p\u2009<\u20090.001). Our model can also detect the presence of intraretinal fluid (AUC: 0.81; 95% CI: 0.81-0.86) and subretinal fluid (AUC 0.88; 95% CI: 0.85-0.91). Using deep learning to make predictions via simple 2D images without sophisticated 3D-imaging equipment\u00a0and with better than specialist performance, has broad relevance to many other applications in medical imaging.", "keywords": ["Center-involved diabetic macular", "diabetic macular edema", "Center-involved diabetic", "macular edema", "vision loss", "diabetic macular", "AUC", "Center-involved", "edema", "loss", "deep learning", "imaging", "diabetic", "macular", "major", "vision", "diagnosis involves", "screening settings", "resulting in high", "false-negative calls"], "paper_title": "Predicting optical coherence tomography-derived diabetic macular edema grades from fundus photographs using deep learning.", "last_updated": "2023/02/04"}, {"id": "0034033552", "domain": "Glaucoma (unspecified)", "model_name": "Luo et al.", "publication_date": "2021/09/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34033552/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34033552", "task": "IWqQC1koJA", "abstract": "With the popularization of computer-aided diagnosis (CAD) technologies, more and more deep learning methods are developed to facilitate the detection of ophthalmic diseases. In this article, the deep learning-based detections for some common eye diseases, including cataract, glaucoma, and age-related macular degeneration (AMD), are analyzed. Generally speaking, morphological change in retina reveals the presence of eye disease. Then, while using some existing deep learning methods to achieve this analysis task, the satisfactory performance may not be given, since fundus images usually suffer from the impact of data imbalance and outliers. It is, therefore, expected that with the exploration of effective and robust deep learning algorithms, the detection performance could be further improved. Here, we propose a deep learning model combined with a novel mixture loss function to automatically detect eye diseases, through the analysis of retinal fundus color images. Specifically, given the good generalization and robustness of focal loss and correntropy-induced loss functions in addressing complex dataset with class imbalance and outliers, we present a mixture of those two losses in deep neural network model to improve the recognition performance of classifier for biomedical data. The proposed model is evaluated on a real-life ophthalmic dataset. Meanwhile, the performance of deep learning model with our proposed loss function is compared with the baseline models, while adopting accuracy, sensitivity, specificity, Kappa, and area under the receiver operating characteristic curve (AUC) as the evaluation metrics. The experimental results verify the effectiveness and robustness of the proposed algorithm.", "keywords": ["deep learning", "deep learning methods", "deep learning model", "computer-aided diagnosis", "eye diseases", "popularization of computer-aided", "developed to facilitate", "common eye diseases", "deep", "deep learning-based detections", "learning methods", "learning", "detect eye diseases", "deep learning algorithms", "CAD", "diseases", "learning model", "existing deep learning", "robust deep learning", "facilitate the detection"], "paper_title": "Ophthalmic Disease Detection via Deep Learning With a Novel Mixture Loss Function.", "last_updated": "2023/02/04"}, {"id": "0031800463", "domain": "Macular degeneration", "model_name": "Told et al.", "publication_date": "2021/09/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31800463/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31800463", "task": null, "abstract": "To compare area measurements between swept source optical coherence tomography angiography (SSOCTA), fluorescein angiography (FA), and indocyanine green angiography (ICGA) after applying a novel deep-learning-assisted algorithm for accurate image registration. We applied an algorithm for the segmentation of blood vessels in FA, ICGA, and SSOCTA images of 24 eyes with treatment-naive neovascular age-related macular degeneration. We trained a model based on U-Net and Mask R-CNN for each imaging modality using vessel annotations and junctions to estimate scaling, translation, and rotation. For fine-tuning of the registration, vessels and the elastix framework were used. Area, perimeter, and circularity measurements were performed manually using ImageJ. Choroidal neovascularization lesion size, perimeter, and circularity delineations showed no significant difference between SSOCTA and ICGA (all P > 0.05). Choroidal neovascularization area showed excellent correlation between SSOCTA and ICGA (r = 0.992) and a Bland-Altman bias of -0.10 \u00b1 0.24 mm. There was no significant difference in foveal avascular zone size between SSOCTA and FA (P = 0.96) and an extremely small bias of 0.0004 \u00b1 0.04 mm and excellent correlation (r = 0.933). Foveal avascular zone perimeter was not significantly different, but foveal avascular zone circularity was significantly different (P = 0.047), indicating that some small cavities or gaps may be missed leading to higher circularity values representing a more round-shaped foveal avascular zone in FA. We found no statistically significant differences between SSOCTA and FA and ICGA area measurements in patients with treatment-naive neovascular age-related macular degeneration after applying a deep-learning-assisted approach for image registration. These findings encourage a paradigm shift to using SSOCTA as a first-line diagnostic tool in neovascular age-related macular degeneration.", "keywords": ["coherence tomography angiography", "indocyanine green angiography", "swept source optical", "source optical coherence", "optical coherence tomography", "fluorescein angiography", "tomography angiography", "green angiography", "accurate image registration", "SSOCTA", "angiography", "foveal avascular zone", "swept source", "source optical", "optical coherence", "coherence tomography", "indocyanine green", "ICGA", "algorithm for accurate", "image registration"], "paper_title": "SWEPT SOURCE OPTICAL COHERENCE TOMOGRAPHY ANGIOGRAPHY, FLUORESCEIN ANGIOGRAPHY, AND INDOCYANINE GREEN ANGIOGRAPHY COMPARISONS REVISITED: Using a Novel Deep-Learning-Assisted Approach for Image Registration.", "last_updated": "2023/02/04"}, {"id": "0035001904", "domain": "Disc hemorrhage", "model_name": "Albahli et al.", "publication_date": "2022/03/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35001904/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "35001904", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy is an eye deficiency that affects retina as a result of the patient having diabetes mellitus caused by high sugar levels, which may eventually lead to macular edema. The objective of this study is to design and compare several deep learning models that detect severity of diabetic retinopathy, determine risk of leading to macular edema, and segment different types of disease patterns using retina images. Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset was used for disease grading and segmentation. Since images of the dataset have different brightness and contrast, we employed three techniques for generating processed images from the original images, which include brightness, color and, contrast (BCC) enhancing, color jitters (CJ), and contrast limited adaptive histogram equalization (CLAHE). After image preporcessing, we used pre-trained ResNet50, VGG16, and VGG19 models on these different preprocessed images both for determining the severity of the retinopathy and also the chances of macular edema. UNet was also applied to segment different types of diseases. To train and test these models, image dataset was divided into training, testing, and validation data at 70%, 20%, and 10% ratios, respectively. During model training, data augmentation method was also applied to increase the number of training images. Study results show that for detecting the severity of retinopathy and macular edema, ResNet50 showed the best accuracy using BCC and original images with an accuracy of 60.2% and 82.5%, respectively, on validation dataset. In segmenting different types of diseases, UNet yielded the highest testing accuracy of 65.22% and 91.09% for microaneurysms and hard exudates using BCC images, 84.83% for optic disc using CJ images, 59.35% and 89.69% for hemorrhages and soft exudates using CLAHE images, respectively. Thus, image preprocessing can play an important role to improve efficacy and performance of deep learning models.", "keywords": ["high sugar levels", "diabetes mellitus caused", "Diabetic retinopathy", "macular edema", "Diabetic Retinopathy Image", "Indian Diabetic Retinopathy", "images", "sugar levels", "Retinopathy Image Dataset", "eye deficiency", "deficiency that affects", "patient having diabetes", "diabetes mellitus", "mellitus caused", "caused by high", "high sugar", "eventually lead", "retinopathy", "edema", "affects retina"], "paper_title": "Automated detection of diabetic retinopathy using custom convolutional neural network.", "last_updated": "2023/02/04"}, {"id": "0035644472", "domain": "Macular degeneration", "model_name": "Zhao et al.", "publication_date": "2022/05/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35644472/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35644472", "task": "aipbNdPTIt", "abstract": "To train a deep learning (DL) algorithm to perform fully automated semantic segmentation of multiple autofluorescence lesion types in Stargardt disease. Cross-sectional study with retrospective imaging data. The study included 193 images from 193 eyes of 97 patients with Stargardt disease. Fundus autofluorescence images obtained from patient visits between 2013 and 2020 were annotated with ground-truth labels. Model training and evaluation were performed using fivefold cross-validation. Dice similarity coefficients, intraclass correlation coefficients, and Bland-Altman analyses comparing algorithm-predicted and grader-labeled segmentations. The overall Dice similarity coefficient across all lesion classes was 0.78 (95% confidence interval [CI], 0.69-0.86). Dice coefficients were 0.90 (95% CI, 0.85-0.94) for areas of definitely decreased autofluorescence (DDAF), 0.55 (95% CI, 0.35-0.76) for areas of questionably decreased autofluorescence (QDAF), and 0.88 (95% CI, 0.73-1.00) for areas of abnormal background autofluorescence (ABAF). Intraclass correlation coefficients comparing the ground-truth and automated methods were 0.997 (95% CI, 0.996-0.998) for DDAF, 0.863 (95% CI, 0.823-0.895) for QDAF, and 0.974 (95% CI, 0.966-0.980) for ABAF. A DL algorithm performed accurate segmentation of autofluorescence lesions in Stargardt disease, demonstrating the feasibility of fully automated segmentation as an alternative to manual or semiautomated labeling methods.", "keywords": ["Stargardt disease", "deep learning", "train a deep", "Stargardt", "types in Stargardt", "autofluorescence", "disease", "multiple autofluorescence lesion", "autofluorescence lesion types", "coefficients", "multiple autofluorescence", "perform fully", "semantic segmentation", "perform fully automated", "fully automated semantic", "lesion types", "automated semantic segmentation", "Dice", "areas", "automated semantic"], "paper_title": "Automated Segmentation of Autofluorescence Lesions in Stargardt Disease.", "last_updated": "2023/02/04"}, {"id": "0035948209", "domain": "Macular degeneration", "model_name": "Vogl et al.", "publication_date": "2022/08/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35948209/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35948209", "task": "0GPcU42tzV", "abstract": "To identify disease activity and effects of intravitreal pegcetacoplan treatment on the topographic progression of geographic atrophy (GA) secondary to age-related macular degeneration quantified in spectral-domain OCT (SD-OCT) by automated deep learning assessment. Retrospective analysis of a phase II clinical trial study evaluating pegcetacoplan in GA patients (FILLY, NCT02503332). SD-OCT scans of 57 eyes with monthly treatment, 46 eyes with every-other-month (EOM) treatment, and 53 eyes with sham injection from baseline and 12-month follow-ups were included, in a total of 312 scans. Retinal pigment epithelium loss, photoreceptor (PR) integrity, and hyperreflective foci (HRF) were automatically segmented using validated deep learning algorithms. Local progression rate (LPR) was determined from a growth model measuring the local expansion of GA margins between baseline and 1 year. For each individual margin point, the eccentricity to the foveal center, the progression direction, mean PR thickness, and HRF concentration in the junctional zone were computed. Mean LPR in disease activity and treatment effect conditioned on these properties were estimated by spatial generalized additive mixed-effect models. LPR of GA, PR thickness, and HRF concentration in \u03bcm. A total of 31,527 local GA margin locations were analyzed. LPR was higher for areas with low eccentricity to the fovea, thinner PR layer thickness, or higher HRF concentration in the GA junctional zone. When controlling for topographic and structural risk factors, we report on average a significantly lower LPR by -28.0% (95% confidence interval [CI], -42.8 to -9.4; P = 0.0051) and -23.9% (95% CI, -40.2 to -3.0; P = 0.027) for monthly and EOM-treated eyes, respectively, compared with sham. Assessing GA progression on a topographic level is essential to capture the pathognomonic heterogeneity in individual lesion growth and therapeutic response. Pegcetacoplan-treated eyes showed a significantly slower GA lesion progression rate compared with sham, and an even slower growth rate toward the fovea. This study may help to identify patient cohorts with faster progressing lesions, in which pegcetacoplan treatment would be particularly beneficial. Automated artificial intelligence-based tools will provide reliable guidance for the management of GA in clinical practice.", "keywords": ["age-related macular degeneration", "macular degeneration quantified", "spectral-domain OCT", "intravitreal pegcetacoplan treatment", "deep learning assessment", "geographic atrophy", "secondary to age-related", "HRF concentration", "age-related macular", "macular degeneration", "degeneration quantified", "quantified in spectral-domain", "HRF", "LPR", "intravitreal pegcetacoplan", "OCT", "pegcetacoplan treatment", "learning assessment", "deep learning", "treatment"], "paper_title": "Predicting Topographic Disease Progression and Treatment Response of Pegcetacoplan in Geographic Atrophy Quantified by Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0033224635", "domain": "Diabetic retinopathy", "model_name": "Goldhagen et al.", "publication_date": "2020/06/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33224635/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33224635", "task": null, "abstract": "In the present article, we will provide an understanding and review of artificial intelligence in the subspecialty of retina and its potential applications within the specialty. Given the significant use of diagnostic imaging within retina, this subspecialty is a fitting area for the incorporation of artificial intelligence. Researchers have aimed at creating models to assist in the diagnosis and management of retinal disease as well as in the prediction of disease course and treatment response. Most of this work thus far has focused on diabetic retinopathy, age-related macular degeneration, and retinopathy of prematurity, although other retinal diseases have started to be explored as well. Artificial intelligence is well-suited to transform the practice of ophthalmology. A basic understanding of the technology is important for its effective implementation and growth.", "keywords": ["artificial intelligence", "present article", "potential applications", "review of artificial", "artificial", "intelligence", "subspecialty", "retina", "incorporation of artificial", "article", "specialty", "understanding and review", "provide an understanding", "present", "provide", "review", "potential", "applications", "retinal disease", "diagnostic imaging"], "paper_title": "Diving Deep into Deep Learning: An Update on Artificial Intelligence in Retina.", "last_updated": "2023/02/04"}, {"id": "0034038502", "domain": "Glaucoma (unspecified)", "model_name": "expanded-macoct", "publication_date": "2021/06/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34038502/", "code_link": "https://github.com/uw-biomedical-ml/expanded-macoct", "model_type": "random forest", "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "34038502", "task": null, "abstract": "Optical coherence tomography (OCT) is widely used in the management of retinal pathologies, including age-related macular degeneration (AMD), diabetic macular edema (DME), and primary open-angle glaucoma (POAG). We used machine learning techniques to understand diagnostic performance gains from expanding macular OCT B-scans compared with foveal-only OCT B-scans for these conditions. Electronic medical records were extracted to obtain 61 B-scans per eye from patients with AMD, diabetic retinopathy, or POAG. We constructed deep neural networks and random forest ensembles and generated area under the receiver operating characteristic (AUROC) and area under the precision recall (AUPR) curves. After extracting 630,000 OCT images, we achieved improved AUROC and AUPR curves when comparing the central image (one B-scan) to all images (61 B-scans). The AUROC and AUPR points of diminishing return for diagnostic accuracy for macular OCT coverage were found to be within 2.75 to 4.00 mm (14-19 B-scans), 4.25 to 4.50 mm (20-21 B-scans), and 4.50 to 6.25 mm (21-28 B-scans) for AMD, DME, and POAG, respectively. All models with >0.25 mm of coverage had statistically significantly improved AUROC/AUPR curves for all diseases (P < 0.05). Systematically expanded macular coverage models demonstrated significant differences in total macular coverage required for improved diagnostic accuracy, with the largest macular area being relevant in POAG followed by DME and then AMD. These findings support our hypothesis that the extent of macular coverage by OCT imaging in the clinical setting, for any of the three major disorders, has a measurable impact on the functionality of artificial intelligence decision support. We used machine learning techniques to improve OCT imaging standards for common retinal disease diagnoses.", "keywords": ["Optical coherence tomography", "primary open-angle glaucoma", "including age-related macular", "age-related macular degeneration", "OCT", "B-scans", "OCT B-scans", "Optical coherence", "coherence tomography", "including age-related", "open-angle glaucoma", "OCT B-scans compared", "foveal-only OCT B-scans", "diabetic macular edema", "macular OCT B-scans", "primary open-angle", "AMD", "AUROC", "AUPR", "macular"], "paper_title": "Assessing the Clinical Utility of Expanded Macular OCTs Using Machine Learning.", "last_updated": "2023/02/04"}, {"id": "0035903415", "domain": "Diabetic retinopathy", "model_name": "Lin et al.", "publication_date": "2022/07/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35903415/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35903415", "task": "IWqQC1koJA", "abstract": "<b>Purpose:</b> Retinopathy screening via digital imaging is promising for early detection and timely treatment, and tracking retinopathic abnormality over time can help to reveal the risk of disease progression. We developed an innovative physician-oriented artificial intelligence-facilitating diagnosis aid system for retinal diseases for screening multiple retinopathies and monitoring the regions of potential abnormality over time. <b>Approach:</b> Our dataset contains 4908 fundus images from 304 eyes with image-level annotations, including diabetic retinopathy, age-related macular degeneration, cellophane maculopathy, pathological myopia, and healthy control (HC). The screening model utilized a VGG-based feature extractor and multiple-binary convolutional neural network-based classifiers. Images in time series were aligned via affine transforms estimated through speeded-up robust features. Heatmaps of retinopathy were generated from the feature extractor using gradient-weighted class activation mapping++, and individual candidate retinopathy sites were identified from the heatmaps using clustering algorithm. Nested cross-validation with a train-to-test split of 80% to 20% was used to evaluate the performance of the screening model. <b>Results:</b> Our screening model achieved 99% accuracy, 93% sensitivity, and 97% specificity in discriminating between patients with retinopathy and HCs. For discriminating between types of retinopathy, our model achieved an averaged performance of 80% accuracy, 78% sensitivity, 94% specificity, 79% <i>F</i>1-score, and Cohen's kappa coefficient of 0.70. Moreover, visualization results were also shown to provide reasonable candidate sites of retinopathy. <b>Conclusions:</b> Our results demonstrated the capability of the proposed model for extracting diagnostic information of the abnormality and lesion locations, which allows clinicians to focus on patient-centered treatment and untangles the pathological plausibility hidden in deep learning models.", "keywords": ["tracking retinopathic abnormality", "digital imaging", "imaging is promising", "promising for early", "early detection", "detection and timely", "tracking retinopathic", "reveal the risk", "disease progression", "Purpose", "retinopathic abnormality", "Retinopathy", "screening model", "screening", "model", "abnormality", "time", "abnormality over time", "timely treatment", "screening multiple retinopathies"], "paper_title": "PADAr: physician-oriented artificial intelligence-facilitating diagnosis aid for retinal diseases.", "last_updated": "2023/02/04"}, {"id": "0035767116", "domain": "Diabetic retinopathy", "model_name": "Asif et al.", "publication_date": "2022/06/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35767116/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35767116", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy occurs due to damage to the blood vessels in the retina, and it is a major health problem in recent years that progresses slowly without recognizable symptoms. Optical coherence tomography (OCT) is a popular and widely used noninvasive imaging modality for the diagnosis of diabetic retinopathy. Accurate and early diagnosis of this disease using OCT images is crucial for the prevention of blindness. In recent years, several deep learning methods have been very successful in automating the process of detecting retinal diseases from OCT images. However, most methods face reliability and interpretability issues. In this study, we propose a deep residual network for the classification of four classes of retinal diseases, namely diabetic macular edema (DME), choroidal neovascularization (CNV), DRUSEN and NORMAL in OCT images. The proposed model is based on the popular architecture called ResNet50, which eliminates the vanishing gradient problem and is pre-trained on large dataset such as ImageNet and trained end-to-end on the publicly available OCT image dataset. We removed the fully connected layer of ResNet50 and placed our new fully connected block on top to improve the classification accuracy and avoid overfitting in the proposed model. The proposed model was trained and evaluated using different performance metrics, including receiver operating characteristic (ROC) curve on a dataset of 84,452 OCT images with expert disease grading as DRUSEN, CNV, DME and NORMAL. The proposed model provides an improved overall classification accuracy of 99.48% with only 5 misclassifications out of 968 test samples and outperforms existing methods on the same dataset. The results show that the proposed model is well suited for the diagnosis of retinal diseases in ophthalmology clinics.", "keywords": ["retinopathy occurs due", "Diabetic retinopathy occurs", "OCT images", "major health problem", "proposed model", "OCT", "Diabetic retinopathy", "recognizable symptoms", "occurs due", "due to damage", "blood vessels", "major health", "progresses slowly", "slowly without recognizable", "OCT image dataset", "retinopathy occurs", "retinal diseases", "proposed", "OCT image", "recent years"], "paper_title": "Deep Residual Network for Diagnosis of Retinal Diseases Using Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0036122302", "domain": "Diabetic retinopathy", "model_name": "Zhao et al.", "publication_date": "2022/10/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36122302/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36122302", "task": null, "abstract": "The automatic generation algorithm of optical coherence tomography (OCT) images based on generative adversarial networks (GAN) can generate a large number of simulation images by a relatively small number of real images, which can effectively improve the classification performance. We proposed\u00a0an automatic generation algorithm for retinal OCT images based on GAN to alleviate the problem of insufficient images with high quality in deep learning, and put the diagnosis algorithm toward clinical application. We designed a generation network based on GAN and trained the network with a data set constructed by 2014_BOE_Srinivasan and OCT2017 to acquire three models. Then, we generated a large number of images by the three models to augment age-related macular degeneration (AMD), diabetic macular edema (DME), and normal images. We evaluated the generated images by subjective visual observation, Fr\u00e9chet inception distance (FID) scores, and a classification experiment. Visual observation shows that the generated images have clear and similar features compared with the real images. Also, the lesion regions containing similar features in the real image and the generated image are randomly distributed in the image field of view. When the FID scores of the three types of generated images are lowest, three local optimal models are obtained for AMD, DME, and normal images, indicating the generated images have high quality and diversity. Moreover, the classification experiment results show that the model performance trained with the mixed images is better than that of the model trained with real images, in which the accuracy, sensitivity, and specificity are improved by 5.56%, 8.89%, and 2.22%. In addition, compared with the generation method based on variational auto-encoder (VAE), the method improved the accuracy, sensitivity, and specificity by 1.97%, 2.97%, and 0.99%, for the same test set. The results show that our method can augment the three kinds of OCT images, not only effectively alleviating the problem of insufficient images with high quality but also improving the diagnosis performance.", "keywords": ["OCT images based", "automatic generation algorithm", "optical coherence tomography", "images", "OCT images", "generative adversarial networks", "retinal OCT images", "generated images", "based on GAN", "generation network based", "images based", "generation algorithm", "real images", "automatic generation", "OCT", "GAN", "coherence tomography", "generated", "optical coherence", "generative adversarial"], "paper_title": "Automatic generation of retinal optical coherence tomography images based on generative adversarial networks.", "last_updated": "2023/02/04"}, {"id": "0032789526", "domain": "Macular degeneration", "model_name": "Pfau et al.", "publication_date": "2021/02/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32789526/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32789526", "task": "0GPcU42tzV", "abstract": "Sensitive outcome measures for disease progression are needed for treatment trials in geographic atrophy (GA) secondary to age-related macular degeneration (AMD). To quantify photoreceptor degeneration outside regions of GA in eyes with nonexudative AMD, to evaluate its association with future GA progression, and to characterize its spatio-temporal progression. Monocenter cohort study (Directional Spread in Geographic Atrophy [NCT02051998]) and analysis of data from a normative data study at a tertiary referral center. One hundred fifty-eight eyes of 89 patients with a mean (SD) age of 77.7 (7.1) years, median area of GA of 8.87 mm2 (IQR, 4.09-15.60), and median follow-up of 1.1 years (IQR, 0.52-1.7 years), as well as 93 normal eyes from 93 participants. Longitudinal spectral-domain optical coherence tomography (SD-OCT) volume scans (121 B-scans across 30\u00b0\u2009\u00d7\u200925\u00b0) were segmented with a deep-learning pipeline and standardized in a pointwise manner with age-adjusted normal data (z scores). Outer nuclear layer (ONL), photoreceptor inner segment (IS), and outer segment (OS) thickness were quantified along evenly spaced contour lines surrounding GA lesions. Linear mixed models were applied to assess the association between photoreceptor-related imaging features and GA progression rates and characterize the pattern of photoreceptor degeneration over time. Association of ONL thinning with follow-up time (after adjusting for age, retinal topography [z score], and distance to the GA boundary). The study included 158 eyes of 89 patients (51 women and 38 men) with a mean (SD) age of 77.7 (7.1) years. The fully automated B-scan segmentation was accurate (dice coefficient, 0.82; 95% CI, 0.80-0.85; compared with manual markings) and revealed a marked interpatient variability in photoreceptor degeneration. The ellipsoid zone (EZ) loss-to-GA boundary distance and OS thickness were prognostic for future progression rates. Outer nuclear layer and IS thinning over time was significant even when adjusting for age and proximity to the GA boundary (estimates of -0.16 \u03bcm/y; 95% CI, -0.30 to -0.02; and -0.17 \u03bcm/y; 95% CI, -0.26 to -0.09). Distinct and progressive alterations of photoreceptor laminae (exceeding GA spatially) were detectable and quantifiable. The degree of photoreceptor degeneration outside of regions of retinal pigment epithelium atrophy varied markedly between eyes and was associated with future GA progression. Macula-wide photoreceptor laminae thinning represents a potential candidate end point to monitor treatment effects beyond mere GA lesion size progression.", "keywords": ["Sensitive outcome measures", "age-related macular degeneration", "geographic atrophy", "Sensitive outcome", "secondary to age-related", "photoreceptor degeneration", "outcome measures", "measures for disease", "age-related macular", "trials in geographic", "photoreceptor", "nonexudative AMD", "AMD", "progression", "Spread in Geographic", "degeneration", "years", "Directional Spread", "macular degeneration", "disease progression"], "paper_title": "Progression of Photoreceptor Degeneration in Geographic Atrophy Secondary to Age-related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0035259614", "domain": "Macular degeneration", "model_name": "Sotoudeh-Paima et al.", "publication_date": "2022/03/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35259614/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35259614", "task": "IWqQC1koJA", "abstract": "Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries, especially in people over 60 years of age. The workload of specialists and the healthcare system in this field has increased in recent years mainly due to three reasons: 1) increased use of retinal optical coherence tomography (OCT) imaging technique, 2) prevalence of population aging worldwide, and 3) chronic nature of AMD. Recent advancements in the field of deep learning have provided a unique opportunity for the development of fully automated diagnosis frameworks. Considering the presence of AMD-related retinal pathologies in varying sizes in OCT images, our objective was to propose a multi-scale convolutional neural network (CNN) that can capture inter-scale variations and improve performance using a feature fusion strategy across convolutional blocks. Our proposed method introduces a multi-scale CNN based on the feature pyramid network (FPN) structure. This method is used for the reliable diagnosis of normal and two common clinical characteristics of dry and wet AMD, namely drusen and choroidal neovascularization (CNV). The proposed method is evaluated on the national dataset gathered at Hospital (NEH) for this study, consisting of 12649 retinal OCT images from 441 patients, and the UCSD public dataset, consisting of 108312 OCT images from 4686 patients. Experimental results show the superior performance of our proposed multi-scale structure over several well-known OCT classification frameworks. This feature combination strategy has proved to be effective on all tested backbone models, with improvements ranging from 0.4% to 3.3%. In addition, gradual learning has proved to be effective in improving performance in two consecutive stages. In the first stage, the performance was boosted from 87.2%\u00b12.5% to 92.0%\u00b11.6% using pre-trained ImageNet weights. In the second stage, another performance boost from 92.0%\u00b11.6% to 93.4%\u00b11.4% was observed as a result of fine-tuning the previous model on the UCSD dataset. Lastly, generating heatmaps provided additional proof for the effectiveness of our multi-scale structure, enabling the detection of retinal pathologies appearing in different sizes. The promising quantitative results of the proposed architecture, along with qualitative evaluations through generating heatmaps, prove the suitability of the proposed method to be used as a screening tool in healthcare centers assisting ophthalmologists in making better diagnostic decisions.", "keywords": ["Age-related macular degeneration", "OCT images", "Age-related macular", "macular degeneration", "developed countries", "retinal OCT images", "blindness in developed", "AMD", "OCT", "proposed method", "proposed", "retinal OCT", "proposed multi-scale structure", "wet AMD", "performance", "retinal", "recent years", "method", "years of age", "multi-scale CNN based"], "paper_title": "Multi-scale convolutional neural network for automated AMD classification using retinal OCT images.", "last_updated": "2023/02/04"}, {"id": "0035131605", "domain": "Macular degeneration", "model_name": "Pham et al.", "publication_date": "2022/01/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35131605/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35131605", "task": null, "abstract": "Age-related macular degeneration (AMD) is one of the most common diseases that can lead to blindness worldwide. Recently, various fundus image analyzing studies are done using deep learning methods to classify fundus images to aid diagnosis and monitor AMD disease progression. But until now, to the best of our knowledge, no attempt was made to generate future synthesized fundus images that can predict AMD progression. In this paper, we developed a deep learning model using fundus images for AMD patients with different time elapses to generate synthetic future fundus images. We exploit generative adversarial networks (GANs) with additional drusen masks to maintain the pathological information. The dataset included 8196 fundus images from 1263 AMD patients. A proposed GAN-based model, called Multi-Modal GAN (MuMo-GAN), was trained to generate synthetic predicted-future fundus images. The proposed deep learning model indicates that the additional drusen masks can help to learn the AMD progression. Our model can generate future fundus images with appropriate pathological features. The drusen development over time is depicted well. Both qualitative and quantitative experiments show that our model is more efficient to monitor the AMD disease as compared to other studies. This study could help individualized risk prediction for AMD patients. Compared to existing methods, the experimental results show a significant improvement in terms of tracking the AMD stage in both image-level and pixel-level.", "keywords": ["Age-related macular degeneration", "AMD", "fundus images", "AMD disease progression", "AMD progression", "AMD disease", "AMD patients", "fundus", "Age-related macular", "macular degeneration", "blindness worldwide", "images", "future fundus images", "lead to blindness", "predict AMD progression", "monitor AMD disease", "deep learning", "common diseases", "deep learning model", "model"], "paper_title": "Generating future fundus images for early age-related macular degeneration based on generative adversarial networks.", "last_updated": "2023/02/04"}, {"id": "0035322564", "domain": "Macular degeneration", "model_name": "Potapenko et al.", "publication_date": "2022/03/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35322564/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35322564", "task": null, "abstract": "In this study, we investigate the potential of a novel artificial intelligence-based system for autonomous follow-up of patients treated for neovascular age-related macular degeneration (AMD). A temporal deep learning model was trained on a data set of 84\u2009489 optical coherence tomography scans from AMD patients to recognize disease activity, and its performance was compared with a published non-temporal model trained on the same data (Acta Ophthalmol, 2021). An autonomous follow-up system was created by augmenting the AI model with deterministic logic to suggest treatment according to the observe-and-plan regimen. To validate the AI-based system, a data set comprising clinical decisions and imaging data from 200 follow-up consultations was collected prospectively. In each case, both the autonomous AI decision and original clinical decision were compared with an expert panel consensus. The temporal AI model proved superior at detecting disease activity compared with the model without temporal input (area under the curve 0.900 (95% CI 0.894-0.906) and 0.857 (95% CI 0.846-0.867) respectively). The AI-based follow-up system could make an autonomous decision in 73% of the cases, 91.8% of which were in agreement with expert consensus. This was on par with the 87.7% agreement rate between decisions made in the clinic and expert consensus (p\u2009=\u20090.33). The proposed autonomous follow-up system was shown to be safe and compliant with expert consensus on par with clinical practice. The system could in the future ease the pressure on public ophthalmology services from an increasing number of AMD patients.", "keywords": ["age-related macular degeneration", "neovascular age-related macular", "artificial intelligence-based system", "AMD patients", "autonomous follow-up system", "macular degeneration", "follow-up system", "Acta Ophthalmol", "autonomous follow-up", "investigate the potential", "artificial intelligence-based", "treated for neovascular", "neovascular age-related", "age-related macular", "AMD", "system", "patients treated", "follow-up", "model", "autonomous"], "paper_title": "Automated artificial intelligence-based system for clinical follow-up of patients with age-related macular degeneration.", "last_updated": "2023/02/04"}, {"id": "0035660417", "domain": "Macular degeneration", "model_name": "Agr\u00f3n et al.", "publication_date": "2022/05/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35660417/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35660417", "task": "0GPcU42tzV", "abstract": "To analyze reticular pseudodrusen (RPD) as an independent risk factor for progression to late age-related macular degeneration (AMD), alongside traditional macular risk factors (soft drusen and pigmentary abnormalities) considered simultaneously. Post hoc analysis of 2 clinical trial cohorts: Age-Related Eye Disease Study (AREDS) and AREDS2. Eyes with no late AMD at baseline in AREDS (6959 eyes, 3780 participants) and AREDS2 (3355 eyes, 2056 participants). Color fundus photographs (CFPs) from annual visits were graded for soft drusen, pigmentary abnormalities, and late AMD. Presence of RPD was from grading of fundus autofluorescence images (AREDS2) and deep learning grading of CFPs (AREDS). Proportional hazards regression analyses were performed, considering AREDS AMD severity scales (modified simplified severity scale [person] and 9-step scale [eye]) and RPD presence simultaneously. Progression to late AMD, geographic atrophy (GA), and neovascular AMD. In AREDS, for late AMD analyses by person, in a model considering the simplified severity scale simultaneously, RPD presence was associated with a higher risk of progression: hazard ratio (HR), 2.15 (95% confidence interval [CI], 1.75-2.64). However, the risk associated with RPD presence differed at different severity scale levels: HR, 3.23 (95% CI, 1.60-6.51), HR, 3.81 (95% CI, 2.38-6.10), HR, 2.28 (95% CI, 1.59-3.27), and HR, 1.64 (95% CI, 1.20-2.24), at levels 0-1, 2, 3, and 4, respectively. Considering the 9-step scale (by eye), RPD presence was associated with higher risk: HR, 2.54 (95% CI, 2.07-3.13). The HRs were 5.11 (95% CI, 3.93-6.66) at levels 1-6 and 1.78 (95% CI, 1.43-2.22) at levels 7 and 8. In AREDS2, by person, RPD presence was not associated with higher risk: HR, 1.18 (95% CI, 0.90-1.56); by eye, it was HR, 1.57 (95% CI, 1.31-1.89). In both cohorts, RPD presence carried a higher risk for GA than neovascular AMD. Reticular pseudodrusen represent an important risk factor for progression to late AMD, particularly GA. However, the added risk varies markedly by severity level, with highly increased risk at lower/moderate levels and less increased risk at higher levels. Reticular pseudodrusen status should be included in updated AMD classification systems, risk calculators, and clinical trials.", "keywords": ["alongside traditional macular", "RPD presence", "age-related macular degeneration", "late AMD", "AMD", "Eye Disease Study", "RPD", "traditional macular risk", "late age-related macular", "risk", "Disease Study", "AREDS AMD severity", "AREDS", "Age-Related Eye Disease", "macular degeneration", "traditional macular", "Presence", "late", "AREDS AMD", "alongside traditional"], "paper_title": "Reticular Pseudodrusen: The Third Macular Risk Feature for Progression to Late Age-Related Macular Degeneration: Age-Related Eye Disease Study 2 Report 30.", "last_updated": "2023/02/04"}, {"id": "0034844251", "domain": "Macular degeneration", "model_name": "Hess et al.", "publication_date": "2021/11/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34844251/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34844251", "task": null, "abstract": "The aim of the study was to quantify choriocapillaris (CC) flow alterations in early Sorsby fundus dystrophy (SFD) and to investigate the relationship of the CC flow deficits with the choroidal and outer retinal microstructure. In this prospective case-control study, 18 eyes of 11 patients with early SFD and 31 eyes of 31 controls without ocular pathology underwent multimodal imaging, including spectral-domain optical coherence tomography (OCT), followed by deep-learning-based layer segmentation. OCT angiography (OCTA) was performed to quantify CC flow signal deficits (FDs). Differences in CC FD density between SFD patients and controls were determined, and the relationships with choroidal thickness, retinal pigment epithelium-drusen complex (RPEDC) thickness and outer retinal layer thicknesses were analyzed using mixed-model analysis. SFD patients exhibited a significantly greater CC FD density than controls (estimate [95% CI]: +20.0%FD [13.3; 26.7], p < 0.001 for SFD patients), even when adjusted for age. Square-root transformed choroidal thickness was a structural OCT surrogate of the CC FD density (-2.1%FD per \u221a\u00b5m, p < 0.001), whereas RPEDC thickness was not informative regarding CC FD (p = 0.061). The CC FD density was associated with an altered microstructure of the overlying photoreceptors (outer segments, inner segments, and outer nuclear layer thinning of -0.19 \u03bcm, -0.08 \u03bcm and -0.30 \u03bcm per %FD, respectively, all p < 0.001). Patients with early SFD exhibit pronounced abnormalities of CC flow signal on OCTA, which are not limited to areas of sub-RPE deposits seen on OCT imaging. Thus, analysis of the CC flow may enable clinical trials at earlier disease stages in SFD.", "keywords": ["Sorsby fundus dystrophy", "early Sorsby fundus", "Sorsby fundus", "early Sorsby", "SFD", "SFD patients", "early SFD", "fundus dystrophy", "OCT", "quantify choriocapillaris", "Sorsby", "patients", "early SFD exhibit", "flow", "outer retinal", "flow alterations", "SFD patients exhibited", "outer", "investigate the relationship", "early"], "paper_title": "Choriocapillaris Flow Signal Impairment in Sorsby Fundus Dystrophy.", "last_updated": "2023/02/04"}, {"id": "0035704327", "domain": "Macular degeneration", "model_name": "Tang et al.", "publication_date": "2022/06/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35704327/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35704327", "task": "aipbNdPTIt", "abstract": "To develop deep learning models based on color fundus photographs that can automatically grade myopic maculopathy, diagnose pathologic myopia, and identify and segment myopia-related lesions. Photographs were graded and annotated by four ophthalmologists and were then divided into a high-consistency subgroup or a low-consistency subgroup according to the consistency between the results of the graders. ResNet-50 network was used to develop the classification model, and DeepLabv3+ network was used to develop the segmentation model for lesion identification. The two models were then combined to develop the classification-and-segmentation-based co-decision model. This study included 1395 color fundus photographs from 895 patients. The grading accuracy of the co-decision model was 0.9370, and the quadratic-weighted \u03ba coefficient was 0.9651; the co-decision model achieved an area under the receiver operating characteristic curve of 0.9980 in diagnosing pathologic myopia. The photograph-level F1 values of the segmentation model identifying optic disc, peripapillary atrophy, diffuse atrophy, patchy atrophy, and macular atrophy were all >0.95; the pixel-level F1 values for segmenting optic disc and peripapillary atrophy were both >0.9; the pixel-level F1 values for segmenting diffuse atrophy, patchy atrophy, and macular atrophy were all >0.8; and the photograph-level recall/sensitivity for detecting lacquer cracks was 0.9230. The models could accurately and automatically grade myopic maculopathy, diagnose pathologic myopia, and identify and monitor progression of the lesions. The models can potentially help with the diagnosis, screening, and follow-up for pathologic myopic in clinical practice.", "keywords": ["develop deep learning", "deep learning models", "learning models based", "segment myopia-related lesions", "color fundus photographs", "atrophy", "deep learning", "segment myopia-related", "diagnose pathologic myopia", "co-decision model", "fundus photographs", "develop deep", "model", "pathologic myopia", "develop", "learning models", "models based", "myopia-related lesions", "grade myopic maculopathy", "photographs"], "paper_title": "An Artificial-Intelligence-Based Automated Grading and Lesions Segmentation System for Myopic Maculopathy Based on Color Fundus Photographs.", "last_updated": "2023/02/04"}, {"id": "0032974088", "domain": "Macular degeneration", "model_name": "Kawczynski et al.", "publication_date": "2020/09/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32974088/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32974088", "task": null, "abstract": "To develop deep learning (DL) models to predict best-corrected visual acuity (BCVA) from optical coherence tomography (OCT) images from patients with neovascular age-related macular degeneration (nAMD). Retrospective analysis of OCT images and associated BCVA measurements from the phase\u00a03 HARBOR trial (NCT00891735). DL regression models were developed to predict BCVA at the concurrent visit and 12 months from baseline using OCT images. Binary classification models were developed to predict BCVA of Snellen equivalent of <20/40, <20/60, and \u226420/200 at the concurrent visit and 12 months from baseline. The regression model to predict BCVA at the concurrent visit had <i>R</i><sup>2</sup> = 0.67 (root-mean-square error [RMSE] = 8.60) in study eyes and <i>R</i><sup>2</sup> = 0.84 (RMSE = 9.01) in fellow eyes. The best classification model to predict BCVA at the concurrent visit had an area under the receiver operating characteristic curve (AUC) of 0.92 in study eyes and 0.98 in fellow eyes. The regression model to predict BCVA at month\u00a012 using baseline OCT had <i>R</i><sup>2</sup> = 0.33 (RMSE = 14.16) in study eyes and <i>R</i><sup>2</sup> = 0.75 (RMSE = 11.27) in fellow eyes. The best classification model to predict BCVA at month\u00a012 had AUC = 0.84 in study eyes and AUC = 0.96 in fellow eyes. DL shows promise in predicting BCVA from OCTs in nAMD. Further research should elucidate the utility of models in clinical settings. DL models predicting BCVA could be used to enhance understanding of structure-function relationships and develop more efficient clinical trials.", "keywords": ["predict BCVA", "BCVA", "best-corrected visual acuity", "optical coherence tomography", "age-related macular degeneration", "neovascular age-related macular", "predict best-corrected visual", "predict", "eyes", "study eyes", "fellow eyes", "concurrent visit", "RMSE", "OCT images", "develop deep learning", "OCT", "model to predict", "deep learning", "visual acuity", "coherence tomography"], "paper_title": "Development of Deep Learning Models to Predict Best-Corrected Visual Acuity from Optical Coherence Tomography.", "last_updated": "2023/02/04"}, {"id": "0034400662", "domain": "Macular degeneration", "model_name": "Rauf et al.", "publication_date": "2021/08/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34400662/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34400662", "task": "IWqQC1koJA", "abstract": "Pathological myopia is a severe case of myopia, i.e., nearsightedness. Pathological myopia is also known as degenerative myopia because it ultimately leads to blindness. In pathological myopia, certain myopia-specific pathologies occur at the eye's posterior i.e., Foster-Fuchs's spot, Cystoid degeneration, Liquefaction, Macular degeneration, Vitreous opacities, Weiss's reflex, Posterior staphyloma, etc. This research is aimed at developing a machine learning (ML) approach for the automatic detection of pathological myopia based on fundus images. A deep learning technique of convolutional neural network (CNN) is employed for this purpose. A CNN model is developed in Spyder. The fundus images are first preprocessed. The preprocessed images are then fed to the designed CNN model. The CNN model automatically extracts the features from the input images and classifies the images i.e., normal image or pathological myopia. The best performing CNN model achieved an AUC score of 0.9845. The best validation loss obtained is 0.1457. The results show that the model can be successfully employed to detect pathological myopia from the fundus images.", "keywords": ["Pathological myopia", "CNN model", "myopia", "severe case", "CNN", "Pathological", "images", "fundus images", "model", "pathological myopia based", "detect pathological myopia", "Cystoid degeneration", "Macular degeneration", "fundus", "posterior", "designed CNN model", "CNN model automatically", "Vitreous opacities", "Weiss reflex", "performing CNN model"], "paper_title": "Automatic detection of pathological myopia using machine learning.", "last_updated": "2023/02/04"}, {"id": "0034751189", "domain": "Macular degeneration", "model_name": "Derradji et al.", "publication_date": "2021/11/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34751189/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34751189", "task": "aipbNdPTIt", "abstract": "Age-related macular degeneration (AMD) is a progressive retinal disease, causing vision loss. A more detailed characterization of its atrophic form became possible thanks to the introduction of Optical Coherence Tomography (OCT). However, manual atrophy quantification in 3D retinal scans is a tedious task and prevents taking full advantage of the accurate retina depiction. In this study we developed a fully automated algorithm segmenting Retinal Pigment Epithelial and Outer Retinal Atrophy (RORA) in dry AMD on macular OCT. 62 SD-OCT scans from eyes with atrophic AMD (57 patients) were collected and split into train and test sets. The training set was used to develop a Convolutional Neural Network (CNN). The performance of the algorithm was established by cross validation and comparison to the test set with ground-truth annotated by two graders. Additionally, the effect of using retinal layer segmentation during training was investigated. The algorithm achieved mean Dice scores of 0.881 and 0.844, sensitivity of 0.850 and 0.915 and precision of 0.928 and 0.799 in comparison with Expert 1 and Expert 2, respectively. Using retinal layer segmentation improved the model performance. The proposed model identified RORA with performance matching human experts. It has a potential to rapidly identify atrophy with high consistency.", "keywords": ["causing vision loss", "Optical Coherence Tomography", "progressive retinal disease", "Age-related macular degeneration", "causing vision", "vision loss", "Coherence Tomography", "Retinal Pigment Epithelial", "Optical Coherence", "Outer Retinal Atrophy", "retinal", "retinal disease", "progressive retinal", "Convolutional Neural Network", "Age-related macular", "macular degeneration", "retinal layer segmentation", "AMD", "macular OCT", "segmenting Retinal Pigment"], "paper_title": "Fully-automated atrophy segmentation in dry age-related macular degeneration in optical coherence tomography.", "last_updated": "2023/02/04"}, {"id": "0034922038", "domain": "Macular degeneration", "model_name": "Riedl et al.", "publication_date": "2021/12/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34922038/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34922038", "task": null, "abstract": "To investigate the functional associations of intraretinal fluid (IRF) and subretinal fluid (SRF) volumes at baseline and after the loading dose as well as fluid change after the first injection with best-corrected visual acuity (BCVA) in patients with neovascular age-related macular degeneration (nAMD) who received an anti-VEGF treatment over 24 months. Post hoc analysis of a phase III, randomized, multicenter trial in which ranibizumab was administered monthly or in a pro re nata regimen (HARBOR). Study eyes of 1094 treatment-na\u00efve patients with nAMD. IRF and SRF volumes were segmented automatically on monthly spectral domain OCT images. Fluid volumes and changes thereof were included as covariates into longitudinal mixed-effects models, which modeled BCVA trajectories. BCVA estimates corresponding to baseline, follow-up, and persistent IRF/SRF volumes after the loading dose; BCVA estimates of change in fluid volumes after the first injection; and marginal and conditional R<sup>2</sup>. Analysis of 22 494 volumetric scans revealed that foveal IRF consistently shows a negative correlation with BCVA at baseline and subsequent visits (-3.23 and\u00a0-4.32 letters/100 nL, respectively). After the first injection, BCVA increased by\u00a0+2.13 letters/100 nL decrease in foveal IRF. Persistent IRF was associated with lower baseline BCVA and less improvement. Foveal SRF correlated with better BCVA at baseline and subsequent visits (+6.52 and\u00a0+1.42 letters/100 nL, respectively). After the first injection, SRF decrease was associated with significant vision gain (+5.88 letters/100 nL). Foveal fluid correlated more with BCVA than parafoveal IRF/SRF. Although IRF consistently correlates with decreased function and recovery throughout therapy, SRF is associated with a more pronounced functional improvement. Moreover, SRF resolution provides increased benefit. Fluid-function correlation represents an essential base for the development of personalized treatment regimens, optimizing functional outcomes, and reducing treatment burden.", "keywords": ["best-corrected visual acuity", "age-related macular degeneration", "neovascular age-related macular", "BCVA", "IRF", "SRF", "visual acuity", "macular degeneration", "associations of intraretinal", "best-corrected visual", "neovascular age-related", "age-related macular", "received an anti-VEGF", "SRF volumes", "fluid", "foveal IRF", "volumes", "baseline", "letters", "persistent IRF"], "paper_title": "Impact of Intra- and Subretinal Fluid on Vision Based on Volume Quantification in the HARBOR Trial.", "last_updated": "2023/02/04"}, {"id": "0035585773", "domain": "Diabetic retinopathy", "model_name": "Saleh et al.", "publication_date": "2022/05/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35585773/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35585773", "task": "IWqQC1koJA", "abstract": "The incidence of vision impairment is rapidly increasing. Diagnosis and classifying retinal abnormalities in ophthalmological applications is a significant challenge. Using Optical Coherence Tomography (OCT), the study aims to develop a computer aided diagnosis system for detecting and classifying retinal disorders. Choroidal neovascularization, diabetic macular edema, drusen, and normal cases are the investigated groups. Both deep learning and machine learning are combined to build the system. The SqueezeNet neural network was modified to\u00a0extract features. The Support Vector Machine (SVM), K-Nearest Neighbor (K-NN), Decision Tree (DT), and Ensemble Model (EM) algorithms were used for disorder classification. The Bayesian optimization technique was also used to determine the best hyperparameters for each model. The model' performance was evaluated through nine criteria using 12,000 OCT images. The results have demonstrated accuracies of 97.39, 97.47, 96.98, and 95.25% for the SVM, K-NN, DT, and EM, respectively. When results are compared to relevant studies in terms of accuracy and\u00a0tested samples, they show superior performance. As a\u00a0result, a novel computer-aided diagnosis system for detecting and classifying retinal diseases has been developed, reducing human error while also saving time.", "keywords": ["Optical Coherence Tomography", "rapidly increasing", "classifying retinal", "incidence of vision", "vision impairment", "impairment is rapidly", "classifying retinal abnormalities", "classifying retinal disorders", "detecting and classifying", "Coherence Tomography", "diagnosis system", "Optical Coherence", "Support Vector Machine", "classifying retinal diseases", "classifying", "aided diagnosis system", "retinal", "Diagnosis", "Model", "system"], "paper_title": "Computer-aided diagnosis system for retinal disorder classification using optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0035640981", "domain": "Macular degeneration", "model_name": "DeepLUCIA", "publication_date": "2022/11/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35640981/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35640981", "task": null, "abstract": "The importance of chromatin loops in gene regulation is broadly accepted. There are mainly two approaches to predict chromatin loops: transcription factor (TF) binding-dependent approach and genomic variation-based approach. However, neither of these approaches provides an adequate understanding of gene regulation in human tissues. To address this issue, we developed a deep learning-based chromatin loop prediction model called Deep Learning-based Universal Chromatin Interaction Annotator (DeepLUCIA). Although DeepLUCIA does not use TF binding profile data which previous TF binding-dependent methods critically rely on, its prediction accuracies are comparable to those of the previous TF binding-dependent methods. More importantly, DeepLUCIA enables the tissue-specific chromatin loop predictions from tissue-specific epigenomes that cannot be handled by genomic variation-based approach. We demonstrated the utility of the DeepLUCIA by predicting several novel target genes of SNPs identified in genome-wide association studies targeting Brugada syndrome, COVID-19 severity and age-related macular degeneration. Availability and implementation DeepLUCIA is freely available at https://github.com/bcbl-kaist/DeepLUCIA. Supplementary data are available at Bioinformatics online.", "keywords": ["broadly accepted", "Chromatin Interaction Annotator", "Universal Chromatin Interaction", "chromatin", "chromatin loops", "Learning-based Universal Chromatin", "learning-based chromatin loop", "Deep Learning-based Universal", "deep learning-based", "deep learning-based chromatin", "predict chromatin loops", "gene regulation", "Interaction Annotator", "DeepLUCIA", "chromatin loop prediction", "binding-dependent", "Universal Chromatin", "Chromatin Interaction", "genomic variation-based approach", "Learning-based Universal"], "paper_title": "DeepLUCIA: predicting tissue-specific chromatin loops using Deep Learning-based Universal Chromatin Interaction Annotator.", "last_updated": "2023/02/04"}, {"id": "0032447042", "domain": "Macular degeneration", "model_name": "Keenan et al.", "publication_date": "2020/05/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32447042/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32447042", "task": "IWqQC1koJA", "abstract": "To develop deep learning models for detecting reticular pseudodrusen (RPD) using fundus autofluorescence (FAF) images or, alternatively, color fundus photographs (CFP) in the context of age-related macular degeneration (AMD). Application of deep learning models to the Age-Related Eye Disease Study 2 (AREDS2) dataset. FAF and CFP images (n = 11\u2009535) from 2450 AREDS2 participants. Gold standard labels from reading center grading of the FAF images were transferred to the corresponding CFP images. A deep learning model was trained to detect RPD in eyes with intermediate to late AMD using FAF images (FAF model). Using label transfer from FAF to CFP images, a deep learning model was trained to detect RPD from CFP (CFP model). Performance was compared with 4 ophthalmologists using a random subset from the full test set. Area under the receiver operating characteristic curve (AUC), \u03ba value, accuracy, and F1 score. The FAF model had an AUC of 0.939 (95% confidence interval [CI], 0.927-0.950), a \u03ba value of 0.718 (95% CI, 0.685-0.751), and accuracy of 0.899 (95% CI, 0.887-0.911). The CFP model showed equivalent values of 0.832 (95% CI, 0.812-0.851), 0.470 (95% CI, 0.426-0.511), and 0.809 (95% CI, 0.793-0.825), respectively. The FAF model demonstrated superior performance to 4 ophthalmologists, showing a higher \u03ba value of 0.789 (95% CI, 0.675-0.875) versus a range of 0.367 to 0.756 and higher accuracy of 0.937 (95% CI, 0.907-0.963) versus a range of 0.696 to 0.933. The CFP model demonstrated substantially superior performance to 4 ophthalmologists, showing a higher \u03ba value of 0.471 (95% CI, 0.330-0.606) versus a range of 0.105 to 0.180 and higher accuracy of 0.844 (95% CI, 0.798-0.886) versus a range of 0.717 to 0.814. Deep learning-enabled automated detection of RPD presence from FAF images achieved a high level of accuracy, equal or superior to that of ophthalmologists. Automated RPD detection using CFP achieved a lower accuracy that still surpassed that of ophthalmologists. Deep learning models can assist, and even augment, the detection of this clinically important AMD-associated lesion.", "keywords": ["color fundus photographs", "detecting reticular pseudodrusen", "Eye Disease Study", "FAF", "deep learning models", "age-related macular degeneration", "FAF images", "CFP", "FAF model", "CFP images", "CFP model", "deep learning", "Age-Related Eye Disease", "develop deep learning", "fundus autofluorescence", "color fundus", "fundus photographs", "learning models", "Disease Study", "model"], "paper_title": "Deep Learning Automated Detection of Reticular Pseudodrusen from Fundus Autofluorescence Images or Color Fundus Photographs in AREDS2.", "last_updated": "2023/02/04"}, {"id": "0036196946", "domain": "Diabetic retinopathy", "model_name": "Mohan et al.", "publication_date": "2022/08/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36196946/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36196946", "task": "IWqQC1koJA", "abstract": "Deep learning in medical image analysis has indicated increasing interest in the classification of signs of abnormalities. In this study, a new convolutional neural network (CNN) architecture (MIDNet18) Medical Image Detection Network was proposed for the classification of retinal diseases using optical coherence tomography (OCT) images. The model consists of 14 convolutional layers, seven Max Pooling layers, four dense layers, and one classification layer. A multi-class classification layer in the MIDNet18 is used to classify the OCT images into either normal or any of the three abnormal types: Choroidal Neovascularization (CNV), Drusen, and Diabetic Macular Edema (DME). The dataset consists of 83,484 training images, 41,741 validation images, and 968 test images. According to the experimental results, MIDNet18 obtains an accuracy of 98.86%, and their performances are compared with other standard CNN models; ResNet-50 (83.26%), MobileNet (93.29%) and DenseNet (92.5%). Also, MIDNet18 with a <i>p</i>-value < 0.001 has been proved to be statistically significant than other standard CNN architectures in classifying retinal diseases using OCT images.", "keywords": ["medical image analysis", "Medical Image Detection", "Image Detection Network", "Deep learning", "signs of abnormalities", "Diabetic Macular Edema", "medical image", "increasing interest", "Max Pooling layers", "Detection Network", "OCT images", "images", "classification", "OCT", "medical", "learning in medical", "Max Pooling", "standard CNN", "convolutional neural network", "Choroidal Neovascularization"], "paper_title": "Comparison of the proposed DCNN model with standard CNN architectures for retinal diseases classification.", "last_updated": "2023/02/04"}, {"id": "0031302456", "domain": "Macular degeneration", "model_name": "Qiu et al.", "publication_date": "2019/06/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31302456/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31302456", "task": "IWqQC1koJA", "abstract": "We present self-supervised iterative refinement learning (SIRL) as a pipeline to improve a type of macular optical coherence tomography (OCT) volumetric image classification algorithms. In this type of algorithms, first, two-dimensional (2D) image classification algorithms are applied to each B-scan in an OCT volume, and then B-scan level classification results are combined to obtain the classification result of the volume. Specifically, SIRL consists of repetitive training-sieving-relabeling steps. In the initialization stage, the label of each 2D image is assigned as the label of the volume they belong to, yielding an initial label set. In the training stage, the network is trained using the current label set. In the sieving and relabeling stage, the label of each 2D image is renewed based on the classification result of the trained network, and a new label set is obtained. Experiments are conducted on a clinical dataset and public dataset, on which the performances of the models trained by a normal scheme and our proposed methods are compared under a five-fold cross validation. Our proposed method achieves sensitivity, specificity, and accuracy of 89.74%, 94.87%, and 93.18%, respectively, on the clinical dataset. On the public dataset, the results of the corresponding three metrics are 98.22%, 90.43% and 95.88%. The results demonstrate the effectiveness of our proposed method as an approach to improve the B-scan-classification-based macular OCT volumetric image classification algorithms.", "keywords": ["image classification algorithms", "iterative refinement learning", "optical coherence tomography", "present self-supervised iterative", "self-supervised iterative refinement", "classification algorithms", "volumetric image classification", "OCT volumetric image", "image classification", "macular optical coherence", "B-scan level classification", "refinement learning", "coherence tomography", "present self-supervised", "self-supervised iterative", "iterative refinement", "optical coherence", "classification", "label set", "OCT"], "paper_title": "Self-supervised iterative refinement learning for macular OCT volumetric data classification.", "last_updated": "2023/02/04"}, {"id": "0034792759", "domain": "Diabetic retinopathy", "model_name": "Danesh et al.", "publication_date": "2021/11/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34792759/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34792759", "task": null, "abstract": "Nowadays, retinal optical coherence tomography (OCT) plays an important role in ophthalmology and automatic analysis of the OCT is of real importance: image denoising facilitates a better diagnosis and image segmentation and classification are undeniably critical in treatment evaluation. Synthetic OCT was recently considered to provide a benchmark for quantitative comparison of automatic algorithms and to be utilized in the training stage of novel solutions based on deep learning. Due to complicated data structure in retinal OCTs, a limited number of delineated OCT datasets are already available in presence of abnormalities; furthermore, the intrinsic three-dimensional (3D) structure of OCT is ignored in many public 2D datasets. We propose a new synthetic method, applicable to 3D data and feasible in presence of abnormalities like diabetic macular edema (DME). In this method, a limited number of OCT data is used during the training step and the Active Shape Model is used to produce synthetic OCTs plus delineation of retinal boundaries and location of abnormalities. Statistical comparison of thickness maps showed that synthetic dataset can be used as a statistically acceptable representative of the original dataset (p\u2009>\u20090.05). Visual inspection of the synthesized vessels was also promising. Regarding the texture features of the synthesized datasets, Q-Q plots were used, and even in cases that the points have slightly digressed from the straight line, the p-values of the Kolmogorov-Smirnov test rejected the null hypothesis and showed the same distribution in texture features of the real and the synthetic data. The proposed algorithm provides a unique benchmark for comparison of OCT enhancement methods and a tailored augmentation method to overcome the limited number of OCTs in deep learning algorithms.", "keywords": ["image denoising facilitates", "optical coherence tomography", "retinal optical coherence", "OCT", "image denoising", "image segmentation", "coherence tomography", "plays an important", "treatment evaluation", "optical coherence", "important role", "role in ophthalmology", "denoising facilitates", "segmentation and classification", "classification are undeniably", "undeniably critical", "critical in treatment", "automatic analysis", "OCTs", "limited number"], "paper_title": "Synthetic OCT data in challenging conditions: three-dimensional OCT and presence of abnormalities.", "last_updated": "2023/02/04"}, {"id": "0036006018", "domain": "Diabetic retinopathy", "model_name": "Wang et al.", "publication_date": "2022/08/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36006018/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36006018", "task": "0GPcU42tzV", "abstract": "Previous studies that identify putative genes associated with diabetic retinopathy are only focusing on specific clinical stages, thus resulting genes are not necessarily reflective of disease progression. This study identified genes associated with the severity level of diabetic retinopathy using the likelihood-ratio test (LRT) and ordinal logistic regression (OLR) model, as well as to profile immune and retinal cell landscape in progressive diabetic retinopathy using a machine learning deconvolution approach. This study used a published transcriptomic dataset (GSE160306) from macular regions of donors with different degrees of diabetic retinopathy (10 healthy controls, 10 cases of diabetes, 9 cases of nonproliferative diabetic retinopathy, and 10 cases of proliferative diabetic retinopathy or combined with diabetic macular edema). LRT and OLR models were applied to identify severity-associated genes. In addition, CIBERSORTx was used to estimate proportional changes of immune and retinal cells in progressive diabetic retinopathy. By controlling for gender and age using LRT and OLR, 50 genes were identified to be significantly increased in expression with the severity of diabetic retinopathy. Functional enrichment analyses suggested these severity-associated genes are related to inflammation and immune responses. CCND1 and FCGR2B are further identified as key regulators to interact with many other severity-associated genes and are crucial to inflammation. Deconvolution analyses demonstrated that the proportions of memory B cells, M2 macrophages, and M\u00fcller glia were significantly increased with the progression of diabetic retinopathy. These findings demonstrate that deep analyses of transcriptomic data can advance our understanding of progressive ocular diseases, such as diabetic retinopathy, by applying LRT and OLR models as well as bulk gene expression deconvolution.", "keywords": ["diabetic retinopathy", "specific clinical stages", "diabetic", "retinopathy", "progressive diabetic retinopathy", "LRT and OLR", "Previous studies", "clinical stages", "OLR", "OLR models", "focusing on specific", "specific clinical", "necessarily reflective", "LRT", "genes", "identify putative genes", "progressive diabetic", "nonproliferative diabetic retinopathy", "severity-associated genes", "cases"], "paper_title": "Retinal Transcriptome and Cellular Landscape in Relation to the Progression of Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0034185055", "domain": "Diabetic retinopathy", "model_name": "Pfau et al.", "publication_date": "2021/07/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34185055/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34185055", "task": "0GPcU42tzV", "abstract": "To probabilistically forecast needed anti-vascular endothelial growth factor (anti-VEGF) treatment frequency using volumetric spectral domain-optical coherence tomography (SD-OCT) biomarkers in neovascular age-related macular degeneration from real-world settings. SD-OCT volume scans were segmented with a custom deep-learning-based analysis pipeline. Retinal thickness and reflectivity values were extracted for the central and the four inner Early Treatment Diabetic Retinopathy Study (ETDRS) subfields for six retinal layers (inner retina, outer nuclear layer, inner segments [IS], outer segments [OS], retinal pigment epithelium-drusen complex [RPEDC] and the choroid). Machine-learning models were probed to predict the anti-VEGF treatment frequency within the next 12 months. Probabilistic forecasting was performed using natural gradient boosting (NGBoost), which outputs a full probability distribution. The mean absolute error (MAE) between the predicted versus actual anti-VEGF treatment frequency was the primary outcome measure. In a total of 138 visits of 99 eyes with neovascular AMD (96 patients) from two clinical centers, the prediction of future anti-VEGF treatment frequency was observed with an accuracy (MAE [95% confidence interval]) of 2.60 injections/year [2.25-2.96] (R2 = 0.390) using random forest regression and 2.66 injections/year [2.31-3.01] (R2 = 0.094) using NGBoost, respectively. Prediction intervals were well calibrated and reflected the true uncertainty of NGBoost-based predictions. Standard deviation of RPEDC-thickness in the central ETDRS-subfield constituted an important predictor across models. The proposed, fully automated pipeline enables probabilistic forecasting of future anti-VEGF treatment frequency in real-world settings. Prediction of a probability distribution allows the physician to inspect the underlying uncertainty. Predictive uncertainty estimates are essential to highlight cases where human-inspection and/or reversion to a fallback alternative is warranted.", "keywords": ["endothelial growth factor", "domain-optical coherence tomography", "probabilistically forecast needed", "forecast needed anti-vascular", "needed anti-vascular endothelial", "anti-vascular endothelial growth", "volumetric spectral domain-optical", "spectral domain-optical coherence", "age-related macular degeneration", "anti-VEGF treatment frequency", "treatment frequency", "neovascular age-related macular", "Early Treatment Diabetic", "Treatment Diabetic Retinopathy", "anti-VEGF treatment", "Diabetic Retinopathy Study", "future anti-VEGF treatment", "growth factor", "coherence tomography", "probabilistically forecast"], "paper_title": "Probabilistic Forecasting of Anti-VEGF Treatment Frequency in Neovascular Age-Related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0036565376", "domain": "Diabetic retinopathy", "model_name": "Sun et al.", "publication_date": "2022/12/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36565376/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36565376", "task": "IWqQC1koJA", "abstract": "To design and evaluate a deep learning model based on ultra-widefield images (UWFIs) that can detect several common fundus diseases. Based on 4574 UWFIs, a deep learning model was trained and validated that can identify normal fundus and eight common fundus diseases, namely referable diabetic retinopathy, retinal vein occlusion, pathologic myopia, retinal detachment, retinitis pigmentosa, age-related macular degeneration, vitreous opacity, and optic neuropathy. The model was tested on three test sets with data volumes of 465, 979, and 525. The performance of the three deep learning networks, EfficientNet-B7, DenseNet, and ResNet-101, was evaluated on the internal test set. Additionally, we compared the performance of the deep learning model with that of doctors in a tertiary referral hospital. Compared to the other two deep learning models, EfficientNet-B7 achieved the best performance. The area under the receiver operating characteristic curves of the EfficientNet-B7 model on the internal test set, external test set\u00a0A and external test set\u00a0B were 0.9708 (0.8772, 0.9849) to 1.0000 (1.0000, 1.0000), 0.9683 (0.8829, 0.9770) to 1.0000 (0.9975, 1.0000), and 0.8919 (0.7150, 0.9055) to 0.9977 (0.9165, 1.0000), respectively. On a data\u00a0set of 100 images, the total accuracy of the deep learning model was 93.00%, the average accuracy of three ophthalmologists who had been working for 2\u00a0years and three ophthalmologists who had been working in fundus imaging for more than 5\u00a0years was 88.00% and 94.00%, respectively. High performance was achieved on all three test sets using our UWFI multidisease classification model with a small sample size and fast model inference. The performance of the artificial intelligence model was comparable to that of a physician with 2-5\u00a0years of experience in fundus diseases at a tertiary referral hospital. The model is expected to be used as an effective aid for fundus disease screening.", "keywords": ["deep learning model", "deep learning", "learning model", "learning model based", "model", "common fundus diseases", "test set", "learning", "deep", "test", "design and evaluate", "internal test set", "common fundus", "external test set", "based on ultra-widefield", "fundus", "fundus diseases", "deep learning networks", "test sets", "performance"], "paper_title": "Deep Learning for the Detection of Multiple Fundus Diseases Using Ultra-widefield Images.", "last_updated": "2023/02/04"}, {"id": "0033339630", "domain": "Diabetic retinopathy", "model_name": "A P et al.", "publication_date": "2020/11/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33339630/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33339630", "task": "IWqQC1koJA", "abstract": "Retinal diseases are becoming a major health problem in recent years. Their early detection and ensuing treatment are essential to prevent visual damage, as the number of people affected by diabetes is expected to grow exponentially. Retinal diseases progress slowly, without any discernible symptoms. Optical Coherence Tomography (OCT) is a diagnostic tool capable of analyzing and identifying the quantitative discrimination in the disease affected retinal layers with high resolution. This paper proposes a deep neural network-based classifier for the computer-aided classification of Diabetic Macular Edema (DME), drusen, Choroidal NeoVascularization (CNV) from normal OCT images of the retina. In the proposed method, we demonstrate the feasibility of classifying and detecting severe retinal pathologies from OCT images using a deep convolutional neural network having six convolutional blocks. The classification results are explained using a gradient-based class activation mapping algorithm. Training and validation of the model are performed on a public dataset of 83,484 images with expert-level disease grading of CNV, DME, and drusen, in addition to normal retinal image. We achieved a precision of 99.69%, recall of 99.69%, and accuracy of 99.69% with only three misclassifications out of 968 test cases. In the proposed work, downsampling and weight sharing were introduced to improve the training efficiency and were found to reduce the trainable parameters significantly. The class activation mapping was also performed, and the output image was similar to the retina's actual color OCT images. The proposed network used only 6.9% of learnable parameters compared to the existing ResNet-50 model and yet outperformed it in classification. The proposed work can be potentially employed in real-time applications due to reduced complexity and fewer learnable parameters over other models.", "keywords": ["major health problem", "OCT images", "recent years", "major health", "health problem", "problem in recent", "Diabetic Macular Edema", "OCT", "disease affected retinal", "normal OCT images", "Retinal diseases", "Retinal", "Optical Coherence Tomography", "Retinal diseases progress", "color OCT images", "images", "Coherence Tomography", "proposed", "normal retinal image", "Macular Edema"], "paper_title": "OctNET: A Lightweight CNN for Retinal Disease Classification from Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0032439844", "domain": "Diabetic retinopathy", "model_name": "Lo et al.", "publication_date": "2020/05/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32439844/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32439844", "task": "IWqQC1koJA", "abstract": "Previous deep learning studies on optical coherence tomography (OCT) mainly focused on diabetic retinopathy and age-related macular degeneration. We proposed a deep learning model that can identify epiretinal membrane (ERM) in OCT with ophthalmologist-level performance. Cross-sectional study. A total of 3,618 central fovea cross section OCT images from 1,475 eyes of 964 patients. We retrospectively collected 7,652 OCT images from 1,197 patients. From these images, 2,171 were normal and 1,447 were ERM OCT. A total of 3,141 OCT images was used as training dataset and 477 images as testing dataset. DL algorithm was used to train the interpretation model. Diagnostic results by four board-certified non-retinal specialized ophthalmologists on the testing dataset were compared with those generated by the DL model. We calculated for the derived DL model the following characteristics: sensitivity, specificity, F1 score and area under curve (AUC) of the receiver operating characteristic (ROC) curve. These were calculated according to the gold standard results which were parallel diagnoses of the retinal specialist. Performance of the DL model was finally compared with that of non-retinal specialized ophthalmologists. Regarding the diagnosis of ERM in OCT images, the trained DL model had the following characteristics in performance: sensitivity: 98.7%, specificity: 98.0%, and F1 score: 0.945. The accuracy on the training dataset was 99.7% (95% CI: 99.4 - 99.9%), and for the testing dataset, diagnostic accuracy was 98.1% (95% CI: 96.5 - 99.1%). AUC of the ROC curve was 0.999. The DL model slightly outperformed the average non-retinal specialized ophthalmologists. An ophthalmologist-level DL model was built here to accurately identify ERM in OCT images. The performance of the model was slightly better than the average non-retinal specialized ophthalmologists. The derived model may play a role to assist clinicians to promote the efficiency and safety of healthcare in the future.", "keywords": ["OCT images", "optical coherence tomography", "age-related macular degeneration", "Previous deep learning", "OCT", "deep learning studies", "deep learning model", "model", "deep learning", "ERM OCT", "images", "non-retinal specialized ophthalmologists", "section OCT images", "coherence tomography", "macular degeneration", "studies on optical", "optical coherence", "focused on diabetic", "diabetic retinopathy", "retinopathy and age-related"], "paper_title": "Epiretinal Membrane Detection at the Ophthalmologist Level using Deep Learning of Optical Coherence Tomography.", "last_updated": "2023/02/04"}, {"id": "0033260118", "domain": "Macular degeneration", "model_name": "Zhang et al.", "publication_date": "2020/11/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33260118/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33260118", "task": null, "abstract": "The automated prediction of geographic atrophy (GA) lesion growth can help ophthalmologists understand how the GA progresses, and assess the efficiency of current treatment and the prognosis of the disease. We developed an integrated time adaptive prediction model for identifying the location of future GA growth. The proposed model was comprised of bi-directional long short-term memory (BiLSTM) network-based prediction module and convolutional neural network (CNN)-based refinement module. Considering the discontinuity of time intervals among sequential follow-up visits, we integrated time factors into BiLSTM-based prediction module to control the time attribute expediently. Then, the results from prediction module were refined by a CNN-based strategy to obtain the final locations of future GA growth. The 10 scenarios were designed to evaluate the prediction accuracy of our proposed model. The 1-6th scenarios demonstrated the importance of the prior information similarity, the 7-8th scenarios verified the effect of time factors and refinement methods respectively and the 9th scenario compared the prediction results between those using a single follow-up visit for training and using 2 sequential follow-up visits for training. The 10th scenario showed the model generalization performance across regions. The average dice indexes (DI) of the predicted GA regions in the 1-6th scenarios are 0.86, 0.89, 0.89, 0.92 and 0.88, 0.90, respectively. By integrating time factors to the BiLSTM models, the prediction accuracy was improved by almost 10%. The CNN-based refinement strategy can remove the wrong GA regions effectively to preserve the actual GA regions and improve the prediction accuracy further. The prediction results based on 2 sequential follow-up visits showed higher correlations than that based on single follow-up visit. The proposed model presented a good generalization performance while training patients and testing patients were from different regions. Experimental results demonstrated the importance of prior information to the prediction accuracy. We demonstrate the feasibility of creating a model for disease prediction.", "keywords": ["prediction", "prediction accuracy", "sequential follow-up visits", "prediction module", "follow-up visits", "geographic atrophy", "ophthalmologists understand", "assess the efficiency", "efficiency of current", "current treatment", "time", "automated prediction", "time adaptive prediction", "model", "adaptive prediction model", "proposed model", "prediction results", "time factors", "lesion growth", "single follow-up visit"], "paper_title": "An integrated time adaptive geographic atrophy prediction model for SD-OCT images.", "last_updated": "2023/02/04"}, {"id": "0033407448", "domain": "Macular degeneration", "model_name": null, "publication_date": "2021/01/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33407448/", "code_link": "https://github.com/ophthal-cdm/", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "33407448", "task": null, "abstract": "Although ophthalmic devices have made remarkable progress and are widely used, most lack standardization of both image review and results reporting systems, making interoperability unachievable. We developed and validated new software for extracting, transforming, and storing information from report images produced by ophthalmic examination devices to generate standardized, structured, and interoperable information to assist ophthalmologists in eye clinics. We selected report images derived from optical coherence tomography (OCT). The new software consists of three parts: (1) The Area Explorer, which determines whether the designated area in the configuration file contains numeric values or tomographic images; (2) The Value Reader, which converts images to text according to ophthalmic measurements; and (3) The Finding Classifier, which classifies pathologic findings from tomographic images included in the report. After assessment of Value Reader accuracy by human experts, all report images were converted and stored in a database. We applied the Value Reader, which achieved 99.67% accuracy, to a total of 433,175 OCT report images acquired in a single tertiary hospital from 07/04/2006 to 08/31/2019. The Finding Classifier provided pathologic findings (e.g., macular edema and subretinal fluid) and disease activity. Patient longitudinal data could be easily reviewed to document changes in measurements over time. The final results were loaded into a common data model (CDM), and the cropped tomographic images were loaded into the Picture Archive Communication System. The newly developed software extracts valuable information from OCT images and may be extended to other types of report image files produced by medical devices. Furthermore, powerful databases such as the CDM may be implemented or augmented by adding the information captured through our program.", "keywords": ["making interoperability unachievable", "made remarkable progress", "report images", "images", "making interoperability", "interoperability unachievable", "OCT report images", "made remarkable", "remarkable progress", "lack standardization", "Finding Classifier", "results reporting systems", "report", "tomographic images", "ophthalmic examination devices", "image review", "reporting systems", "Reader", "report images produced", "Archive Communication System"], "paper_title": "An innovative strategy for standardized, structured, and interoperable results in ophthalmic examinations.", "last_updated": "2023/02/04"}, {"id": "0031416547", "domain": "Diabetic retinopathy", "model_name": "Perdomo et al.", "publication_date": "2019/06/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31416547/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31416547", "task": "IWqQC1koJA", "abstract": "Spectral Domain Optical Coherence Tomography (SD-OCT) is a volumetric imaging technique that allows measuring patterns between layers such as small amounts of fluid. Since 2012, automatic medical image analysis performance has steadily increased through the use of deep learning models that automatically learn relevant features for specific tasks, instead of designing visual features manually. Nevertheless, providing insights and interpretation of the predictions made by the model is still a challenge. This paper describes a deep learning model able to detect medically interpretable information in relevant images from a volume to classify diabetes-related retinal diseases. This article presents a new deep learning model, OCT-NET, which is a customized convolutional neural network for processing scans extracted from optical coherence tomography volumes. OCT-NET is applied to the classification of three conditions seen in SD-OCT volumes. Additionally, the proposed model includes a feedback stage that highlights the areas of the scans to support the interpretation of the results. This information is potentially useful for a medical specialist while assessing the prediction produced by the model. The proposed model was tested on the public SERI-CUHK and A2A SD-OCT data sets containing healthy, diabetic retinopathy, diabetic macular edema and age-related macular degeneration. The experimental evaluation shows that the proposed method outperforms conventional convolutional deep learning models from the state of the art reported on the SERI+CUHK and A2A SD-OCT data sets with a precision of 93% and an area under the ROC curve (AUC) of 0.99 respectively. The proposed method is able to classify the three studied retinal diseases with high accuracy. One advantage of the method is its ability to produce interpretable clinical information in the form of highlighting the regions of the image that most contribute to the classifier decision.", "keywords": ["Spectral Domain Optical", "Domain Optical Coherence", "volumetric imaging technique", "Optical Coherence Tomography", "Spectral Domain", "deep learning models", "Domain Optical", "deep learning", "amounts of fluid", "volumetric imaging", "imaging technique", "measuring patterns", "patterns between layers", "small amounts", "coherence tomography volumes", "Optical Coherence", "Coherence Tomography", "learning models", "model", "proposed model"], "paper_title": "Classification of diabetes-related retinal diseases using a deep learning approach in optical coherence tomography.", "last_updated": "2023/02/04"}, {"id": "0033918998", "domain": "Diabetic retinopathy", "model_name": "Kim et al.", "publication_date": "2021/04/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33918998/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33918998", "task": "IWqQC1koJA", "abstract": "Artificial intelligence (AI)-based diagnostic tools have been accepted in ophthalmology. The use of retinal images, such as fundus photographs, is a promising approach for the development of AI-based diagnostic platforms. Retinal pathologies usually occur in a broad spectrum of eye diseases, including neovascular or dry age-related macular degeneration, epiretinal membrane, rhegmatogenous retinal detachment, retinitis pigmentosa, macular hole, retinal vein occlusions, and diabetic retinopathy. Here, we report a fundus image-based AI model for differential diagnosis of retinal diseases. We classified retinal images with three convolutional neural network models: ResNet50, VGG19, and Inception v3. Furthermore, the performance of several dense (fully connected) layers was compared. The prediction accuracy for diagnosis of nine classes of eight retinal diseases and normal control was 87.42% in the ResNet50 model, which added a dense layer with 128 nodes. Furthermore, our AI tool augments ophthalmologist's performance in the diagnosis of retinal disease. These results suggested that the fundus image-based AI tool is applicable for the medical diagnosis process of retinal diseases.", "keywords": ["Artificial intelligence", "accepted in ophthalmology", "retinal", "based diagnostic tools", "based diagnostic", "retinal diseases", "AI-based diagnostic platforms", "diseases", "retinal images", "diagnosis", "diagnostic", "fundus", "diagnostic platforms", "fundus image-based", "AI-based diagnostic", "Artificial", "intelligence", "based", "ophthalmology", "accepted"], "paper_title": "Development of a Fundus Image-Based Deep Learning Diagnostic Tool for Various Retinal Diseases.", "last_updated": "2023/02/04"}, {"id": "0031416566", "domain": "Macular degeneration", "model_name": "Varga et al.", "publication_date": "2019/06/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31416566/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31416566", "task": "aipbNdPTIt", "abstract": "The leading cause of vision loss in the Western World is Age-related Macular Degeneration (AMD), but together with modern medicines, tracking the number of Hyperreflective Foci (HF) on Optical Coherence Tomography (OCT) images should assist the treatment of patients. Here, we developed a framework based on deep learning for the automatic segmentation of HF in OCT images. We collected OCT images and annotated them, then these images underwent image preprocessing, and feature extraction steps. Using the prepared data we trained different types of Conventional-, Deep- and Convolutional Neural Networks to perform the task of the automatic segmentation of HF. We evaluated the various Neural Networks, by performing HF segmentation of clinical data belonging to patients, whose data were excluded from the training process. The results suggest that our systems can achieve reasonably high Dice Coefficient values, and they are comparable with (i.e., in most cases above 95%) the similarity between manual annotations performed by different physicians. From the results, it can be concluded that neural networks can be used to accurately segment HF in OCT images. The results are sufficiently accurate for us to incorporate them into the next phase of the research, building a decision support system for everyday clinical practice.", "keywords": ["Age-related Macular Degeneration", "Optical Coherence Tomography", "Macular Degeneration", "Hyperreflective Foci", "Coherence Tomography", "Western World", "World is Age-related", "Age-related Macular", "Optical Coherence", "OCT images", "number of Hyperreflective", "collected OCT images", "Neural Networks", "Convolutional Neural Networks", "modern medicines", "tracking the number", "vision loss", "assist the treatment", "OCT", "images underwent image"], "paper_title": "Automatic segmentation of hyperreflective foci in OCT images.", "last_updated": "2023/02/04"}, {"id": "0032568984", "domain": "Macular degeneration", "model_name": "Lee et al.", "publication_date": "2021/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32568984/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32568984", "task": null, "abstract": "To develop a deep learning model to generate posttreatment optical coherence tomography (OCT) images of neovascular age-related macular degeneration. Two hundred ninety-eight patients with neovascular age-related macular degeneration were included. The conditional generative adversarial network was trained using 15,183 augmented paired OCT B-scan images obtained from 723 scans of 241 patients at baseline and 1 month after 3 loading doses of an anti-vascular endothelial growth factor treatment. The network was also trained using baseline fluorescein angiography (FA) or indocyanine green angiography (ICGA) images together with baseline OCT images. A test set of 150 images of 50 eyes was used to evaluate its ability to predict the presence of intraretinal fluid, subretinal fluid, PED, and subretinal hyperreflective material. Posttreatment OCT images were compared with images generated from baseline OCT with or without FA and indocyanine green angiography images. The predicted images inferred from baseline OCT images achieved an acceptable accuracy, specificity, and negative predictive value for four lesions (range: 77.0-91.9, 94.1-95.1, and 54.7-96.5%, respectively). The addition of both FA and indocyanine green angiography images improved the accuracy, specificity, and negative predictive value (range: 80.7-96.3, 97.3-99.0, and 59.0-98.3%, respectively). A conditional generative adversarial network is able to generate posttreatment OCT images from baseline OCT, FA, and indocyanine green angiography images.", "keywords": ["neovascular age-related macular", "age-related macular degeneration", "optical coherence tomography", "deep learning model", "baseline OCT", "OCT", "OCT images", "baseline OCT images", "indocyanine green angiography", "Posttreatment OCT images", "OCT B-scan images", "images", "posttreatment optical coherence", "green angiography images", "age-related macular", "neovascular age-related", "macular degeneration", "paired OCT B-scan", "green angiography", "indocyanine green"], "paper_title": "POST-TREATMENT PREDICTION OF OPTICAL COHERENCE TOMOGRAPHY USING A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK IN AGE-RELATED MACULAR DEGENERATION.", "last_updated": "2023/02/04"}, {"id": "0030605812", "domain": "Macular degeneration", "model_name": "Xu et al.", "publication_date": "2018/12/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30605812/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "30605812", "task": "aipbNdPTIt", "abstract": "Automatic and reliable segmentation for geographic atrophy in spectral-domain optical coherence tomography (SD-OCT) images is a challenging task. To develop an effective segmentation method, a two-stage deep learning framework based on an auto-encoder is proposed. Firstly, the axial data of cross-section images were used as samples instead of the projection images of SD-OCT images. Next, a two-stage learning model that includes offline-learning and self-learning was designed based on a stacked sparse auto-encoder to obtain deep discriminative representations. Finally, a fusion strategy was used to refine the segmentation results based on the two-stage learning results. The proposed method was evaluated on two datasets consisting of 55 and 56 cubes, respectively. For the first dataset, our method obtained a mean overlap ratio (OR) of 89.85\u202f\u00b1\u202f6.35% and an absolute area difference (AAD) of 4.79\u202f\u00b1\u202f7.16%. For the second dataset, the mean OR and AAD were 84.48\u202f\u00b1\u202f11.98%, 11.09\u202f\u00b1\u202f13.61%, respectively. Compared with the state-of-the-art algorithms, experiments indicate that the proposed algorithm can provide more accurate segmentation results on these two datasets without using retinal layer segmentation.", "keywords": ["optical coherence tomography", "spectral-domain optical coherence", "Automatic and reliable", "coherence tomography", "challenging task", "geographic atrophy", "atrophy in spectral-domain", "spectral-domain optical", "optical coherence", "segmentation", "images", "reliable segmentation", "two-stage learning", "two-stage", "based", "two-stage deep learning", "learning framework based", "segmentation results", "SD-OCT images", "learning"], "paper_title": "Automated geographic atrophy segmentation for SD-OCT images based on two-stage learning model.", "last_updated": "2023/02/04"}, {"id": "0035978087", "domain": "Diabetic retinopathy", "model_name": "Almasi et al.", "publication_date": "2022/08/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35978087/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35978087", "task": "IWqQC1koJA", "abstract": "Microaneurysms (MAs) are pathognomonic signs that help clinicians to detect diabetic retinopathy (DR) in the early stages. Automatic detection of MA in retinal images is an active area of research due to its application in screening processes for DR which is one of the main reasons of blindness amongst the working-age population. The focus of these works is on the automatic detection of MAs in en face retinal images like fundus color and Fluorescein Angiography (FA). On the other hand, detection of MAs from Optical Coherence Tomography (OCT) images has 2 main advantages: first, OCT is a non-invasive imaging technique that does not require injection, therefore is safer. Secondly, because of the proven application of OCT in detection of Age-Related Macular Degeneration, Diabetic Macular Edema, and normal cases, thanks to detecting MAs in OCT, extensive information is obtained by using this imaging technique. In this research, the concentration is on the diagnosis of MAs using deep learning in the OCT images which represent in-depth structure of retinal layers. To this end, OCT B-scans should be divided into strips and MA patterns should be searched in the resulted strips. Since we need a dataset comprising OCT image strips with suitable labels and such large labelled datasets are not yet available, we have created it. For this purpose, an exact registration method is utilized to align OCT images with FA photographs. Then, with the help of corresponding FA images, OCT image strips are created from OCT B-scans in four labels, namely MA, normal, abnormal, and vessel. Once the dataset of image strips is prepared, a stacked generalization (stacking) ensemble of four fine-tuned, pre-trained convolutional neural networks is trained to classify the strips of OCT images into the mentioned classes. FA images are used once to create OCT strips for training process and they are no longer needed for subsequent steps. Once the stacking ensemble model is obtained, it will be used to classify the OCT strips in the test process. The results demonstrate that the proposed framework classifies overall OCT image strips and OCT strips containing MAs with accuracy scores of 0.982 and 0.987, respectively.", "keywords": ["OCT", "OCT image strips", "detect diabetic retinopathy", "OCT images", "OCT image", "OCT strips", "OCT B-scans", "Optical Coherence Tomography", "strips", "images", "Diabetic Macular Edema", "image strips", "early stages", "pathognomonic signs", "clinicians to detect", "MAs", "diabetic retinopathy", "comprising OCT image", "Fluorescein Angiography", "detect diabetic"], "paper_title": "Automatic detection of microaneurysms in optical coherence tomography images of retina using convolutional neural networks and transfer learning.", "last_updated": "2023/02/04"}, {"id": "0035342179", "domain": "Diabetic retinopathy", "model_name": "Zheng et al.", "publication_date": "2022/05/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35342179/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35342179", "task": "IWqQC1koJA", "abstract": "To develop and test semi-supervised generative adversarial networks (GANs) that detect retinal disorders on optical coherence tomography (OCT) images using a small-labeled dataset. From a public database, we randomly chose a small supervised dataset with 400 OCT images (100 choroidal neovascularization, 100 diabetic macular edema, 100 drusen, and 100 normal) and assigned all other OCT images to unsupervised dataset (107,912 images without labeling). We adopted a semi-supervised GAN and a supervised deep learning (DL) model for automatically detecting retinal disorders from OCT images. The performance of the 2 models was compared in 3 testing datasets with different OCT devices. The evaluation metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curves. The local validation dataset included 1000 images with 250 from each category. The independent clinical dataset included 366 OCT images using Cirrus OCT Shanghai Shibei Hospital and 511 OCT images using RTVue OCT from Xinhua Hospital respectively. The semi-supervised GANs classifier achieved better accuracy than supervised DL model (0.91 vs 0.86 for local cell validation dataset, 0.91 vs 0.86 in the Shanghai Shibei Hospital testing dataset, and 0.93 vs 0.92 in Xinhua Hospital testing dataset). For detecting urgent referrals (choroidal neo-vascularization and diabetic macular edema) from nonurgent referrals (drusen and normal) on OCT images, the semi-supervised GANs classifier also achieved better area under the receiver operating characteristic curves than supervised DL model (0.99 vs 0.97, 0.97 vs 0.96, and 0.99 vs 0.99, respectively). A semi-supervised GAN can achieve better performance than that of a supervised DL model when the labeled dataset is limited. The current study offers utility to various research and clinical studies using DL with relatively small datasets. Semi-supervised GANs can detect retinal disorders from OCT images using relatively small dataset.", "keywords": ["OCT images", "OCT", "generative adversarial networks", "optical coherence tomography", "Shanghai Shibei Hospital", "OCT Shanghai Shibei", "images", "Cirrus OCT Shanghai", "test semi-supervised generative", "semi-supervised generative adversarial", "dataset", "Shibei Hospital", "Xinhua Hospital", "semi-supervised GANs", "Hospital testing dataset", "retinal disorders", "adversarial networks", "coherence tomography", "Shanghai Shibei", "Hospital"], "paper_title": "Development and Clinical Validation of Semi-Supervised Generative Adversarial Networks for Detection of Retinal Disorders in Optical Coherence Tomography Images Using Small Dataset.", "last_updated": "2023/02/04"}, {"id": "0033133774", "domain": "Macular degeneration", "model_name": "oct-stargardtretina-seg", "publication_date": "2020/10/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33133774/", "code_link": "https://github.com/jakugel/oct-stargardtretina-seg", "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "33133774", "task": "aipbNdPTIt", "abstract": "To use a deep learning model to develop a fully automated method (fully semantic network and graph search [FS-GS]) of retinal segmentation for optical coherence tomography (OCT) images from patients with Stargardt disease. Eighty-seven manually segmented (ground truth) OCT volume scan sets (5171 B-scans) from 22 patients with Stargardt disease were used for training, validation and testing of a novel retinal boundary detection approach (FS-GS) that combines a fully semantic deep learning segmentation method, which generates a per-pixel class prediction map with a graph-search method to extract retinal boundary positions. The performance was evaluated using the mean absolute boundary error and the differences in two clinical metrics (retinal thickness and volume) compared with the ground truth. The performance of a separate deep learning method and two publicly available software algorithms were also evaluated against the ground truth. FS-GS showed an excellent agreement with the ground truth, with a boundary mean absolute error of 0.23 and 1.12 pixels for the internal limiting membrane and the base of retinal pigment epithelium or Bruch's membrane, respectively. The mean difference in thickness and volume across the central 6 mm zone were 2.10 \u00b5m and 0.059 mm<sup>3</sup>. The performance of the proposed method was more accurate and consistent than the publicly available OCTExplorer and AURA tools. The FS-GS method delivers good performance in segmentation of OCT images of pathologic retina in Stargardt disease. Deep learning models can provide a robust method for retinal segmentation and support a high-throughput analysis pipeline for measuring retinal thickness and volume in Stargardt disease.", "keywords": ["optical coherence tomography", "fully semantic network", "ground truth", "Stargardt disease", "fully semantic deep", "deep learning", "fully automated method", "fully semantic", "semantic deep learning", "graph search", "coherence tomography", "OCT volume scan", "retinal", "network and graph", "optical coherence", "fully automated", "Stargardt", "OCT", "deep learning segmentation", "semantic network"], "paper_title": "Retinal Boundary Segmentation in Stargardt Disease Optical Coherence Tomography Images Using Automated Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0034097114", "domain": "Macular degeneration", "model_name": "Zhao et al.", "publication_date": "2021/06/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34097114/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34097114", "task": null, "abstract": "To predict short-term anti-vascular endothelial growth factor (anti-VEGF) treatment responder/non-responder for neovascular age-related macular degeneration (nAMD) patients based on optical coherence tomography (OCT) images. A total of 4944 OCT scans from 206 patients with nAMD were involved to develop and evaluate a responder/non-responder prediction method for the short-term effect of anti-VEGF therapy. A deep learning architecture named sensitive structure guided network (SSG-Net) was proposed to make the prediction leveraging a sensitive structure guidance module trained from pre- and post-treatment images. To verify its clinical efficiency, other 2 deep learning methods and 4 experienced ophthalmologists were involved to evaluate the performance of the developed model. For the testing dataset, SSG-Net could predict the response by an accuracy of 84.6% and an area under the receiver curve (AUC) of 0.83, with a sensitivity of 0.692 and specificity of 1. In contrast, the 2 compared deep learning methods achieved an accuracy of 65.4% with a sensitivity of 0.461 and specificity of 0.846, and an accuracy of 73.1% with a sensitivity of 0.692 and specificity of 0.846, respectively. The predicted accuracy for 4 experienced ophthalmologists was 53.8 to 76.9%, with sensitivity of 0.538 to 0.923 and specificity of 0.385 to 0.846, respectively. Our proposed SSG-Net shows effective prediction on the short-term efficacy of anti-VEGF treatment for nAMD patients. This technique could potentially help clinicians explain the necessity of anti-VEGF treatment to the potential responder and avoid unnecessary treatment for the non-responder.", "keywords": ["endothelial growth factor", "age-related macular degeneration", "optical coherence tomography", "anti-vascular endothelial growth", "neovascular age-related macular", "short-term anti-vascular endothelial", "predict short-term anti-vascular", "growth factor", "macular degeneration", "coherence tomography", "deep learning methods", "anti-vascular endothelial", "endothelial growth", "neovascular age-related", "age-related macular", "based on optical", "optical coherence", "deep learning", "OCT scans", "short-term anti-vascular"], "paper_title": "Optical coherence tomography-based short-term effect prediction of anti-vascular endothelial growth factor treatment in neovascular age-related macular degeneration using sensitive structure guided network.", "last_updated": "2023/02/04"}, {"id": "0033260113", "domain": "Diabetic retinopathy", "model_name": "oct_preprocess", "publication_date": "2020/10/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33260113/", "code_link": "https://github.com/heyufan1995/oct_preprocess", "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "33260113", "task": "aipbNdPTIt", "abstract": "Optical coherence tomography\u00a0(OCT) is a noninvasive imaging modality with micrometer resolution which has been widely used for scanning the retina. Retinal layers are important biomarkers for many diseases. Accurate automated algorithms for segmenting smooth continuous layer surfaces with correct hierarchy\u00a0(topology) are important for automated retinal thickness and surface shape analysis. State-of-the-art methods typically use a two step process. Firstly, a trained classifier is used to label each pixel into either background and layers or boundaries and non-boundaries. Secondly, the desired smooth surfaces with the correct topology are extracted by graph methods\u00a0(e.g., graph cut). Data driven methods like deep networks have shown great ability for the pixel classification step, but to date have not been able to extract structured smooth continuous surfaces with topological constraints in the second step. In this paper, we combine these two steps into a unified deep learning framework by directly modeling the distribution of the surface positions. Smooth, continuous, and topologically correct surfaces are obtained in a single feed forward operation. The proposed method was evaluated on two publicly available data sets of healthy controls and subjects with either multiple sclerosis or diabetic macular edema, and is shown to achieve state-of-the art performance with sub-pixel accuracy.", "keywords": ["Optical coherence tomography", "noninvasive imaging modality", "Optical coherence", "coherence tomography", "scanning the retina", "noninvasive imaging", "imaging modality", "modality with micrometer", "micrometer resolution", "OCT", "smooth", "surfaces", "Optical", "tomography", "retina", "continuous", "step", "correct", "methods", "smooth continuous"], "paper_title": "Structured layer surface segmentation for retina OCT using fully convolutional regression networks.", "last_updated": "2023/02/04"}, {"id": "0031170065", "domain": "Macular degeneration", "model_name": "Seebock et al.", "publication_date": "2019/05/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31170065/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31170065", "task": "aipbNdPTIt", "abstract": "Diagnosis and treatment guidance are aided by detecting relevant biomarkers in medical images. Although supervised deep learning can perform accurate segmentation of pathological areas, it is limited by requiring a priori definitions of these regions, large-scale annotations, and a representative patient cohort in the training set. In contrast, anomaly detection is not limited to specific definitions of pathologies and allows for training on healthy samples without annotation. Anomalous regions can then serve as candidates for biomarker discovery. Knowledge about normal anatomical structure brings implicit information for detecting anomalies. We propose to take advantage of this property using Bayesian deep learning, based on the assumption that epistemic uncertainties will correlate with anatomical deviations from a normal training set. A Bayesian U-Net is trained on a well-defined healthy environment using weak labels of healthy anatomy produced by existing methods. At test time, we capture epistemic uncertainty estimates of our model using Monte Carlo dropout. A novel post-processing technique is then applied to exploit these estimates and transfer their layered appearance to smooth blob-shaped segmentations of the anomalies. We experimentally validated this approach in retinal optical coherence tomography (OCT) images, using weak labels of retinal layers. Our method achieved a Dice index of 0.789 in an independent anomaly test set of age-related macular degeneration (AMD) cases. The resulting segmentations allowed very high accuracy for separating healthy and diseased cases with late wet AMD, dry geographic atrophy (GA), diabetic macular edema (DME) and retinal vein occlusion (RVO). Finally, we qualitatively observed that our approach can also detect other deviations in normal scans such as cut edge artifacts.", "keywords": ["Diagnosis and treatment", "treatment guidance", "guidance are aided", "detecting relevant biomarkers", "relevant biomarkers", "detecting relevant", "medical images", "training set", "Bayesian deep learning", "training", "deep learning", "healthy", "representative patient cohort", "large-scale annotations", "supervised deep learning", "normal training set", "Diagnosis", "biomarkers in medical", "set", "weak labels"], "paper_title": "Exploiting Epistemic Uncertainty of Anatomy Segmentation for Anomaly Detection in Retinal OCT.", "last_updated": "2023/02/04"}, {"id": "0033927240", "domain": "Macular degeneration", "model_name": "Hwang et al.", "publication_date": "2021/04/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33927240/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33927240", "task": null, "abstract": "This cross-sectional study aimed to build a deep learning model for detecting neovascular age-related macular degeneration (AMD) and to distinguish retinal angiomatous proliferation (RAP) from polypoidal choroidal vasculopathy (PCV) using a convolutional neural network (CNN). Patients from a single tertiary center were enrolled from January 2014 to January 2020. Spectral-domain optical coherence tomography (SD-OCT) images of patients with RAP or PCV and a control group were analyzed with a deep CNN. Sensitivity, specificity, accuracy, and area under the receiver operating characteristic curve (AUROC) were used to evaluate the model's ability to distinguish RAP from PCV. The performances of the new model, the VGG-16, Resnet-50, Inception, and eight ophthalmologists were compared. A total of 3951 SD-OCT images from 314 participants (229 AMD, 85 normal controls) were analyzed. In distinguishing the PCV and RAP cases, the proposed model showed an accuracy, sensitivity, and specificity of 89.1%, 89.4%, and 88.8%, respectively, with an AUROC of 95.3% (95% CI 0.727-0.852). The proposed model showed better diagnostic performance than VGG-16, Resnet-50, and Inception-V3 and comparable performance with the eight ophthalmologists. The novel model performed well when distinguishing between PCV and RAP. Thus, automated deep learning systems may support ophthalmologists in distinguishing RAP from PCV.", "keywords": ["age-related macular degeneration", "retinal angiomatous proliferation", "polypoidal choroidal vasculopathy", "convolutional neural network", "cross-sectional study aimed", "detecting neovascular age-related", "neovascular age-related macular", "distinguish retinal angiomatous", "PCV", "macular degeneration", "angiomatous proliferation", "choroidal vasculopathy", "neural network", "RAP", "cross-sectional study", "study aimed", "aimed to build", "detecting neovascular", "neovascular age-related", "age-related macular"], "paper_title": "Distinguishing retinal angiomatous proliferation from polypoidal choroidal vasculopathy with a deep neural network based on optical coherence tomography.", "last_updated": "2023/02/04"}, {"id": "0029994522", "domain": "Glaucoma (unspecified)", "model_name": "Cheng et al.", "publication_date": "2018/05/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29994522/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "29994522", "task": null, "abstract": "Retinal fundus photographs have been used in the diagnosis of many ocular diseases such as glaucoma, pathological myopia, age-related macular degeneration, and diabetic retinopathy. With the development of computer science, computer aided diagnosis has been developed to process and analyze the retinal images automatically. One of the challenges in the analysis is that the quality of the retinal image is often degraded. For example, a cataract in human lens will attenuate the retinal image, just as a cloudy camera lens which reduces the quality of a photograph. It often obscures the details in the retinal images and posts challenges in retinal image processing and analyzing tasks. In this paper, we approximate the degradation of the retinal images as a combination of human-lens attenuation and scattering. A novel structure-preserving guided retinal image filtering (SGRIF) is then proposed to restore images based on the attenuation and scattering model. The proposed SGRIF consists of a step of global structure transferring and a step of global edge-preserving smoothing. Our results show that the proposed SGRIF method is able to improve the contrast of retinal images, measured by histogram flatness measure, histogram spread, and variability of local luminosity. In addition, we further explored the benefits of SGRIF for subsequent retinal image processing and analyzing tasks. In the two applications of deep learning-based optic cup segmentation and sparse learning-based cup-to-disk ratio (CDR) computation, our results show that we are able to achieve more accurate optic cup segmentation and CDR measurements from images processed by SGRIF.", "keywords": ["age-related macular degeneration", "retinal image", "Retinal", "Retinal fundus photographs", "retinal image processing", "pathological myopia", "age-related macular", "macular degeneration", "diabetic retinopathy", "retinal images automatically", "ocular diseases", "images", "Retinal fundus", "SGRIF", "image", "retinal image filtering", "computer aided diagnosis", "proposed SGRIF", "guided retinal image", "subsequent retinal image"], "paper_title": "Structure-Preserving Guided Retinal Image Filtering and Its Application for Optic Disk Analysis.", "last_updated": "2023/02/04"}, {"id": "0032569310", "domain": "Diabetic retinopathy", "model_name": "Shaban et al.", "publication_date": "2020/06/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32569310/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32569310", "task": null, "abstract": "Diabetic retinopathy (DR) is a serious retinal disease and is considered as a leading cause of blindness in the world. Ophthalmologists use optical coherence tomography (OCT) and fundus photography for the purpose of assessing the retinal thickness, and structure, in addition to detecting edema, hemorrhage, and scars. Deep learning models are mainly used to analyze OCT or fundus images, extract unique features for each stage of DR and therefore classify images and stage the disease. Throughout this paper, a deep Convolutional Neural Network (CNN) with 18 convolutional layers and 3 fully connected layers is proposed to analyze fundus images and automatically distinguish between controls (i.e. no DR), moderate DR (i.e. a combination of mild and moderate Non Proliferative DR (NPDR)) and severe DR (i.e. a group of severe NPDR, and Proliferative DR (PDR)) with a validation accuracy of 88%-89%, a sensitivity of 87%-89%, a specificity of 94%-95%, and a Quadratic Weighted Kappa Score of 0.91-0.92 when both 5-fold, and 10-fold cross validation methods were used respectively. A prior pre-processing stage was deployed where image resizing and a class-specific data augmentation were used. The proposed approach is considerably accurate in objectively diagnosing and grading diabetic retinopathy, which obviates the need for a retina specialist and expands access to retinal care. This technology enables both early diagnosis and objective tracking of disease progression which may help optimize medical therapy to minimize vision loss.", "keywords": ["Quadratic Weighted Kappa", "Weighted Kappa Score", "Convolutional Neural Network", "analyze fundus images", "fundus images", "deep Convolutional Neural", "Diabetic retinopathy", "analyze OCT", "optical coherence tomography", "retinal disease", "retinal", "retinal thickness", "Neural Network", "Quadratic Weighted", "Weighted Kappa", "Kappa Score", "OCT", "grading diabetic retinopathy", "fundus", "images"], "paper_title": "A convolutional neural network for the screening and staging of diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0036769653", "domain": "Macular degeneration", "model_name": "Han et al.", "publication_date": "2023/01/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36769653/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36769653", "task": "IWqQC1koJA", "abstract": "Neovascular age-related macular degeneration (nAMD) and central serous chorioretinopathy (CSC) are two of the most common macular diseases. This study proposes a convolutional neural network (CNN)-based deep learning model for classifying the subtypes of nAMD (polypoidal choroidal vasculopathy, retinal angiomatous proliferation, and typical nAMD) and CSC (chronic CSC and acute CSC) and healthy individuals using single spectral-domain optical coherence tomography (SD-OCT) images. The proposed model was trained and tested using 6063 SD-OCT images from 521 patients and 47 healthy participants. We used three well-known CNN architectures (VGG-16, VGG-19, and ResNet) and two customized classification layers. Additionally, transfer learning and mix-up-based data augmentation were applied to improve robustness and accuracy. Our model demonstrated high accuracies of 99.7% and 91.1% in the nAMD and CSC classification and retinopathy (nAMD and CSC) subtype classification, including normal participants, respectively. Furthermore, we performed an external test to compare the classification accuracy with that of eight ophthalmologists, and our model showed the highest accuracy. The region determined to be important for classification by the model was confirmed using gradient-weighted class activation mapping. The model's clinical criteria were similar to that of the ophthalmologists.", "keywords": ["Neovascular age-related macular", "age-related macular degeneration", "common macular diseases", "central serous chorioretinopathy", "CSC", "macular degeneration", "macular diseases", "age-related macular", "common macular", "Neovascular age-related", "serous chorioretinopathy", "central serous", "nAMD", "chronic CSC", "acute CSC", "CSC classification", "nAMD and CSC", "model", "macular", "classification"], "paper_title": "Detecting Macular Disease Based on Optical Coherence Tomography Using a Deep Convolutional Network.", "last_updated": "2023/02/04"}, {"id": "0035995411", "domain": "Macular degeneration", "model_name": "Chiang et al.", "publication_date": "2022/08/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35995411/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35995411", "task": null, "abstract": "To assess and validate a deep learning algorithm to automatically detect incomplete retinal pigment epithelial and outer retinal atrophy (iRORA) and complete retinal pigment epithelial and outer retinal atrophy (cRORA) in eyes with age-related macular degeneration. In a retrospective machine learning analysis, a deep learning model was trained to jointly classify the presence of iRORA and cRORA within a given B-scan. The algorithm was evaluated using 2 separate and independent datasets. OCT B-scan volumes from 71 patients with nonneovascular age-related macular degeneration captured at the Doheny-University of California Los Angeles Eye Centers and the following 2 external OCT B-scans testing datasets: (1) University of Pennsylvania, University of Miami, and Case Western Reserve University and (2) Doheny Image Reading Research Laboratory. The images were annotated by an experienced grader for the presence of iRORA and cRORA. A Resnet18 model was trained to classify these annotations for each B-scan using OCT volumes collected at the Doheny-University of California Los Angeles Eye Centers. The model was applied to 2 testing datasets to assess out-of-sample model performance. Model performance was quantified in terms of area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). Sensitivity, specificity, and positive predictive value were also compared against additional clinician annotators. On an independently collected test set, consisting of 1117 volumes from the general population, the model predicted iRORA and cRORA presence within the entire volume with nearly perfect AUROC performance and AUPRC scores (iRORA, 0.61; 95% confidence interval [CI] [0.45, 0.82]: cRORA, 0.83; 95% CI [0.68, 0.95]). On another independently collected set, consisting of 60 OCT B-scans enriched for iRORA and cRORA lesions, the model performed with AUROC (iRORA: 0.68, 95% CI [0.54, 0.81]; cRORA: 0.84, 95% CI [0.75, 0.94]) and AUPRC (iRORA: 0.70, 95% CI [0.55, 0.86]; cRORA: 0.82, 95% CI [0.70, 0.93]). A deep learning model can accurately and precisely identify both iRORA and cRORA lesions within the OCT B-scan volume. The model can achieve similar sensitivity compared with human graders, which potentially obviates a laborious and time-consuming annotation process and could be developed into a diagnostic screening tool.", "keywords": ["outer retinal atrophy", "retinal pigment epithelial", "incomplete retinal pigment", "detect incomplete retinal", "retinal atrophy", "retinal pigment", "outer retinal", "pigment epithelial", "epithelial and outer", "Los Angeles Eye", "automatically detect incomplete", "deep learning model", "Angeles Eye Centers", "California Los Angeles", "incomplete retinal", "complete retinal", "deep learning", "retinal", "OCT B-scan", "deep learning algorithm"], "paper_title": "Automated Identification of Incomplete and Complete Retinal Epithelial Pigment and Outer Retinal Atrophy Using Machine Learning.", "last_updated": "2023/02/04"}, {"id": "0032327254", "domain": "Macular degeneration", "model_name": "Schmidt-Erfurth et al.", "publication_date": "2020/03/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32327254/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32327254", "task": null, "abstract": "Anti-vascular endothelial growth factor (VEGF) treatment of neovascular age-related macular degeneration (AMD) is a highly effective advance in the retinal armentarium. OCT offering 3-dimensional imaging of the retina is widely used to guide treatment. Although poor outcomes reported from clinical practice are multifactorial, availability of reliable, reproducible, and quantitative evaluation tools to accurately measure the fluid response, that is, a \"VEGF meter,\" may be a better means of monitoring and treating than the current purely qualitative evaluation used in clinical practice. Post hoc analysis of a phase III, randomized, multicenter study. Study eyes of 1095 treatment-naive subjects receiving pro re nata (PRN) or monthly ranibizumab therapy according to protocol-specified criteria in the HARBOR study. A deep learning method for localization and quantification of fluid in all retinal compartments was applied for automated segmentation of fluid with every voxel classified by a convolutional neural network (CNN). Three-dimensional volumes (nanoliters) for intraretinal fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachment (PED) were determined in 24\u2009362 volume scans obtained from 1095 patients treated over 24 months in a phase III clinical trial with randomization to 2 drug dosages (0.5 mg and 2.0 mg ranibizumab) and 2 regimens (monthly and PRN). A multivariable mixed-effects regression model was used to test for differences in fluid between the arms and for fluid/function correlation. Fluid volume in nanoliters, structure-function as Pearson's correlation coefficient, and as a coefficient of determination (R<sup>2</sup>). Fluid volumes were quantified in all visits of all patients. Automated segmentation demonstrated characteristic response patterns for each fluid compartment individually: Intraretinal fluid showed the greatest and most rapid resolution, followed by SRF and PED the least. The loading dose treatment achieved resolution of all fluid types close to the lowest levels attainable. Dosage and regimen parameters correlated directly with resulting fluid volumes. Fluid/function correlation showed a volume-dependent negative impact of IRF on vision and weak positive prognostic effect of SRF. Automated quantification of the fluid response may improve therapeutic management of neovascular AMD, avoid discrepancies between clinicians/investigators, and establish structure/function correlations.", "keywords": ["Anti-vascular endothelial growth", "endothelial growth factor", "age-related macular degeneration", "highly effective advance", "fluid", "neovascular age-related macular", "Anti-vascular endothelial", "growth factor", "macular degeneration", "endothelial growth", "age-related macular", "highly effective", "effective advance", "VEGF", "phase III", "VEGF meter", "OCT offering", "retinal armentarium", "treatment", "SRF"], "paper_title": "Application of Automated Quantification of Fluid Volumes to Anti-VEGF Therapy of Neovascular Age-Related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0031686211", "domain": "Macular degeneration", "model_name": "Yang et al.", "publication_date": "2019/11/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31686211/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31686211", "task": "IWqQC1koJA", "abstract": "To investigate the feasibility of training an artificial intelligence (AI) on a public-available AI platform to diagnose polypoidal choroidal vasculopathy (PCV) using indocyanine green angiography (ICGA). Two methods using AI models were trained by a data set including 430 ICGA images of normal, neovascular age-related macular degeneration (nvAMD), and PCV eyes on a public-available AI platform. The one-step method distinguished normal, nvAMD, and PCV images simultaneously. The two-step method identifies normal and abnormal ICGA images at the first step and diagnoses PCV from the abnormal ICGA images at the second step. The method with higher performance was used to compare with retinal specialists and ophthalmologic residents on the performance of diagnosing PCV. The two-step method had better performance, in which the precision was 0.911 and the recall was 0.911 at the first step, and the precision was 0.783, and the recall was 0.783 at the second step. For the test data set, the two-step method distinguished normal and abnormal images with an accuracy of 1 and diagnosed PCV with an accuracy of 0.83, which was comparable to retinal specialists and superior to ophthalmologic residents. In this evaluation of ICGA images from normal, nvAMD, and PCV eyes, the models trained on a public-available AI platform had comparable performance to retinal specialists for diagnosing PCV. The utility of public-available AI platform might help everyone including ophthalmologists who had no AI-related resources, especially those in less developed areas, for future studies.", "keywords": ["polypoidal choroidal vasculopathy", "indocyanine green angiography", "ICGA images", "abnormal ICGA images", "PCV", "diagnose polypoidal choroidal", "ICGA", "abnormal ICGA", "public-available AI platform", "PCV images simultaneously", "artificial intelligence", "choroidal vasculopathy", "green angiography", "PCV eyes", "method distinguished normal", "diagnosing PCV", "investigate the feasibility", "feasibility of training", "training an artificial", "polypoidal choroidal"], "paper_title": "Utility of a public-available artificial intelligence in diagnosis of polypoidal choroidal vasculopathy.", "last_updated": "2023/02/04"}, {"id": "0035717830", "domain": "Macular degeneration", "model_name": "Ibrahim et al.", "publication_date": "2022/06/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35717830/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "35717830", "task": null, "abstract": "The choroid, a dense vascular structure in the posterior segment of the eye, maintains the health of the retina by supplying oxygen and nutrients, and assumes clinical significance in screening ocular diseases including age-related macular degeneration (AMD) and central serous chorioretinopathy (CSCR). As a technological assist, algorithmic estimation of choroidal biomarkers has been suggested based on sectional (B-scan) optical coherence tomography (OCT) images. However, most such 2D estimation techniques are compute-intensive, yet enjoy limited accuracy and have only been validated on OCT image datasets of healthy eyes. Not surprisingly, fine-scale analyses, including those involving Haller's sublayer, remain relatively rare and unsophisticated. Against this backdrop, we propose an efficient algorithm to quantify desired biomarkers with improved accuracy based on volume OCT scans. Specifically, we attempted an accurate, computationally light volumetric segmentation method involving stratified smoothing to detect choroid and Haller's sublayer. For detecting the various boundaries of the choroid and the Haller's sublayer, we propose a common volumetric method that performs suitable exponential enhancement and maintains smooth spatial continuity across 2D B-scans. Further, we achieve suitable volumetric smoothing by primarily deploying light-duty linear regression, and sparingly using compute-intensive tensor voting, and hence significantly reduce overall complexity. The proposed methodology is tested on five health and five diseased OCT volumes considering various metrics including volumetric Dice coefficient and corresponding quotient measures to facilitate comparison vis-\u00e0-vis intra-observer repeatability. On five healthy and five diseased OCT volumes, respectively, the proposed method for choroid segmentation recorded volumetric Dice coefficients of 93.53 % and 93.30 %, which closely approximate the respective reference observer repeatability values of 95.60 % and 95.49 %. In terms of related quotient measures, our method achieved more than 50 % improvement over a recently reported method. In detecting Haller's sublayer as well, our algorithm records statistical performance closely matching that of reference manual method. Advancing the state-of-the-art, the proposed volumetric segmentation, tested on both healthy and diseased datasets, demonstrated close match with the manual reference. Our method assumes significance in accurate screening of chorioretinal diseases including AMD, CSCR and pachychoroid. Further, it enables generating accurate training data for developing deep learning models for improved detection of choroid and Haller's sublayer.", "keywords": ["age-related macular degeneration", "central serous chorioretinopathy", "dense vascular structure", "Haller sublayer", "including age-related macular", "oxygen and nutrients", "macular degeneration", "serous chorioretinopathy", "diseased OCT volumes", "dense vascular", "vascular structure", "posterior segment", "retina by supplying", "supplying oxygen", "age-related macular", "central serous", "OCT", "Haller", "diseased OCT", "involving Haller sublayer"], "paper_title": "Volumetric quantification of choroid and Haller's sublayer using OCT scans: An accurate and unified approach based on stratified smoothing.", "last_updated": "2023/02/04"}, {"id": "0036403310", "domain": "Diabetic retinopathy", "model_name": "Wang et al.", "publication_date": "2022/10/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36403310/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36403310", "task": "IWqQC1koJA", "abstract": "Supervised deep learning has achieved prominent success in various diabetic macular edema (DME) recognition tasks from optical coherence tomography (OCT) volumetric images. A common problematic issue that frequently occurs in this field is the shortage of labeled data due to the expensive fine-grained annotations, which increases substantial difficulty in accurate analysis by supervised learning. The morphological changes in the retina caused by DME might be distributed sparsely in B-scan images of the OCT volume, and OCT data is often coarsely labeled at the volume level. Hence, the DME identification task can be formulated as a multiple instance classification problem that could be addressed by multiple instance learning (MIL) techniques. Nevertheless, none of previous studies utilize unlabeled data simultaneously to promote the classification accuracy, which is particularly significant for a high quality of analysis at the minimum annotation cost. To this end, we present a novel deep semi-supervised multiple instance learning framework to explore the feasibility of leveraging a small amount of coarsely labeled data and a large amount of unlabeled data to tackle this problem. Specifically, we come up with several modules to further improve the performance according to the availability and granularity of their labels. To warm up the training, we propagate the bag labels to the corresponding instances as the supervision of training, and propose a self-correction strategy to handle the label noise in the positive bags. This strategy is based on confidence-based pseudo-labeling with consistency regularization. The model uses its prediction to generate the pseudo-label for each weakly augmented input only if it is highly confident about the prediction, which is subsequently used to supervise the same input in a strongly augmented version. This learning scheme is also applicable to unlabeled data. To enhance the discrimination capability of the model, we introduce the Student-Teacher architecture and impose consistency constraints between two models. For demonstration, the proposed approach was evaluated on two large-scale DME OCT image datasets. Extensive results indicate that the proposed method improves DME classification with the incorporation of unlabeled data and outperforms competing MIL methods significantly, which confirm the feasibility of deep semi-supervised multiple instance learning at a low annotation cost.", "keywords": ["diabetic macular edema", "optical coherence tomography", "achieved prominent success", "DME OCT image", "multiple instance learning", "large-scale DME OCT", "unlabeled data", "DME OCT", "multiple instance", "macular edema", "coherence tomography", "instance learning", "achieved prominent", "prominent success", "diabetic macular", "optical coherence", "DME", "Supervised deep learning", "data", "OCT"], "paper_title": "Deep semi-supervised multiple instance learning with self-correction for DME classification from OCT images.", "last_updated": "2023/02/04"}, {"id": "0036709332", "domain": "Macular degeneration", "model_name": "Koidala et al.", "publication_date": "2023/01/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36709332/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36709332", "task": "IWqQC1koJA", "abstract": "Various vision-threatening eye diseases including age-related macular degeneration (AMD) and central serous chorioretinopathy (CSCR) are caused due to the dysfunctions manifested in the highly vascular choroid layer of the posterior segment of the eye. In the current clinical practice, screening choroidal structural changes is widely based on optical coherence tomography (OCT) images. Accordingly, to assist clinicians, several automated choroidal biomarker detection methods using OCT images are developed. However, the performance of these algorithms is largely constrained by the quality of the OCT scan. Consequently, determining the quality of choroidal features in OCT scans is significant in building standardized quantification tools and hence constitutes our main objective. This study includes a dataset of 1593 good and 2581 bad quality Spectralis OCT images graded by an expert. Noting the efficacy of deep-learning (DL) in medical image analysis, we propose to train three state-of-the-art DL models: ResNet18, EfficientNet-B0 and EfficientNet-B3 to detect the quality of OCT images. The choice of these models was inspired by their ability to preserve the salient features across all the layers without information loss. To evaluate the attention of DL models on the choroid, we introduced color transparency maps (CTMs) based on GradCAM explanations. Further, we proposed two subjective grading scores: overall choroid coverage (OCC) and choroid coverage in the visible region(CCVR) based on CTMs to objectively correlate visual explanations vis-\u00e0-vis DL model attentions. We observed that the average accuracy and F-scores for the three DL models are greater than 96%. Further, the OCC and CCVR scores achieved for the three DL models under consideration substantiate that they mostly focus on the choroid layer in making the decision. In particular, of the three DL models, EfficientNet-B3 is in close agreement with the clinician's inference. The proposed DL-based framework demonstrated high detection accuracy as well as attention on the choroid layer, where EfficientNet-B3 reported superior performance. Our work assumes significance in bench-marking the automated choroid biomarker detection tools and facilitating high-throughput screening. Further, the methods proposed in this work can be adopted for evaluating the attention of DL-based approaches developed for other region-specific quality assessment tasks.", "keywords": ["vision-threatening eye diseases", "eye diseases including", "age-related macular degeneration", "central serous chorioretinopathy", "diseases including age-related", "including age-related macular", "highly vascular choroid", "OCT images", "Spectralis OCT images", "quality Spectralis OCT", "vision-threatening eye", "eye diseases", "OCT", "vascular choroid layer", "macular degeneration", "serous chorioretinopathy", "diseases including", "including age-related", "age-related macular", "central serous"], "paper_title": "Deep learning based diagnostic quality assessment of choroidal OCT features with expert-evaluated explainability.", "last_updated": "2023/02/04"}, {"id": "0032040475", "domain": "Diabetic retinopathy", "model_name": "DMENet", "publication_date": "2020/02/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32040475/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32040475", "task": "IWqQC1koJA", "abstract": "Diabetic Macular Edema (DME) is an advanced stage of Diabetic Retinopathy (DR) and can lead to permanent vision loss. Currently, it affects 26.7 million people globally and on account of such a huge number of DME cases and the limited number of ophthalmologists, it is desirable to automate the diagnosis process. Computer-assisted, deep learning based diagnosis could help in early detection, following which precision medication can help to mitigate the vision loss. In order to automate the screening of DME, we propose a novel DMENet Algorithm which is built on the pillars of Convolutional Neural Networks (CNNs). DMENet analyses the preprocessed color fundus images and passes it through a two-stage pipeline. The first stage detects the presence or absence of DME whereas the second stage takes only the positive cases and grades the images based on severity. In both the stages, we use a novel Hierarchical Ensemble of CNNs (HE-CNN). This paper uses two of the popular publicly available datasets IDRiD and MESSIDOR for classification. Preprocessing on the images is performed using morphological opening and gaussian kernel. The dataset is augmented to solve the class imbalance problem for better performance of the proposed model. The proposed methodology achieved an average Accuracy of 96.12%, Sensitivity of 96.32%, Specificity of 95.84%, and F-1 score of 0.9609 on MESSIDOR and IDRiD datasets. These excellent results establish the validity of the proposed methodology for use in DME screening and solidifies the applicability of the HE-CNN classification technique in the domain of biomedical imaging.", "keywords": ["Diabetic Macular Edema", "Macular Edema", "Diabetic Retinopathy", "Diabetic Macular", "permanent vision loss", "lead to permanent", "Convolutional Neural Networks", "DME", "vision loss", "Diabetic", "permanent vision", "Edema", "Retinopathy", "advanced stage", "Macular", "DME cases", "DME screening", "number of DME", "million people globally", "Neural Networks"], "paper_title": "DMENet: Diabetic Macular Edema diagnosis using Hierarchical Ensemble of CNNs.", "last_updated": "2023/02/04"}, {"id": "0030231879", "domain": "Diabetic retinopathy", "model_name": "Jiang et al.", "publication_date": "2018/09/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30231879/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30231879", "task": null, "abstract": "Fundus fluorescein angiography (FFA) imaging is a standard diagnostic tool for many retinal diseases such as age-related macular degeneration and diabetic retinopathy. High-resolution FFA images facilitate the detection of small lesions such as microaneurysms, and other landmark changes, in the early stages; this can help an ophthalmologist improve a patient's cure rate. However, only low-resolution images are available in most clinical cases. Super-resolution (SR), which is a method to improve the resolution of an image, has been successfully employed for natural and remote sensing images. To the best of our knowledge, no one has applied SR techniques to FFA imaging so far. In this work, we propose a SR method-based pipeline for FFA imaging. The aim of this pipeline is to enhance the image quality of FFA by using SR techniques. Several SR frameworks including neighborhood embedding, sparsity-based, locally-linear regression and deep learning-based approaches are investigated. Based on a clinical FFA dataset collected from Second Affiliated Hospital to Xuzhou Medical University, each SR method is implemented and evaluated for the pipeline to improve the resolution of FFA images. As shown in our results, most SR algorithms have a positive impact on the enhancement of FFA images. Super-resolution forests (SRF), a random forest-based SR method has displayed remarkable high effectiveness and outperformed other methods. Hence, SRF should be one potential way to benefit ophthalmologists by obtaining high-resolution FFA images in a clinical setting.", "keywords": ["Fundus fluorescein angiography", "standard diagnostic tool", "age-related macular degeneration", "FFA", "FFA images", "Fundus fluorescein", "fluorescein angiography", "diabetic retinopathy", "standard diagnostic", "diagnostic tool", "retinal diseases", "age-related macular", "macular degeneration", "degeneration and diabetic", "FFA imaging", "High-resolution FFA images", "High-resolution FFA", "images", "FFA images facilitate", "Xuzhou Medical University"], "paper_title": "A super-resolution method-based pipeline for fundus fluorescein angiography imaging.", "last_updated": "2023/02/04"}, {"id": "0030346281", "domain": "Macular degeneration", "model_name": "Seebock et al.", "publication_date": "2018/10/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30346281/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "30346281", "task": null, "abstract": "The identification and quantification of markers in medical images is critical for diagnosis, prognosis, and disease management. Supervised machine learning enables the detection and exploitation of findings that are known a priori after annotation of training examples by experts. However, supervision does not scale well, due to the amount of necessary training examples, and the limitation of the marker vocabulary to known entities. In this proof-of-concept study, we propose unsupervised identification of anomalies as candidates for markers in retinal optical coherence tomography (OCT) imaging data without a constraint to a priori definitions. We identify and categorize marker candidates occurring frequently in the data and demonstrate that these markers show a predictive value in the task of detecting disease. A careful qualitative analysis of the identified data driven markers reveals how their quantifiable occurrence aligns with our current understanding of disease course, in early- and late age-related macular degeneration (AMD) patients. A multi-scale deep denoising autoencoder is trained on healthy images, and a one-class support vector machine identifies anomalies in new data. Clustering in the anomalies identifies stable categories. Using these markers to classify healthy-, early AMD- and late AMD cases yields an accuracy of 81.40%. In a second binary classification experiment on a publicly available data set (healthy versus intermediate AMD), the model achieves an area under the ROC curve of 0.944.", "keywords": ["critical for diagnosis", "markers", "AMD", "disease management", "data", "medical images", "prognosis", "Supervised machine learning", "disease", "diagnosis", "management", "identification and quantification", "anomalies", "machine learning enables", "healthy", "training", "quantification", "medical", "critical", "images is critical"], "paper_title": "Unsupervised Identification of Disease Marker Candidates in Retinal OCT Imaging Data.", "last_updated": "2023/02/04"}, {"id": "0032822291", "domain": "Disc hemorrhage", "model_name": "Moosavi et al.", "publication_date": "2021/05/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32822291/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "32822291", "task": null, "abstract": "Diabetic Macular Edema (DME) and macular edema secondary to retinal occlusion (RVO) are the two most common retinal vascular causes of visual impairment and leading cause of worldwide vision loss. The blood-retinal barrier is the key barrier for maintaining fluid balance within the retinal tissue. Vascular Endothelial Growth Factor (VEGF) has a significant role in the permeability of the blood-retinal barrier, which also leads to appearance of leakage foci. Intravitreal anti-VEGF therapy is the current gold standard treatment and has been demonstrated to improve macular thickening, improve vision acuity and reduce vascular leakage. However, treatment response and required dosing interval can vary widely across patients. Given the role of the blood-retinal barrier and vascular leakage in the pathogenesis of these disorders, the goal of this study was to present and evaluate new computer extracted features relating to morphology, spatial architecture and tortuosity of vessels and leakages from baseline ultra-widefield fluorescein angiography (UWFA) images. Specifically, we sought to evaluate the role of these computer extracted features from baseline UWFA images. Notably, these UWFA images were obtained from IRB-approved PERMEATE clinical trial [1], [2] to distinguish eyes tolerating extended dosing intervals (n = 16) who are referred to as non-rebounders and those who require more frequent dosing (n = 12) and are called rebounders based on visual acuity loss with extended dosing challenges. A total of 64 features encapsulating different morphological and geometrical attributes of leakage patches including the anatomical (shape, size, density, area, minor and major axis, orientation, area, extent ratio, perimeter, radii) and geometrical characteristics (the proximity of each leakage foci to main vessels, to other leakage foci and to optical disc) as well as 54 tortuosity features (tortuosity of whole vessel network, local tortuosity of vessels in the vicinity of leakage foci) were extracted. The most significant and predictive biomarkers related to treatment response were proximity of leakage nodes to major and minor eye vessels as well as local vasculature tortuosity in the vicinity of the leakages. The imaging features were then used in conjunction with a Linear Discriminant Analysis (LDA) classifier to distinguish rebounders from non-rebounders. The 3-fold cross-validated Area Under Curve (AUC) was found to be 0.82 for the morphological based features and 0.85 for the tortuosity based features. Our findings suggest higher variation in leakage node proximity to retinal vessels in eyes tolerating extended interval dosing. In contrast, eyes with increased local vascular tortuosity demonstrated less tolerance of increased dosing interval. Moreover, a class activation map generated by a deep learning model identified regions that corresponded to regions of leakages proximal to the vessels, providing confirmation of the validity of predictive image features extracted from these regions in this study.", "keywords": ["Diabetic Macular Edema", "macular edema secondary", "Macular Edema", "edema secondary", "Endothelial Growth Factor", "common retinal vascular", "Vascular Endothelial Growth", "leakage", "Edema", "blood-retinal barrier", "leakage foci", "impairment and leading", "features", "Diabetic Macular", "retinal occlusion", "tortuosity", "common retinal", "DME", "RVO", "retinal"], "paper_title": "Imaging Features of Vessels and Leakage Patterns Predict Extended Interval Aflibercept Dosing Using Ultra-Widefield Angiography in Retinal Vascular Disease: Findings From the PERMEATE Study.", "last_updated": "2023/02/04"}, {"id": "0032597828", "domain": "Macular degeneration", "model_name": "Vellakani et al.", "publication_date": "2021/10/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32597828/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32597828", "task": "IWqQC1koJA", "abstract": "Human eye is affected by the different eye diseases including choroidal neovascularization (CNV), diabetic macular edema (DME) and age-related macular degeneration (AMD). This work aims to design an artificial intelligence (AI) based clinical decision support system for eye disease detection and classification to assist the ophthalmologists more effectively detecting and classifying CNV, DME and drusen by using the Optical Coherence Tomography (OCT) images depicting different tissues. The methodology used for designing this system involves different deep learning convolutional neural network (CNN) models and long short-term memory networks (LSTM). The best image captioning model is selected after performance analysis by comparing nine different image captioning systems for assisting ophthalmologists to detect and classify eye diseases. The quantitative data analysis results obtained for the image captioning models designed using DenseNet201 with LSTM have superior performance in terms of overall accuracy of 0.969, positive predictive value of 0.972 and true-positive rate of 0.969using OCT images enhanced by the generative adversarial network (GAN). The corresponding performance values for the Xception with LSTM image captioning models are 0.969, 0.969 and 0.938, respectively. Thus, these two models yield superior performance and have potential to assist ophthalmologists in making optimal diagnostic decision.", "keywords": ["diabetic macular edema", "age-related macular degeneration", "including choroidal neovascularization", "Optical Coherence Tomography", "diseases including choroidal", "eye diseases including", "diabetic macular", "macular edema", "macular degeneration", "age-related macular", "choroidal neovascularization", "including choroidal", "Coherence Tomography", "eye diseases", "image captioning", "Optical Coherence", "image captioning models", "eye disease detection", "Human eye", "classifying CNV"], "paper_title": "An enhanced OCT image captioning system to assist ophthalmologists in detecting and classifying eye diseases.", "last_updated": "2023/02/04"}, {"id": "0030615698", "domain": "Macular degeneration", "model_name": "ncbi\" aria-label=\"github\">", "publication_date": "2018/08/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30615698/", "code_link": "https://github.com/ncbi\" aria-label=\"github\">", "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "30615698", "task": "aipbNdPTIt", "abstract": "Automated segmentation of object boundaries or surfaces is crucial for quantitative image analysis in numerous biomedical applications. For example, retinal surfaces in optical coherence tomography (OCT) images play a vital role in the diagnosis and management of retinal diseases. Recently, graph based surface segmentation and contour modeling have been developed and optimized for various surface segmentation tasks. These methods require expertly designed, application specific transforms, including cost functions, constraints and model parameters. However, deep learning based methods are able to directly learn the model and features from training data. In this paper, we propose a convolutional neural network (CNN) based framework to segment multiple surfaces simultaneously. We demonstrate the application of the proposed method by training a single CNN to segment three retinal surfaces in two types of OCT images - normal retinas and retinas affected by intermediate age-related macular degeneration (AMD). The trained network directly infers the segmentations for each B-scan in one pass. The proposed method was validated on 50 retinal OCT volumes (3000 B-scans) including 25 normal and 25 intermediate AMD subjects. Our experiment demonstrated statistically significant improvement of segmentation accuracy compared to the optimal surface segmentation method with convex priors (OSCS) and two deep learning based UNET methods for both types of data. The average computation time for segmenting an entire OCT volume (consisting of 60 B-scans each) for the proposed method was 12.3 seconds, demonstrating low computation costs and higher performance compared to the graph based optimal surface segmentation and UNET based methods.", "keywords": ["quantitative image analysis", "numerous biomedical applications", "object boundaries", "crucial for quantitative", "analysis in numerous", "numerous biomedical", "surface segmentation", "based", "OCT", "segmentation", "quantitative image", "image analysis", "surfaces", "surface", "proposed method", "Automated segmentation", "methods", "method", "biomedical applications", "OCT images"], "paper_title": "Multiple surface segmentation using convolution neural nets: application to retinal layer segmentation in OCT images.", "last_updated": "2023/02/04"}, {"id": "0032940026", "domain": "Diabetic retinopathy", "model_name": "Sun et al.", "publication_date": "2021/09/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32940026/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32940026", "task": "IWqQC1koJA", "abstract": "Automatic and accurate classification of three-dimensional (3-D) retinal optical coherence tomography (OCT) images is essential for assisting ophthalmologist in the diagnosis and grading of macular diseases. Therefore, more effective OCT volume classification for automatic recognition of macular diseases is needed. For OCT volumes in which only OCT volume-level labels are known, OCT volume classifiers based on its global feature and deep learning are designed, validated, and compared with other methods. We present a general framework to classify OCT volume for automatic recognizing macular diseases. The architecture of the framework consists of three modules: B-scan feature extractor, two-dimensional (2-D) feature map generation, and volume-level classifier. Our architecture could address OCT volume classification using two 2-D image machine learning classification algorithms. Specifically, a convolutional neural network (CNN) model is trained and used as a B-scan feature extractor to construct a 2-D feature map of an OCT volume and volume-level classifiers such as support vector machine and CNN with/without attention mechanism for 2-D feature maps are described. Our proposed methods are validated on the publicly available Duke dataset, which consists of 269 intermediate age-related macular degeneration (AMD) volumes and 115 normal volumes. Fivefold cross-validation was done, and average accuracy, sensitivity, and specificity of 98.17%, 99.26%, and 95.65%, respectively, are achieved. The experiments show that our methods outperform the state-of-the-art methods. Our methods are also validated on our private clinical OCT volume dataset, consisting of 448 AMD volumes and 462 diabetic macular edema volumes. We present a general framework of OCT volume classification based on its 2-D feature map and CNN with attention mechanism and describe its implementation schemes. Our proposed methods could classify OCT volumes automatically and effectively with high accuracy, and they are a potential practical tool for screening of ophthalmic diseases from OCT volume.", "keywords": ["OCT volume", "OCT volume classification", "OCT", "retinal optical coherence", "optical coherence tomography", "volume", "OCT volume classifiers", "effective OCT volume", "feature", "OCT volume dataset", "volumes", "volume classification", "macular diseases", "classify OCT volume", "retinal optical", "coherence tomography", "classify OCT", "B-scan feature extractor", "optical coherence", "essential for assisting"], "paper_title": "Automatic diagnosis of macular diseases from OCT volume based on its two-dimensional feature map and convolutional neural network with attention mechanism.", "last_updated": "2023/02/04"}, {"id": "0035859144", "domain": "Diabetic retinopathy", "model_name": "Liu et al.", "publication_date": "2022/07/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35859144/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35859144", "task": null, "abstract": "To assess the feasibility and clinical utility of artificial intelligence (AI)-based screening for diabetic retinopathy (DR) and macular edema (ME) by combining fundus photos and optical coherence tomography (OCT) images in a community hospital. Fundus photos and OCT images were taken for 600 diabetic patients in a community hospital. Ophthalmologists graded these fundus photos according to the International Clinical Diabetic Retinopathy (ICDR) Severity Scale as the ground truth. Two existing trained AI models were used to automatically classify the fundus images into DR grades according to ICDR, and to detect concomitant ME from OCT images, respectively. The criteria for referral were DR grades 2-4 and/or the presence of ME. The sensitivity and specificity of AI grading were evaluated. The number of referable DR cases confirmed by ophthalmologists and AI was calculated, respectively. DR was detected in 81 (13.5%) participants by ophthalmologists and in 94 (15.6%) by AI, and 45 (7.5%) and 53 (8.8%) participants were diagnosed with referable DR by ophthalmologists and by AI, respectively. The sensitivity, specificity and area under the curve (AUC) of AI for detecting DR were 91.67%, 96.92% and 0.944, respectively. For detecting referable DR, the sensitivity, specificity and AUC of AI were 97.78%, 98.38% and 0.981, respectively. ME was detected from OCT images in 49 (8.2%) participants by ophthalmologists and in 57 (9.5%) by AI, and the sensitivity, specificity and AUC of AI were 91.30%, 97.46% and 0.944, respectively. When combining fundus photos and OCT images, the number of referrals identified by ophthalmologists increased from 45 to 75 and from 53 to 85 by AI. AI-based DR screening has high sensitivity and specificity and may feasibly improve the referral rate of community DR.", "keywords": ["optical coherence tomography", "Clinical Diabetic Retinopathy", "OCT images", "International Clinical Diabetic", "fundus photos", "diabetic retinopathy", "combining fundus photos", "OCT", "community hospital", "artificial intelligence", "macular edema", "coherence tomography", "assess the feasibility", "utility of artificial", "optical coherence", "images", "Clinical Diabetic", "clinical utility", "fundus", "Ophthalmologists"], "paper_title": "Application of artificial intelligence-based dual-modality analysis combining fundus photography and optical coherence tomography in diabetic retinopathy screening in a community hospital.", "last_updated": "2023/02/04"}, {"id": "0034460650", "domain": "Macular degeneration", "model_name": "env_dmla", "publication_date": "2020/06/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34460650/", "code_link": "https://github.com/gouzmi/env_dmla", "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "34460650", "task": "0GPcU42tzV", "abstract": "Age-Related Macular Degeneration (ARMD) is a progressive eye disease that slowly causes patients to go blind. For several years now, it has been an important research field to try to understand how the disease progresses and find effective medical treatments. Researchers have been mostly interested in studying the evolution of the lesions using different techniques ranging from manual annotation to mathematical models of the disease. However, artificial intelligence for ARMD image analysis has become one of the main research focuses to study the progression of the disease, as accurate manual annotation of its evolution has proved difficult using traditional methods even for experienced practicians. In this paper, we propose a deep learning architecture that can detect changes in the eye fundus images and assess the progression of the disease. Our method is based on joint autoencoders and is fully unsupervised. Our algorithm has been applied to pairs of images from different eye fundus images time series of 24 ARMD patients. Our method has been shown to be quite effective when compared with other methods from the literature, including non-neural network based algorithms that still are the current standard to follow the disease progression and change detection methods from other fields.", "keywords": ["Age-Related Macular Degeneration", "Macular Degeneration", "Age-Related Macular", "progressive eye disease", "disease", "Degeneration", "Macular", "eye fundus images", "ARMD", "progressive eye", "ARMD image analysis", "manual annotation", "ARMD image", "fundus images", "eye fundus", "ARMD patients", "disease progression", "progression", "images", "methods"], "paper_title": "Analyzing Age-Related Macular Degeneration Progression in Patients with Geographic Atrophy Using Joint Autoencoders for Unsupervised Change Detection.", "last_updated": "2023/02/04"}, {"id": "0034460779", "domain": "Macular degeneration", "model_name": "unsupervised-segmentation-of-dry-armd-lesions-in-cslo-images-using-w-nets", "publication_date": "2021/08/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34460779/", "code_link": "https://github.com/clement-royer/unsupervised-segmentation-of-dry-armd-lesions-in-cslo-images-using-w-nets", "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "34460779", "task": "aipbNdPTIt", "abstract": "Age-related macular degeneration (ARMD), a major cause of sight impairment for elderly people, is still not well understood despite intensive research. Measuring the size of the lesions in the fundus is the main biomarker of the severity of the disease and as such is widely used in clinical trials yet only relies on manual segmentation. Artificial intelligence, in particular automatic image analysis based on neural networks, has a major role to play in better understanding the disease, by analyzing the intrinsic optical properties of dry ARMD lesions from patient images. In this paper, we propose a comparison of automatic segmentation methods (classical computer vision method, machine learning method and deep learning method) in an unsupervised context applied on cSLO IR images. Among the methods compared, we propose an adaptation of a fully convolutional network, called W-net, as an efficient method for the segmentation of ARMD lesions. Unlike supervised segmentation methods, our algorithm does not require annotated data which are very difficult to obtain in this application. Our method was tested on a dataset of 328 images and has shown to reach higher quality results than other compared unsupervised methods with a F1 score of 0.87, while having a more stable model, even though in some specific cases, texture/edges-based methods can produce relevant results.", "keywords": ["Age-related macular degeneration", "Age-related macular", "macular degeneration", "elderly people", "intensive research", "sight impairment", "impairment for elderly", "understood despite intensive", "ARMD lesions", "dry ARMD lesions", "ARMD", "methods", "method", "segmentation", "lesions", "dry ARMD", "segmentation methods", "images", "major", "Age-related"], "paper_title": "Unsupervised Approaches for the Segmentation of Dry ARMD Lesions in Eye Fundus cSLO Images.", "last_updated": "2023/02/04"}, {"id": "0033883573", "domain": "Macular degeneration", "model_name": "Munk et al.", "publication_date": "2021/04/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33883573/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33883573", "task": null, "abstract": "In this paper we analyse the performance of machine learning methods in predicting patient information such as age or sex solely from retinal imaging modalities in a heterogeneous clinical population. Our dataset consists of N\u00a0=\u00a0135,667 fundus images and N\u00a0=\u00a085,536 volumetric OCT scans. Deep learning models were trained to predict the patient's age and sex from fundus images, OCT cross sections and OCT volumes. For sex prediction, a ROC AUC of 0.80 was achieved for fundus images, 0.84 for OCT cross sections and 0.90 for OCT volumes. Age prediction mean absolute errors of 6.328 years for fundus, 5.625\u00a0years for OCT cross sections and 4.541 for OCT volumes were observed. We assess the performance of OCT scans containing different biomarkers and note a peak performance of AUC\u00a0=\u00a00.88 for OCT cross sections and 0.95 for volumes when there is no pathology on scans. Performance drops in case of drusen, fibrovascular pigment epitheliuum detachment and geographic atrophy present. We conclude that deep learning based methods are capable of classifying the patient's sex and age from color fundus photography and OCT for a broad spectrum of patients irrespective of underlying disease or image quality. Non-random sex prediction using fundus images seems only possible if the eye fovea and optic disc are visible.", "keywords": ["OCT cross sections", "heterogeneous clinical population", "retinal imaging modalities", "OCT cross", "OCT", "OCT volumes", "predicting patient information", "cross sections", "OCT scans", "fundus images", "clinical population", "paper we analyse", "solely from retinal", "retinal imaging", "imaging modalities", "heterogeneous clinical", "volumetric OCT scans", "fundus", "cross", "sections"], "paper_title": "Assessment of patient specific information in the wild on fundus photography and optical coherence tomography.", "last_updated": "2023/02/04"}, {"id": "0036720585", "domain": "Glaucoma (unspecified)", "model_name": "Pandey et al.", "publication_date": "2023/01/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36720585/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36720585", "task": "IWqQC1koJA", "abstract": "To develop an algorithm to classify multiple retinal pathologies accurately and reliably from fundus photographs and to validate its performance against human experts. We trained a deep convolutional ensemble (DCE), an ensemble of five convolutional neural networks (CNNs), to classify retinal fundus photographs into diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD) and normal eyes. The CNN architecture was based on the InceptionV3 model, and initial weights were pretrained on the ImageNet dataset. We used 43\u2009055 fundus images from 12 public datasets. Five trained ensembles were then tested on an 'unseen' set of 100 images. Seven board-certified ophthalmologists were asked to classify these test images. Board-certified ophthalmologists achieved a mean accuracy of 72.7% over all classes, while the DCE achieved a mean accuracy of 79.2% (p=0.03). The DCE had a statistically significant higher mean F1-score for DR classification compared with the ophthalmologists (76.8% vs 57.5%; p=0.01) and greater but statistically non-significant mean F1-scores for glaucoma (83.9% vs 75.7%; p=0.10), AMD (85.9% vs 85.2%; p=0.69) and normal eyes (73.0% vs 70.5%; p=0.39). The DCE had a greater mean agreement between accuracy and confident of 81.6% vs 70.3% (p<0.001). We developed a deep learning model and found that it could more accurately and reliably classify four categories of fundus images compared with board-certified ophthalmologists. This work provides proof-of-principle that an algorithm is capable of accurate and reliable recognition of multiple retinal diseases using only fundus photographs.", "keywords": ["human experts", "fundus photographs", "validate its performance", "performance against human", "retinal pathologies accurately", "retinal fundus photographs", "multiple retinal pathologies", "DCE", "retinal pathologies", "classify retinal fundus", "fundus", "photographs", "classify", "multiple retinal", "retinal", "board-certified ophthalmologists", "AMD", "fundus images", "classify multiple retinal", "images"], "paper_title": "An ensemble of deep convolutional neural networks is more accurate and reliable than board-certified ophthalmologists at detecting multiple diseases in retinal fundus photographs.", "last_updated": "2023/02/04"}, {"id": "0036821970", "domain": "Macular degeneration", "model_name": "Li et al.", "publication_date": "2023/02/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36821970/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36821970", "task": "aipbNdPTIt", "abstract": "Accurate measurements of the size, shape and volume of macular edema can provide important biomarkers to jointly assess disease progression and treatment outcome. Although many deep learning-based segmentation algorithms have achieved remarkable success in semantic segmentation, these methods have difficulty obtaining satisfactory segmentation results in retinal optical coherence tomography (OCT) fluid segmentation tasks due to low contrast, blurred boundaries, and varied distribution. Moreover, directly applying a well-trained model on one device to test the images from other devices may cause the performance degradation in the joint analysis of multi-domain OCT images. In this paper, we propose a self-training adversarial learning framework for unsupervised domain adaptation in retinal OCT fluid segmentation tasks. Specifically, we develop an image style transfer module and a fine-grained feature transfer module to reduce discrepancies in the appearance and high-level features of images from different devices. Importantly, we transfer the target images to the appearance of source images to ensure that no image information of the source domain for supervised training is lost. To capture specific features of the target domain, we design a self-training module based on a discrepancy and similarity strategy to select the images with better segmentation results from the target domain and then introduce them into the source domain for the iterative training segmentation model. Extensive experiments on two challenging datasets demonstrate the effectiveness of our proposed method. In Particular, our proposed method achieves comparable results on cross-domain retinal OCT fluid segmentation compared with the state-of-the-art methods.", "keywords": ["provide important biomarkers", "jointly assess disease", "assess disease progression", "retinal OCT fluid", "OCT fluid segmentation", "OCT fluid", "Accurate measurements", "shape and volume", "treatment outcome", "retinal OCT", "volume of macular", "macular edema", "edema can provide", "provide important", "important biomarkers", "biomarkers to jointly", "jointly assess", "assess disease", "disease progression", "progression and treatment"], "paper_title": "Self-training adversarial learning for cross-domain retinal OCT fluid segmentation.", "last_updated": "2023/02/04"}, {"id": "0036875027", "domain": "Diabetic retinopathy", "model_name": "Pan et al.", "publication_date": "2023/02/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36875027/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36875027", "task": "IWqQC1koJA", "abstract": "<b>Purpose:</b> We aim to present effective and computer aided diagnostics in the field of ophthalmology and improve eye health. This study aims to create an automated deep learning based system for categorizing fundus images into three classes: normal, macular degeneration and tessellated fundus for the timely recognition and treatment of diabetic retinopathy and other diseases. <b>Methods:</b> A total of 1,032 fundus images were collected from 516 patients using fundus camera from Health Management Center, Shenzhen University General Hospital Shenzhen University, Shenzhen 518055, Guangdong, China. Then, Inception V3 and ResNet-50 deep learning models are used to classify fundus images into three classes, Normal, Macular degeneration and tessellated fundus for the timely recognition and treatment of fundus diseases. <b>Results:</b> The experimental results show that the effect of model recognition is the best when the Adam is used as optimizer method, the number of iterations is 150, and 0.00 as the learning rate. According to our proposed approach we, achieved the highest accuracy of 93.81% and 91.76% by using ResNet-50 and Inception V3 after fine-tuned and adjusted hyper parameters according to our classification problem. <b>Conclusion:</b> Our research provides a reference to the clinical diagnosis or screening for diabetic retinopathy and other eye diseases. Our suggested computer aided diagnostics framework will prevent incorrect diagnoses caused by the low image quality and individual experience, and other factors. In future implementations, the ophthalmologists can implement more advanced learning algorithms to improve the accuracy of diagnosis.", "keywords": ["Shenzhen University General", "Hospital Shenzhen University", "General Hospital Shenzhen", "Health Management Center", "University General Hospital", "Shenzhen University", "fundus images", "present effective", "field of ophthalmology", "fundus", "aim to present", "macular degeneration", "categorizing fundus images", "tessellated fundus", "Hospital Shenzhen", "Purpose", "Health Management", "University General", "Shenzhen", "classify fundus images"], "paper_title": "Fundus image classification using Inception V3 and ResNet-50 for the early diagnostics of fundus diseases.", "last_updated": "2023/02/04"}, {"id": "0029757338", "domain": "Macular degeneration", "model_name": "Chen et al.", "publication_date": "2017/09/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29757338/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "29757338", "task": "aipbNdPTIt", "abstract": "The choroid plays a critical role in maintaining the portions of the eye responsible for vision. Specific alterations in the choroid have been associated with several disease states, including age-related macular degeneration (AMD), central serous choroiretinopathy, retinitis pigmentosa and diabetes. In addition, choroid thickness measures have been shown as a predictive biomarker for treatment response and visual function. Where several approaches currently exist for segmenting the choroid in optical coherence tomography (OCT) images of healthy retina, very few are capable of addressing images with retinal pathology. The difficulty is due to existing methods relying on first detecting the retinal boundaries before performing the choroidal segmentation. Performance suffers when these boundaries are disrupted or suffer large morphological changes due to disease, and cannot be found accurately. In this work, we show that a learning based approach using convolutional neural networks can allow for the detection and segmentation of the choroid without the prerequisite delineation of the retinal layers. This avoids the need to model and delineate unpredictable pathological changes in the retina due to disease. Experimental validation was performed using 62 manually delineated choroid segmentations of retinal enhanced depth OCT images from patients with AMD. Our results show segmentation accuracy that surpasses those reported by state of the art approaches on healthy retinal images, and overall high values in images with pathology, which are difficult to address by existing methods without pathology specific heuristics.", "keywords": ["responsible for vision", "plays a critical", "critical role", "role in maintaining", "maintaining the portions", "eye responsible", "choroid", "choroid plays", "retinal", "images", "central serous choroiretinopathy", "including age-related macular", "age-related macular degeneration", "AMD", "choroid thickness measures", "due", "choroid segmentations", "disease", "segmentation", "delineated choroid segmentations"], "paper_title": "Automated segmentation of the choroid in EDI-OCT images with retinal pathology using convolution neural networks.", "last_updated": "2023/02/04"}, {"id": "0033282479", "domain": "Macular degeneration", "model_name": "IA-net", "publication_date": "2020/10/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33282479/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "33282479", "task": "aipbNdPTIt", "abstract": "Choroidal neovascularization (CNV) is a characteristic feature of wet age-related macular degeneration (AMD). Quantification of CNV is useful to clinicians in the diagnosis and treatment of CNV disease. Before quantification, CNV lesion should be delineated by automatic CNV segmentation technology. Recently, deep learning methods have achieved significant success for medical image segmentation. However, some CNVs are small objects which are hard to discriminate, resulting in performance degradation. In addition, it's difficult to train an effective network for accurate segmentation due to the complicated characteristics of CNV in OCT images. In order to tackle these two challenges, this paper proposed a novel Informative Attention Convolutional Neural Network (IA-net) for automatic CNV segmentation in OCT images. Considering that the attention mechanism has the ability to enhance the discriminative power of the interesting regions in the feature maps, the attention enhancement block is developed by introducing the additional attention constraint. It has the ability to force the model to pay high attention on CNV in the learned feature maps, improving the discriminative ability of the learned CNV features, which is useful to improve the segmentation performance on small CNV. For accurate pixel classification, the novel informative loss is proposed with the incorporation of an informative attention map. It can focus training on a set of informative samples that are difficult to be predicted. Therefore, the trained model has the ability to learn enough information to classify these informative samples, further improving the performance. The experimental results on our database demonstrate that the proposed method outperforms traditional CNV segmentation methods.", "keywords": ["age-related macular degeneration", "wet age-related macular", "CNV", "CNV segmentation", "automatic CNV segmentation", "Choroidal neovascularization", "macular degeneration", "wet age-related", "age-related macular", "automatic CNV", "CNV segmentation methods", "CNV segmentation technology", "AMD", "Attention", "segmentation", "Informative", "learned CNV features", "OCT images", "traditional CNV segmentation", "Attention Convolutional Neural"], "paper_title": "IA-net: informative attention convolutional neural network for choroidal neovascularization segmentation in OCT images.", "last_updated": "2023/02/04"}, {"id": "0036481530", "domain": "Macular degeneration", "model_name": "Morano et al.", "publication_date": "2022/12/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36481530/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36481530", "task": "IWqQC1koJA", "abstract": "Age-related macular degeneration (AMD) is a degenerative disorder affecting the macula, a key area of the retina for visual acuity. Nowadays, AMD is the most frequent cause of blindness in developed countries. Although some promising treatments have been proposed that effectively slow down its development, their effectiveness significantly diminishes in the advanced stages. This emphasizes the importance of large-scale screening programs for early detection. Nevertheless, implementing such programs for a disease like AMD is usually unfeasible, since the population at risk is large and the diagnosis is challenging. For the characterization of the disease, clinicians have to identify and localize certain retinal lesions. All this motivates the development of automatic diagnostic methods. In this sense, several works have achieved highly positive results for AMD detection using convolutional neural networks (CNNs). However, none of them incorporates explainability mechanisms linking the diagnosis to its related lesions to help clinicians to better understand the decisions of the models. This is specially relevant, since the absence of such mechanisms limits the application of automatic methods in the clinical practice. In that regard, we propose an explainable deep learning approach for the diagnosis of AMD via the joint identification of its associated retinal lesions. In our proposal, a CNN with a custom architectural setting is trained end-to-end for the joint identification of AMD and its associated retinal lesions. With the proposed setting, the lesion identification is directly derived from independent lesion activation maps; then, the diagnosis is obtained from the identified lesions. The training is performed end-to-end using image-level labels. Thus, lesion-specific activation maps are learned in a weakly-supervised manner. The provided lesion information is of high clinical interest, as it allows clinicians to assess the developmental stage of the disease. Additionally, the proposed approach allows to explain the diagnosis obtained by the models directly from the identified lesions and their corresponding activation maps. The training data necessary for the approach can be obtained without much extra work on the part of clinicians, since the lesion information is habitually present in medical records. This is an important advantage over other methods, including fully-supervised lesion segmentation methods, which require pixel-level labels whose acquisition is arduous. The experiments conducted in 4 different datasets demonstrate that the proposed approach is able to identify AMD and its associated lesions with satisfactory performance. Moreover, the evaluation of the lesion activation maps shows that the models trained using the proposed approach are able to identify the pathological areas within the image and, in most cases, to correctly determine to which lesion they correspond. The proposed approach provides meaningful information-lesion identification and lesion activation maps-that conveniently explains and complements the diagnosis, and is of particular interest to clinicians for the diagnostic process. Moreover, the data needed to train the networks using the proposed approach is commonly easy to obtain, what represents an important advantage in fields with particularly scarce data, such as medical imaging.", "keywords": ["Age-related macular degeneration", "degenerative disorder affecting", "AMD", "proposed approach", "lesion", "lesions", "lesion activation maps", "lesion activation", "Age-related macular", "macular degeneration", "affecting the macula", "visual acuity", "proposed", "degenerative disorder", "disorder affecting", "retina for visual", "approach", "retinal lesions", "diagnosis", "activation maps"], "paper_title": "Weakly-supervised detection of AMD-related lesions in color fundus images using explainable deep learning.", "last_updated": "2023/02/04"}, {"id": "0036793539", "domain": "Macular degeneration", "model_name": "ME", "publication_date": "2023/01/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36793539/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36793539", "task": "IWqQC1koJA", "abstract": "A common ocular manifestation, macular edema (ME) is the primary cause of visual deterioration. In this study, an artificial intelligence method based on multi-feature fusion was introduced to enable automatic ME classification on spectral-domain optical coherence tomography (SD-OCT) images, to provide a convenient method of clinical diagnosis. First, 1,213 two-dimensional (2D) cross-sectional OCT images of ME were collected from the Jiangxi Provincial People's Hospital between 2016 and 2021. According to OCT reports of senior ophthalmologists, there were 300 images with diabetic (DME), 303 images with age-related macular degeneration (AMD), 304 images with retinal-vein occlusion (RVO), and 306 images with central serous chorioretinopathy (CSC). Then, traditional omics features of the images were extracted based on the first-order statistics, shape, size, and texture. After extraction by the alexnet, inception_v3, resnet34, and vgg13 models and selected by dimensionality reduction using principal components analysis (PCA), the deep-learning features were fused. Next, the gradient-weighted class-activation map (Grad-CAM) was used to visualize the-deep-learning process. Finally, the fusion features set, which was fused from the traditional omics features and the deep-fusion features, was used to establish the final classification models. The performance of the final models was evaluated by accuracy, confusion matrix, and the receiver operating characteristic (ROC) curve. Compared with other classification models, the performance of the support vector machine (SVM) model was best, with an accuracy of 93.8%. The area under curves AUC of micro- and macro-averages were 99%, and the AUC of the AMD, DME, RVO, and CSC groups were 100, 99, 98, and 100%, respectively. The artificial intelligence model in this study could be used to classify DME, AME, RVO, and CSC accurately from SD-OCT images.", "keywords": ["common ocular manifestation", "Jiangxi Provincial People", "Provincial People Hospital", "ocular manifestation", "visual deterioration", "common ocular", "images", "cross-sectional OCT images", "macular edema", "OCT images", "Jiangxi Provincial", "Provincial People", "People Hospital", "features", "traditional omics features", "RVO", "DME", "models", "CSC", "classification models"], "paper_title": "Artificial intelligence method based on multi-feature fusion for automatic macular edema (ME) classification on spectral-domain optical coherence tomography (SD-OCT) images.", "last_updated": "2023/02/04"}, {"id": "0034749634", "domain": "Diabetic retinopathy", "model_name": "Chen et al.", "publication_date": "2021/11/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34749634/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34749634", "task": "IWqQC1koJA", "abstract": "Doctors can detect symptoms of diabetic retinopathy (DR) early by using retinal ophthalmoscopy, and they can improve diagnostic efficiency with the assistance of deep learning to select treatments and support personnel workflow. Conventionally, most deep learning methods for DR diagnosis categorize retinal ophthalmoscopy images into training and validation data sets according to the 80/20 rule, and they use the synthetic minority oversampling technique (SMOTE) in data processing (e.g., rotating, scaling, and translating training images) to increase the number of training samples. Oversampling training may lead to overfitting of the training model. Therefore, untrained or unverified images can yield erroneous predictions. Although the accuracy of prediction results is 90%-99%, this overfitting of training data may distort training module variables. This study uses a 2-stage training method to solve the overfitting problem. In the training phase, to build the model, the Learning module 1 used to identify the DR and no-DR. The Learning module 2 on SMOTE synthetic datasets to identify the mild-NPDR, moderate NPDR, severe NPDR and proliferative DR classification. These two modules also used early stopping and data dividing methods to reduce overfitting by oversampling. In the test phase, we use the DIARETDB0, DIARETDB1, eOphtha, MESSIDOR, and DRIVE datasets to evaluate the performance of the training network. The prediction accuracy achieved to 85.38%, 84.27%, 85.75%, 86.73%, and 92.5%. Based on the experiment, a general deep learning model for detecting DR was developed, and it could be used with all DR databases. We provided a simple method of addressing the imbalance of DR databases, and this method can be used with other medical images.", "keywords": ["support personnel workflow", "improve diagnostic efficiency", "deep learning", "retinal ophthalmoscopy images", "retinal ophthalmoscopy", "training", "categorize retinal ophthalmoscopy", "Doctors can detect", "diabetic retinopathy", "personnel workflow", "detect symptoms", "symptoms of diabetic", "improve diagnostic", "diagnostic efficiency", "select treatments", "treatments and support", "support personnel", "deep learning methods", "deep learning model", "learning"], "paper_title": "General deep learning model for detecting diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0033686525", "domain": "Disc hemorrhage", "model_name": "Bhardwaj et al.", "publication_date": "2021/03/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33686525/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "33686525", "task": null, "abstract": "The diabetic retinopathy accounts in the deterioration of retinal blood vessels leading to a serious compilation affecting the eyes. The automated DR diagnosis frameworks are critically important for the early identification and detection of these eye-related problems, helping the ophthalmic experts in providing the second opinion for effectual treatment. The deep learning techniques have evolved as an improvement over the conventional approaches, which are dependent on the handcrafted feature extraction. To address the issue of proficient DR discrimination, the authors have proposed a quadrant ensemble automated DR grading approach by implementing InceptionResnet-V2 deep neural network framework. The presented model incorporates histogram equalization, optical disc localization, and quadrant cropping along with the data augmentation step for improving the network performance. A superior accuracy performance of 93.33% is observed for the proposed framework, and a significant reduction of 0.325 is noticed in the cross-entropy loss function for MESSIDOR benchmark dataset; however, its validation utilizing the latest IDRiD dataset establishes its generalization ability. The accuracy improvement of 13.58% is observed when the proposed QEIRV-2 model is compared\u00a0with the classical Inception-V3 CNN model.\u00a0To justify the viability of the proposed framework, its performance is compared with the existing state-of-the-art approaches and 25.23% of accuracy improvement is observed.", "keywords": ["diabetic retinopathy accounts", "retinal blood vessels", "blood vessels leading", "affecting the eyes", "diabetic retinopathy", "retinopathy accounts", "deterioration of retinal", "retinal blood", "blood vessels", "vessels leading", "compilation affecting", "proposed framework", "proposed", "accuracy improvement", "neural network framework", "quadrant ensemble automated", "framework", "deep neural network", "observed", "deep learning techniques"], "paper_title": "Deep Learning-Based Diabetic Retinopathy Severity Grading System Employing Quadrant Ensemble Model.", "last_updated": "2023/02/04"}, {"id": "0036611557", "domain": "Diabetic retinopathy", "model_name": "Bhimavarapu et al.", "publication_date": "2022/12/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36611557/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36611557", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is an eye disease triggered due to diabetes, which may lead to blindness. To prevent diabetic patients from becoming blind, early diagnosis and accurate detection of DR are vital. Deep learning models, such as convolutional neural networks (CNNs), are largely used in DR detection through the classification of blood vessel pixels from the remaining pixels. In this paper, an improved activation function was proposed for diagnosing DR from fundus images that automatically reduces loss and processing time. The DIARETDB0, DRIVE, CHASE, and Kaggle datasets were used to train and test the enhanced activation function in the different CNN models. The ResNet-152 model has the highest accuracy of 99.41% with the Kaggle dataset. This enhanced activation function is suitable for DR diagnosis from retinal fundus images.", "keywords": ["eye disease triggered", "disease triggered due", "due to diabetes", "lead to blindness", "eye disease", "disease triggered", "triggered due", "Diabetic retinopathy", "activation function", "enhanced activation function", "prevent diabetic patients", "enhanced activation", "activation", "prevent diabetic", "diabetic patients", "Diabetic", "function", "retinopathy", "diabetes", "blindness"], "paper_title": "Deep Learning for the Detection and Classification of Diabetic Retinopathy with an Improved Activation Function.", "last_updated": "2023/02/04"}, {"id": "0034837030", "domain": "Diabetic retinopathy", "model_name": "Ryu et al.", "publication_date": "2021/11/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34837030/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34837030", "task": null, "abstract": "As the prevalence of diabetes increases, millions of people need to be screened for diabetic retinopathy (DR). Remarkable advances in technology have made it possible to use artificial intelligence to screen DR from retinal images with high accuracy and reliability, resulting in reducing human labor by processing large amounts of data in a shorter time. We developed a fully automated classification algorithm to diagnose DR and identify referable status using optical coherence tomography angiography (OCTA) images with convolutional neural network (CNN) model and verified its feasibility by comparing its performance with that of conventional machine learning model. Ground truths for classifications were made based on ultra-widefield fluorescein angiography to increase the accuracy of data annotation. The proposed CNN classifier achieved an accuracy of 91-98%, a sensitivity of 86-97%, a specificity of 94-99%, and an area under the curve of 0.919-0.976. In the external validation, overall similar performances were also achieved. The results were similar regardless of the size and depth of the OCTA images, indicating that DR could be satisfactorily classified even with images comprising narrow area of the macular region and a single image slab of retina. The CNN-based classification using OCTA is expected to create a novel diagnostic workflow for DR detection and referral.", "keywords": ["millions of people", "diabetic retinopathy", "prevalence of diabetes", "screened for diabetic", "diabetes increases", "images", "reducing human labor", "processing large amounts", "OCTA images", "OCTA", "CNN classifier achieved", "accuracy", "CNN", "retinal images", "millions", "retinopathy", "proposed CNN classifier", "fully automated classification", "automated classification algorithm", "prevalence"], "paper_title": "A deep learning model for identifying diabetic retinopathy using optical coherence tomography angiography.", "last_updated": "2023/02/04"}, {"id": "0034073541", "domain": "Diabetic retinopathy", "model_name": "Alyoubi et al.", "publication_date": "2021/05/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34073541/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34073541", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a disease resulting from diabetes complications, causing non-reversible damage to retina blood vessels. DR is a leading cause of blindness if not detected early. The currently available DR treatments are limited to stopping or delaying the deterioration of sight, highlighting the importance of regular scanning using high-efficiency computer-based systems to diagnose cases early. The current work presented fully automatic diagnosis systems that exceed manual techniques to avoid misdiagnosis, reducing time, effort and cost. The proposed system classifies DR images into five stages-no-DR, mild, moderate, severe and proliferative DR-as well as localizing the affected lesions on retain surface. The system comprises two deep learning-based models. The first model (CNN512) used the whole image as an input to the CNN model to classify it into one of the five DR stages. It achieved an accuracy of 88.6% and 84.1% on the DDR and the APTOS Kaggle 2019 public datasets, respectively, compared to the state-of-the-art results. Simultaneously, the second model used an adopted YOLOv3 model to detect and localize the DR lesions, achieving a 0.216 mAP in lesion localization on the DDR dataset, which improves the current state-of-the-art results. Finally, both of the proposed structures, CNN512 and YOLOv3, were fused to classify DR images and localize DR lesions, obtaining an accuracy of 89% with 89% sensitivity, 97.3 specificity and that exceeds the current state-of-the-art results.", "keywords": ["causing non-reversible damage", "retina blood vessels", "Diabetic retinopathy", "diabetes complications", "causing non-reversible", "blood vessels", "disease resulting", "resulting from diabetes", "non-reversible damage", "damage to retina", "retina blood", "model", "results", "diagnose cases early", "detected early", "lesions", "current", "high-efficiency computer-based systems", "early", "Diabetic"], "paper_title": "Diabetic Retinopathy Fundus Image Classification and Lesions Localization System Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0036611416", "domain": "Diabetic retinopathy", "model_name": "Mondal et al.", "publication_date": "2022/12/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36611416/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36611416", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is an ophthalmological disease that causes damage in the blood vessels of the eye. DR causes clotting, lesions or haemorrhage in the light-sensitive region of the retina. Person suffering from DR face loss of vision due to the formation of exudates or lesions in the retina. The detection of DR is critical to the successful treatment of patients suffering from DR. The retinal fundus images may be used for the detection of abnormalities leading to DR. In this paper, an automated ensemble deep learning model is proposed for the detection and classification of DR. The ensembling of a deep learning model enables better predictions and achieves better performance than any single contributing model. Two deep learning models, namely modified DenseNet101 and ResNeXt, are ensembled for the detection of diabetic retinopathy. The ResNeXt model is an improvement over the existing ResNet models. The model includes a shortcut from the previous block to next block, stacking layers and adapting split-transform-merge strategy. The model has a cardinality parameter that specifies the number of transformations. The DenseNet model gives better feature use efficiency as the dense blocks perform concatenation. The ensembling of these two models is performed using normalization over the classes followed by maximum a posteriori over the class outputs to compute the final class label. The experiments are conducted on two datasets APTOS19 and DIARETDB1. The classifications are carried out for both two classes and five classes. The images are pre-processed using CLAHE method for histogram equalization. The dataset has a high-class imbalance and the images of the non-proliferative type are very low, therefore, GAN-based augmentation technique is used for data augmentation. The results obtained from the proposed method are compared with other existing methods. The comparison shows that the proposed method has higher accuracy, precision and recall for both two classes and five classes. The proposed method has an accuracy of 86.08 for five classes and 96.98% for two classes. The precision and recall for two classes are 0.97. For five classes also, the precision and recall are high, i.e., 0.76 and 0.82, respectively.", "keywords": ["classes", "deep learning model", "model", "ophthalmological disease", "blood vessels", "learning model", "deep learning", "proposed method", "detection", "models", "method", "proposed", "precision and recall", "Diabetic retinopathy", "learning", "learning model enables", "deep", "precision", "retina", "recall"], "paper_title": "EDLDR: An Ensemble Deep Learning Technique for Detection and Classification of Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0034892084", "domain": "Diabetic retinopathy", "model_name": "Mohamed et al.", "publication_date": "2021/12/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34892084/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34892084", "task": null, "abstract": "Diabetic retinopathy (DR) is one of the most common chronic diseases around the world. Early screening and diagnosis of DR patients through retinal fundus is always preferred. However, image screening and diagnosis is a highly time-consuming task for clinicians. So, there is a high need for automatic diagnosis. The objective of our study is to develop and validate a new automated deep learning-based approach for diabetic retinopathy multi-class detection and classification. In this study we evaluate the contribution of the DR features in each color channel then we pick the most significant channels and calculate their principal components (PCA) which are then fed to the deep learning model, and the grading decision is decided based on a majority voting scheme applied to the out of the deep learning model. The developed models were trained on a publicly available dataset with around 80K color fundus images and were tested on our local dataset with around 100 images. Our results show a significant improvement in DR multi-class classification with 85% accuracy, 89% sensitivity, and 96% specificity.", "keywords": ["common chronic diseases", "common chronic", "chronic diseases", "deep learning model", "Diabetic retinopathy", "screening and diagnosis", "diagnosis", "diabetic retinopathy multi-class", "deep learning", "learning model", "deep", "Early screening", "world", "common", "chronic", "diseases", "screening", "image screening", "Diabetic", "retinopathy"], "paper_title": "Improved Automatic Grading of Diabetic Retinopathy Using Deep Learning and Principal Component Analysis.", "last_updated": "2023/02/04"}, {"id": "0034541004", "domain": "Diabetic retinopathy", "model_name": "Wang et al.", "publication_date": "2021/09/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34541004/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34541004", "task": null, "abstract": "The objective of this study was to establish diagnostic technology to automatically grade the severity of diabetic retinopathy (DR) according to the ischemic index and leakage index with ultra-widefield fluorescein angiography (UWFA) and the Early Treatment Diabetic Retinopathy Study (ETDRS) 7-standard field (7-SF). This is a cross-sectional study. UWFA samples from 280 diabetic patients and 119 normal patients were used to train and test an artificial intelligence model to differentiate PDR and NPDR based on the ischemic index and leakage index with UWFA. A panel of retinal specialists determined the ground truth for our data set before experimentation. A confusion matrix as a metric was used to measure the precision of our algorithm, and a simple linear regression function was implemented to explore the discrimination of indexes on the DR grades. In addition, the model was tested with simulated 7-SF. The model classification of DR in the original UWFA images achieved 88.50% accuracy and 73.68% accuracy in the simulated 7-SF images. A simple linear regression function demonstrated that there is a significant relationship between the ischemic index and leakage index and the severity of DR. These two thresholds were set to classify the grade of DR, which achieved 76.8% accuracy. The optimization of the cycle generative adversarial network (CycleGAN) and convolutional neural network (CNN) model classifier achieved DR grading based on the ischemic index and leakage index with UWFA and simulated 7-SF and provided accurate inference results. The classification accuracy with UWFA is slightly higher than that of simulated 7-SF.", "keywords": ["Early Treatment Diabetic", "Treatment Diabetic Retinopathy", "Diabetic Retinopathy Study", "Early Treatment", "ultra-widefield fluorescein angiography", "diabetic retinopathy", "establish diagnostic technology", "Retinopathy Study", "Treatment Diabetic", "ischemic index", "leakage index", "index", "index and leakage", "UWFA", "retinopathy", "fluorescein angiography", "establish diagnostic", "diagnostic technology", "technology to automatically", "ultra-widefield fluorescein"], "paper_title": "Automated Grading of Diabetic Retinopathy with Ultra-Widefield Fluorescein Angiography and Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0036037631", "domain": "Diabetic retinopathy", "model_name": "Saini et al.", "publication_date": "2022/08/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36037631/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36037631", "task": null, "abstract": "Screening and diagnosis of diabetic retinopathy disease is a well known problem in the biomedical domain. The use of medical imagery from a patient's eye for detecting the damage caused to blood vessels is a part of the computer-aided diagnosis that has immensely progressed over the past few years due to the advent and success of deep learning. The challenges related to imbalanced datasets, inconsistent annotations, less number of sample images and inappropriate performance evaluation metrics has caused an adverse impact on the performance of the deep learning models. In order to tackle the effect caused by class imbalance, we have done extensive comparative analysis between various state-of-the-art methods on three benchmark datasets of diabetic retinopathy: - Kaggle DR detection, IDRiD and DDR, for classification, object detection and segmentation tasks. This research could serve as a concrete baseline for future research in this field to find appropriate approaches and deep learning architectures for imbalanced datasets.", "keywords": ["diabetic retinopathy disease", "biomedical domain", "deep learning", "diabetic retinopathy", "deep learning models", "retinopathy disease", "deep learning architectures", "Screening and diagnosis", "imbalanced datasets", "computer-aided diagnosis", "deep", "learning", "diagnosis", "caused", "datasets", "Screening", "domain", "diabetic", "learning models", "diagnosis of diabetic"], "paper_title": "Diabetic retinopathy screening using deep learning for multi-class imbalanced datasets.", "last_updated": "2023/02/04"}, {"id": "0035624922", "domain": "Diabetic retinopathy", "model_name": "Jabbar et al.", "publication_date": "2022/04/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35624922/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35624922", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a visual obstacle caused by diabetic disease, which forms because of long-standing diabetes mellitus, which damages the retinal blood vessels. This disease is considered one of the principal causes of sightlessness and accounts for more than 158 million cases all over the world. Since early detection and classification could diminish the visual impairment, it is significant to develop an automated DR diagnosis method. Although deep learning models provide automatic feature extraction and classification, training such models from scratch requires a larger annotated dataset. The availability of annotated training datasets is considered a core issue for implementing deep learning in the classification of medical images. The models based on transfer learning are widely adopted by the researchers to overcome annotated data insufficiency problems and computational overhead. In the proposed study, features are extracted from fundus images using the pre-trained network VGGNet and combined with the concept of transfer learning to improve classification performance. To deal with data insufficiency and unbalancing problems, we employed various data augmentation operations differently on each grade of DR. The results of the experiment indicate that the proposed framework (which is evaluated on the benchmark dataset) outperformed advanced methods in terms of accurateness. Our technique, in combination with handcrafted features, could be used to improve classification accuracy.", "keywords": ["long-standing diabetes mellitus", "retinal blood vessels", "visual obstacle caused", "Diabetic retinopathy", "diabetes mellitus", "blood vessels", "diabetic disease", "obstacle caused", "long-standing diabetes", "damages the retinal", "retinal blood", "caused by diabetic", "visual obstacle", "Diabetic", "classification", "learning", "deep learning", "improve classification", "transfer learning", "deep learning models"], "paper_title": "Transfer Learning-Based Model for Diabetic Retinopathy Diagnosis Using Retinal Images.", "last_updated": "2023/02/04"}, {"id": "0035493724", "domain": "Diabetic retinopathy", "model_name": "Chandrasekaran et al.", "publication_date": "2022/04/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35493724/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35493724", "task": null, "abstract": "Recent developments reveal the prominence of Diabetic Retinopathy (DR) grading. In the past few decades, Wavelet-based DR classification has shown successful impacts and the Deep Learning models, like Convolutional Neural Networks (CNN's), have evolved in offering the highest prediction accuracy. In this work, the features of the input image are enhanced with the integration of Multi-Resolution Analysis (MRA) and a CNN framework without costing more convolution filters. The bottleneck with conventional activation functions, used in CNN's, is the nullification of the feature maps that are negative in value. In this work, a novel Hyper-analytic Wavelet (<b>HW) phase activation function</b> is formulated with unique characteristics for the wavelet sub-bands. Instead of dismissal, the function transforms these negative coefficients that correspond to significant edge feature maps<b>.</b> The hyper-analytic wavelet phase forms the imaginary part of the complex activation. And the hyper-parameter of the activation function is selected such that the corresponding magnitude spectrum produces monotonic and effective activations. The performance of 3 CNN models (1 custom, shallow CNN, ResNet with Soft attention, Alex Net for DR) with spatial-Wavelet quilts is better. With the spatial-Wavelet quilts, the Alex Net for DR has an improvement with an 11% of accuracy level (from 87 to 98%). The highest accuracy level of 98% and the highest Sensitivity of 99% are attained through Modified Alex Net for DR. The proposal also illustrates the visualization of the negative edge preservation with assumed image patches. From this study, the researcher infers that models with spatial-Wavelet quilts, with the hyper-analytic activations, have better generalization ability. And the visualization of heat maps provides evidence of better learning of the feature maps from the wavelet sub-bands.", "keywords": ["Diabetic Retinopathy", "Recent developments reveal", "Convolutional Neural Networks", "prominence of Diabetic", "Alex Net", "CNN", "Recent developments", "developments reveal", "reveal the prominence", "Modified Alex Net", "Neural Networks", "Convolutional Neural", "feature maps", "Deep Learning models", "Hyper-analytic Wavelet", "activation function", "Retinopathy", "Diabetic", "hyper-analytic wavelet phase", "phase activation function"], "paper_title": "Retinopathy grading with deep learning and wavelet hyper-analytic activations.", "last_updated": "2023/02/04"}, {"id": "0035665292", "domain": "Glaucoma (unspecified)", "model_name": "Gundluru et al.", "publication_date": "2022/05/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35665292/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35665292", "task": "IWqQC1koJA", "abstract": "In today's world, diabetic retinopathy is a very severe health issue, which is affecting many humans of different age groups. Due to the high levels of blood sugar, the minuscule blood vessels in the retina may get damaged in no time and further may lead to retinal detachment and even sometimes lead to glaucoma blindness. If diabetic retinopathy can be diagnosed at the early stages, then many of the affected people will not be losing their vision and also human lives can be saved. Several machine learning and deep learning methods have been applied on the available data sets of diabetic retinopathy, but they were unable to provide the better results in terms of accuracy in preprocessing and optimizing the classification and feature extraction process. To overcome the issues like feature extraction and optimization in the existing systems, we have considered the Diabetic Retinopathy Debrecen Data Set from the UCI machine learning repository and designed a deep learning model with principal component analysis (PCA) for dimensionality reduction, and to extract the most important features, Harris hawks optimization algorithm is used further to optimize the classification and feature extraction process. The results shown by the deep learning model with respect to specificity, precision, accuracy, and recall are very much satisfactory compared to the existing systems.", "keywords": ["severe health issue", "diabetic retinopathy", "Diabetic Retinopathy Debrecen", "today world", "age groups", "severe health", "feature extraction process", "Retinopathy Debrecen Data", "deep learning model", "feature extraction", "deep learning", "Debrecen Data Set", "UCI machine learning", "diabetic", "retinopathy", "learning", "health issue", "extraction process", "machine learning", "minuscule blood vessels"], "paper_title": "Enhancement of Detection of Diabetic Retinopathy Using Harris Hawks Optimization with Deep Learning Model.", "last_updated": "2023/02/04"}, {"id": "0034325853", "domain": "Diabetic retinopathy", "model_name": "CARE", "publication_date": "2021/08/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34325853/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34325853", "task": null, "abstract": "Medical artificial intelligence (AI) has entered the clinical implementation phase, although real-world performance of deep-learning systems (DLSs) for screening fundus disease remains unsatisfactory. Our study aimed to train a clinically applicable DLS for fundus diseases using data derived from the real world, and externally test the model using fundus photographs collected prospectively from the settings in which the model would most likely be adopted. In this national real-world evidence study, we trained a DLS, the Comprehensive AI Retinal Expert (CARE) system, to identify the 14 most common retinal abnormalities using 207\u2008228 colour fundus photographs derived from 16 clinical settings with different disease distributions. CARE was internally validated using 21\u2008867 photographs and externally tested using 18\u2008136 photographs prospectively collected from 35 real-world settings across China where CARE might be adopted, including eight tertiary hospitals, six community hospitals, and 21 physical examination centres. The performance of CARE was further compared with that of 16 ophthalmologists and tested using datasets with non-Chinese ethnicities and previously unused camera types. This study was registered with ClinicalTrials.gov, NCT04213430, and is currently closed. The area under the receiver operating characteristic curve (AUC) in the internal validation set was 0\u00b7955 (SD 0\u00b7046). AUC values in the external test set were 0\u00b7965 (0\u00b7035) in tertiary hospitals, 0\u00b7983 (0\u00b7031) in community hospitals, and 0\u00b7953 (0\u00b7042) in physical examination centres. The performance of CARE was similar to that of ophthalmologists. Large variations in sensitivity were observed among the ophthalmologists in different regions and with varying experience. The system retained strong identification performance when tested using the non-Chinese dataset (AUC 0\u00b7960, 95% CI 0\u00b7957-0\u00b7964 in referable diabetic retinopathy). Our DLS (CARE) showed satisfactory performance for screening multiple retinal abnormalities in real-world settings using prospectively collected fundus photographs, and so could allow the system to be implemented and adopted for clinical care. This study was funded by the National Key R&D Programme of China, the Science and Technology Planning Projects of Guangdong Province, the National Natural Science Foundation of China, the Natural Science Foundation of Guangdong Province, and the Fundamental Research Funds for the Central Universities. For the Chinese translation of the abstract see Supplementary Materials section.", "keywords": ["Medical artificial intelligence", "disease remains unsatisfactory", "fundus disease remains", "clinical implementation phase", "fundus photographs", "CARE", "Natural Science Foundation", "fundus photographs derived", "fundus photographs collected", "collected fundus photographs", "Medical artificial", "artificial intelligence", "implementation phase", "remains unsatisfactory", "screening fundus disease", "fundus", "prospectively collected fundus", "colour fundus photographs", "clinically applicable DLS", "National Natural Science"], "paper_title": "Application of Comprehensive Artificial intelligence Retinal Expert (CARE) system: a national real-world evidence study.", "last_updated": "2023/02/04"}, {"id": "0031946303", "domain": "Diabetic retinopathy", "model_name": "Jiang et al.", "publication_date": "2020/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31946303/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31946303", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is one kind of eye disease that is caused by overtime diabetes. Lots of patients around the world suffered from DR which may bring about blindness. Early detection of DR is a rigid quest which can remind the DR patients to seek corresponding treatments in time. This paper presents an automatic image-level DR detection system using multiple well-trained deep learning models. Besides, several deep learning models are integrated using the Adaboost algorithm in order to reduce the bias of each single model. To explain the results of DR detection, this paper provides weighted class activation maps (CAMs) that can illustrate the suspected position of lesions. In the pre-processing stage, eight image transformation ways are also introduced to help augment the diversity of fundus images. Experiments demonstrate that the method proposed by this paper has stronger robustness and acquires more excellent performance than that of individual deep learning model.", "keywords": ["Diabetic retinopathy", "overtime diabetes", "kind of eye", "eye disease", "caused by overtime", "deep learning models", "deep learning", "learning models", "learning", "well-trained deep learning", "deep", "detection", "paper", "Diabetic", "retinopathy", "diabetes", "individual deep learning", "kind", "eye", "disease"], "paper_title": "An Interpretable Ensemble Deep Learning Model for Diabetic Retinopathy Disease Classification.", "last_updated": "2023/02/04"}, {"id": "0033018290", "domain": "Diabetic retinopathy", "model_name": "Jiang et al.", "publication_date": "2020/10/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33018290/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33018290", "task": "IWqQC1koJA", "abstract": "The characteristics of diabetic retinopathy (DR) fundus images generally consist of multiple types of lesions which provided strong evidence for the ophthalmologists to make diagnosis. It is particularly significant to figure out an efficient method to not only accurately classify DR fundus images but also recognize all kinds of lesions on them. In this paper, a deep learning-based multi-label classification model with Gradient-weighted Class Activation Mapping (Grad-CAM) was proposed, which can both make DR classification and automatically locate the regions of different lesions. To reducing laborious annotation work and improve the efficiency of labeling, this paper innovatively considered different types of lesions as different labels for a fundus image so that this paper changed the task of lesion detection into that of image classification. A total of five labels were pre-defined and 3228 fundus images were collected for developing our model. The architecture of deep learning model was designed by ourselves based on ResNet. Through experiments on the test images, this method acquired a sensitive of 93.9% and a specificity of 94.4% on DR classification. Moreover, the corresponding regions of lesions were reasonably outlined on the DR fundus images.", "keywords": ["provided strong evidence", "images generally consist", "fundus images generally", "Class Activation Mapping", "Gradient-weighted Class Activation", "fundus images", "diabetic retinopathy", "characteristics of diabetic", "generally consist", "consist of multiple", "provided strong", "strong evidence", "fundus", "lesions", "make diagnosis", "images", "images generally", "types of lesions", "Activation Mapping", "multiple types"], "paper_title": "A Multi-Label Deep Learning Model with Interpretable Grad-CAM for Diabetic Retinopathy Classification.", "last_updated": "2023/02/04"}, {"id": "0034591173", "domain": "Diabetic retinopathy", "model_name": "Zhang et al.", "publication_date": "2021/09/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34591173/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34591173", "task": "IWqQC1koJA", "abstract": "The purpose of this study is to develop and validate the intelligent diagnosis of severe DR with lesion recognition based on color fundus photography. The Kaggle public dataset for DR grading is used in the project, including 53,576 fundus photos in the test set, 28,101 in the training set, and 7,025 in the validation set. We randomly select 4,192 images for lesion annotation. Inception V3 structure is adopted as the classification algorithm. Both 299\u2009\u00d7\u2009299 pixel images and 896\u2009\u00d7\u2009896 pixel images are used as the input size. ROC curve, AUC, sensitivity, specificity, and their harmonic mean are used to evaluate the performance of the models. The harmonic mean and AUC of the model of 896\u2009\u00d7\u2009896 input are higher than those of the 299\u2009\u00d7\u2009299 input model. The sensitivity, specificity, harmonic mean, and AUC of the method with 896\u2009\u00d7\u2009896 resolution images as input for severe DR are 0.925, 0.907, 0.916, and 0.968, respectively. The prediction error mainly occurs in moderate NPDR, and cases with more hard exudates and cotton wool spots are easily predicted as severe cases. Cases with preretinal hemorrhage and vitreous hemorrhage are easily identified as severe cases, and IRMA is the most difficult lesion to recognize. We have studied the intelligent diagnosis of severe DR based on color fundus photography. This artificial intelligence-based technology offers a possibility to increase the accessibility and efficiency of severe DR screening.", "keywords": ["develop and validate", "color fundus photography", "lesion recognition based", "severe", "Kaggle public dataset", "fundus photography", "color fundus", "lesion recognition", "fundus", "AUC", "severe cases", "set", "images", "Kaggle public", "validate the intelligent", "recognition based", "cases", "pixel images", "input", "lesion"], "paper_title": "Automated detection of severe diabetic retinopathy using deep learning method.", "last_updated": "2023/02/04"}, {"id": "0035845987", "domain": "Diabetic retinopathy", "model_name": "Zhang et al.", "publication_date": "2022/07/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35845987/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35845987", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is one of the most threatening complications in diabetic patients, leading to permanent blindness without timely treatment. However, DR screening is not only a time-consuming task that requires experienced ophthalmologists but also easy to produce misdiagnosis. In recent years, deep learning techniques based on convolutional neural networks have attracted increasing research attention in medical image analysis, especially for DR diagnosis. However, dataset labeling is expensive work and it is necessary for existing deep-learning-based DR detection models. For this study, a novel domain adaptation method (multi-model domain adaptation) is developed for unsupervised DR classification in unlabeled retinal images. At the same time, it only exploits discriminative information from multiple source models without access to any data. In detail, we integrate a weight mechanism into the multi-model-based domain adaptation by measuring the importance of each source domain in a novel way, and a weighted pseudo-labeling strategy is attached to the source feature extractors for training the target DR classification model. Extensive experiments are performed on four source datasets (DDR, IDRiD, Messidor, and Messidor-2) to a target domain APTOS 2019, showing that MMDA produces competitive performance for present state-of-the-art methods for DR classification. As a novel DR detection approach, this article presents a new domain adaptation solution for medical image analysis when the source data is unavailable.", "keywords": ["Diabetic retinopathy", "diabetic patients", "leading to permanent", "timely treatment", "threatening complications", "permanent blindness", "blindness without timely", "complications in diabetic", "Diabetic", "domain adaptation", "domain", "source", "adaptation", "medical image analysis", "medical image", "image analysis", "classification", "retinopathy", "patients", "leading"], "paper_title": "Multi-Model Domain Adaptation for Diabetic Retinopathy Classification.", "last_updated": "2023/02/04"}, {"id": "0035430558", "domain": "Diabetic retinopathy", "model_name": "Toledo-Cort\u00e9s et al.", "publication_date": "2022/04/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35430558/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35430558", "task": "IWqQC1koJA", "abstract": "Although for many diseases there is a progressive diagnosis scale, automatic analysis of grade-based medical images is quite often addressed as a binary classification problem, missing the finer distinction and intrinsic relation between the different possible stages or grades. Ordinal regression (or classification) considers the order of the values of the categorical labels and thus takes into account the order of grading scales used to assess the severity of different medical conditions. This paper presents a quantum-inspired deep probabilistic learning ordinal regression model for medical image diagnosis that takes advantage of the representational power of deep learning and the intrinsic ordinal information of disease stages. The method is evaluated on two different medical image analysis tasks: prostate cancer diagnosis and diabetic retinopathy grade estimation on eye fundus images. The experimental results show that the proposed method not only improves the diagnosis performance on the two tasks but also the interpretability of the results by quantifying the uncertainty of the predictions in comparison to conventional deep classification and regression architectures. The code and datasets are available at https://github.com/stoledoc/DQOR.", "keywords": ["binary classification problem", "grade-based medical images", "progressive diagnosis scale", "medical image diagnosis", "medical image analysis", "missing the finer", "finer distinction", "medical image", "grade-based medical", "classification problem", "binary classification", "learning ordinal regression", "Ordinal regression", "intrinsic relation", "automatic analysis", "image analysis tasks", "progressive diagnosis", "intrinsic ordinal information", "medical", "ordinal regression model"], "paper_title": "Grading diabetic retinopathy and prostate cancer diagnostic images with deep quantum ordinal regression.", "last_updated": "2023/02/04"}, {"id": "0036086040", "domain": "Diabetic retinopathy", "model_name": "Karakaya et al.", "publication_date": "2022/09/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36086040/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36086040", "task": "IWqQC1koJA", "abstract": "Convolutional Neural Networks (CNNs) are an emerging research area for detection of Diabetic Retinopathy (DR) development in fundus images with highly reliable results. However, its accuracy depends on the availability of big datasets to train such a deep network. Due to the privacy concerns, the strict rules on medical data limit accessibility of images in publicly available datasets. In this paper, we propose a collaborative learning approach to train CNN models with multiple datasets while preserving the privacy of datasets in a distributed learning environment without sharing them. First, CNN networks are trained with private datasets, and tested with the same publicly available images. Based on their initial accuracies, the CNN model with the lowest performance among datasets is forwarded to second lowest performed dataset to retrain it using the transfer learning approach. Then, the retrained network is forwarded to next dataset. This procedure is repeated for each dataset from the lowest performed dataset to the highest. With this ascending chain order fashion, the network is retrained again and again using different datasets and its performance is improved over the time. Based on our experimental results on five different retina image datasets, DR detection accuracy is increased to 93.5% compared with the accuracies of merged datasets (84%) and individual datasets (73%, 78%, 83%, 85%).", "keywords": ["Diabetic Retinopathy", "Convolutional Neural Networks", "emerging research area", "Convolutional Neural", "Neural Networks", "highly reliable results", "datasets", "development in fundus", "emerging research", "research area", "highly reliable", "Retinopathy", "Neural", "Diabetic", "detection of Diabetic", "CNN", "train CNN models", "fundus images", "CNN networks", "dataset"], "paper_title": "Collaborative Deep Learning for Privacy Preserving Diabetic Retinopathy Detection.", "last_updated": "2023/02/04"}, {"id": "0034033015", "domain": "Diabetic retinopathy", "model_name": "Gayathri et al.", "publication_date": "2021/05/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34033015/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34033015", "task": "IWqQC1koJA", "abstract": "Eye care professionals generally use fundoscopy to confirm the occurrence of Diabetic Retinopathy (DR) in patients. Early DR detection and accurate DR grading are critical for the care and management of this disease. This work proposes an automated DR grading method in which features can be extracted from the fundus images and categorized based on severity using deep learning and Machine Learning (ML) algorithms. A Multipath Convolutional Neural Network (M-CNN) is used for global and local feature extraction from images. Then, a machine learning classifier is used to categorize the input according to the severity. The proposed model is evaluated across different publicly available databases (IDRiD, Kaggle (for DR detection), and MESSIDOR) and different ML classifiers (Support Vector Machine (SVM), Random Forest, and J48). The metrics selected for model evaluation are the False Positive Rate (FPR), Specificity, Precision, Recall, F1-score, K-score, and Accuracy. The experiments show that the best response is produced by the M-CNN network with the J48 classifier. The classifiers are evaluated across the pre-trained network features and existing DR grading methods. The average accuracy obtained for the proposed work is 99.62% for DR grading. The experiments and evaluation results show that the proposed method works well for accurate DR grading and early disease detection.", "keywords": ["Diabetic Retinopathy", "Eye care professionals", "care professionals generally", "occurrence of Diabetic", "professionals generally", "generally use fundoscopy", "fundoscopy to confirm", "confirm the occurrence", "Multipath Convolutional Neural", "Eye care", "Support Vector Machine", "care professionals", "Machine Learning", "machine learning classifier", "Convolutional Neural Network", "Retinopathy", "Diabetic", "grading", "False Positive Rate", "Machine"], "paper_title": "Diabetic retinopathy classification based on multipath CNN and machine learning classifiers.", "last_updated": "2023/02/04"}, {"id": "0035002666", "domain": "Diabetic retinopathy", "model_name": "Ai et al.", "publication_date": "2021/12/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35002666/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35002666", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is one of the common chronic complications of diabetes and the most common blinding eye disease. If not treated in time, it might lead to visual impairment and even blindness in severe cases. Therefore, this article proposes an algorithm for detecting diabetic retinopathy based on deep ensemble learning and attention mechanism. First, image samples were preprocessed and enhanced to obtain high quality image data. Second, in order to improve the adaptability and accuracy of the detection algorithm, we constructed a holistic detection model DR-IIXRN, which consists of Inception V3, InceptionResNet V2, Xception, ResNeXt101, and NASNetLarge. For each base classifier, we modified the network model using transfer learning, fine-tuning, and attention mechanisms to improve its ability to detect DR. Finally, a weighted voting algorithm was used to determine which category (normal, mild, moderate, severe, or proliferative DR) the images belonged to. We also tuned the trained network model on the hospital data, and the real test samples in the hospital also confirmed the advantages of the algorithm in the detection of the diabetic retina. Experiments show that compared with the traditional single network model detection algorithm, the auc, accuracy, and recall rate of the proposed method are improved to 95, 92, and 92%, respectively, which proves the adaptability and correctness of the proposed method.", "keywords": ["common chronic complications", "common blinding eye", "blinding eye disease", "common chronic", "common blinding", "eye disease", "chronic complications", "complications of diabetes", "blinding eye", "Diabetic retinopathy", "detecting diabetic retinopathy", "diabetic retinopathy based", "common", "network model", "algorithm", "model detection algorithm", "detection algorithm", "network model detection", "model", "detection"], "paper_title": "DR-IIXRN : Detection Algorithm of Diabetic Retinopathy Based on Deep Ensemble Learning and Attention Mechanism.", "last_updated": "2023/02/04"}, {"id": "0036140666", "domain": "Diabetic retinopathy", "model_name": "Mujeeb Rahman et al.", "publication_date": "2022/09/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36140666/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36140666", "task": null, "abstract": "Diabetic Retinopathy is a vision impairment caused by blood vessel degeneration in the retina. It is becoming more widespread as it is linked to diabetes. Diabetic retinopathy can lead to blindness. Early detection of diabetic retinopathy by an ophthalmologist can help avoid vision loss and other complications. Diabetic retinopathy is currently diagnosed by visually recognizing irregularities on fundus pictures. This procedure, however, necessitates the use of ophthalmic imaging technologies to acquire fundus images as well as a detailed visual analysis of the stored photos, resulting in a costly and time-consuming diagnosis. The fundamental goal of this project is to create an easy-to-use machine learning model tool that can accurately predict diabetic retinopathy using pre-recorded digital fundus images. To create the suggested classifier model, we gathered annotated fundus images from publicly accessible data repositories and used two machine learning methods, support vector machine (SVM) and deep neural network (DNN). On test data, the proposed SVM model had a mean area under the receiver operating characteristic curve (AUC) of 97.11%, whereas the DNN model had a mean AUC of 99.15%.", "keywords": ["blood vessel degeneration", "vision impairment caused", "Diabetic Retinopathy", "impairment caused", "caused by blood", "blood vessel", "vessel degeneration", "Retinopathy", "Diabetic", "vision impairment", "fundus images", "fundus", "predict diabetic retinopathy", "model", "images", "retina", "machine", "SVM", "DNN", "impairment"], "paper_title": "Automatic Screening of Diabetic Retinopathy Using Fundus Images and Machine Learning Algorithms.", "last_updated": "2023/02/04"}, {"id": "0030570648", "domain": "Glaucoma (unspecified)", "model_name": "Keel et al.", "publication_date": "2019/11/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30570648/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30570648", "task": "IWqQC1koJA", "abstract": "Convolutional neural networks have recently been applied to ophthalmic diseases; however, the rationale for the outputs generated by these systems is inscrutable to clinicians. A visualization tool is needed that would enable clinicians to understand important exposure variables in real time. To systematically visualize the convolutional neural networks of 2 validated deep learning models for the detection of referable diabetic retinopathy (DR) and glaucomatous optic neuropathy (GON). The GON and referable DR algorithms were previously developed and validated (holdout method) using 48\u202f116 and 66\u202f790 retinal photographs, respectively, derived from a third-party database (LabelMe) of deidentified photographs from various clinical settings in China. In the present cross-sectional study, a random sample of 100 true-positive photographs and all false-positive cases from each of the GON and DR validation data sets were selected. All data were collected from March to June 2017. The original color fundus images were processed using an adaptive kernel visualization technique. The images were preprocessed by applying a sliding window with a size of 28\u2009\u00d7\u200928 pixels and a stride of 3 pixels to crop images into smaller subimages to produce a feature map. Threshold scales were adjusted to optimal levels for each model to generate heat maps highlighting localized landmarks on the input image. A single optometrist allocated each image to predefined categories based on the generated heat map. Visualization regions of the fundus. In the GON data set, 90 of 100 true-positive cases (90%; 95% CI, 82%-95%) and 15 of 22 false-positive cases (68%; 95% CI, 45%-86%) displayed heat map visualization within regions of the optic nerve head only. Lesions typically seen in cases of referable DR (exudate, hemorrhage, or vessel abnormality) were identified as the most important prognostic regions in 96 of 100 true-positive DR cases (96%; 95% CI, 90%-99%). In 39 of 46 false-positive DR cases (85%; 95% CI, 71%-94%), the heat map displayed visualization of nontraditional fundus regions with or without retinal venules. These findings suggest that this visualization method can highlight traditional regions in disease diagnosis, substantiating the validity of the deep learning models investigated. This visualization technique may promote the clinical adoption of these models.", "keywords": ["Convolutional neural networks", "recently been applied", "applied to ophthalmic", "systems is inscrutable", "GON", "Convolutional neural", "neural networks", "visualization", "GON data set", "cases", "heat map", "regions", "GON data", "map", "heat", "outputs generated", "inscrutable to clinicians", "referable", "ophthalmic diseases", "clinicians"], "paper_title": "Visualizing Deep Learning Models for the Detection of Referable Diabetic Retinopathy and Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0034399196", "domain": "Diabetic retinopathy", "model_name": "Garifullin et al.", "publication_date": "2021/08/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34399196/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34399196", "task": null, "abstract": "Early diagnosis of retinopathy is essential for preventing retinal complications and visual impairment due to diabetes. For the detection of retinopathy lesions from retinal images, several automatic approaches based on deep neural networks have been developed in the recent years. Most of the proposed methods produce point estimates of pixels belonging to the lesion areas and give no or little information on the uncertainty of method predictions. However, the latter can be essential in the examination of the medical condition of the patient when the goal is early detection of abnormalities. This work extends the recent research with a Bayesian framework by considering the parameters of a convolutional neural network as random variables and utilizing stochastic variational dropout based approximation for uncertainty quantification. The framework includes an extended validation procedure and it allows analyzing lesion segmentation distributions, model calibration and prediction uncertainties. Also the challenges related to the deep probabilistic model and uncertainty quantification are presented. The proposed method achieves area under precision-recall curve of 0.84 for hard exudates, 0.641 for soft exudates, 0.593 for haemorrhages, and 0.484 for microaneurysms on IDRiD dataset.", "keywords": ["visual impairment due", "preventing retinal complications", "due to diabetes", "complications and visual", "visual impairment", "impairment due", "diagnosis of retinopathy", "preventing retinal", "retinal complications", "Early diagnosis", "retinopathy lesions", "detection of retinopathy", "deep neural networks", "retinal images", "automatic approaches based", "uncertainty quantification", "retinopathy", "retinal", "essential for preventing", "convolutional neural network"], "paper_title": "Deep Bayesian baseline for segmenting diabetic retinopathy lesions: Advances and challenges.", "last_updated": "2023/02/04"}, {"id": "0036246944", "domain": "Diabetic retinopathy", "model_name": "federated-learning-oct", "publication_date": "2021/10/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36246944/", "code_link": "https://github.com/borg-sfu/federated-learning-oct", "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "36246944", "task": "aipbNdPTIt", "abstract": "To evaluate the performance of a federated learning framework for deep neural network-based retinal microvasculature segmentation and referable diabetic retinopathy (RDR) classification using OCT and OCT angiography (OCTA). Retrospective analysis of clinical OCT and OCTA scans of control participants and patients with diabetes. The 153 OCTA en face images used for microvasculature segmentation were acquired from 4 OCT instruments with fields of view ranging from 2 \u00d7 2-mm to 6 \u00d7 6-mm. The 700 eyes used for RDR classification consisted of OCTA en face images and structural OCT projections acquired from 2 commercial OCT systems. OCT angiography images used for microvasculature segmentation were delineated manually and verified by retina experts. Diabetic retinopathy (DR) severity was evaluated by retinal specialists and was condensed into 2 classes: non-RDR and RDR. The federated learning configuration was demonstrated via simulation using 4 clients for microvasculature segmentation and was compared with other collaborative training methods. Subsequently, federated learning was applied over multiple institutions for RDR classification and was compared with models trained and tested on data from the same institution (internal models) and different institutions (external models). For microvasculature segmentation, we measured the accuracy and Dice similarity coefficient (DSC). For severity classification, we measured accuracy, area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve, balanced accuracy, F1 score, sensitivity, and specificity. For both applications, federated learning achieved similar performance as internal models. Specifically, for microvasculature segmentation, the federated learning model achieved similar performance (mean DSC across all test sets, 0.793) as models trained on a fully centralized dataset (mean DSC, 0.807). For RDR classification, federated learning achieved a mean AUROC of 0.954 and 0.960; the internal models attained a mean AUROC of 0.956 and 0.973. Similar results are reflected in the other calculated evaluation metrics. Federated learning showed similar results to traditional deep learning in both applications of segmentation and classification, while maintaining data privacy. Evaluation metrics highlight the potential of collaborative learning for increasing domain diversity and the generalizability of models used for the classification of OCT data.", "keywords": ["microvasculature segmentation", "OCT", "federated learning", "RDR classification", "neural network-based retinal", "RDR", "OCTA", "segmentation", "OCT angiography", "learning", "microvasculature", "deep neural network-based", "federated", "OCT angiography images", "network-based retinal microvasculature", "federated learning framework", "neural network-based", "classification", "models", "retinal microvasculature segmentation"], "paper_title": "Federated Learning for Microvasculature Segmentation and Diabetic Retinopathy Classification of OCT Data.", "last_updated": "2023/02/04"}, {"id": "0035703566", "domain": "Diabetic retinopathy", "model_name": "Ryu et al.", "publication_date": "2022/06/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35703566/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35703566", "task": "IWqQC1koJA", "abstract": "To develop an automated diabetic retinopathy (DR) staging system using optical coherence tomography angiography (OCTA) images with a convolutional neural network (CNN) and to verify the feasibility of the system. In this retrospective cross-sectional study, a total of 918 data sets of 3 \u00d7 3 mm2 OCTA images and 917 data sets of 6 \u00d7 6 mm2 OCTA images were obtained from 1118 eyes. A deep CNN and four traditional machine learning models were trained with annotations made by a retinal specialist based on ultra-widefield fluorescein angiography. Separately, the same images of the test data sets were independently graded by two human experts. The results of the CNN algorithm were compared with those of traditional machine learning-based classifiers and human experts. The proposed CNN achieved an accuracy of 0.728, a sensitivity of 0.675, a specificity of 0.944, an F1 score of 0.683, and a quadratic weighted \u03ba of 0.908 for a six-level staging task, which were far superior to the results of traditional machine learning methods or human experts. The CNN algorithm showed a better performance using 6 \u00d7 6 mm2 rather than 3 \u00d7 3 mm2 sized OCTA images and using combined data rather than a separate OCTA layer alone. CNN-based classification using OCTA images can provide reliable assistance to clinicians for DR classification. This CNN algorithm can guide the clinical decision for invasive angiography or referrals to ophthalmology specialists, helping to create more efficient diagnostic workflow in primary care settings.", "keywords": ["automated diabetic retinopathy", "convolutional neural network", "optical coherence tomography", "coherence tomography angiography", "OCTA images", "OCTA", "diabetic retinopathy", "neural network", "CNN", "CNN algorithm", "develop an automated", "automated diabetic", "optical coherence", "coherence tomography", "convolutional neural", "verify the feasibility", "data sets", "images", "human experts", "traditional machine learning"], "paper_title": "A Deep Learning Algorithm for Classifying Diabetic Retinopathy Using Optical Coherence Tomography Angiography.", "last_updated": "2023/02/04"}, {"id": "0035859243", "domain": "Diabetic retinopathy", "model_name": "Ahsan et al.", "publication_date": "2022/07/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35859243/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35859243", "task": "IWqQC1koJA", "abstract": "In recent years, deep learning (DL) techniques have provided state-of-the-art performance in medical imaging. However, good quality (annotated) medical data is in general hard to find due to the usually high cost of medical images, limited availability of expert annotators (e.g., radiologists), and the amount of time required for annotation. In addition, DL is data-hungry and its training requires extensive computational resources. Furthermore, DL being a black-box method lacks transparency on its inner working and lacks fundamental understanding behind decisions made by the model and consequently, this notion enhances the uncertainty on its predictions. To this end, we address these challenges by proposing a hybrid model, which uses a Bayesian convolutional neural network (BCNN) for uncertainty quantification, and an active learning approach for annotating the unlabeled data. The BCNN is used as a feature descriptor and these features are then used for training a model, in an active learning setting. We evaluate the proposed framework for diabetic retinopathy classification problem and demonstrate state-of-the-art performance in terms of different metrics.", "keywords": ["techniques have provided", "recent years", "medical imaging", "deep learning", "medical", "medical images", "active learning", "years", "deep", "techniques", "provided", "imaging", "medical data", "good quality", "learning", "model", "performance", "BCNN", "recent", "limited availability"], "paper_title": "An active learning method for diabetic retinopathy classification with uncertainty quantification.", "last_updated": "2023/02/04"}, {"id": "0035204628", "domain": "Diabetic retinopathy", "model_name": "Nneji et al.", "publication_date": "2022/02/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35204628/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35204628", "task": null, "abstract": "It is a well-known fact that diabetic retinopathy (DR) is one of the most common causes of visual impairment between the ages of 25 and 74 around the globe. Diabetes is caused by persistently high blood glucose levels, which leads to blood vessel aggravations and vision loss. Early diagnosis can minimise the risk of proliferated diabetic retinopathy, which is the advanced level of this disease, and having higher risk of severe impairment. Therefore, it becomes important to classify DR stages. To this effect, this paper presents a weighted fusion deep learning network (WFDLN) to automatically extract features and classify DR stages from fundus scans. The proposed framework aims to treat the issue of low quality and identify retinopathy symptoms in fundus images. Two channels of fundus images, namely, the contrast-limited adaptive histogram equalization (CLAHE) fundus images and the contrast-enhanced canny edge detection (CECED) fundus images are processed by WFDLN. Fundus-related features of CLAHE images are extracted by fine-tuned Inception V3, whereas the features of CECED fundus images are extracted using fine-tuned VGG-16. Both channels' outputs are merged in a weighted approach, and softmax classification is used to determine the final recognition result. Experimental results show that the proposed network can identify the DR stages with high accuracy. The proposed method tested on the Messidor dataset reports an accuracy level of 98.5%, sensitivity of 98.9%, and specificity of 98.0%, whereas on the Kaggle dataset, the proposed model reports an accuracy level of 98.0%, sensitivity of 98.7%, and specificity of 97.8%. Compared with other models, our proposed network achieves comparable performance.", "keywords": ["fundus images", "well-known fact", "images", "CECED fundus images", "diabetic retinopathy", "fundus", "blood glucose levels", "visual impairment", "accuracy level", "proposed", "proliferated diabetic retinopathy", "level", "blood vessel aggravations", "classify DR stages", "retinopathy", "CLAHE images", "proposed network", "fact that diabetic", "stages", "CECED fundus"], "paper_title": "Identification of Diabetic Retinopathy Using Weighted Fusion Deep Learning Based on Dual-Channel Fundus Scans.", "last_updated": "2023/02/04"}, {"id": "0036171054", "domain": "Diabetic retinopathy", "model_name": "Gao et al.", "publication_date": "2022/09/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36171054/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36171054", "task": null, "abstract": "Fundus fluorescein angiography (FFA) is an important technique to evaluate diabetic retinopathy (DR) and other retinal diseases. The interpretation of FFA images is complex and time-consuming, and the ability of diagnosis is uneven among different ophthalmologists. The aim of the study is to develop a clinically usable multilevel classification deep learning model for FFA images, including prediagnosis assessment and lesion classification. A total of 15 599 FFA images of 1558 eyes from 845 patients diagnosed with DR were collected and annotated. Three convolutional neural network (CNN) models were trained to generate the label of image quality, location, laterality of eye, phase and five lesions. Performance of the models was evaluated by accuracy, F-1 score, the area under the curve and human-machine comparison. The images with false positive and false negative results were analysed in detail. Compared with LeNet-5 and VGG16, ResNet18 got the best result, achieving an accuracy of 80.79%-93.34% for prediagnosis assessment and an accuracy of 63.67%-88.88% for lesion detection. The human-machine comparison showed that the CNN had similar accuracy with junior ophthalmologists. The false positive and false negative analysis indicated a direction of improvement. This is the first study to do automated standardised labelling on FFA images. Our model is able to be applied in clinical practice, and will make great contributions to the development of intelligent diagnosis of FFA images.", "keywords": ["Fundus fluorescein angiography", "evaluate diabetic retinopathy", "FFA images", "FFA", "Fundus fluorescein", "fluorescein angiography", "diabetic retinopathy", "retinal diseases", "important technique", "technique to evaluate", "evaluate diabetic", "images", "accuracy", "false", "false positive", "prediagnosis assessment", "false negative", "CNN", "Fundus", "angiography"], "paper_title": "Automatic interpretation and clinical evaluation for fundus fluorescein angiography images of diabetic retinopathy patients by deep learning.", "last_updated": "2023/02/04"}, {"id": "0032307321", "domain": "Diabetic retinopathy", "model_name": "Hsieh et al.", "publication_date": "2020/04/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32307321/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32307321", "task": null, "abstract": "To develop a deep learning image assessment software VeriSee\u2122 and to validate its accuracy in grading the severity of diabetic retinopathy (DR). Diabetic patients who underwent single-field, nonmydriatic, 45-degree color retinal fundus photography at National Taiwan University Hospital between July 2007 and June 2017 were retrospectively recruited. A total of 7524 judgeable color fundus images were collected and were graded for the severity of DR by ophthalmologists. Among these pictures, 5649 along with another 31,612 color fundus images from the EyePACS dataset were used for model training of VeriSee\u2122. The other 1875 images were used for validation and were graded for the severity of DR by VeriSee\u2122, ophthalmologists, and internal physicians. Area under the receiver operating characteristic curve (AUC) for VeriSee\u2122, and the sensitivities and specificities for VeriSee\u2122, ophthalmologists, and internal physicians in diagnosing DR were calculated. The AUCs for VeriSee\u2122 in diagnosing any DR, referable DR and proliferative diabetic retinopathy (PDR) were 0.955, 0.955 and 0.984, respectively. VeriSee\u2122 had better sensitivities in diagnosing any DR and PDR (92.2% and 90.9%, respectively) than internal physicians (64.3% and 20.6%, respectively) (P\u00a0<\u00a00.001 for both). VeriSee\u2122 also had better sensitivities in diagnosing any DR and referable DR (92.2% and 89.2%, respectively) than ophthalmologists (86.9% and 71.1%, respectively) (P\u00a0<\u00a00.001 for both), while ophthalmologists had better specificities. VeriSee\u2122 had good sensitivity and specificity in grading the severity of DR from color fundus images. It may offer clinical assistance to non-ophthalmologists in DR screening with nonmydriatic retinal fundus photography.", "keywords": ["National Taiwan University", "Taiwan University Hospital", "assessment software VeriSee", "color fundus images", "deep learning image", "learning image assessment", "image assessment software", "VeriSee", "fundus images", "color fundus", "Hospital between July", "National Taiwan", "Taiwan University", "University Hospital", "develop a deep", "deep learning", "assessment software", "validate its accuracy", "fundus", "software VeriSee"], "paper_title": "Application of deep learning image assessment software VeriSee\u2122 for diabetic retinopathy screening.", "last_updated": "2023/02/04"}, {"id": "0036981520", "domain": "Diabetic retinopathy", "model_name": "Alwakid et al.", "publication_date": "2023/03/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36981520/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36981520", "task": null, "abstract": "Vision loss can be avoided if diabetic retinopathy (DR) is diagnosed and treated promptly. The main five DR stages are none, moderate, mild, proliferate, and severe. In this study, a deep learning (DL) model is presented that diagnoses all five stages of DR with more accuracy than previous methods. The suggested method presents two scenarios: case 1 with image enhancement using a contrast limited adaptive histogram equalization (CLAHE) filtering algorithm in conjunction with an enhanced super-resolution generative adversarial network (ESRGAN), and case 2 without image enhancement. Augmentation techniques were then performed to generate a balanced dataset utilizing the same parameters for both cases. Using Inception-V3 applied to the Asia Pacific Tele-Ophthalmology Society (APTOS) datasets, the developed model achieved an accuracy of 98.7% for case 1 and 80.87% for case 2, which is greater than existing methods for detecting the five stages of DR. It was demonstrated that using CLAHE and ESRGAN improves a model's performance and learning ability.", "keywords": ["Vision loss", "diabetic retinopathy", "treated promptly", "avoided if diabetic", "diagnosed and treated", "case", "stages", "Vision", "retinopathy", "promptly", "Pacific Tele-Ophthalmology Society", "Asia Pacific", "CLAHE", "ESRGAN", "image enhancement", "Asia Pacific Tele-Ophthalmology", "model", "loss", "avoided", "diabetic"], "paper_title": "Deep Learning-Based Prediction of Diabetic Retinopathy Using CLAHE and ESRGAN for Enhancement.", "last_updated": "2023/02/04"}, {"id": "0030553900", "domain": "Diabetic retinopathy", "model_name": "Sayres et al.", "publication_date": "2018/12/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30553900/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30553900", "task": null, "abstract": "To understand the impact of deep learning diabetic retinopathy (DR) algorithms on physician readers in computer-assisted settings. Evaluation of diagnostic technology. One thousand seven hundred ninety-six retinal fundus images from 1612 diabetic patients. Ten ophthalmologists (5 general ophthalmologists, 4 retina specialists, 1 retina fellow) read images for DR severity based on the International Clinical Diabetic Retinopathy disease severity scale in each of 3 conditions: unassisted, grades only, or grades plus heatmap. Grades-only assistance comprised a histogram of DR predictions (grades) from a trained deep-learning model. For grades plus heatmap, we additionally showed explanatory heatmaps. For each experiment arm, we computed sensitivity and specificity of each reader and the algorithm for different levels of DR severity against an adjudicated reference standard. We also measured accuracy (exact 5-class level agreement and Cohen's quadratically weighted \u03ba), reader-reported confidence (5-point Likert scale), and grading time. Readers graded more accurately with model assistance than without for the grades-only condition (P < 0.001). Grades plus heatmaps improved accuracy for patients with DR (P < 0.001), but reduced accuracy for patients without DR (P\u00a0= 0.006). Both forms of assistance increased readers' sensitivity moderate-or-worse DR: unassisted: mean, 79.4% [95% confidence interval (CI), 72.3%-86.5%]; grades only: mean, 87.5% [95% CI, 85.1%-89.9%]; grades plus heatmap: mean, 88.7% [95% CI, 84.9%-92.5%] without a corresponding drop in specificity (unassisted: mean, 96.6% [95% CI, 95.9%-97.4%]; grades only: mean, 96.1% [95% CI, 95.5%-96.7%]; grades plus heatmap: mean, 95.5% [95% CI, 94.8%-96.1%]). Algorithmic assistance increased the accuracy of retina specialists above that of the unassisted reader or model alone; and increased grading confidence and grading time across all readers. For most cases, grades plus heatmap was only as effective as grades only. Over the course of the experiment, grading time decreased across all conditions, although most sharply for grades plus heatmap. Deep learning algorithms can improve the accuracy of, and confidence in, DR diagnosis in an assisted read setting. They also may increase grading time, although these effects may be ameliorated with experience.", "keywords": ["grades", "Clinical Diabetic Retinopathy", "International Clinical Diabetic", "diabetic retinopathy", "understand the impact", "heatmap", "grades plus heatmap", "learning diabetic retinopathy", "grading", "accuracy", "grading time", "diabetic", "Diabetic Retinopathy disease", "International Clinical", "Clinical Diabetic", "time", "retinopathy", "assistance", "confidence", "unassisted"], "paper_title": "Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0033896293", "domain": "Diabetic retinopathy", "model_name": "Zingman et al.", "publication_date": "2020/12/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33896293/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33896293", "task": "IWqQC1koJA", "abstract": "Proliferative retinopathies, such as diabetic retinopathy and retinopathy of prematurity, are leading causes of vision impairment. A common feature is a loss of retinal capillary vessels resulting in hypoxia and neuronal damage. The oxygen-induced retinopathy model is widely used to study revascularization of an ischemic area in the mouse retina. The presence of endothelial tip cells indicates vascular recovery; however, their quantification relies on manual counting in microscopy images of retinal flat mount preparations. Recent advances in deep neural networks (DNNs) allow the automation of such tasks. We demonstrate a workflow for detection of tip cells in retinal images using the DNN-based Single Shot Detector (SSD). The SSD was designed for detection of objects in natural images. We adapt the SSD architecture and training procedure to the tip cell detection task and retrain the DNN using labeled tip cells in images of fluorescently stained retina flat mounts. Transferring knowledge from the pretrained DNN and extensive data augmentation reduced the amount of required labeled data. Our system shows a performance comparable to the human level, while providing highly consistent results. Therefore, such a system can automate counting of tip cells, a readout frequently used in retinopathy research, thereby reducing routine work for biomedical experts.", "keywords": ["Proliferative retinopathies", "vision impairment", "tip cells", "Single Shot Detector", "tip", "diabetic retinopathy", "cells", "tip cell detection", "retinopathy", "SSD", "images", "labeled tip cells", "endothelial tip cells", "retinal capillary vessels", "retinal", "Shot Detector", "capillary vessels resulting", "DNN-based Single Shot", "detection", "tip cell"], "paper_title": "Deep Learning-Based Detection of Endothelial Tip Cells in the Oxygen-Induced Retinopathy Model.", "last_updated": "2023/02/04"}, {"id": "0035885512", "domain": "Diabetic retinopathy", "model_name": "Butt et al.", "publication_date": "2022/07/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35885512/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35885512", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) is a medical condition present in patients suffering from long-term diabetes. If a diagnosis is not carried out at an early stage, it can lead to vision impairment. High blood sugar in diabetic patients is the main source of DR. This affects the blood vessels within the retina. Manual detection of DR is a difficult task since it can affect the retina, causing structural changes such as Microaneurysms (MAs), Exudates (EXs), Hemorrhages (HMs), and extra blood vessel growth. In this work, a hybrid technique for the detection and classification of Diabetic Retinopathy in fundus images of the eye is proposed. Transfer learning (TL) is used on pre-trained Convolutional Neural Network (CNN) models to extract features that are combined to generate a hybrid feature vector. This feature vector is passed on to various classifiers for binary and multiclass classification of fundus images. System performance is measured using various metrics and results are compared with recent approaches for DR detection. The proposed method provides significant performance improvement in DR detection for fundus images. For binary classification, the proposed modified method achieved the highest accuracy of 97.8% and 89.29% for multiclass classification.", "keywords": ["medical condition present", "long-term diabetes", "medical condition", "condition present", "suffering from long-term", "Diabetic Retinopathy", "patients suffering", "Convolutional Neural Network", "Diabetic", "fundus images", "classification", "Retinopathy", "detection", "diabetic patients", "blood", "present in patients", "fundus", "images", "diabetes", "Neural Network"], "paper_title": "Diabetic Retinopathy Detection from Fundus Images of the Eye Using Hybrid Deep Learning Features.", "last_updated": "2023/02/04"}, {"id": "0035212716", "domain": "Diabetic retinopathy", "model_name": "Chen et al.", "publication_date": "2022/04/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35212716/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35212716", "task": "aipbNdPTIt", "abstract": "To investigate the correlation between choroidal thickness and myopia progression using a deep learning method. Two data sets, data set A and data set B, comprising of 123 optical coherence tomography (OCT) volumes, were collected to establish the model and verify its clinical utility. The proposed mask region-based convolutional neural network (R-CNN) model, trained with the pretrained weights from the Common Objects in Context database as well as the manually labeled OCT images from data set A, was used to automatically segment the choroid. To verify its clinical utility, the mask R-CNN model was tested with data set B, and the choroidal thickness estimated by the model was also used to explore its relationship with myopia. Compared with the result of manual segmentation in data set B, the error of the automatic choroidal inner and outer boundary segmentation was 6.72 \u00b1 2.12 and 13.75 \u00b1 7.57 \u00b5m, respectively. The mean dice coefficient between the region segmented by automatic and manual methods was 93.87% \u00b1 2.89%. The mean difference in choroidal thickness over the Early Treatment Diabetic Retinopathy Study zone between the two methods was 10.52 \u00b5m. Additionally, the choroidal thickness estimated using the proposed model was thinner in high-myopic eyes, and axial length was the most significant predictor. The mask R-CNN model has excellent performance in choroidal segmentation and quantification. In addition, the choroid of high myopia is significantly thinner than that of nonhigh myopia. This work lays the foundations for mask R-CNN models that could aid in the evaluation of more intricate changes occurring in chorioretinal diseases.", "keywords": ["data set", "deep learning method", "mask R-CNN model", "choroidal thickness", "choroidal thickness estimated", "investigate the correlation", "deep learning", "set", "data", "mask R-CNN", "model", "choroidal", "R-CNN model", "data sets", "Common Objects", "Objects in Context", "myopia progression", "Early Treatment Diabetic", "Treatment Diabetic Retinopathy", "Diabetic Retinopathy Study"], "paper_title": "Application of Artificial Intelligence and Deep Learning for Choroid Segmentation in Myopia.", "last_updated": "2023/02/04"}, {"id": "0035492360", "domain": "Diabetic retinopathy", "model_name": "Zhang et al.", "publication_date": "2022/04/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35492360/", "code_link": null, "model_type": "graph embedding", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35492360", "task": null, "abstract": "Diabetic retinopathy, as a severe public health problem associated with vision loss, should be diagnosed early using an accurate screening tool. While many previous deep learning models have been proposed for this disease, they need sufficient professional annotation data to train the model, requiring more expensive and time-consuming screening skills. This study aims to economize manual power and proposes a deep graph correlation network (DGCN) to develop automated diabetic retinopathy grading without any professional annotations. DGCN involves the novel deep learning algorithm of a graph convolutional network to exploit inherent correlations from independent retinal image features learned by a convolutional neural network. Three designed loss functions of graph-center, pseudo-contrastive, and transformation-invariant constrain the optimisation and application of the DGCN model in an automated diabetic retinopathy grading task. To evaluate the DGCN model, this study employed EyePACS-1 and Messidor-2 sets to perform grading results. It achieved an accuracy of 89.9% (91.8%), sensitivity of 88.2% (90.2%), and specificity of 91.3% (93.0%) on EyePACS-1 (Messidor-2) data set with a confidence index of 95% and commendable effectiveness on receiver operating characteristic (ROC) curve and t-SNE plots. The grading capability of this study is close to that of retina specialists, but superior to that of trained graders, which demonstrates that the proposed DGCN provides an innovative route for automated diabetic retinopathy grading and other computer-aided diagnostic systems.", "keywords": ["severe public health", "public health problem", "accurate screening tool", "automated diabetic retinopathy", "diabetic retinopathy grading", "Diabetic retinopathy", "severe public", "public health", "health problem", "diagnosed early", "automated diabetic", "retinopathy grading", "DGCN", "screening tool", "accurate screening", "DGCN model", "Diabetic", "retinopathy", "time-consuming screening skills", "grading"], "paper_title": "Diabetic Retinopathy Grading by Deep Graph Correlation Network on Retinal Images Without Manual Annotations.", "last_updated": "2023/02/04"}, {"id": "0031906601", "domain": "Diabetic retinopathy", "model_name": "Riaz et al.", "publication_date": "2020/01/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31906601/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31906601", "task": "IWqQC1koJA", "abstract": "Diabetes has recently emerged as a worldwide problem, and diabetic retinopathy is an abnormal state associated with the human retina. Due to the increase in daily screen-related activities of modern human beings, diabetic retinopathy is more prevalent among adults, leading to minor and major blindness. Doctors and clinicians are unable to perform early diagnoses due to the large number of patients. To solve this problem, this study introduces a classification model for retinal images that distinguishes between the various stages of diabetic retinopathy. This work involves deploying deep and densely connected networks for retinal image analysis with training from scratch. Dense connections between the convolutional layers of the network are an essential factor to enhance accuracy owing to the deeper supervision between layers. Another factor is the growth rate that further assists our model in learning more sophisticated feature maps regarding retinal images from every stage of the network. We compute the area under the curve, sensitivity, and specificity, particularly for messidor-2 and EyePACS. Compared to existing approaches, our method achieved better results, with an approximate rise rate of 0.01, 0.03, and 0.01, respectively. Therefore, computer-aided programs can help in diagnostic centers as automated detection systems.", "keywords": ["diabetic retinopathy", "Diabetes has recently", "recently emerged", "abnormal state", "human retina", "worldwide problem", "retinopathy", "retinal images", "diabetic", "daily screen-related activities", "modern human", "retinal", "human", "retinal image analysis", "early diagnoses due", "Diabetes", "retina", "problem", "recently", "emerged"], "paper_title": "Deep and Densely Connected Networks for Classification of Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0036471042", "domain": "Diabetic retinopathy", "model_name": "Ryu et al.", "publication_date": "2022/12/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36471042/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36471042", "task": null, "abstract": "", "keywords": [], "paper_title": "Author Correction: A deep learning model for identifying diabetic retinopathy using optical coherence tomography angiography.", "last_updated": "2023/02/04"}, {"id": "0033897905", "domain": "Diabetic retinopathy", "model_name": "DRISTI", "publication_date": "2021/04/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33897905/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33897905", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a significant reason for the global increase in visual loss. Studies show that timely treatment can significantly bring down such incidents. Hence, it is essential to distinguish the stages and severity of DR to recommend needed medical attention. In this view, this paper presents DRISTI (Diabetic Retinopathy classIfication by analySing reTinal Images), where a hybrid deep learning model composed of VGG16 and capsule network is proposed, which yields statistically significant performance improvement over the state of the art. To validate our claim, we have reported detailed experimental and ablation studies. We have also created an augmented dataset to increase the APTOS dataset's size and check how robust the model is. The five-class training and validation accuracy for the expanded dataset is <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>99.21</mn> <mo>%</mo></mrow> </math> and <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>75.50</mn> <mo>%</mo></mrow> </math> . The two-class training and validation accuracy on augmented APTOS is <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>99.96</mn> <mo>%</mo></mrow> </math> and <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>97.05</mn> <mo>%</mo></mrow> </math> . Extending the two-class model for the mixed dataset, we get a training and validation accuracy of <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>99.92</mn> <mo>%</mo></mrow> </math> and <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>91.43</mn> <mo>%</mo></mrow> </math> , respectively. We have also performed cross-dataset and mixed dataset testing to demonstrate the efficiency of DRISTI.", "keywords": ["math xmlns", "mrow", "math", "Diabetic retinopathy", "visual loss", "xmlns", "Diabetic Retinopathy classIfication", "dataset", "Diabetic", "significant reason", "retinopathy", "global increase", "training and validation", "validation accuracy", "DRISTI", "validation", "APTOS", "model", "training", "accuracy"], "paper_title": "DRISTI: a hybrid deep neural network for diabetic retinopathy diagnosis.", "last_updated": "2023/02/04"}, {"id": "0035066704", "domain": "Diabetic retinopathy", "model_name": "Gao et al.", "publication_date": "2022/01/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35066704/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35066704", "task": null, "abstract": "To develop and validate a deep learning system for diabetic retinopathy (DR) grading based on fundus fluorescein angiography (FFA) images. A total of 11,214 FFA images from 705 patients were collected to form the internal dataset. Three convolutional neural networks, namely VGG16, RestNet50, and DenseNet, were trained using a nine-square grid input, and heat maps were generated. Subsequently, a comparison between human graders and the algorithm was performed. Lastly, the best model was tested on two external datasets (Xian dataset and Ningbo dataset). VGG16 performed the best, with a maximum accuracy of 94.17%, and had an AUC of 0.972, 0.922, and 0.994 for levels 1, 2, and 3, respectively. For Xian dataset, our model reached the accuracy of 82.47% and AUC of 0.910, 0.888, and 0.976 for levels 1, 2, and 3. As for Ningbo dataset, the network performed with the accuracy of 88.89% and AUC of 0.972, 0.756, and 0.945 for levels 1, 2, and 3. A deep learning system for DR staging was trained based on FFA images and evaluated through human-machine comparisons as well as external dataset testing. The proposed system will help clinical practitioners to diagnose and treat DR patients, and lay a foundation for future applications of other ophthalmic or general diseases.", "keywords": ["fundus fluorescein angiography", "FFA images", "diabetic retinopathy", "fluorescein angiography", "develop and validate", "fundus fluorescein", "FFA", "Xian dataset", "dataset", "Ningbo dataset", "deep learning system", "images", "grading based", "AUC", "validate a deep", "based on fundus", "levels", "deep learning", "learning system", "accuracy"], "paper_title": "End-to-end diabetic retinopathy grading based on fundus fluorescein angiography images using deep learning.", "last_updated": "2023/02/04"}, {"id": "0035233182", "domain": "Disc hemorrhage", "model_name": "Gupta et al.", "publication_date": "2022/02/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35233182/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "35233182", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) is defined as the Diabetes Mellitus difficulty that harms the blood vessels in the retina. It is also known as a silent disease and cause mild vision issues or no symptoms. In order to enhance the chances of effective treatment, yearly eye tests are vital for premature discovery. Hence, it uses fundus cameras for capturing retinal images, but due to its size and cost, it is a troublesome for extensive screening. Therefore, the smartphones are utilized for scheming low-power, small-sized, and reasonable retinal imaging schemes to activate automated DR detection and DR screening. In this article, the new DIY (do it yourself) smartphone enabled camera is used for smartphone based DR detection. Initially, the preprocessing like green channel transformation and CLAHE (Contrast Limited Adaptive Histogram Equalization) are performed. Further, the segmentation process starts with optic disc segmentation by WT (watershed transform) and abnormality segmentation (Exudates, microaneurysms, haemorrhages, and IRMA) by Triplet half band filter bank (THFB). Then the different features are extracted by Haralick and ADTCWT (Anisotropic Dual Tree Complex Wavelet Transform) methods. Using life choice-based optimizer (LCBO) algorithm, the optimal features are chosen from the mined features. Then the selected features are applied to the optimized hybrid ML (machine learning) classifier with the combination of NN and DCNN (Deep Convolutional Neural Network) in which the SSD (Social Ski-Driver) is utilized for the best weight values of hybrid classifier to categorize the severity level as mild DR, severe DR, normal, moderate DR, and Proliferative DR. The proposed work is simulated in python environment and to test the efficiency of the proposed scheme the datasets like APTOS-2019-Blindness-Detection, and EyePacs are used. The model has been evaluated using different performance metrics. The simulation results verified that the suggested scheme is provides well accuracy for each dataset than other current approaches.", "keywords": ["Diabetes Mellitus difficulty", "Diabetes Mellitus", "Diabetic Retinopathy", "Mellitus difficulty", "difficulty that harms", "harms the blood", "blood vessels", "Contrast Limited Adaptive", "Adaptive Histogram Equalization", "Limited Adaptive Histogram", "Retinopathy", "Diabetes", "Mellitus", "Complex Wavelet Transform", "Anisotropic Dual Tree", "Dual Tree Complex", "Tree Complex Wavelet", "mild vision issues", "Deep Convolutional Neural", "Convolutional Neural Network"], "paper_title": "Optimized hybrid machine learning approach for smartphone based diabetic retinopathy detection.", "last_updated": "2023/02/04"}, {"id": "0034450766", "domain": "Diabetic retinopathy", "model_name": "Tang et al.", "publication_date": "2021/08/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34450766/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34450766", "task": "IWqQC1koJA", "abstract": "Proliferative Diabetic Retinopathy (PDR) is a severe retinal disease that threatens diabetic patients. It is characterized by neovascularization in the retina and the optic disk. PDR clinical features contain highly intense retinal neovascularization and fibrous spreads, leading to visual distortion if not controlled. Different image processing techniques have been proposed to detect and diagnose neovascularization from fundus images. Recently, deep learning methods are getting popular in neovascularization detection due to artificial intelligence advancement in biomedical image processing. This paper presents a semantic segmentation convolutional neural network architecture for neovascularization detection. First, image pre-processing steps were applied to enhance the fundus images. Then, the images were divided into small patches, forming a training set, a validation set, and a testing set. A semantic segmentation convolutional neural network was designed and trained to detect the neovascularization regions on the images. Finally, the network was tested using the testing set for performance evaluation. The proposed model is entirely automated in detecting and localizing neovascularization lesions, which is not possible with previously published methods. Evaluation results showed that the model could achieve accuracy, sensitivity, specificity, precision, Jaccard similarity, and Dice similarity of 0.9948, 0.8772, 0.9976, 0.8696, 0.7643, and 0.8466, respectively. We demonstrated that this model could outperform other convolutional neural network models in neovascularization detection.", "keywords": ["Proliferative Diabetic Retinopathy", "threatens diabetic patients", "Diabetic Retinopathy", "Proliferative Diabetic", "diabetic patients", "threatens diabetic", "severe retinal disease", "Diabetic", "disease that threatens", "neovascularization", "Retinopathy", "PDR", "convolutional neural network", "severe retinal", "retinal disease", "PDR clinical features", "convolutional neural", "neural network", "PDR clinical", "neovascularization detection"], "paper_title": "Neovascularization Detection and Localization in Fundus Images Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0036241374", "domain": "Diabetic retinopathy", "model_name": "Lee et al.", "publication_date": "2022/10/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36241374/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36241374", "task": "aipbNdPTIt", "abstract": "Retinal capillary non-perfusion (NP) and neovascularisation (NV) are two of the most important angiographic changes in diabetic retinopathy (DR). This study investigated the feasibility of using deep learning (DL) models to automatically segment NP and NV on ultra-widefield fluorescein angiography (UWFA) images from patients with DR. Retrospective cross-sectional chart review study. In total, 951 UWFA images were collected from patients with severe non-proliferative DR (NPDR) or proliferative DR (PDR). Each image was segmented and labelled for NP, NV, disc, background and outside areas. Using the labelled images, DL models were trained and validated (80%) using convolutional neural networks (CNNs) for automated segmentation and tested (20%) on test sets. Accuracy of each model and each label were assessed. The best accuracy from CNN models for each label was 0.8208, 0.8338, 0.9801, 0.9253 and 0.9766 for NP, NV, disc, background and outside areas, respectively. The best Intersection over Union for each label was 0.6806, 0.5675, 0.7107, 0.8551 and 0.924 and mean mean boundary F1 score (BF score) was 0.6702, 0.8742, 0.9092, 0.8103 and 0.9006, respectively. DL models can detect NV and NP as well as disc and outer margins on UWFA with good performance. This automated segmentation of important UWFA features will aid physicians in DR clinics and in overcoming grader subjectivity.", "keywords": ["Retinal capillary non-perfusion", "Retinal capillary", "capillary non-perfusion", "diabetic retinopathy", "UWFA", "important angiographic", "models", "ultra-widefield fluorescein angiography", "UWFA images", "images", "label", "Retinal", "non-perfusion", "neovascularisation", "retinopathy", "important UWFA", "disc", "chart review study", "capillary", "angiographic"], "paper_title": "Automated segmentation of ultra-widefield fluorescein angiography of diabetic retinopathy using deep learning.", "last_updated": "2023/02/04"}, {"id": "0036143239", "domain": "Diabetic retinopathy", "model_name": "Shaukat et al.", "publication_date": "2022/09/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36143239/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36143239", "task": "aipbNdPTIt", "abstract": "Diabetic retinopathy (DR) is a drastic disease. DR embarks on vision impairment when it is left undetected. In this article, learning-based techniques are presented for the segmentation and classification of DR lesions. The pre-trained Xception model is utilized for deep feature extraction in the segmentation phase. The extracted features are fed to Deeplabv3 for semantic segmentation. For the training of the segmentation model, an experiment is performed for the selection of the optimal hyperparameters that provided effective segmentation results in the testing phase. The multi-classification model is developed for feature extraction using the fully connected (FC) MatMul layer of efficient-net-b0 and pool-10 of the squeeze-net. The extracted features from both models are fused serially, having the dimension of N \u00d7 2020, amidst the best N \u00d7 1032 features chosen by applying the marine predictor algorithm (MPA). The multi-classification of the DR lesions into grades 0, 1, 2, and 3 is performed using neural network and KNN classifiers. The proposed method performance is validated on open access datasets such as DIARETDB1, e-ophtha-EX, IDRiD, and Messidor. The obtained results are better compared to those of the latest published works.", "keywords": ["Diabetic retinopathy", "drastic disease", "segmentation", "extracted features", "feature extraction", "pre-trained Xception model", "features", "segmentation model", "Xception model", "model", "deep feature extraction", "segmentation phase", "Diabetic", "retinopathy", "disease", "effective segmentation results", "drastic", "feature", "segmentation results", "left undetected"], "paper_title": "Three-Dimensional Semantic Segmentation of Diabetic Retinopathy Lesions and Grading Using Transfer Learning.", "last_updated": "2023/02/04"}, {"id": "0032704196", "domain": "Diabetic retinopathy", "model_name": "Hacisoftaoglu et al.", "publication_date": "2020/05/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32704196/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32704196", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) may result in various degrees of vision loss and even blindness if not diagnosed in a timely manner. Therefore, having an annual eye exam helps early detection to prevent vision loss in earlier stages, especially for diabetic patients. Recent technological advances made smartphone-based retinal imaging systems available on the market to perform small-sized, low-powered, and affordable DR screening in diverse environments. However, the accuracy of DR detection depends on the field of view and image quality. Since smartphone-based retinal imaging systems have much more compact designs than a traditional fundus camera, captured images are likely to be the low quality with a smaller field of view. Our motivation in this paper is to develop an automatic DR detection model for smartphone-based retinal images using the deep learning approach with the ResNet50 network. This study first utilized the well-known AlexNet, GoogLeNet, and ResNet50 architectures, using the transfer learning approach. Second, these frameworks were retrained with retina images from several datasets including EyePACS, Messidor, IDRiD, and Messidor-2 to investigate the effect of using images from the single, cross, and multiple datasets. Third, the proposed ResNet50 model is applied to smartphone-based synthetic images to explore the DR detection accuracy of smartphone-based retinal imaging systems. Based on the vision-threatening diabetic retinopathy detection results, the proposed approach achieved a high classification accuracy of 98.6%, with a 98.2% sensitivity and a 99.1% specificity while its AUC was 0.9978 on the independent test dataset. As the main contributions, DR detection accuracy was improved using the deep transfer learning approach for the ResNet50 network with publicly available datasets and the effect of the field of view in smartphone-based retinal imaging was studied. Although a smaller number of images were used in the training set compared with the existing studies, considerably acceptable high accuracies for validation and testing data were obtained.", "keywords": ["smartphone-based retinal imaging", "retinal imaging systems", "smartphone-based retinal", "retinal imaging", "vision loss", "prevent vision loss", "timely manner", "diabetic retinopathy detection", "Diabetic Retinopathy", "imaging systems", "smartphone-based", "retinal", "detection", "field of view", "images", "learning approach", "made smartphone-based retinal", "detection accuracy", "Diabetic", "imaging"], "paper_title": "Deep Learning Frameworks for Diabetic Retinopathy Detection with Smartphone-based Retinal Imaging Systems.", "last_updated": "2023/02/04"}, {"id": "0036275805", "domain": "Diabetic retinopathy", "model_name": "Lin et al.", "publication_date": "2022/10/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36275805/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36275805", "task": null, "abstract": "Diabetic macular edema (DME) is one of the leading causes of visual impairment in diabetic retinopathy (DR). Physicians rely on optical coherence tomography (OCT) and baseline visual acuity (VA) to tailor therapeutic regimen. However, best-corrected visual acuity (BCVA) from chart-based examinations may not wholly reflect DME status. Chart-based examinations are subjected findings dependent on the patient's recognition functions and are often confounded by concurrent corneal, lens, retinal, optic nerve, or extraocular disorders. The ability to infer VA from objective optical coherence tomography (OCT) images provides the predicted VA from objective macular structures directly and a better understanding of diabetic macular health. Deviations from chart-based and artificial intelligence (AI) image-based VA will prompt physicians to assess other ocular abnormalities affecting the patients VA and whether pursuing anti-VEGF treatment will likely yield increment in VA. We enrolled a retrospective cohort of 251 DME patients from Big Data Center (BDC) of Taipei Veteran General Hospital (TVGH) from February 2011 and August 2019. A total of 3,920 OCT images, labeled as \"visually impaired\" or \"adequate\" according to baseline VA, were grouped into training (2,826), validation (779), and testing cohort (315). We applied confusion matrix and receiver operating characteristic (ROC) curve to evaluate the performance. We developed an OCT-based convolutional neuronal network (CNN) model that could classify two VA classes by the threshold of 0.50 (decimal notation) with an accuracy of 75.9%, a sensitivity of 78.9%, and an area under the ROC curve of 80.1% on the testing cohort. This study demonstrated the feasibility of inferring VA from routine objective retinal images. Serves as a pilot study to encourage further use of deep learning in deriving functional outcomes and secondary surrogate endpoints for retinal diseases.", "keywords": ["Diabetic macular edema", "optical coherence tomography", "visual acuity", "best-corrected visual acuity", "visual impairment", "baseline visual acuity", "macular edema", "diabetic retinopathy", "Diabetic macular", "coherence tomography", "DME", "Big Data Center", "Veteran General Hospital", "OCT", "reflect DME status", "Taipei Veteran General", "chart-based examinations", "wholly reflect DME", "optical coherence", "visual"], "paper_title": "Deep learning to infer visual acuity from optical coherence tomography in diabetic macular edema.", "last_updated": "2023/02/04"}, {"id": "0030603189", "domain": "Diabetic retinopathy", "model_name": "Mansour et al.", "publication_date": "2017/08/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30603189/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30603189", "task": "IWqQC1koJA", "abstract": "The high-pace rise in advanced computing and imaging systems has given rise to a new research dimension called computer-aided diagnosis (CAD) system for various biomedical purposes. CAD-based diabetic retinopathy (DR) can be of paramount significance to enable early disease detection and diagnosis decision. Considering the robustness of deep neural networks (DNNs) to solve highly intricate classification problems, in this paper, AlexNet DNN, which functions on the basis of convolutional neural network (CNN), has been applied to enable an optimal DR CAD solution. The DR model applies a multilevel optimization measure that incorporates pre-processing, adaptive-learning-based Gaussian mixture model (GMM)-based concept region segmentation, connected component-analysis-based region of interest (ROI) localization, AlexNet DNN-based highly dimensional feature extraction, principle component analysis (PCA)- and linear discriminant analysis (LDA)-based feature selection, and support-vector-machine-based classification to ensure optimal five-class DR classification. The simulation results with standard KAGGLE fundus datasets reveal that the proposed AlexNet DNN-based DR exhibits a better performance with LDA feature selection, where it exhibits a DR classification accuracy of 97.93% with FC7 features, whereas with PCA, it shows 95.26% accuracy. Comparative analysis with spatial invariant feature transform (SIFT) technique (accuracy-94.40%) based DR feature extraction also confirms that AlexNet DNN-based DR outperforms SIFT-based DR.", "keywords": ["research dimension called", "dimension called computer-aided", "called computer-aided diagnosis", "high-pace rise", "imaging systems", "biomedical purposes", "advanced computing", "computing and imaging", "research dimension", "dimension called", "called computer-aided", "computer-aided diagnosis", "AlexNet DNN-based", "rise in advanced", "rise", "LDA feature selection", "feature", "CAD", "CAD solution", "CAD-based diabetic retinopathy"], "paper_title": "Deep-learning-based automatic computer-aided diagnosis system for diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0032904519", "domain": "Diabetic retinopathy", "model_name": "Chetoui et al.", "publication_date": "2020/08/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32904519/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32904519", "task": "IWqQC1koJA", "abstract": "<b>Purpose</b>: Diabetic retinopathy (DR) is characterized by retinal lesions affecting people having diabetes for several years. It is one of the leading causes of visual impairment worldwide. To diagnose this disease, ophthalmologists need to manually analyze retinal fundus images. Computer-aided diagnosis systems can help alleviate this burden by automatically detecting DR on retinal images, thus saving physicians' precious time and reducing costs. The objective of this study is to develop a deep learning algorithm capable of detecting DR on retinal fundus images. Nine public datasets and more than 90,000 images are used to assess the efficiency of the proposed technique. In addition, an explainability algorithm is developed to visually show the DR signs detected by the deep model. <b>Approach</b>: The proposed deep learning algorithm fine-tunes a pretrained deep convolutional neural network for DR detection. The model is trained on a subset of EyePACS dataset using a cosine annealing strategy for decaying the learning rate with warm up, thus improving the training accuracy. Tests are conducted on the nine datasets. An explainability algorithm based on gradient-weighted class activation mapping is developed to visually show the signs selected by the model to classify the retina images as DR. <b>Result</b>: The proposed network leads to higher classification rates with an area under curve (AUC) of 0.986, sensitivity = 0.958, and specificity = 0.971 for EyePACS. For MESSIDOR, MESSIDOR-2, DIARETDB0, DIARETDB1, STARE, IDRID, E-ophtha, and UoA-DR, the AUC is 0.963, 0.979, 0.986, 0.988, 0.964, 0.957, 0.984, and 0.990, respectively. <b>Conclusions</b>: The obtained results achieve state-of-the-art performance and outperform past published works relying on training using only publicly available datasets. The proposed approach can robustly classify fundus images and detect DR. An explainability model was developed and showed that our model was able to efficiently identify different signs of DR and detect this health issue.", "keywords": ["lesions affecting people", "Diabetic retinopathy", "retinal lesions affecting", "retinal fundus images", "lesions affecting", "affecting people", "people having diabetes", "fundus images", "images", "Purpose", "Diabetic", "retinal lesions", "retinal fundus", "deep learning algorithm", "retinal", "model", "analyze retinal fundus", "algorithm", "proposed", "deep"], "paper_title": "Explainable end-to-end deep learning for diabetic retinopathy detection across multiple datasets.", "last_updated": "2023/02/04"}, {"id": "0036010325", "domain": "Glaucoma (unspecified)", "model_name": "Kobat et al.", "publication_date": "2022/08/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36010325/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36010325", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a common complication of diabetes that can lead to progressive vision loss. Regular surveillance with fundal photography, early diagnosis, and prompt intervention are paramount to reducing the incidence of DR-induced vision loss. However, manual interpretation of fundal photographs is subject to human error. In this study, a new method based on horizontal and vertical patch division was proposed for the automated classification of DR images on fundal photographs. The novel sides of this study are given as follows. We proposed a new non-fixed-size patch division model to obtain high classification results and collected a new fundus image dataset. Moreover, two datasets are used to test the model: a newly collected three-class (normal, non-proliferative DR, and proliferative DR) dataset comprising 2355 DR images and the established open-access five-class Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 dataset comprising 3662 images. Two analysis scenarios, Case 1 and Case 2, with three (normal, non-proliferative DR, and proliferative DR) and five classes (normal, mild DR, moderate DR, severe DR, and proliferative DR), respectively, were derived from the APTOS 2019 dataset. These datasets and these cases have been used to demonstrate the general classification performance of our proposal. By applying transfer learning, the last fully connected and global average pooling layers of the DenseNet201 architecture were used to extract deep features from input DR images and each of the eight subdivided horizontal and vertical patches. The most discriminative features are then selected using neighborhood component analysis. These were fed as input to a standard shallow cubic support vector machine for classification. Our new DR dataset obtained 94.06% and 91.55% accuracy values for three-class classification with 80:20 hold-out validation and 10-fold cross-validation, respectively. As can be seen from steps of the proposed model, a new patch-based deep-feature engineering model has been proposed. The proposed deep-feature engineering model is a cognitive model, since it uses efficient methods in each phase. Similar excellent results were seen for three-class classification with the Case 1 dataset. In addition, the model attained 87.43% and 84.90% five-class classification accuracy rates using 80:20 hold-out validation and 10-fold cross-validation, respectively, on the Case 2 dataset, which outperformed prior DR classification studies based on the five-class APTOS 2019 dataset. Our model attained about >2% classification results compared to others. These findings demonstrate the accuracy and robustness of the proposed model for classification of DR images.", "keywords": ["progressive vision loss", "vision loss", "classification", "Diabetic retinopathy", "DR-induced vision loss", "model", "common complication", "complication of diabetes", "lead to progressive", "dataset", "progressive vision", "images", "proposed", "Case", "APTOS", "fundal photographs", "loss", "fundal", "normal", "vision"], "paper_title": "Automated Diabetic Retinopathy Detection Using Horizontal and Vertical Patch Division-Based Pre-Trained DenseNET with Digital Fundus Images.", "last_updated": "2023/02/04"}, {"id": "0029888061", "domain": "Diabetic retinopathy", "model_name": "firstaid", "publication_date": "2018/05/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29888061/", "code_link": "https://github.com/yidarvin/firstaid", "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "29888061", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy is a leading cause of blindness among working-age adults. Early detection of this condition is critical for good prognosis. In this paper, we demonstrate the use of convolutional neural networks (CNNs) on color fundus images for the recognition task of diabetic retinopathy staging. Our network models achieved test metric performance comparable to baseline literature results, with validation sensitivity of 95%. We additionally explored multinomial classification models, and demonstrate that errors primarily occur in the misclassification of mild disease as normal due to the CNNs inability to detect subtle disease features. We discovered that preprocessing with contrast limited adaptive histogram equalization and ensuring dataset fidelity by expert verification of class labels improves recognition of subtle features. Transfer learning on pretrained GoogLeNet and AlexNet models from ImageNet improved peak test set accuracies to 74.5%, 68.8%, and 57.2% on 2-ary, 3-ary, and 4-ary classification models, respectively.", "keywords": ["working-age adults", "blindness among working-age", "Diabetic retinopathy", "diabetic retinopathy staging", "models", "adults", "classification models", "Diabetic", "retinopathy", "retinopathy staging", "leading", "blindness", "working-age", "Early detection", "good prognosis", "condition is critical", "critical for good", "network models achieved", "convolutional neural networks", "features"], "paper_title": "Automated Detection of Diabetic Retinopathy using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0035966243", "domain": "Diabetic retinopathy", "model_name": "octa-analysis", "publication_date": "2022/08/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35966243/", "code_link": "https://github.com/liyuatbjut/octa-analysis", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "35966243", "task": "IWqQC1koJA", "abstract": "As an extension of optical coherence tomography (OCT), optical coherence tomographic angiography (OCTA) provides information on the blood flow status at the microlevel and is sensitive to changes in the fundus vessels. However, due to the distinct imaging mechanism of OCTA, existing models, which are primarily used for analyzing fundus images, do not work well on OCTA images. Effectively extracting and analyzing the information in OCTA images remains challenging. To this end, a deep learning framework that fuses multilevel information in OCTA images is proposed in this study. The effectiveness of the proposed model was demonstrated in the task of diabetic retinopathy (DR) classification. First, a U-Net-based segmentation model was proposed to label the boundaries of large retinal vessels and the foveal avascular zone (FAZ) in OCTA images. Then, we designed an isolated concatenated block (ICB) structure to extract and fuse information from the original OCTA images and segmentation results at different fusion levels. The experiments were conducted on 301 OCTA images. Of these images, 244 were labeled by ophthalmologists as normal images, and 57 were labeled as DR images. An accuracy of 93.1% and a mean intersection over union (mIOU) of 77.1% were achieved using the proposed large vessel and FAZ segmentation model. In the ablation experiment with 6-fold validation, the proposed deep learning framework that combines the proposed isolated and concatenated convolution process significantly improved the DR diagnosis accuracy. Moreover, inputting the merged images of the original OCTA images and segmentation results further improved the model performance. Finally, a DR diagnosis accuracy of 88.1% (95%CI \u00b1 3.6%) and an area under the curve (AUC) of 0.92 were achieved using our proposed classification model, which significantly outperforms the state-of-the-art classification models. As a comparison, an accuracy of 83.7 (95%CI \u00b1 1.5%) and AUC of 0.76 were obtained using EfficientNet. <i>Significance</i>. The visualization results show that the FAZ and the vascular region close to the FAZ provide more information for the model than the farther surrounding area. Furthermore, this study demonstrates that a clinically sophisticated designed deep learning model is not only able to effectively assist in the diagnosis but also help to locate new indicators for certain illnesses.", "keywords": ["optical coherence tomography", "OCTA images", "coherence tomographic angiography", "optical coherence", "blood flow status", "OCTA", "original OCTA images", "images", "OCTA images remains", "coherence tomography", "original OCTA", "analyzing fundus images", "tomographic angiography", "model", "blood flow", "flow status", "proposed", "OCT", "FAZ segmentation model", "information"], "paper_title": "Diagnosing Diabetic Retinopathy in OCTA Images Based on Multilevel Information Fusion Using a Deep Learning Framework.", "last_updated": "2023/02/04"}, {"id": "0031671320", "domain": "Diabetic retinopathy", "model_name": "IDRiD", "publication_date": "2019/10/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31671320/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31671320", "task": "aipbNdPTIt", "abstract": "Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on \"Diabetic Retinopathy - Segmentation and Grading\" was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.", "keywords": ["avoidable vision loss", "avoid vision loss", "vision loss", "Diabetic Retinopathy", "predominantly affecting", "Diabetic Retinopathy Image", "affecting the working-age", "Indian Diabetic Retinopathy", "avoidable vision", "Retinopathy Image Dataset", "working-age population", "global diabetic population", "IEEE International Symposium", "Diabetic", "retinal image analysis", "Screening", "Retinopathy", "challenge", "loss", "diabetic population"], "paper_title": "IDRiD: Diabetic Retinopathy - Segmentation and Grading Challenge.", "last_updated": "2023/02/04"}, {"id": "0036744216", "domain": "Diabetic retinopathy", "model_name": "Dong et al.", "publication_date": "2022/08/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36744216/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36744216", "task": null, "abstract": "Deep learning (DL) is a technique explored within ophthalmology that requires large datasets to distinguish feature representations with high diagnostic performance. There is a need for developing DL approaches to predict therapeutic response, but completed clinical trial datasets are limited in size. Predicting treatment response is more complex than disease diagnosis, where hallmarks of treatment response are subtle. This study seeks to understand the utility of DL for clinical problems in ophthalmology such as predicting treatment response and where large sample sizes for model training are not available. Four DL architectures were trained using cross-validated transfer learning to classify ultra-widefield angiograms (UWFA) and fluid-compartmentalized optical coherence tomography (OCT) images from a completed clinical trial (PERMEATE) dataset (n=29) as tolerating or requiring extended interval Anti-VEGF dosing. UWFA images (n=217) from the Anti-VEGF study were divided into five increasingly larger subsets to evaluate the influence of dataset size on performance. Class activation maps (CAMs) were generated to identify regions of model attention. The best performing DL model had a mean AUC of 0.507 \u00b1 0.042 on UWFA images, and highest observed AUC of 0.503 for fluid-compartmentalized OCT images. DL had a best performing AUC of 0.634 when dataset size was incrementally increased. Resulting CAMs show inconsistent regions of interest. This study demonstrated the limitations of DL for predicting therapeutic response when large datasets were not available for model training. Our findings suggest the need for hand-crafted approaches for complex and data scarce prediction problems in ophthalmology.", "keywords": ["distinguish feature representations", "high diagnostic performance", "technique explored", "distinguish feature", "feature representations", "representations with high", "high diagnostic", "treatment response", "Predicting treatment response", "response", "requires large datasets", "UWFA images", "completed clinical trial", "UWFA", "Deep learning", "diagnostic performance", "AUC", "requires large", "Predicting treatment", "model"], "paper_title": "Evaluating the utility of deep learning for predicting therapeutic response in diabetic eye disease.", "last_updated": "2023/02/04"}, {"id": "0034432726", "domain": "Diabetic retinopathy", "model_name": "Yang et al.", "publication_date": "2022/02/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34432726/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34432726", "task": null, "abstract": "We aimed to develop and test a deep-learning system to perform image quality and diabetic macular ischemia (DMI) assessment on optical coherence tomography angiography (OCTA) images. This study included 7,194 OCTA images with diabetes mellitus for training and primary validation and 960 images from three independent data sets for external testing. A trinary classification for image quality assessment and the presence or absence of DMI for DMI assessment were labeled on all OCTA images. Two DenseNet-161 models were built for both tasks for OCTA images of superficial and deep capillary plexuses, respectively. External testing was performed on three unseen data sets in which one data set using the same model of OCTA device as of the primary data set and two data sets using another brand of OCTA device. We assessed the performance by using the area under the receiver operating characteristic curves with sensitivities, specificities, and accuracies and the area under the precision-recall curves with precision. For the image quality assessment, analyses for gradability and measurability assessment were performed. Our deep-learning system achieved the area under the receiver operating characteristic curves >0.948 and area under the precision-recall curves >0.866 for the gradability assessment, area under the receiver operating characteristic curves >0.960 and area under the precision-recall curves >0.822 for the measurability assessment, and area under the receiver operating characteristic curves >0.939 and area under the precision-recall curves >0.899 for the DMI assessment across three external validation data sets. Grad-CAM demonstrated the capability of our deep-learning system paying attention to regions related to DMI identification. Our proposed multitask deep-learning system might facilitate the development of a simplified assessment of DMI on OCTA images among individuals with diabetes mellitus at high risk for visual loss.", "keywords": ["OCTA images", "diabetic macular ischemia", "coherence tomography angiography", "optical coherence tomography", "OCTA", "operating characteristic curves", "image quality assessment", "receiver operating characteristic", "perform image quality", "area", "image quality", "data sets", "DMI", "curves", "characteristic curves", "precision-recall curves", "assessment", "OCTA device", "images", "macular ischemia"], "paper_title": "A MULTITASK DEEP-LEARNING SYSTEM FOR ASSESSMENT OF DIABETIC MACULAR ISCHEMIA ON OPTICAL COHERENCE TOMOGRAPHY ANGIOGRAPHY IMAGES.", "last_updated": "2023/02/04"}, {"id": "0028359545", "domain": "Diabetic retinopathy", "model_name": "Gargeya et al.", "publication_date": "2017/03/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28359545/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28359545", "task": null, "abstract": "Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal screening examinations on all diabetic patients is an unmet need, and there are many undiagnosed and untreated cases of DR. The objective of this study was to develop robust diagnostic technology to automate DR screening. Referral of eyes with DR to an ophthalmologist for further evaluation and treatment would aid in reducing the rate of vision loss, enabling timely and accurate diagnoses. We developed and evaluated a data-driven deep learning algorithm as a novel diagnostic tool for automated DR detection. The algorithm processed color fundus images and classified them as healthy (no\u00a0retinopathy) or having DR, identifying relevant cases for medical referral. A total of 75\u2009137 publicly available fundus images from diabetic patients were used to train and test an artificial intelligence model to differentiate healthy fundi from those with DR. A panel of retinal specialists determined the ground truth for our data set before experimentation. We also tested our model using the public MESSIDOR 2 and E-Ophtha databases for external validation. Information learned in our automated method was visualized readily through an automatically generated abnormality heatmap, highlighting subregions within each input fundus image for further clinical review. We used area under the receiver operating characteristic curve (AUC) as a metric to measure the precision-recall trade-off of our algorithm, reporting associated sensitivity and specificity metrics on the receiver operating characteristic curve. Our model achieved a 0.97 AUC with a 94% and 98% sensitivity and specificity, respectively, on 5-fold cross-validation using our local data set. Testing against the independent MESSIDOR 2 and E-Ophtha databases achieved a 0.94 and 0.95 AUC score, respectively. A fully data-driven artificial intelligence-based grading algorithm can be used to screen fundus photographs obtained from diabetic patients and to identify, with high reliability, which cases should be referred to an ophthalmologist for further evaluation and treatment. The implementation of such an algorithm on a global basis could reduce drastically the rate of vision loss attributed to DR.", "keywords": ["preventable blindness globally", "blindness globally", "preventable blindness", "diabetic patients", "Diabetic", "algorithm", "AUC", "fundus", "fundus images", "Performing retinal screening", "retinal screening examinations", "Diabetic retinopathy", "patients", "cases", "operating characteristic curve", "model", "globally", "MESSIDOR", "rate of vision", "receiver operating characteristic"], "paper_title": "Automated Identification of Diabetic Retinopathy Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0032092614", "domain": "Diabetic retinopathy", "model_name": "Wang et al.", "publication_date": "2020/02/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32092614/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32092614", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR), which is generally diagnosed by the presence of hemorrhages and hard exudates, is one of the most prevalent causes of visual impairment and blindness. Early detection of hard exudates (HEs) in color fundus photographs can help in preventing such destructive damage. However, this is a challenging task due to high intra-class diversity and high similarity with other structures in the fundus images. Most of the existing methods for detecting HEs are based on characterizing HEs using hand crafted features (HCFs) only, which can not characterize HEs accurately. Deep learning methods are scarce in this domain because they require large-scale sample sets for training which are not generally available for most routine medical imaging research. To address these challenges, we propose a novel methodology for HE detection using deep convolutional neural network (DCNN) and multi-feature joint representation. Specifically, we present a new optimized mathematical morphological approach that first segments HE candidates accurately. Then, each candidate is characterized using combined features based on deep features with HCFs incorporated, which is implemented by a ridge regression-based feature fusion. This method employs multi-space-based intensity features, geometric features, a gray-level co-occurrence matrix (GLCM)-based texture descriptor, a gray-level size zone matrix (GLSZM)-based texture descriptor to construct HCFs, and a DCNN to automatically learn the deep information of HE. Finally, a random forest is employed to identify the true HEs among candidates. The proposed method is evaluated on two benchmark databases. It obtains an F-score of 0.8929 with an area under curve (AUC) of 0.9644 on the e-optha database and an F-score of 0.9326 with an AUC of 0.9323 on the HEI-MED database. These results demonstrate that our approach outperforms state-of-the-art methods. Our model also proves to be suitable for clinical applications based on private clinical images from a local hospital. This newly proposed method integrates the traditional HCFs and deep features learned from DCNN for detecting HEs. It achieves a new state-of-the-art in both detecting HEs and DR screening. Furthermore, the proposed feature selection and fusion strategy reduces feature dimension and improves HE detection performance.", "keywords": ["hard exudates", "Diabetic retinopathy", "impairment and blindness", "presence of hemorrhages", "visual impairment", "HEs", "features", "based", "Deep", "exudates", "DCNN", "generally diagnosed", "hard", "HCFs", "hemorrhages and hard", "detecting HEs", "detection", "detecting", "F-score", "methods"], "paper_title": "Hard exudate detection based on deep model learned information and multi-feature joint representation for diabetic retinopathy screening.", "last_updated": "2023/02/04"}, {"id": "0033120397", "domain": "Diabetic retinopathy", "model_name": "R\u00eago et al.", "publication_date": "2020/10/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33120397/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33120397", "task": "IWqQC1koJA", "abstract": "To evaluate the diagnostic accuracy of a diagnostic system software for the automated screening of diabetic retinopathy (DR) on digital colour fundus photographs, the 2019 Convolutional Neural Network (CNN) model with Inception-V3. In this cross-sectional study, 295 fundus images were analysed by the CNN model and compared to a panel of ophthalmologists. Images were obtained from a dataset acquired within a screening programme. Diagnostic accuracy measures and respective 95% CI were calculated. The sensitivity and specificity of the CNN model in diagnosing referable DR was 81% (95% CI 66-90%) and 97% (95% CI 95-99%), respectively. Positive predictive value was 86% (95% CI 72-94%) and negative predictive value 96% (95% CI 93-98%). The positive likelihood ratio was 33 (95% CI 15-75) and the negative was 0.20 (95% CI 0.11-0.35). Its clinical impact is demonstrated by the change observed in the pre-test probability of referable DR (assuming a prevalence of 16%) to a post-test probability for a positive test result of 86% and for a negative test result of 4%. A CNN model negative test result safely excludes DR, and its use may significantly reduce the burden of ophthalmologists at reading centres.", "keywords": ["Convolutional Neural Network", "Convolutional Neural", "Neural Network", "colour fundus photographs", "digital colour fundus", "diagnostic system software", "CNN model", "diabetic retinopathy", "system software", "digital colour", "CNN model negative", "CNN", "fundus photographs", "colour fundus", "Convolutional", "Network", "automated screening", "diagnostic system", "Neural", "diagnostic accuracy"], "paper_title": "Screening for Diabetic Retinopathy Using an Automated Diagnostic System Based on Deep Learning: Diagnostic Accuracy Assessment.", "last_updated": "2023/02/04"}, {"id": "0035569335", "domain": "Diabetic retinopathy", "model_name": "Islam et al.", "publication_date": "2022/05/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35569335/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35569335", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) is a major complication in human eyes among the diabetic patients. Early detection of the DR can save many patients from permanent blindness. Various artificial intelligent based systems have been proposed and they outperform human analysis in accurate detection of the DR. In most of the traditional deep learning models, the cross-entropy is used as a common loss function in a single stage end-to-end training method. However, it has been recently identified that this loss function has some limitations such as poor margin leading to false results, sensitive to noisy data and hyperparameter variations. To overcome these issues, supervised contrastive learning (SCL) has been introduced. In this study, SCL method, a two-stage training method with supervised contrastive loss function was proposed for the first time to the best of authors' knowledge to identify the DR and its severity stages from fundus images (FIs) using \"APTOS 2019 Blindness Detection\" dataset. \"Messidor-2\" dataset was also used to conduct experiments for further validating the model's performance. Contrast Limited Adaptive Histogram Equalization (CLAHE) was applied for enhancing the image quality and the pre-trained Xception CNN model was deployed as the encoder with transfer learning. To interpret the SCL of the model, t-SNE method was used to visualize the embedding space (unit hyper sphere) composed of 128 D space into a 2 D space. The proposed model achieved a test accuracy of 98.36%, and AUC score of 98.50% to identify the DR (Binary classification) and a test accuracy of 84.364%, and AUC score of 93.819% for five stages grading with the APTOS 2019 dataset. Other evaluation metrics (precision, recall, F1-score) were also determined with APTOS 2019 as well as with Messidor-2 for analyzing the performance of the proposed model. It was also concluded that the proposed method achieved better performance in detecting the DR compared to the conventional CNN without SCL and other state-of-the-art methods.", "keywords": ["Diabetic Retinopathy", "diabetic patients", "major complication", "loss function", "human eyes", "Diabetic", "Retinopathy", "SCL", "model", "Xception CNN model", "proposed", "method", "detection", "proposed model", "Adaptive Histogram Equalization", "Blindness Detection", "APTOS", "Limited Adaptive Histogram", "AUC score", "training method"], "paper_title": "Applying supervised contrastive learning for the detection of diabetic retinopathy and its severity levels from fundus images.", "last_updated": "2023/02/04"}, {"id": "0033323239", "domain": "Diabetic retinopathy", "model_name": "Bellemo et al.", "publication_date": "2019/05/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33323239/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33323239", "task": null, "abstract": "Radical measures are required to identify and reduce blindness due to diabetes to achieve the Sustainable Development Goals by 2030. Therefore, we evaluated the accuracy of an artificial intelligence (AI) model using deep learning in a population-based diabetic retinopathy screening programme in Zambia, a lower-middle-income country. We adopted an ensemble AI model consisting of a combination of two convolutional neural networks (an adapted VGGNet architecture and a residual neural network architecture) for classifying retinal colour fundus images. We trained our model on 76\u2008370 retinal fundus images from 13\u2008099 patients with diabetes who had participated in the Singapore Integrated Diabetic Retinopathy Program, between 2010 and 2013, which has been published previously. In this clinical validation study, we included all patients with a diagnosis of diabetes that attended a mobile screening unit in five urban centres in the Copperbelt province of Zambia from Feb 1 to June 31, 2012. In our model, referable diabetic retinopathy was defined as moderate non-proliferative diabetic retinopathy or worse, diabetic macular oedema, and ungradable images. Vision-threatening diabetic retinopathy comprised severe non-proliferative and proliferative diabetic retinopathy. We calculated the area under the curve (AUC), sensitivity, and specificity for referable diabetic retinopathy, and sensitivities of vision-threatening diabetic retinopathy and diabetic macular oedema compared with the grading by retinal specialists. We did a multivariate analysis for systemic risk factors and referable diabetic retinopathy between AI and human graders. A total of 4504 retinal fundus images from 3093 eyes of 1574 Zambians with diabetes were prospectively recruited. Referable diabetic retinopathy was found in 697 (22\u00b75%) eyes, vision-threatening diabetic retinopathy in 171 (5\u00b75%) eyes, and diabetic macular oedema in 249 (8\u00b71%) eyes. The AUC of the AI system for referable diabetic retinopathy was 0\u00b7973 (95% CI 0\u00b7969-0\u00b7978), with corresponding sensitivity of 92\u00b725% (90\u00b710-94\u00b712) and specificity of 89\u00b704% (87\u00b785-90\u00b728). Vision-threatening diabetic retinopathy sensitivity was 99\u00b742% (99\u00b715-99\u00b768) and diabetic macular oedema sensitivity was 97\u00b719% (96\u00b761-97\u00b777). The AI model and human graders showed similar outcomes in referable diabetic retinopathy prevalence detection and systemic risk factors associations. Both the AI model and human graders identified longer duration of diabetes, higher level of glycated haemoglobin, and increased systolic blood pressure as risk factors associated with referable diabetic retinopathy. An AI system shows clinically acceptable performance in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, and diabetic macular oedema in population-based diabetic retinopathy screening. This shows the potential application and adoption of such AI technology in an under-resourced African population to reduce the incidence of preventable blindness, even when the model is trained in a different population. National Medical Research Council Health Service Research Grant, Large Collaborative Grant, Ministry of Health, Singapore; the SingHealth Foundation; and the Tanoto Foundation.", "keywords": ["Sustainable Development Goals", "diabetic retinopathy", "referable diabetic retinopathy", "Sustainable Development", "Development Goals", "Vision-threatening diabetic retinopathy", "diabetic", "referable diabetic", "diabetic macular oedema", "retinopathy", "diabetic macular", "achieve the Sustainable", "diabetic retinopathy screening", "Vision-threatening diabetic", "Diabetic Retinopathy Program", "Integrated Diabetic Retinopathy", "population-based diabetic retinopathy", "Singapore Integrated Diabetic", "macular oedema", "referable"], "paper_title": "Artificial intelligence using deep learning to screen for referable and vision-threatening diabetic retinopathy in Africa: a clinical validation study.", "last_updated": "2023/02/04"}, {"id": "0035204404", "domain": "Diabetic retinopathy", "model_name": "Alryalat et al.", "publication_date": "2022/01/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35204404/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35204404", "task": null, "abstract": "Diabetic macular edema (DME) is the most common cause of visual impairment among patients with diabetes mellitus. Anti-vascular endothelial growth factors (Anti-VEGFs) are considered the first line in its management. The aim of this research has been to develop a deep learning (DL) model for predicting response to intravitreal anti-VEGF injections among DME patients. The research included treatment naive DME patients who were treated with anti-VEGF. Patient's pre-treatment and post-treatment clinical and macular optical coherence tomography (OCT) were assessed by retina specialists, who annotated pre-treatment images for five prognostic features. Patients were also classified based on their response to treatment in their post-treatment OCT into either good responder, defined as a reduction of thickness by >25% or 50 \u00b5m by 3 months, or poor responder. A novel modified U-net DL model for image segmentation, and another DL EfficientNet-B3 model for response classification were developed and implemented for predicting response to anti-VEGF injections among patients with DME. Finally, the classification DL model was compared with different levels of ophthalmology residents and specialists regarding response classification accuracy. The segmentation deep learning model resulted in segmentation accuracy of 95.9%, with a specificity of 98.9%, and a sensitivity of 87.9%. The classification accuracy of classifying patients' images into good and poor responders reached 75%. Upon comparing the model's performance with practicing ophthalmology residents, ophthalmologists and retina specialists, the model's accuracy is comparable to ophthalmologist's accuracy. The developed DL models can segment and predict response to anti-VEGF treatment among DME patients with comparable accuracy to general ophthalmologists. Further training on a larger dataset is nonetheless needed to yield more accurate response predictions.", "keywords": ["DME patients", "Diabetic macular edema", "DME", "diabetes mellitus", "visual impairment", "patients", "response", "model", "accuracy", "naive DME patients", "anti-VEGF", "Diabetic macular", "macular edema", "classification", "anti-VEGF injections", "naive DME", "OCT", "treatment naive DME", "predicting response", "response classification"], "paper_title": "Deep Learning Prediction of Response to Anti-VEGF among Diabetic Macular Edema Patients: Treatment Response Analyzer System (TRAS).", "last_updated": "2023/02/04"}, {"id": "0034767624", "domain": "Diabetic retinopathy", "model_name": "Wongchaisuwat et al.", "publication_date": "2022/01/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34767624/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34767624", "task": null, "abstract": "To evaluate the clinical performance of an automated diabetic retinopathy (DR) screening model to detect referable cases at Siriraj Hospital, Bangkok, Thailand. A retrospective review of two sets of fundus photographs (Eidon and Nidek) was undertaken. The images were classified by DR staging prior to the development of a DR screening model. In a prospective cross-sectional enrollment of patients with diabetes, automated detection of referable DR was compared with the results of the gold standard, a dilated fundus examination. The study analyzed 2533 Nidek fundus images and 1989 Eidon images. The sensitivities calculated for the Nidek and Eidon images were 0.93 and 0.88 and the specificities were 0.91 and 0.85, respectively. In a clinical verification phase using 982 Nidek and 674 Eidon photographs, the calculated sensitivities and specificities were 0.86 and 0.92 for Nidek along with 0.92 and 0.84 for Eidon, respectively. The 60\u00b0-field images from the Eidon yielded a more desirable performance in differentiating referable DR than did the corresponding images from the Nidek. A conventional fundus examination requires intense healthcare resources. It is time consuming and possibly leads to unavoidable human errors. The deep learning algorithm for the detection of referable DR exhibited a favorable performance and is a promising alternative for DR screening. However, variations in the color and pixels of photographs can cause differences in sensitivity and specificity. The image angle and poor quality of fundus photographs were the main limitations of the automated method. The deep learning algorithm, developed from basic research of image processing, was applied to detect referable DR in a real-word clinical care setting.", "keywords": ["Siriraj Hospital", "cases at Siriraj", "automated diabetic retinopathy", "Nidek fundus images", "Nidek", "Eidon", "diabetic retinopathy", "Eidon images", "detect referable cases", "screening model", "images", "Bangkok", "Thailand", "Hospital", "Siriraj", "fundus", "referable", "Nidek fundus", "referable cases", "automated diabetic"], "paper_title": "In-Person Verification of Deep Learning Algorithm for Diabetic Retinopathy Screening Using Different Techniques Across Fundus Image Devices.", "last_updated": "2023/02/04"}, {"id": "0034423259", "domain": "Diabetic retinopathy", "model_name": "Ogunyemi et al.", "publication_date": "2021/08/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34423259/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34423259", "task": "IWqQC1koJA", "abstract": "Clinical guidelines recommend annual eye examinations to detect diabetic retinopathy (DR) in patients with diabetes. However, timely DR detection remains a problem in medically underserved and under-resourced settings in the United States. Machine learning that identifies patients with latent/undiagnosed DR could help to address this problem. Using electronic health record data from 40 631 unique diabetic patients seen at Los Angeles County Department of Health Services healthcare facilities between January 1, 2015 and December 31, 2017, we compared ten machine learning environments, including five classifier models, for assessing the presence or absence of DR. We also used data from a distinct set of 9300 diabetic patients seen between January 1, 2018 and December 31, 2018 as an external validation set. Following feature subset selection, the classifier with the best AUC on the external validation set was a deep neural network using majority class undersampling, with an AUC of 0.8, the sensitivity of 72.17%, and specificity of 74.2%. A deep neural network produced the best AUCs and sensitivity results on the test set and external validation set. Models are intended to be used to screen guideline noncompliant diabetic patients in an urban safety-net setting. Machine learning on diabetic patients' routinely collected clinical data could help clinicians in safety-net settings to identify and target unscreened diabetic patients who potentially have undiagnosed DR.", "keywords": ["recommend annual eye", "annual eye examinations", "detect diabetic retinopathy", "guidelines recommend annual", "United States", "recommend annual", "annual eye", "eye examinations", "examinations to detect", "Los Angeles County", "Angeles County Department", "diabetic patients", "patients", "external validation set", "diabetic", "Machine learning", "external validation", "validation set", "diabetic retinopathy", "set"], "paper_title": "Detecting diabetic retinopathy through machine learning on electronic health record data from an urban, safety net healthcare system.", "last_updated": "2023/02/04"}, {"id": "0035731541", "domain": "Diabetic retinopathy", "model_name": "Andersen et al.", "publication_date": "2022/06/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35731541/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35731541", "task": "aipbNdPTIt", "abstract": "Classification of diabetic retinopathy (DR) is traditionally based on severity grading, given by the most advanced lesion, but potentially leaving out relevant information for risk stratification. In this study, we aimed to develop a deep learning model able to individually segment seven different DR-lesions, in order to test if this would improve a subsequently developed classification model. First, manual segmentation of 34,075 different DR-lesions was used to construct a segmentation model, with performance subsequently compared to another retinal specialist. Second, we constructed a 5-step classification model using a data set of 31,325 expert-annotated retinal 6-field images and evaluated if performance was improved with the integration of presegmentation given by the segmentation model. The segmentation model had higher average sensitivity across all abnormalities compared to the retinal expert (0.68 and 0.62) at a comparable average F1-score (0.60 and 0.62). Model sensitivity for microaneurysms, retinal hemorrhages and intraretinal microvascular abnormalities was higher by 42.5%, 8.8%, and 67.5% and F1-scores by 15.8%, 6.5%, and 12.5%, respectively. When presegmentation was included, grading performance increased by 29.7%, 6.0%, and 4.5% for average per class accuracy, quadratic weighted kappa, and multiclass macro area under the curve, with values of 70.4%, 0.90, and 0.92, respectively. The segmentation model matched an expert in detecting retinal abnormalities, and presegmentation substantially improved accuracy of the automated classification model. Presegmentation may yield more accurate automated DR grading models and increase interpretability and trust in model decisions.", "keywords": ["diabetic retinopathy", "advanced lesion", "risk stratification", "model", "segmentation model", "traditionally based", "based on severity", "potentially leaving", "leaving out relevant", "relevant information", "information for risk", "classification model", "segmentation", "Classification", "retinal", "developed classification model", "subsequently developed classification", "severity grading", "presegmentation", "performance"], "paper_title": "Automatic Detection of Abnormalities and Grading of Diabetic Retinopathy in 6-Field Retinal Images: Integration of Segmentation Into Classification.", "last_updated": "2023/02/04"}, {"id": "0034865312", "domain": "Disc hemorrhage", "model_name": "Pugal Priya et al.", "publication_date": "2021/12/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34865312/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "34865312", "task": "IWqQC1koJA", "abstract": "Because of retina abnormalities of diabetic patients, the most common vision-threatening disease is diabetic retinopathy (DR). The DR diagnosis and prevention are challenging tasks as they may lead to vision loss. According to the literature analysis, the shortcomings in existing studies, such as failed to reduce the feature dimension, higher execution time, and higher computational cost, unable to tune the hyper-parameters, such as a number of hidden layers and learning rate, more computational complexities, higher cost, and so forth, during DR classification. To tackle these problems, we proposed a deep long- and short-term memory (LSTM) in a neural network with Red Fox optimization (deep LSTM-RFO) algorithm for DR classification. The four major components involved in the proposed methods are image preprocessing, segmentation, feature extraction, and classification. At first, an adaptive histogram equalization and histogram equalization model performs the fundus image preprocessing, thereby neglecting the noise and improving the contrast level of an image. Next, an adaptive watershed segmentation model effectively segments the lesion region based on the optic disc color and size of hemorrhages. At the third stage, we have extracted statistical, intensity, color, and shape features. Finally, the single normal class with three abnormal classes such as mild non-proliferative diabetic retinopathy, moderate NPDR, and severe NPDR are accurately classified using the deep LSTM-RFO algorithm. Experimentally, the MESSIDOR, STARE, and DRIVE datasets are used for both training and validation. MATLAB software performs the implementation process with respect to various evaluation criteria used. However, the proposed method accomplished superior performance, such as 98.45% specificity, 96.78% sensitivity, 97.92% precision, 96.89% recall, and 97.93% F-score results in terms of DR classification than previous methods.", "keywords": ["common vision-threatening disease", "retina abnormalities", "common vision-threatening", "vision-threatening disease", "higher computational cost", "diabetic patients", "Red Fox optimization", "classification", "diabetic retinopathy", "image preprocessing", "Red Fox", "higher", "deep LSTM-RFO", "higher execution time", "proposed", "higher cost", "diabetic", "image", "histogram equalization", "deep"], "paper_title": "Deep long and short term memory based Red Fox optimization algorithm for diabetic retinopathy detection and classification.", "last_updated": "2023/02/04"}, {"id": "0031747632", "domain": "Diabetic retinopathy", "model_name": "Zago et al.", "publication_date": "2019/11/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31747632/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31747632", "task": "IWqQC1koJA", "abstract": "Detecting the early signs of diabetic retinopathy (DR) is essential, as timely treatment might reduce or even prevent vision loss. Moreover, automatically localizing the regions of the retinal image that might contain lesions can favorably assist specialists in the task of detection. In this study, we designed a lesion localization model using a deep network patch-based approach. Our goal was to reduce the complexity of the model while improving its performance. For this purpose, we designed an efficient procedure (including two convolutional neural network models) for selecting the training patches, such that the challenging examples would be given special attention during the training process. Using the labeling of the region, a DR decision can be given to the initial image, without the need for special training. The model is trained on the Standard Diabetic Retinopathy Database, Calibration Level 1 (DIARETDB1) database and is tested on several databases (including Messidor) without any further adaptation. It reaches an area under the receiver operating characteristic curve of 0.912-95%CI(0.897-0.928) for DR screening, and a sensitivity of 0.940-95%CI(0.921-0.959). These values are competitive with other state-of-the-art approaches.", "keywords": ["prevent vision loss", "Detecting the early", "vision loss", "early signs", "timely treatment", "prevent vision", "Standard Diabetic Retinopathy", "Diabetic Retinopathy Database", "diabetic retinopathy", "lesion localization model", "favorably assist specialists", "signs of diabetic", "model", "treatment might reduce", "Standard Diabetic", "neural network models", "Retinopathy Database", "training", "Calibration Level", "Detecting"], "paper_title": "Diabetic retinopathy detection using red lesion localization and convolutional neural networks.", "last_updated": "2023/02/04"}, {"id": "0035545738", "domain": "Diabetic retinopathy", "model_name": "AbdelMaksoud et al.", "publication_date": "2022/05/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35545738/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35545738", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a serious disease that may cause vision loss unawares without any alarm. Therefore, it is essential to scan and audit the DR progress continuously. In this respect, deep learning techniques achieved great success in medical image analysis. Deep convolution neural network (CNN) architectures are widely used in multi-label (ML) classification. It helps in diagnosing normal and various DR grades: mild, moderate, and severe non-proliferative DR (NPDR) and proliferative DR (PDR). DR grades are formulated by appearing multiple DR lesions simultaneously on the color retinal fundus images. Many lesion types have various features that are difficult to segment and distinguished by utilizing conventional and hand-crafted methods. Therefore, the practical solution is to utilize an effective CNN model. In this paper, we present a novel hybrid, deep learning technique, which is called E-DenseNet. We integrated EyeNet and DenseNet models based on transfer learning. We customized the traditional EyeNet by inserting the dense blocks and optimized the resulting hybrid E-DensNet model's hyperparameters. The proposed system based on the E-DenseNet model can accurately diagnose healthy and different DR grades from various small and large ML color fundus images. We trained and tested our model on four different datasets that were published from 2006 to 2019. The proposed system achieved an average accuracy (ACC), sensitivity (SEN), specificity (SPE), Dice similarity coefficient (DSC), the quadratic Kappa score (QKS), and the calculation time (T) in minutes (m) equal [Formula: see text], [Formula: see text], [Formula: see text], [Formula: see text], 0.883, and 3.5m respectively. The experiments show promising results as compared with other systems.", "keywords": ["vision loss unawares", "Diabetic retinopathy", "vision loss", "loss unawares", "Formula", "deep learning", "text", "deep", "deep learning techniques", "model", "grades", "learning", "fundus images", "Diabetic", "retinopathy", "alarm", "CNN", "disease", "vision", "loss"], "paper_title": "A computer-aided diagnosis system for detecting various diabetic retinopathy grades based on a hybrid deep learning technique.", "last_updated": "2023/02/04"}, {"id": "0036425113", "domain": "Diabetic retinopathy", "model_name": "dr-detector", "publication_date": "2022/11/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36425113/", "code_link": "https://github.com/janga-lab/dr-detector", "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "36425113", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a late microvascular complication of Diabetes Mellitus (DM) that could lead to permanent blindness in patients, without early detection. Although adequate management of DM <i>via</i> regular eye examination can preserve vision in in 98% of the DR cases, DR screening and diagnoses based on clinical lesion features devised by expert clinicians; are costly, time-consuming and not sufficiently accurate. This raises the requirements for Artificial Intelligent (AI) systems which can accurately detect DR automatically and thus preventing DR before affecting vision. Hence, such systems can help clinician experts in certain cases and aid ophthalmologists in rapid diagnoses. To address such requirements, several approaches have been proposed in the literature that use Machine Learning (ML) and Deep Learning (DL) techniques to develop such systems. However, these approaches ignore the highly valuable clinical lesion features that could contribute significantly to the accurate detection of DR. Therefore, in this study we introduce a framework called DR-detector that employs the Extreme Gradient Boosting (XGBoost) ML model trained <i>via</i> the combination of the features extracted by the pretrained convolutional neural networks commonly known as transfer learning (TL) models and the clinical retinal lesion features for accurate detection of DR. The retinal lesion features are extracted <i>via</i> image segmentation technique using the UNET DL model and captures exudates (EXs), microaneurysms (MAs), and hemorrhages (HEMs) that are relevant lesions for DR detection. The feature combination approach implemented in DR-detector has been applied to two common TL models in the literature namely VGG-16 and ResNet-50. We trained the DR-detector model using a training dataset comprising of 1,840 color fundus images collected from e-ophtha, retinal lesions and APTOS 2019 Kaggle datasets of which 920 images are healthy. To validate the DR-detector model, we test the model on external dataset that consists of 81 healthy images collected from High-Resolution Fundus (HRF) dataset and MESSIDOR-2 datasets and 81 images with DR signs collected from Indian Diabetic Retinopathy Image Dataset (IDRID) dataset annotated for DR by expert. The experimental results show that the DR-detector model achieves a testing accuracy of 100% in detecting DR after training it with the combination of ResNet-50 and lesion features and 99.38% accuracy after training it with the combination of VGG-16 and lesion features. More importantly, the results also show a higher contribution of specific lesion features toward the performance of the DR-detector model. For instance, using only the hemorrhages feature to train the model, our model achieves an accuracy of 99.38 in detecting DR, which is higher than the accuracy when training the model with the combination of all lesion features (89%) and equal to the accuracy when training the model with the combination of all lesions and VGG-16 features together. This highlights the possibility of using only the clinical features, such as lesions that are clinically interpretable, to build the next generation of robust artificial intelligence (AI) systems with great clinical interpretability for DR detection. The code of the DR-detector framework is available on GitHub at https://github.com/Janga-Lab/DR-detector and can be readily employed for detecting DR from retinal image datasets.", "keywords": ["Diabetes Mellitus", "lesion features", "late microvascular complication", "complication of Diabetes", "clinical lesion features", "features", "model", "retinal lesion features", "DR-detector model", "lesion", "DR-detector", "blindness in patients", "late microvascular", "microvascular complication", "lead to permanent", "permanent blindness", "combination", "clinical lesion", "Mellitus", "lesion features devised"], "paper_title": "Combining transfer learning with retinal lesion features for accurate detection of diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0034505925", "domain": "Diabetic retinopathy", "model_name": "Vasireddi et al.", "publication_date": "2021/09/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34505925/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34505925", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) has become a major cause of blindness in recent years. Diabetic patients should be screened on a regular basis for early detection, which can help them avoid blindness. Furthermore, the number of diabetic patients undergoing these screening procedures is rapidly increasing, resulting in increased workload for ophthalmologists. An efficient screening system that assists ophthalmologists in DR diagnosis saves ophthalmologists a lot of time and effort. To address this issue, an automatic DR detection screening system is required to improve diagnosis speed and detection accuracy. Appropriate treatment can be provided to patients to prevent vision loss if the severity levels of DR are accurately diagnosed in the early stages. A growing number of screening systems for DR diagnosis have been developed in recent years using various deep learning models, and the majority of the published work did not include any optimization algorithm in the neural network for severity classification. The use of an optimization algorithm with the necessary hyper parameter tuning will improve the model's performance. Considering this as motivation, we proposed a five-phase DFNN-LOA model. The DFNN-LOA algorithm presented here has five phases: (i) pre-processing, (ii) optic disc detection, (iii) segmentation, (iv) feature extraction, and (v) severity classification. The proposed model's experimental analysis is carried out on the MESSIDOR dataset. The experimental results show that the proposed DFNN-LOA model has superior characteristics, with maximum accuracy, sensitivity, specificity, F1-score, PPV, and NPV of 97.6%, 98.4%, 90.7%, 96.5%, 94.6%, and 97.1%, respectively.", "keywords": ["Diabetic Retinopathy", "Diabetic patients", "Retinopathy", "diabetic patients undergoing", "screening system", "Diabetic", "screening", "detection screening system", "detection", "recent years", "model", "patients", "avoid blindness", "blindness", "ophthalmologists", "diagnosis", "severity classification", "efficient screening system", "DFNN-LOA model", "severity"], "paper_title": "Deep feed forward neural network-based screening system for diabetic retinopathy severity classification using the lion optimization algorithm.", "last_updated": "2023/02/04"}, {"id": "0036552925", "domain": "Diabetic retinopathy", "model_name": "MSLF-Net", "publication_date": "2022/11/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36552925/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36552925", "task": "aipbNdPTIt", "abstract": "Diabetic Retinopathy (DR) is a diabetic complication that predisposes patients to visual impairments that could lead to blindness. Lesion segmentation using deep learning algorithms is an effective measure to screen and prevent early DR. However, there are several types of DR with varying sizes and high inter-class similarity, making segmentation difficult. In this paper, we propose a supervised segmentation method (MSLF-Net) based on multi-scale-multi-level feature fusion to achieve accurate end-to-end DR lesion segmentation. MSLF-Net builds a Multi-Scale Feature Extraction (MSFE) module to extract multi-scale information and provide more comprehensive features for segmentation. This paper further introduces the Multi-Level Feature Fusion (MLFF) module to improve feature fusion using a cross-layer structure. This structure only fuses low- and high-level features of the same class based on category supervision, avoiding feature contamination. Moreover, this paper produces additional masked images for the dataset and performs image enhancement operations to ensure that the proposed method is trainable and functional on small datasets. The extensive experiments are conducted on public datasets IDRID and e_ophtha. The results showed that our proposed feature enhancement method can perform feature fusion more effectively. Therefore, In the end-to-end DR segmentation neural network model, MSLF Net is superior to other similar models in segmentation, and can effectively improve the DR lesion segmentation performance.", "keywords": ["Diabetic Retinopathy", "diabetic complication", "lead to blindness", "complication that predisposes", "predisposes patients", "patients to visual", "visual impairments", "feature fusion", "segmentation", "Retinopathy", "feature", "Lesion segmentation", "Diabetic", "fusion", "Lesion", "paper", "Multi-Scale Feature Extraction", "blindness", "Feature Extraction", "method"], "paper_title": "MSLF-Net: A Multi-Scale and Multi-Level Feature Fusion Net for Diabetic Retinopathy Segmentation.", "last_updated": "2023/02/04"}, {"id": "0031737428", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2019/11/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31737428/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31737428", "task": "IWqQC1koJA", "abstract": "To achieve automatic diabetic retinopathy (DR) detection in retinal fundus photographs through the use of a deep transfer learning approach using the Inception-v3 network. A total of 19,233 eye fundus color numerical images were retrospectively obtained from 5278 adult patients presenting for DR screening. The 8816 images passed image-quality review and were graded as no apparent DR (1374 images), mild nonproliferative DR (NPDR) (2152 images), moderate NPDR (2370 images), severe NPDR (1984 images), and proliferative DR (PDR) (936 images) by eight retinal experts according to the International Clinical Diabetic Retinopathy severity scale. After image preprocessing, 7935 DR images were selected from the above categories as a training dataset, while the rest of the images were used as validation dataset. We introduced a 10-fold cross-validation strategy to assess and optimize our model. We also selected the publicly independent Messidor-2 dataset to test the performance of our model. For discrimination between no referral (no apparent DR and mild NPDR) and referral (moderate NPDR, severe NPDR, and PDR), we also computed prediction accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and \u03ba value. The proposed approach achieved a high classification accuracy of 93.49% (95% confidence interval [CI], 93.13%-93.85%), with a 96.93% sensitivity (95% CI, 96.35%-97.51%) and a 93.45% specificity (95% CI, 93.12%-93.79%), while the AUC was up to 0.9905 (95% CI, 0.9887-0.9923) on the independent test dataset. The \u03ba value of our best model was 0.919, while the three experts had \u03ba values of 0.906, 0.931, and 0.914, independently. This approach could automatically detect DR with excellent sensitivity, accuracy, and specificity and could aid in making a referral recommendation for further evaluation and treatment with high reliability. This approach has great value in early DR screening using retinal fundus photographs.", "keywords": ["deep transfer learning", "achieve automatic diabetic", "automatic diabetic retinopathy", "Clinical Diabetic Retinopathy", "International Clinical Diabetic", "transfer learning approach", "diabetic retinopathy", "NPDR", "Diabetic Retinopathy severity", "images", "retinal fundus photographs", "moderate NPDR", "severe NPDR", "achieve automatic", "deep transfer", "transfer learning", "automatic diabetic", "retinal fundus", "Clinical Diabetic", "fundus photographs"], "paper_title": "Automatic Detection of Diabetic Retinopathy in Retinal Fundus Photographs Based on Deep Learning Algorithm.", "last_updated": "2023/02/04"}, {"id": "0032523046", "domain": "Disc hemorrhage", "model_name": "Gerrits et al.", "publication_date": "2020/06/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32523046/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "32523046", "task": null, "abstract": "Deep neural networks can extract clinical information, such as diabetic retinopathy status and individual characteristics (e.g. age and sex), from retinal images. Here, we report the first study to train deep learning models with retinal images from 3,000 Qatari citizens participating in the Qatar Biobank study. We investigated whether fundus images can predict cardiometabolic risk\u00a0factors, such as age, sex, blood pressure, smoking status, glycaemic status, total lipid panel, sex steroid hormones and bioimpedance measurements. Additionally, the role of age and sex as mediating factors when predicting cardiometabolic risk factors from fundus images was studied. Predictions at person-level were made by combining information of an optic disc centred and a macula centred image of both eyes with deep learning models using the MobileNet-V2 architecture. An accurate prediction was obtained for age (mean absolute error (MAE): 2.78 years) and sex (area under the curve: 0.97), while an acceptable performance was achieved for systolic blood pressure (MAE: 8.96\u2009mmHg), diastolic blood pressure (MAE: 6.84\u2009mmHg), Haemoglobin A1c (MAE: 0.61%), relative fat mass (MAE: 5.68 units) and testosterone (MAE: 3.76 nmol/L). We discovered that age and sex were mediating factors when predicting cardiometabolic risk factors from fundus images. We have found that deep learning models indirectly predict sex when trained for testosterone. For blood pressure, Haemoglobin A1c and relative fat mass an influence of age and sex was observed. However, achieved performance cannot be fully explained by the influence of age and sex. In conclusion we confirm that age and sex can be predicted reliably from a fundus image and that unique information is stored in the retina that relates to blood pressure, Haemoglobin A1c and relative fat mass. Future research should focus on stratification when predicting person characteristics from a fundus image.", "keywords": ["MAE", "Qatar Biobank study", "diabetic retinopathy status", "Deep neural networks", "cardiometabolic risk factors", "blood pressure", "age", "sex", "extract clinical information", "Qatar Biobank", "deep learning models", "age and sex", "risk factors", "neural networks", "networks can extract", "extract clinical", "diabetic retinopathy", "fundus images", "pressure", "retinal images"], "paper_title": "Age and sex affect deep learning prediction of cardiometabolic risk factors from retinal images.", "last_updated": "2023/02/04"}, {"id": "0034488028", "domain": "Diabetic retinopathy", "model_name": "Sugeno et al.", "publication_date": "2021/08/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34488028/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34488028", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) has become one of the major causes of blindness. Due to the increased prevalence of diabetes worldwide, diabetic patients exhibit high probabilities of developing DR. There is a need to develop a labor-less computer-aided diagnosis system to support the clinical diagnosis. Here, we attempted to develop simple methods for severity grading and lesion detection from retinal fundus images. We developed a severity grading system for DR by transfer learning with a recent convolutional neural network called EfficientNet-B3 and the publicly available Kaggle Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 training dataset, which includes artificial noise. After removing the blurred and duplicated images from the dataset using a numerical threshold, the trained model achieved specificity and sensitivity values\u00a0\u2273\u00a00.98 in the identification of DR retinas. For severity grading, the classification accuracy values of 0.84, 0.95, and 0.98 were recorded for the 1st, 2nd, and 3rd predicted labels, respectively. The utility of EfficientNets-B3 for the severity grading of DR as well as the detailed retinal areas referred were confirmed via visual explanation methods of convolutional neural networks. Lesion extraction was performed by applying an empirically defined threshold value to the enhanced retinal images. Although the extraction of blood vessels and detection of red lesions occurred simultaneously, the red and white lesions, including both soft and hard exudates, were clearly extracted. The detected lesion areas were further confirmed with ground truth using the DIARETDB1 database images with general accuracy. The simple and easily applicable methods proposed in this study will aid in the detection and severity grading of DR, which might help in the selection of appropriate treatment strategies for DR.", "keywords": ["severity grading", "Diabetic retinopathy", "diabetic patients exhibit", "Kaggle Asia Pacific", "severity grading system", "grading", "severity", "diabetic patients", "Pacific Tele-Ophthalmology Society", "Diabetic", "Asia Pacific Tele-Ophthalmology", "images", "patients exhibit high", "exhibit high probabilities", "Kaggle Asia", "Asia Pacific", "convolutional neural", "lesion", "grading system", "computer-aided diagnosis system"], "paper_title": "Simple methods for the lesion detection and severity grading of diabetic retinopathy by image processing and transfer learning.", "last_updated": "2023/02/04"}, {"id": "0029060228", "domain": "Diabetic retinopathy", "model_name": "Ardiyanto et al.", "publication_date": "2018/08/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29060228/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "29060228", "task": null, "abstract": "Diabetic Retinopathy (DR) is a disease which affect the vision ability. The observation by an ophthalmologist usually conducted by analyzing the retinal images of the patient which are marked by some DR features. However some misdiagnosis are usually found due to human error. Here, a deep learning-based low-cost embedded system is established to assist the doctor for grading the severity of the DR from the retinal images. A compact deep learning algorithm named Deep-DR-Net which fits on a small embedded board is afterwards proposed for such purposes. In the heart of Deep-DR-Net, a cascaded encoder-classifier network is arranged using residual style for ensuring the small model size. The usage of different types of convolutional layers subsequently guarantees the features richness of the network for differentiating the grade of the DR. Experimental results show the capability of the proposed system for detecting the existence as well as grading the severity of the DR symptomps.", "keywords": ["Diabetic Retinopathy", "vision ability", "disease which affect", "affect the vision", "retinal images", "Retinopathy", "grading the severity", "analyzing the retinal", "low-cost embedded system", "Diabetic", "ability", "small embedded board", "disease", "affect", "vision", "ophthalmologist usually conducted", "conducted by analyzing", "retinal", "images", "deep learning-based low-cost"], "paper_title": "Deep learning-based Diabetic Retinopathy assessment on embedded system.", "last_updated": "2023/02/04"}, {"id": "0031968187", "domain": "Diabetic retinopathy", "model_name": "Zhao et al.", "publication_date": "2020/01/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31968187/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31968187", "task": "IWqQC1koJA", "abstract": "<b><i>Objective:</i></b> To characterize the relationship between diabetic retinopathy (DR) and diabetic nephropathy (DN) in Chinese patients and to determine whether the severity of DR predicts end-stage renal disease (ESRD). <b><i>Methods:</i></b> Bilateral fundic photographs of 91 Chinese type 2 diabetic patients with biopsy-confirmed DN, not in ESRD stage, were obtained at the time of renal biopsy in this longitudinal study. The baseline severity of DR was determined using the Lesion-aware Deep Learning System (RetinalNET) in an open framework for deep learning and was graded using the Early Treatment Diabetic Retinopathy Study severity scale. Cox proportional hazard models were used to estimate the hazard ratio (HR) for the effect of the severity of diabetic retinopathy on ESRD. <b><i>Results:</i></b> During a median follow-up of 15 months, 25 patients progressed to ESRD. The severity of retinopathy at the time of biopsy was a prognostic factor for progression to ESRD (HR 2.18, 95% confidence interval 1.05 to 4.53, <i>P</i> = .04). At baseline, more severe retinopathy was associated with poor renal function, and more severe glomerular lesions. However, 30% of patients with mild retinopathy and severe glomerular lesions had higher low-density lipo-protein-cholesterol and more severe proteinuria than those with mild glomerular lesions. Additionally, 3% of patients with severe retinopathy and mild glomerular changes were more likely to have had diabetes a long time than those with severe glomerular lesions. <b><i>Conclusion:</i></b> Although the severity of DR predicted diabetic ESRD in patients with type 2 diabetes mellitus and DN, the severities of DR and DN were not always consistent, especially in patients with mild retinopathy or microalbuminuria. <b>Abbreviations: CI</b> = confidence interval; <b>DM</b> = diabetic mellitus; <b>DN</b> = diabetic nephropathy; <b>DR</b> = diabetic retinopathy; <b>eGFR</b> = estimated glomerular filtration rate; <b>ESRD</b> = end-stage renal disease; <b>HbA1c</b> = hemoglobin A1c; <b>HR</b> = hazard ratio; <b>NPDR</b> = nonproliferative diabetic retinopathy; <b>PDR</b> = proliferative diabetic retinopathy; <b>SBP</b> = systolic blood pressure; <b>T2DM</b> = type 2 diabetes mellitus; <b>VEGF</b> = vascular endothelial growth factor.", "keywords": ["diabetic retinopathy", "Diabetic Retinopathy Study", "Treatment Diabetic Retinopathy", "Early Treatment Diabetic", "diabetic", "Retinopathy Study severity", "retinopathy", "ESRD", "Deep Learning System", "severe glomerular lesions", "Lesion-aware Deep Learning", "Chinese patients", "predicted diabetic ESRD", "diabetic ESRD", "severity", "patients", "glomerular lesions", "Deep Learning", "characterize the relationship", "severe glomerular"], "paper_title": "DIABETIC RETINOPATHY, CLASSIFIED USING THE LESION-AWARE DEEP LEARNING SYSTEM, PREDICTS DIABETIC END-STAGE RENAL DISEASE IN CHINESE PATIENTS.", "last_updated": "2023/02/04"}, {"id": "0030798455", "domain": "Diabetic retinopathy", "model_name": "Nagasawa et al.", "publication_date": "2019/02/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30798455/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30798455", "task": "IWqQC1koJA", "abstract": "We investigated using ultrawide-field fundus images with a deep convolutional neural network (DCNN), which is a machine learning technology, to detect treatment-na\u00efve proliferative diabetic retinopathy (PDR). We conducted training with the DCNN using 378 photographic images (132 PDR and 246 non-PDR) and constructed a deep learning model. The area under the curve (AUC), sensitivity, and specificity were examined. The constructed deep learning model demonstrated a high sensitivity of 94.7% and a high specificity of 97.2%, with an AUC of 0.969. Our findings suggested that PDR could be diagnosed using wide-angle camera images and deep learning.", "keywords": ["convolutional neural network", "proliferative diabetic retinopathy", "detect treatment-na\u00efve proliferative", "treatment-na\u00efve proliferative diabetic", "machine learning technology", "deep convolutional neural", "ultrawide-field fundus images", "neural network", "diabetic retinopathy", "investigated using ultrawide-field", "ultrawide-field fundus", "convolutional neural", "detect treatment-na\u00efve", "treatment-na\u00efve proliferative", "proliferative diabetic", "deep learning", "deep learning model", "learning technology", "PDR", "DCNN"], "paper_title": "Accuracy of ultrawide-field fundus ophthalmoscopy-assisted deep learning for detecting treatment-na\u00efve proliferative diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0031407110", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2019/08/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31407110/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31407110", "task": "aipbNdPTIt", "abstract": "Due to insufficient samples, the generalization performance of deep network is insufficient. In order to solve this problem, an improved U-net based image automatic segmentation and diagnosis algorithm was proposed, in which the max-pooling operation in original U-net model was replaced by the convolution operation to keep more feature information. Firstly, the regions of 128\u00d7128 were extracted from all slices of the patients as data samples. Secondly, the patient samples were divided into training sample set and testing sample set, and data augmentation was performed on the training samples. Finally, all the training samples were adopted to train the model. Compared with Fully Convolutional Network (FCN) model and max-pooling based U-net model, DSC and CR coefficients of the proposed method achieve the best results, while PM coefficient is 2.55 percentage lower than the maximum value in the two comparison models, and Average Symmetric Surface Distance is slightly higher than the minimum value of the two comparison models by 0.004. The experimental results show that the proposed model can achieve good segmentation and diagnosis results.", "keywords": ["generalization performance", "performance of deep", "insufficient samples", "U-net model", "original U-net model", "samples", "Fully Convolutional Network", "Due to insufficient", "based U-net model", "training samples", "deep network", "Average Symmetric Surface", "Symmetric Surface Distance", "improved U-net based", "U-net based image", "insufficient", "training sample set", "max-pooling based U-net", "sample set", "model"], "paper_title": "An Intelligent Segmentation and Diagnosis Method for Diabetic Retinopathy Based on Improved U-NET Network.", "last_updated": "2023/02/04"}, {"id": "0031281057", "domain": "Disc hemorrhage", "model_name": "Son et al.", "publication_date": "2019/05/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31281057/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "31281057", "task": null, "abstract": "To develop and evaluate deep learning models that screen multiple abnormal findings in retinal fundus images. Cross-sectional study. For the development and testing of deep learning models, 309\u2009786 readings from 103\u2009262 images were used. Two additional external datasets (the Indian Diabetic Retinopathy Image Dataset and e-ophtha) were used for testing. A third external dataset (Messidor) was used for comparison of the models with human experts. Macula-centered retinal fundus images from the Seoul National University Bundang Hospital Retina Image Archive, obtained at the health screening center and ophthalmology outpatient clinic at Seoul National University Bundang Hospital, were assessed for 12 major findings (hemorrhage, hard exudate, cotton-wool patch, drusen, membrane, macular hole, myelinated nerve fiber, chorioretinal atrophy or scar, any vascular abnormality, retinal nerve fiber layer defect, glaucomatous disc change, and nonglaucomatous disc change) with their regional information using deep learning algorithms. Area under the receiver operating characteristic curve and sensitivity and specificity of the deep learning algorithms at the highest harmonic mean were evaluated and compared with the performance of retina specialists, and visualization of the lesions was qualitatively analyzed. Areas under the receiver operating characteristic curves for all findings were high at 96.2% to 99.9% when tested in the in-house dataset. Lesion heatmaps highlight salient regions effectively in various findings. Areas under the receiver operating characteristic curves for diabetic retinopathy-related findings tested in the Indian Diabetic Retinopathy Image Dataset and e-ophtha dataset were 94.7% to 98.0%. The model demonstrated a performance that rivaled that of human experts, especially in the detection of hemorrhage, hard exudate, membrane, macular hole, myelinated nerve fiber, and glaucomatous disc change. Our deep learning algorithms with region guidance showed reliable performance for detection of multiple findings in macula-centered retinal fundus images. These interpretable, as well as reliable, classification outputs open the possibility for clinical use as an automated screening system for retinal fundus images.", "keywords": ["retinal fundus images", "Retinopathy Image Dataset", "National University Bundang", "Diabetic Retinopathy Image", "Seoul National University", "University Bundang Hospital", "Indian Diabetic Retinopathy", "deep learning", "fundus images", "deep learning algorithms", "deep learning models", "evaluate deep learning", "screen multiple abnormal", "Retinopathy Image", "Image Dataset", "develop and evaluate", "Hospital Retina Image", "National University", "University Bundang", "retinal fundus"], "paper_title": "Development and Validation of Deep Learning Models for Screening Multiple Abnormal Findings in Retinal Fundus Images.", "last_updated": "2023/02/04"}, {"id": "0033735066", "domain": "Diabetic retinopathy", "model_name": "Tham et al.", "publication_date": "2021/04/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33735066/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33735066", "task": null, "abstract": "In current approaches to vision screening in the community, a simple and efficient process is needed to identify individuals who should be referred to tertiary eye care centres for vision loss related to eye diseases. The emergence of deep learning technology offers new opportunities to revolutionise this clinical referral pathway. We aimed to assess the performance of a newly developed deep learning algorithm for detection of disease-related visual impairment. In this proof-of-concept study, using retinal fundus images from 15\u2008175 eyes with complete data related to best-corrected visual acuity or pinhole visual acuity from the Singapore Epidemiology of Eye Diseases Study, we first developed a single-modality deep learning algorithm based on retinal photographs alone for detection of any disease-related visual impairment (defined as eyes from patients with major eye diseases and best-corrected visual acuity of <20/40), and moderate or worse disease-related visual impairment (eyes with disease and best-corrected visual acuity of <20/60). After development of the algorithm, we tested it internally, using a new set of 3803 eyes from the Singapore Epidemiology of Eye Diseases Study. We then tested it externally using three population-based studies (the Beijing Eye study [6239 eyes], Central India Eye and Medical study [6526 eyes], and Blue Mountains Eye Study [2002 eyes]), and two clinical studies (the Chinese University of Hong Kong's Sight Threatening Diabetic Retinopathy study [971 eyes] and the Outram Polyclinic Study [1225 eyes]). The algorithm's performance in each dataset was assessed on the basis of the area under the receiver operating characteristic curve (AUC). In the internal test dataset, the AUC for detection of any disease-related visual impairment was 94\u00b72% (95% CI 93\u00b70-95\u00b73; sensitivity 90\u00b77% [87\u00b70-93\u00b76]; specificity 86\u00b78% [85\u00b76-87\u00b79]). The AUC for moderate or worse disease-related visual impairment was 93\u00b79% (95% CI 92\u00b72-95\u00b76; sensitivity 94\u00b76% [89\u00b76-97\u00b76]; specificity 81\u00b73% [80\u00b70-82\u00b75]). Across the five external test datasets (16\u2008993 eyes), the algorithm achieved AUCs ranging between 86\u00b76% (83\u00b74-89\u00b77; sensitivity 87\u00b75% [80\u00b77-92\u00b75]; specificity 70\u00b70% [66\u00b77-73\u00b71]) and 93\u00b76% (92\u00b74-94\u00b78; sensitivity 87\u00b78% [84\u00b71-90\u00b79]; specificity 87\u00b71% [86\u00b72-88\u00b70]) for any disease-related visual impairment, and the AUCs for moderate or worse disease-related visual impairment ranged between 85\u00b79% (81\u00b78-90\u00b71; sensitivity 84\u00b77% [73\u00b70-92\u00b78]; specificity 74\u00b74% [71\u00b74-77\u00b72]) and 93\u00b75% (91\u00b77-95\u00b73; sensitivity 90\u00b73% [84\u00b72-94\u00b76]; specificity 84\u00b72% [83\u00b72-85\u00b71]). This proof-of-concept study shows the potential of a single-modality, function-focused tool in identifying visual impairment related to major eye diseases, providing more timely and pinpointed referral of patients with disease-related visual impairment from the community to tertiary eye hospitals. National Medical Research Council, Singapore.", "keywords": ["disease-related visual impairment", "Eye Diseases Study", "visual impairment", "eye diseases", "disease-related visual", "eye care centres", "major eye diseases", "worse disease-related visual", "Beijing Eye study", "Mountains Eye Study", "eye", "eyes", "visual", "best-corrected visual acuity", "vision loss related", "Eye study", "visual acuity", "study", "Diseases Study", "Central India Eye"], "paper_title": "Referral for disease-related visual impairment using retinal photograph-based deep learning: a proof-of-concept, model development study.", "last_updated": "2023/02/04"}, {"id": "0033403156", "domain": "Diabetic retinopathy", "model_name": "Van Craenendonck et al.", "publication_date": "2020/12/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33403156/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33403156", "task": "IWqQC1koJA", "abstract": "Heatmapping techniques can support explainability of deep learning (DL) predictions in medical image analysis. However, individual techniques have been mainly applied in a descriptive way without an objective and systematic evaluation. We investigated comparative performances using diabetic retinopathy lesion detection as a benchmark task. The Indian Diabetic Retinopathy Image Dataset (IDRiD) publicly available database contains fundus images of diabetes patients with pixel level annotations of diabetic retinopathy (DR) lesions, the ground truth for this study. Three in advance trained DL models (ResNet50, VGG16 or InceptionV3) were used for DR detection in these images. Next, explainability was visualized with each of the 10 most used heatmapping techniques. The quantitative correspondence between the output of a heatmap and the ground truth was evaluated with the Explainability Consistency Score (ECS), a metric between 0 and 1, developed for this comparative task. In case of the overall DR lesions detection, the ECS ranged from 0.21 to 0.51 for all model/heatmapping combinations. The highest score was for VGG16+Grad-CAM (ECS = 0.51; 95% confidence interval [CI]: [0.46; 0.55]). For individual lesions, VGG16+Grad-CAM performed best on hemorrhages and hard exudates. ResNet50+SmoothGrad performed best for soft exudates and ResNet50+Guided Backpropagation performed best for microaneurysms. Our empirical evaluation on the IDRiD database demonstrated that the combination DL model/heatmapping affects explainability when considering common DR lesions. Our approach found considerable disagreement between regions highlighted by heatmaps and expert annotations. We warrant a more systematic investigation and analysis of heatmaps for reliable explanation of image-based predictions of deep learning models.", "keywords": ["diabetic retinopathy", "Indian Diabetic Retinopathy", "Retinopathy Image Dataset", "Diabetic Retinopathy Image", "diabetic retinopathy lesion", "medical image analysis", "medical image", "Heatmapping techniques", "techniques", "support explainability", "diabetic", "retinopathy", "lesions", "explainability", "retinopathy lesion detection", "ECS", "Indian Diabetic", "Heatmapping", "Image Dataset", "Explainability Consistency Score"], "paper_title": "Systematic Comparison of Heatmapping Techniques in Deep Learning in the Context of Diabetic Retinopathy Lesion Detection.", "last_updated": "2023/02/04"}, {"id": "0035062506", "domain": "Diabetic retinopathy", "model_name": "Mateen et al.", "publication_date": "2022/01/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35062506/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35062506", "task": "IWqQC1koJA", "abstract": "In diabetic retinopathy (DR), the early signs that may lead the eyesight towards complete vision loss are considered as microaneurysms (MAs). The shape of these MAs is almost circular, and they have a darkish color and are tiny in size, which means they may be missed by manual analysis of ophthalmologists. In this case, accurate early detection of microaneurysms is helpful to cure DR before non-reversible blindness. In the proposed method, early detection of MAs is performed using a hybrid feature embedding approach of pre-trained CNN models, named as VGG-19 and Inception-v3. The performance of the proposed approach was evaluated using publicly available datasets, namely \"E-Ophtha\" and \"DIARETDB1\", and achieved 96% and 94% classification accuracy, respectively. Furthermore, the developed approach outperformed the state-of-the-art approaches in terms of sensitivity and specificity for microaneurysms detection.", "keywords": ["complete vision loss", "diabetic retinopathy", "lead the eyesight", "eyesight towards complete", "complete vision", "vision loss", "loss are considered", "early signs", "early detection", "accurate early detection", "MAs", "detection", "early", "microaneurysms", "retinopathy", "pre-trained CNN models", "approach", "microaneurysms detection", "diabetic", "signs"], "paper_title": "Deep Learning Approach for Automatic Microaneurysms Detection.", "last_updated": "2023/02/04"}, {"id": "0035204552", "domain": "Diabetic retinopathy", "model_name": "Elsharkawy et al.", "publication_date": "2022/02/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35204552/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35204552", "task": "IWqQC1koJA", "abstract": "Early diagnosis of diabetic retinopathy (DR) is of critical importance to suppress severe damage to the retina and/or vision loss. In this study, an optical coherence tomography (OCT)-based computer-aided diagnosis (CAD) method is proposed to detect DR early using structural 3D retinal scans. This system uses prior shape knowledge to automatically segment all retinal layers of the 3D-OCT scans using an adaptive, appearance-based method. After the segmentation step, novel texture features are extracted from the segmented layers of the OCT B-scans volume for DR diagnosis. For every layer, Markov-Gibbs random field (MGRF) model is used to extract the 2nd-order reflectivity. In order to represent the extracted image-derived features, we employ cumulative distribution function (CDF) descriptors. For layer-wise classification in 3D volume, using the extracted Gibbs energy feature, an artificial neural network (ANN) is fed the extracted feature for every layer. Finally, the classification outputs for all twelve layers are fused using a majority voting schema for global subject diagnosis. A cohort of 188 3D-OCT subjects are used for system evaluation using different <i>k</i>-fold validation techniques and different validation metrics. Accuracy of 90.56%, 93.11%, and 96.88% are achieved using 4-, 5-, and 10-fold cross-validation, respectively. Additional comparison with deep learning networks, which represent the state-of-the-art, documented the promise of our system's ability to diagnose the DR early.", "keywords": ["suppress severe damage", "diabetic retinopathy", "vision loss", "critical importance", "importance to suppress", "suppress severe", "severe damage", "OCT B-scans volume", "OCT B-scans", "Early", "based computer-aided diagnosis", "diagnosis", "Early diagnosis", "extracted", "optical coherence tomography", "OCT", "layers", "retinal layers", "retinopathy", "loss"], "paper_title": "A Novel Computer-Aided Diagnostic System for Early Detection of Diabetic Retinopathy Using 3D-OCT Higher-Order Spatial Appearance Model.", "last_updated": "2023/02/04"}, {"id": "0033727785", "domain": "Diabetic retinopathy", "model_name": "Morya et al.", "publication_date": "2021/03/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33727785/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33727785", "task": null, "abstract": "Deep Learning (DL) and Artificial Intelligence (AI) have become widespread due to the advanced technologies and availability of digital data. Supervised learning algorithms have shown human-level performance or even better and are better feature extractor-quantifier than unsupervised learning algorithms. To get huge dataset with good quality control, there is a need of an annotation tool with a customizable feature set. This paper evaluates the viability of having an in house annotation tool which works on a smartphone and can be used in a healthcare setting. We developed a smartphone-based grading system to help researchers in grading multiple retinal fundi. The process consisted of designing the flow of user interface (UI) keeping in view feedback from experts. Quantitative and qualitative analysis of change in speed of a grader over time and feature usage statistics was done. The dataset size was approximately 16,000 images with adjudicated labels by a minimum of 2 doctors. Results for an AI model trained on the images graded using this tool and its validation over some public datasets were prepared. We created a DL model and analysed its performance for a binary referrable DR Classification task, whether a retinal image has Referrable DR or not. A total of 32 doctors used the tool for minimum of 20 images each. Data analytics suggested significant portability and flexibility of the tool. Grader variability for images was in favour of agreement on images annotated. Number of images used to assess agreement is 550. Mean of 75.9% was seen in agreement. Our aim was to make Annotation of Medical imaging easier and to minimize time taken for annotations without quality degradation. The user feedback and feature usage statistics confirm our hypotheses of incorporation of brightness and contrast variations, green channels and zooming add-ons in correlation to certain disease types. Simulation of multiple review cycles and establishing quality control can boost the accuracy of AI models even further. Although our study aims at developing an annotation tool for diagnosing and classifying diabetic retinopathy fundus images but same concept can be used for fundus images of other ocular diseases as well as other streams of medical science such as radiology where image-based diagnostic applications are utilised.", "keywords": ["Artificial Intelligence", "learning algorithms", "Supervised learning algorithms", "Deep Learning", "widespread due", "advanced technologies", "technologies and availability", "availability of digital", "Learning", "annotation tool", "images", "tool", "Intelligence", "Supervised learning", "Artificial", "annotation", "feature", "feature usage statistics", "digital data", "feature usage"], "paper_title": "Evaluating the Viability of a Smartphone-Based Annotation Tool for Faster and Accurate Image Labelling for Artificial Intelligence in Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0032995069", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2020/09/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32995069/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32995069", "task": null, "abstract": "Numerous angiographic images with high variability in quality are obtained during each ultra-widefield fluorescein angiography (UWFA) acquisition session. This study evaluated the feasibility of an automated system for image quality classification and selection using deep learning. The training set was comprised of 3543 UWFA images. Ground-truth image quality was assessed by expert image review and classified into one of four categories (ungradable, poor, good, or best) based on contrast, field of view, media opacity, and obscuration from external features. Two test sets, including randomly selected 392 images separated from the training set and an independent balanced image set composed of 50 ungradable/poor and 50 good/best images, assessed the model performance and bias. In the randomly selected and balanced test sets, the automated quality assessment system showed overall accuracy of 89.0% and 94.0% for distinguishing between gradable and ungradable images, with sensitivity of 90.5% and 98.6% and specificity of 87.0% and 81.5%, respectively. The receiver operating characteristic curve measuring performance of two-class classification (ungradable and gradable) had an area under the curve of 0.920 in the randomly selected set and 0.980 in the balanced set. A deep learning classification model demonstrates the feasibility of automatic classification of UWFA image quality. Clinical application of this system might greatly reduce manual image grading workload, allow quality-based image presentation to clinicians, and provide near-instantaneous feedback on image quality during image acquisition for photographers. The UWFA image quality classification tool may significantly reduce manual grading for clinical- and research-related work, providing instantaneous and reliable feedback on image quality.", "keywords": ["ultra-widefield fluorescein angiography", "Numerous angiographic images", "UWFA image quality", "image quality", "image", "quality", "Numerous angiographic", "fluorescein angiography", "UWFA image", "high variability", "ultra-widefield fluorescein", "UWFA", "image quality classification", "images", "set", "randomly selected", "angiographic images", "classification", "quality classification", "ungradable"], "paper_title": "Automated Quality Assessment and Image Selection of Ultra-Widefield Fluorescein Angiography Images through Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0031315159", "domain": "Diabetic retinopathy", "model_name": "Yu et al.", "publication_date": "2019/08/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31315159/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31315159", "task": "aipbNdPTIt", "abstract": "The purpose of this study was to automatically and accurately segment hyper-reflective foci (HRF) in spectral domain optical coherence tomography (SD-OCT) images with diabetic retinopathy (DR) using deep convolutional neural networks. An automatic HRF segmentation model for SD-OCT images based on deep networks was constructed. The model segmented small lesions through pixel-wise predictions based on small image patches. We used an approach for discriminative features extraction for small patches by introducing small kernels and strides in convolutional and pooling layers, which was applied on the state-of-the-art deep classification networks (GoogLeNet and ResNet). The features extracted by the adapted deep networks were fed into a softmax layer to produce the probabilities of HRF. We trained different models on a dataset with 16 HRF eyes by using different sizes of patches, and then, we fused these models to generate optimal results. Experimental results on 18 eyes demonstrated that our method is effective for the HRF segmentation. The dice similarity coefficient (DSC) for the foci area in B-scan, projection images, and foci amount in B-scan images reaches 67.81%, 74.09%, and 72.45%, respectively. The proposed segmentation model can accurately segment HRF in SD-OCT images with DR and outperforms traditional methods. Our model may provide reliable segmentations for small lesions in SD-OCT images and may be helpful in the clinical diagnosis of diseases.", "keywords": ["optical coherence tomography", "spectral domain optical", "domain optical coherence", "coherence tomography", "diabetic retinopathy", "HRF", "spectral domain", "domain optical", "optical coherence", "deep convolutional neural", "convolutional neural networks", "images", "SD-OCT images", "HRF segmentation", "small", "neural networks", "deep networks", "deep", "networks", "SD-OCT"], "paper_title": "Hyper-reflective foci segmentation in SD-OCT retinal images with diabetic retinopathy using deep convolutional neural networks.", "last_updated": "2023/02/04"}, {"id": "0036292119", "domain": "Diabetic retinopathy", "model_name": "ham10000_dataset", "publication_date": "2022/10/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36292119/", "code_link": "https://github.com/ptschandl/ham10000_dataset", "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "36292119", "task": "IWqQC1koJA", "abstract": "Medical image classification is a novel technology that presents a new challenge. It is essential that pathological images are automatically and correctly classified to enable doctors to provide precise treatment. Convolutional neural networks have demonstrated their effectiveness in classifying images in deep learning, which may have dozens or hundreds of layers, to illustrate the relationship between them in terms of their different neural network features. Convolutional layers consisting of small kernels take weights as input and guide them through an activation function as output. The main advantage of using convolutional neural networks (CNNs) instead of traditional neural networks is that they reduce the model parameters for greater accuracy. However, many studies have simply been focused on finding the best CNN model and classification results from a single medical image classification. Therefore, we applied a common deep learning network model in an attempt to identify the best model framework by training and validating different model parameters to classify medical images. After conducting experiments on six publicly available databases of pathological images, including colorectal cancer tissue, chest X-rays, common skin lesions, diabetic retinopathy, pediatric chest X-ray, and breast ultrasound image datasets, we were able to confirm that the recognition accuracy of the Inception V3 method was significantly better than that of other existing deep learning models.", "keywords": ["technology that presents", "neural networks", "Convolutional neural networks", "neural", "Medical image classification", "images", "model", "Convolutional neural", "deep learning", "image classification", "Medical image", "image", "classification", "Convolutional", "networks", "pathological images", "Medical", "deep", "learning", "challenge"], "paper_title": "Deep Learning Technology Applied to Medical Image Tissue Classification.", "last_updated": "2023/02/04"}, {"id": "0031932886", "domain": "Diabetic retinopathy", "model_name": "Pan et al.", "publication_date": "2020/01/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31932886/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31932886", "task": "IWqQC1koJA", "abstract": "To automatically detect and classify the lesions of diabetic retinopathy (DR) in fundus fluorescein angiography (FFA) images using deep learning algorithm through comparing 3 convolutional neural networks (CNNs). A total of 4067 FFA images from Eye Center at the Second Affiliated Hospital of Zhejiang University School of Medicine were annotated with 4 kinds of lesions of DR, including non-perfusion regions (NP), microaneurysms, leakages, and laser scars. Three CNNs including DenseNet, ResNet50, and VGG16 were trained to achieve multi-label classification, which means the algorithms could identify 4 retinal lesions above at the same time. The area under the curve (AUC) of DenseNet reached 0.8703, 0.9435, 0.9647, and 0.9653 for detecting NP, microaneurysms, leakages, and laser scars, respectively. For ResNet50, AUC was 0.8140 for NP, 0.9097 for microaneurysms, 0.9585 for leakages, and 0.9115 for laser scars. And for VGG16, AUC was 0.7125 for NP, 0.5569 for microaneurysms, 0.9177 for leakages, and 0.8537 for laser scars. Experimental results demonstrate that DenseNet is a suitable model to automatically detect and distinguish retinal lesions in the FFA images with multi-label classification, which lies the foundation of automatic analysis for FFA images and comprehensive diagnosis and treatment decision-making for DR.", "keywords": ["convolutional neural networks", "fundus fluorescein angiography", "Zhejiang University School", "FFA images", "deep learning algorithm", "laser scars", "diabetic retinopathy", "fluorescein angiography", "convolutional neural", "neural networks", "FFA", "fundus fluorescein", "deep learning", "Eye Center", "Affiliated Hospital", "Hospital of Zhejiang", "Zhejiang University", "University School", "School of Medicine", "images"], "paper_title": "Multi-label classification of retinal lesions in diabetic retinopathy for automatic analysis of fundus fluorescein angiography based on deep learning.", "last_updated": "2023/02/04"}, {"id": "0029531299", "domain": "Diabetic retinopathy", "model_name": "Keel et al.", "publication_date": "2018/03/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29531299/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "29531299", "task": null, "abstract": "The purpose of this study is to evaluate the feasibility and patient acceptability of a novel artificial intelligence (AI)-based diabetic retinopathy (DR) screening model within endocrinology outpatient settings. Adults with diabetes were recruited from two urban endocrinology outpatient clinics and single-field, non-mydriatic fundus photographs were taken and graded for referable DR (\u2009\u2265\u2009pre-proliferative DR). Each participant underwent; (1) automated screening model; where a deep learning algorithm (DLA) provided real-time reporting of results; and (2) manual model where retinal images were transferred to a retinal grading centre and manual grading outcomes were distributed to the patient within 2 weeks of assessment. Participants completed a questionnaire on the day of examination and 1-month following assessment to determine overall satisfaction and the preferred model of care. In total, 96 participants were screened for DR and the mean assessment time for automated screening was 6.9\u2009minutes. Ninety-six percent of participants reported that they were either satisfied or very satisfied with the automated screening model and 78% reported that they preferred the automated model over manual. The sensitivity and specificity of the DLA for correct referral was 92.3% and 93.7%, respectively. AI-based DR screening in endocrinology outpatient settings appears to be feasible and well accepted by patients.", "keywords": ["based diabetic retinopathy", "endocrinology outpatient settings", "endocrinology outpatient", "artificial intelligence", "based diabetic", "diabetic retinopathy", "automated screening model", "urban endocrinology outpatient", "endocrinology outpatient clinics", "evaluate the feasibility", "screening model", "automated screening", "model", "outpatient settings", "screening", "outpatient", "patient acceptability", "endocrinology", "automated", "outpatient clinics"], "paper_title": "Feasibility and patient acceptability of a novel artificial intelligence-based screening model for diabetic retinopathy at endocrinology outpatient services: a pilot study.", "last_updated": "2023/02/04"}, {"id": "0036924893", "domain": "Diabetic retinopathy", "model_name": "Jacoba et al.", "publication_date": "2023/03/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36924893/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36924893", "task": "IWqQC1koJA", "abstract": "To create and validate code-free automated deep learning models (autoML) for diabetic retinopathy (DR) classification from handheld retinal images. Prospective development and validation of autoML models for DR image classification. 17,829 de-identified retinal images from 3,566 eyes with diabetes acquired using handheld retinal cameras in a community-based DR screening program. AutoML models were generated based on previously acquired 5-field (macula-centered, disc-centered, superior, inferior, temporal macula) handheld retinal images. Each individual image was labeled using the International DR and diabetic macular edema (DME) classification scale by four certified graders at a centralized reading center under oversight by a senior retina specialist. Images for model development were split 8-1-1 for training, optimization, and testing to detect referable DR [(refDR), defined as moderate nonproliferative DR or worse or any level of DME]. Internal validation was performed using a published image set from the same patient population (N=450 images from 225 eyes). External validation was performed using a publicly available retinal imaging dataset from the Asia Pacific Tele-Ophthalmology Society (N=3,662 images). Area under the precision-recall curve (AUPRC), sensitivity, specificity, positive predictive value, negative predictive value, (SN, SP, PPV, NPV, respectively) accuracy, and F1 scores. RefDR was present in 17.3%, 39.1% and 48.0% of the training set, internal and external validation sets respectively. The model's AUPRC was 0.995 with a precision and recall of 97% using a score threshold of 0.5. Internal validation showed SN, SP, PPV, NPV, accuracy and F1 scores were 0.96 (95% CI:0.884-0.99), 0.98 (95% CI:0.937-0.995), 0.96 (95% CI:0.884-0.99), 0.98 (95% CI:0.937-0.995), 0.97 and 0.96, respectively. External validation showed SN, SP, PPV, NPV, accuracy and F1 scores were 0.94 (95% CI:0.929-0.951), 0.97 (95% CI:0.957-0.974), 0.96 (95% CI:0.952-0.971), 0.95 (95% CI:0.935-0.956), 0.97 and 0.96, respectively. This study demonstrates the accuracy and feasibility of code-free autoML models for identifying refDR developed using handheld retinal imaging in a community-based screening program. Potentially, the use of autoML may increase access to machine learning models that may be adapted for specific programs that are guided by the clinical need to rapidly address disparities in healthcare delivery.", "keywords": ["handheld retinal", "handheld retinal images", "automated deep learning", "validate code-free automated", "code-free automated deep", "retinal images", "create and validate", "automated deep", "retinal", "autoML models", "images", "autoML", "validation", "handheld", "models", "PPV", "NPV", "deep learning models", "External validation", "diabetic retinopathy"], "paper_title": "Performance of Automated Machine Learning for Diabetic Retinopathy Image Classification from Multi-field Handheld Retinal Images.", "last_updated": "2023/02/04"}, {"id": "0036050474", "domain": "Diabetic retinopathy", "model_name": "Khalili Pour et al.", "publication_date": "2022/09/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36050474/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36050474", "task": "IWqQC1koJA", "abstract": "The study aims to classify the eyes with proliferative diabetic retinopathy (PDR) and non-proliferative diabetic retinopathy (NPDR) based on the optical coherence tomography angiography (OCTA) vascular density maps using a supervised machine learning algorithm. OCTA vascular density maps (at superficial capillary plexus (SCP), deep capillary plexus (DCP), and total retina (R) levels) of 148 eyes from 78 patients with diabetic retinopathy (45 PDR and 103 NPDR) was used to classify the images to NPDR and PDR groups based on a supervised machine learning algorithm known as the support vector machine (SVM) classifier optimized by a genetic evolutionary algorithm. The implemented algorithm in three different models reached up to 85% accuracy in classifying PDR and NPDR in all three levels of vascular density maps. The deep retinal layer vascular density map demonstrated the best performance with a 90% accuracy in discriminating between PDR and NPDR. The current study on a limited number of patients with diabetic retinopathy demonstrated that a supervised machine learning-based method known as SVM can be used to differentiate PDR and NPDR patients using OCTA vascular density maps.", "keywords": ["OCTA vascular density", "vascular density maps", "coherence tomography angiography", "supervised machine learning", "optical coherence tomography", "vascular density", "machine learning algorithm", "density maps", "proliferative diabetic retinopathy", "non-proliferative diabetic retinopathy", "diabetic retinopathy", "supervised machine", "OCTA vascular", "PDR", "NPDR", "PDR groups based", "machine learning", "tomography angiography", "density", "PDR and NPDR"], "paper_title": "Automated machine learning-based classification of proliferative and non-proliferative diabetic retinopathy using optical coherence tomography angiography vascular density maps.", "last_updated": "2023/02/04"}, {"id": "0036615186", "domain": "Diabetic retinopathy", "model_name": "firstaid", "publication_date": "2023/01/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36615186/", "code_link": "https://github.com/yidarvin/firstaid", "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "36615186", "task": "aipbNdPTIt", "abstract": "With the progression of diabetic retinopathy (DR) from the non-proliferative (NPDR) to proliferative (PDR) stage, the possibility of vision impairment increases significantly. Therefore, it is clinically important to detect the progression to PDR stage for proper intervention. We propose a segmentation-assisted DR classification methodology, that builds on (and improves) current methods by using a fully convolutional network (FCN) to segment retinal neovascularizations (NV) in retinal images prior to image classification. This study utilizes the Kaggle EyePacs dataset, containing retinal photographs from patients with varying degrees of DR (mild, moderate, severe NPDR and PDR. Two graders annotated the NV (a board-certified ophthalmologist and a trained medical student). Segmentation was performed by training an FCN to locate neovascularization on 669 retinal fundus photographs labeled with PDR status according to NV presence. The trained segmentation model was used to locate probable NV in images from the classification dataset. Finally, a CNN was trained to classify the combined images and probability maps into categories of PDR. The mean accuracy of segmentation-assisted classification was 87.71% on the test set (SD = 7.71%). Segmentation-assisted classification of PDR achieved accuracy that was 7.74% better than classification alone. Our study shows that segmentation assistance improves identification of the most severe stage of diabetic retinopathy and has the potential to improve deep learning performance in other imaging problems with limited data availability.", "keywords": ["impairment increases significantly", "vision impairment increases", "increases significantly", "possibility of vision", "vision impairment", "impairment increases", "PDR", "PDR stage", "classification", "stage", "NPDR", "retinal", "progression", "detect the progression", "severe NPDR", "FCN", "Kaggle EyePacs dataset", "diabetic retinopathy", "images", "segmentation-assisted"], "paper_title": "Segmentation-Assisted Fully Convolutional Neural Network Enhances Deep Learning Performance to Identify Proliferative Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0032832207", "domain": "Diabetic retinopathy", "model_name": "non_uniform_label_smoothing", "publication_date": "2020/06/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32832207/", "code_link": "https://github.com/agaldran/non_uniform_label_smoothing", "model_type": "CNN", "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "32832207", "task": null, "abstract": "Introducing a new technique to improve deep learning (DL) models designed for automatic grading of diabetic retinopathy (DR) from retinal fundus images by enhancing predictions' consistency. A convolutional neural network (CNN) was optimized in three different manners to predict DR grade from eye fundus images. The optimization criteria were (1) the standard cross-entropy (CE) loss; (2) CE supplemented with label smoothing (LS), a regularization approach widely employed in computer vision tasks; and (3) our proposed non-uniform label smoothing (N-ULS), a modification of LS that models the underlying structure of expert annotations. Performance was measured in terms of quadratic-weighted \u03ba score (quad-\u03ba) and average area under the receiver operating curve (AUROC), as well as with suitable metrics for analyzing diagnostic consistency, like weighted precision, recall, and F1 score, or Matthews correlation coefficient. While LS generally harmed the performance of the CNN, N-ULS statistically significantly improved performance with respect to CE in terms quad-\u03ba score (73.17 vs. 77.69, <i>P</i> < 0.025), without any performance decrease in average AUROC. N-ULS achieved this while simultaneously increasing performance for all other analyzed metrics. For extending standard modeling approaches from DR detection to the more complex task of DR grading, it is essential to consider the underlying structure of expert annotations. The approach introduced in this article can be easily implemented in conjunction with deep neural networks to increase their consistency without sacrificing per-class performance. A straightforward modification of current standard training practices of CNNs can substantially improve consistency in DR grading, better modeling expert annotations and human variability.", "keywords": ["retinal fundus images", "enhancing predictions' consistency", "diabetic retinopathy", "fundus images", "designed for automatic", "enhancing predictions'", "eye fundus images", "retinal fundus", "improve deep learning", "models designed", "Performance", "expert annotations", "label smoothing", "deep learning", "automatic grading", "predictions' consistency", "images by enhancing", "fundus", "images", "AUROC"], "paper_title": "Non-uniform Label Smoothing for Diabetic Retinopathy Grading from Retinal Fundus Images with Deep Neural Networks.", "last_updated": "2023/02/04"}, {"id": "0034693536", "domain": "Diabetic retinopathy", "model_name": "Wu et al.", "publication_date": "2021/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34693536/", "code_link": null, "model_type": "transformer", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34693536", "task": null, "abstract": "In the domain of natural language processing, Transformers are recognized as state-of-the-art models, which opposing to typical convolutional neural networks (CNNs) do not rely on convolution layers. Instead, Transformers employ multi-head attention mechanisms as the main building block to capture long-range contextual relations between image pixels. Recently, CNNs dominated the deep learning solutions for diabetic retinopathy grade recognition. However, spurred by the advantages of Transformers, we propose a Transformer-based method that is appropriate for recognizing the grade of diabetic retinopathy. The purposes of this work are to demonstrate that (i) the pure attention mechanism is suitable for diabetic retinopathy grade recognition and (ii) Transformers can replace traditional CNNs for diabetic retinopathy grade recognition. This paper proposes a Vision Transformer-based method to recognize the grade of diabetic retinopathy. Fundus images are subdivided into non-overlapping patches, which are then converted into sequences by flattening, and undergo a linear and positional embedding process to preserve positional information. Then, the generated sequence is input into several multi-head attention layers to generate the final representation. The first token sequence is input to a softmax classification layer to produce the recognition output in the classification stage. The dataset for training and testing employs fundus images of different resolutions, subdivided into patches. We challenge our method against current CNNs and extreme learning machines and achieve an appealing performance. Specifically, the suggested deep learning architecture attains an accuracy of 91.4%, specificity\u00a0=\u00a00.977 (95% confidence interval (CI) (0.951-1)), precision\u00a0=\u00a00.928 (95% CI (0.852-1)), sensitivity\u00a0=\u00a00.926 (95% CI (0.863-0.989)), quadratic weighted kappa score\u00a0=\u00a00.935, and area under curve (AUC)\u00a0=\u00a00.986. Our comparative experiments against current methods conclude that our model is competitive and highlight that an attention mechanism based on a Vision Transformer model is promising for the diabetic retinopathy grade recognition task.", "keywords": ["diabetic retinopathy grade", "natural language processing", "convolutional neural networks", "retinopathy grade recognition", "typical convolutional neural", "diabetic retinopathy", "retinopathy grade", "grade recognition", "language processing", "neural networks", "retinopathy", "domain of natural", "natural language", "opposing to typical", "typical convolutional", "convolutional neural", "rely on convolution", "diabetic", "grade", "Vision Transformer model"], "paper_title": "Vision Transformer-based recognition of diabetic retinopathy grade.", "last_updated": "2023/02/04"}, {"id": "0035749336", "domain": "Diabetic retinopathy", "model_name": "SplitAVG", "publication_date": "2022/09/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35749336/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35749336", "task": null, "abstract": "Federated learning is an emerging research paradigm for enabling collaboratively training deep learning models without sharing patient data. However, the data from different institutions are usually heterogeneous across institutions, which may reduce the performance of models trained using federated learning. In this study, we propose a novel heterogeneity-aware federated learning method, SplitAVG, to overcome the performance drops from data heterogeneity in federated learning. Unlike previous federated methods that require complex heuristic training or hyper parameter tuning, our SplitAVG leverages the simple network split and feature map concatenation strategies to encourage the federated model training an unbiased estimator of the target data distribution. We compare SplitAVG with seven state-of-the-art federated learning methods, using centrally hosted training data as the baseline on a suite of both synthetic and real-world federated datasets. We find that the performance of models trained using all the comparison federated learning methods degraded significantly with the increasing degrees of data heterogeneity. In contrast, SplitAVG method achieves comparable results to the baseline method under all heterogeneous settings, that it achieves 96.2% of the accuracy and 110.4% of the mean absolute error obtained by the baseline in a diabetic retinopathy binary classification dataset and a bone age prediction dataset, respectively, on highly heterogeneous data partitions. We conclude that SplitAVG method can effectively overcome the performance drops from variability in data distributions across institutions. Experimental results also show that SplitAVG can be adapted to different base convolutional neural networks (CNNs) and generalized to various types of medical imaging tasks. The code is publicly available at https://github.com/zm17943/SplitAVG.", "keywords": ["emerging research paradigm", "Federated learning", "federated learning methods", "sharing patient data", "deep learning models", "enabling collaboratively training", "collaboratively training deep", "training deep learning", "learning", "Federated", "data", "emerging research", "research paradigm", "paradigm for enabling", "enabling collaboratively", "sharing patient", "learning methods", "deep learning", "SplitAVG", "heterogeneity-aware federated learning"], "paper_title": "SplitAVG: A Heterogeneity-Aware Federated Deep Learning Method for Medical Imaging.", "last_updated": "2023/02/04"}, {"id": "0035673105", "domain": "Diabetic retinopathy", "model_name": "Aquino-Br\u00edtez et al.", "publication_date": "2022/06/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35673105/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35673105", "task": "IWqQC1koJA", "abstract": "Due to the presence of high glucose levels, diabetes mellitus (DM) is a widespread disease that can damage blood vessels in the retina and lead to loss of the visual system. To combat this disease, called Diabetic Retinopathy (DR), retinography, using images of the fundus of the retina, is the most used method for the diagnosis of Diabetic Retinopathy. The Deep Learning (DL) area achieved high performance for the classification of retinal images and even achieved almost the same human performance in diagnostic tasks. However, the performance of DL architectures is highly dependent on the optimal configuration of the hyperparameters. In this article, we propose the use of Neuroevolutionary Algorithms to optimize the hyperparameters corresponding to the DL model for the diagnosis of DR. The results obtained prove that the proposed method outperforms the results obtained by the classical approach.", "keywords": ["damage blood vessels", "high glucose levels", "called Diabetic Retinopathy", "Diabetic Retinopathy", "diabetes mellitus", "glucose levels", "visual system", "damage blood", "blood vessels", "lead to loss", "widespread disease", "achieved high performance", "high glucose", "area achieved high", "called Diabetic", "retina and lead", "Deep Learning", "Retinopathy", "Diabetic", "presence of high"], "paper_title": "Automatic Diagnosis of Diabetic Retinopathy from Fundus Images Using Neuro-Evolutionary Algorithms.", "last_updated": "2023/02/04"}, {"id": "0035388319", "domain": "Diabetic retinopathy", "model_name": "Zhang et al.", "publication_date": "2022/03/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35388319/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35388319", "task": null, "abstract": "Diabetic retinopathy (DR) is currently one of the severe complications leading to blindness, and computer-aided, diagnosis technology-assisted DR grading has become a popular research trend especially for the development of deep learning methods. However, most deep learning-based DR grading models require a large number of annotations to provide data guidance, and it is laborious for experts to find subtle lesion areas from fundus images, making accurate annotation more expensive than other vision tasks. In contrast, large-scale unlabeled data are easily accessible, becoming a potential solution to reduce the annotating workload in DR grading. Thus, this paper explores the internal correlations from unknown fundus images assisted by limited labeled fundus images to solve the semisupervised DR grading problem and proposes an augmentation-consistent clustering network (ACCN) to address the above-mentioned challenges. Specifically, the augmentation provides an efficient cue for the similarity information of unlabeled fundus images, assisting the supervision from the labeled data. By mining the consistent correlations from augmentation and raw images, the ACCN can discover subtle lesion features by clustering with fewer annotations. Experiments on Messidor and APTOS 2019 datasets show that the ACCN surpasses many state-of-the-art methods in a semisupervised manner.", "keywords": ["severe complications leading", "popular research trend", "deep learning methods", "Diabetic retinopathy", "leading to blindness", "diagnosis technology-assisted", "severe complications", "complications leading", "popular research", "research trend", "fundus images", "deep learning", "technology-assisted DR grading", "grading models require", "grading", "unlabeled fundus images", "images", "learning methods", "fundus", "provide data guidance"], "paper_title": "Augmentation-Consistent Clustering Network for Diabetic Retinopathy Grading with Fewer Annotations.", "last_updated": "2023/02/04"}, {"id": "0034896689", "domain": "Glaucoma (unspecified)", "model_name": "To\u011fa\u00e7ar et al.", "publication_date": "2021/12/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34896689/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34896689", "task": "aipbNdPTIt", "abstract": "Diabetes-related cases can cause glaucoma, cataracts, optic neuritis, paralysis of the eye muscles, or various retinal damages over time. Diabetic retinopathy is the most common form of blindness that occurs with diabetes. Diabetic retinopathy is a disease that occurs when the blood vessels in the retina of the eye become damaged, leading to loss of vision in advanced stages. This disease can occur in any diabetic patient, and the most important factor in treating the disease is early diagnosis. Nowadays, deep learning models and machine learning methods, which are open to technological developments, are already used in early diagnosis systems. In this study, two publicly available datasets were used. The datasets consist of five types according to the severity of diabetic retinopathy. The objectives of the proposed approach in diabetic retinopathy detection are to positively contribute to the performance of CNN models by processing fundus images through preprocessing steps (morphological gradient and segmentation approaches). The other goal is to detect efficient sets from type-based activation sets obtained from CNN models using Atom Search Optimization method and increase the classification success. The proposed approach consists of three steps. In the first step, the Morphological Gradient method is used to prevent parasitism in each image, and the ocular vessels in fundus images are extracted using the segmentation method. In the second step, the datasets are trained with transfer learning models and the activations for each class type in the last fully connected layers of these models are extracted. In the last step, the Atom Search optimization method is used, and the most dominant activation class is selected from the extracted activations on a class basis. When classified by the severity of diabetic retinopathy, an overall accuracy of 99.59% was achieved for dataset #1 and 99.81% for dataset #2. In this study, it was found that the overall accuracy achieved with the proposed approach increased. To achieve this increase, the application of preprocessing steps and the selection of the dominant activation sets from the deep learning models were implemented using the Atom Search optimization method.", "keywords": ["Atom Search Optimization", "Search Optimization method", "Diabetic retinopathy", "Atom Search", "optic neuritis", "Search Optimization", "Diabetes-related cases", "damages over time", "retinal damages", "Diabetic", "Optimization method", "models", "learning models", "retinopathy", "CNN models", "eye muscles", "method", "deep learning models", "proposed approach", "Search"], "paper_title": "Detection of retinopathy disease using morphological gradient and segmentation approaches in fundus images.", "last_updated": "2023/02/04"}, {"id": "0036405815", "domain": "Diabetic retinopathy", "model_name": "Paul et al.", "publication_date": "2022/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36405815/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36405815", "task": null, "abstract": "To compare the performance of four deep active learning (DAL) approaches to optimize label efficiency for training diabetic retinopathy (DR) classification deep learning models. 88,702 color retinal fundus photographs from 44,351 patients with DR grades from the publicly available EyePACS dataset were used. Four DAL approaches [entropy sampling (ES), Bayesian active learning by disagreement (BALD), core set, and adversarial active learning (ADV)] were compared to conventional naive random sampling. Models were compared at various dataset sizes using Cohen's kappa (CK) and area under the receiver operating characteristic curve on an internal test set of 10,000 images. An independent test set of 3662 fundus photographs was used to assess generalizability. On the internal test set, 3 out of 4 DAL methods resulted in statistically significant performance improvements ( <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>p</mi> <mo><</mo> <mn>1</mn> <mo>\u00d7</mo> <msup><mrow><mn>10</mn></mrow> <mrow><mo>-</mo> <mn>4</mn></mrow> </msup> </mrow> </math> ) compared to random sampling for multiclass classification, with the largest observed differences in CK ranging from 0.051 for BALD to 0.053 for ES. Improvements in multiclass classification generalized to the independent test set, with the largest differences in CK ranging from 0.126 to 0.135. However, no statistically significant improvements were seen for binary classification. Similar performance was seen across DAL methods, except ADV, which performed similarly to random sampling. Uncertainty-based and feature descriptor-based deep active learning methods outperformed random sampling on both the internal and independent test sets at multiclass classification. However, binary classification performance remained similar across random sampling and active learning methods.", "keywords": ["training diabetic retinopathy", "optimize label efficiency", "mrow", "test set", "diabetic retinopathy", "active learning", "DAL", "optimize label", "label efficiency", "efficiency for training", "training diabetic", "test", "learning", "random sampling", "set", "classification", "internal test set", "sampling", "independent test set", "active"], "paper_title": "Efficient labeling of retinal fundus photographs using deep active learning.", "last_updated": "2023/02/04"}, {"id": "0036322995", "domain": "Diabetic retinopathy", "model_name": "CoT-XNet", "publication_date": "2022/12/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36322995/", "code_link": null, "model_type": "transformer", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36322995", "task": null, "abstract": "<i>Objective.</i>Diabetic retinopathy (DR) grading is primarily performed by assessing fundus images. Many types of lesions, such as microaneurysms, hemorrhages, and soft exudates, are available simultaneously in a single image. However, their sizes may be small, making it difficult to differentiate adjacent DR grades even using deep convolutional neural networks (CNNs). Recently, a vision transformer has shown comparable or even superior performance to CNNs, and it also learns different visual representations from CNNs. Inspired by this finding, we propose a two-path contextual transformer with Xception network (CoT-XNet) to improve the accuracy of DR grading.<i>Approach.</i>The representations learned by CoT through one path and those by the Xception network through another path are concatenated before the fully connected layer. Meanwhile, the dedicated pre-processing, data resampling, and test time augmentation strategies are implemented. The performance of CoT-XNet is evaluated in the publicly available datasets of DDR, APTOS2019, and EyePACS, which include over 50 000 images. Ablation experiments and comprehensive comparisons with various state-of-the-art (SOTA) models have also been performed.<i>Main results.</i>Our proposed CoT-XNet shows better performance than available SOTA models, and the accuracy and Kappa are 83.10% and 0.8496, 84.18% and 0.9000 and 84.10% and 0.7684 respectively, in the three datasets (listed above). Class activation maps of CoT and Xception networks are different and complementary in most images.<i>Significance.</i>By concatenating the different visual representations learned by CoT and Xception networks, CoT-XNet can accurately grade DR from fundus images and present good generalizability. CoT-XNet will promote the application of artificial intelligence-based systems in the DR screening of large-scale populations.", "keywords": ["Diabetic retinopathy", "grading is primarily", "primarily performed", "performed by assessing", "assessing fundus images", "Xception network", "Objective.", "Xception", "Diabetic", "assessing fundus", "CoT-XNet", "fundus images", "visual representations", "representations learned", "CNNs", "networks", "images", "representations", "visual representations learned", "CoT and Xception"], "paper_title": "CoT-XNet: contextual transformer with Xception network for diabetic retinopathy grading.", "last_updated": "2023/02/04"}, {"id": "0035694596", "domain": "Diabetic retinopathy", "model_name": "deep-diabetic-retinopathy-image-dataset-deepdrid", "publication_date": "2022/06/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35694596/", "code_link": "https://github.com/deepdrdoc/deep-diabetic-retinopathy-image-dataset-deepdrid", "model_type": "CNN", "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "35694596", "task": null, "abstract": "Microvascular problems of diabetes, such as diabetic retinopathy and macular edema, can be seen in the eye's retina, and the retinal images are being used to screen for and diagnose the illness manually. Using deep learning to automate this time-consuming process might be quite beneficial. In this paper, a deep neural network, i.e., convolutional neural network, has been proposed for predicting diabetes through retinal images. Before applying the deep neural network, the dataset is preprocessed and normalised for classification. Deep neural network is constructed by using 7 layers, 5 kernels, and ReLU activation function, and MaxPooling is implemented to combine important features. Finally, the model is implemented to classify whether the retinal image belongs to a diabetic or nondiabetic class. The parameters used for evaluating the model are accuracy, precision, recall, and F1 score. The implemented model has achieved a training accuracy of more than 95%, which is much better than the other states of the art algorithms.", "keywords": ["deep neural network", "neural network", "Microvascular problems", "macular edema", "eye retina", "illness manually", "retinopathy and macular", "diagnose the illness", "deep neural", "retinal images", "convolutional neural network", "neural", "deep", "network", "diabetic retinopathy", "retinal image belongs", "retinal", "problems of diabetes", "implemented", "model"], "paper_title": "Prediction of Diabetes through Retinal Images Using Deep Neural Network.", "last_updated": "2023/02/04"}, {"id": "0035847781", "domain": "Diabetic retinopathy", "model_name": "Miao et al.", "publication_date": "2022/06/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35847781/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35847781", "task": null, "abstract": "To develop artificial intelligence (AI)-based deep learning (DL) models for automatically detecting the ischemia type and the non-perfusion area (NPA) from color fundus photographs (CFPs) of patients with branch retinal vein occlusion (BRVO). This was a retrospective analysis of 274 CFPs from patients diagnosed with BRVO. All DL models were trained using a deep convolutional neural network (CNN) based on 45 degree CFPs covering the fovea and the optic disk. We first trained a DL algorithm to identify BRVO patients with or without the necessity of retinal photocoagulation from 219 CFPs and validated the algorithm on 55 CFPs. Next, we trained another DL algorithm to segment NPA from 104 CFPs and validated it on 29 CFPs, in which the NPA was manually delineated by 3 experienced ophthalmologists according to fundus fluorescein angiography. Both DL models have been cross-validated 5-fold. The recall, precision, accuracy, and area under the curve (AUC) were used to evaluate the DL models in comparison with three types of independent ophthalmologists of different seniority. In the first DL model, the recall, precision, accuracy, and area under the curve (AUC) were 0.75 \u00b1 0.08, 0.80 \u00b1 0.07, 0.79 \u00b1 0.02, and 0.82 \u00b1 0.03, respectively, for predicting the necessity of laser photocoagulation for BRVO CFPs. The second DL model was able to segment NPA in CFPs of BRVO with an AUC of 0.96 \u00b1 0.02. The recall, precision, and accuracy for segmenting NPA was 0.74 \u00b1 0.05, 0.87 \u00b1 0.02, and 0.89 \u00b1 0.02, respectively. The performance of the second DL model was nearly comparable with the senior doctors and significantly better than the residents. These results indicate that the DL models can directly identify and segment retinal NPA from the CFPs of patients with BRVO, which can further guide laser photocoagulation. Further research is needed to identify NPA of the peripheral retina in BRVO, or other diseases, such as diabetic retinopathy.", "keywords": ["develop artificial intelligence", "retinal vein occlusion", "color fundus photographs", "branch retinal vein", "based deep learning", "CFPs", "NPA", "BRVO", "artificial intelligence", "vein occlusion", "develop artificial", "automatically detecting", "detecting the ischemia", "identify BRVO patients", "models", "segment NPA", "patients", "BRVO patients", "segment retinal NPA", "deep learning"], "paper_title": "Deep Learning Models for Segmenting Non-perfusion Area of Color Fundus Photographs in Patients With Branch Retinal Vein Occlusion.", "last_updated": "2023/02/04"}, {"id": "0036973632", "domain": "Diabetic retinopathy", "model_name": "Oulhadj et al.", "publication_date": "2023/03/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36973632/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36973632", "task": null, "abstract": "Diabetic retinopathy (DR) is one of the most common consequences of diabetes. It affects the retina, causing blood vessel damage which can lead to loss of vision. Saving patients from losing their sight or at least slowing the progress of this disease depends mainly on the early detection of this pathology, on top of the detection of its specific stage. Furthermore, the early detection of diabetic retinopathy and the follow-up of the patient's condition remains an arduous task, whether for an experienced expert ophthalmologist or a computer-aided diagnosis technician. In this paper, we aim to propose a new automatic diabetic retinopathy severity level detection method. The proposed approach merges the pyramid hierarchy of the discrete wavelet transform of the retina fundus image with the modified capsule network and the modified inception block proposed, in addition to a new deep hybrid model that concatenates the inception block with capsule networks. The performance of our proposed approach has been validated on the APTOS dataset, as it achieved a high training accuracy of 97.71% and a high testing accuracy score of 86.54%, which is considered one of the best scores achieved in this field using the same dataset.", "keywords": ["consequences of diabetes", "common consequences", "Diabetic retinopathy", "early detection", "detection", "causing blood vessel", "automatic diabetic retinopathy", "diabetic retinopathy severity", "blood vessel damage", "proposed approach", "Diabetic", "retinopathy", "inception block", "inception block proposed", "proposed", "level detection method", "diabetes", "severity level detection", "common", "consequences"], "paper_title": "Diabetic Retinopathy Prediction Based on Wavelet Decomposition and Modified Capsule Network.", "last_updated": "2023/02/04"}, {"id": "0035938881", "domain": "Diabetic retinopathy", "model_name": "AOSLO-net", "publication_date": "2022/08/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35938881/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35938881", "task": "aipbNdPTIt", "abstract": "Accurate segmentation of microaneurysms (MAs) from adaptive optics scanning laser ophthalmoscopy (AOSLO) images is crucial for identifying MA morphologies and assessing the hemodynamics inside the MAs. Herein, we introduce AOSLO-net to perform automatic MA segmentation from AOSLO images of diabetic retinas. AOSLO-net is composed of a deep neural network based on UNet with a pretrained EfficientNet as the encoder. We have designed customized preprocessing and postprocessing policies for AOSLO images, including generation of multichannel images, de-noising, contrast enhancement, ensemble and union of model predictions, to optimize the MA segmentation. AOSLO-net is trained and tested using 87 MAs imaged from 28 eyes of 20 subjects with varying severity of diabetic retinopathy (DR), which is the largest available AOSLO dataset for MA detection. To avoid the overfitting in the model training process, we augment the training data by flipping, rotating, scaling the original image to increase the diversity of data available for model training. The validity of the model is demonstrated by the good agreement between the predictions of AOSLO-net and the MA masks generated by ophthalmologists and skillful trainees on 87 patient-specific MA images. Our results show that AOSLO-net outperforms the state-of-the-art segmentation model (nnUNet) both in accuracy (e.g., intersection over union and Dice scores), as well as computational cost. We demonstrate that AOSLO-net provides high-quality of MA segmentation from AOSLO images that enables correct MA morphological classification. As the first attempt to automatically segment retinal MAs from AOSLO images, AOSLO-net could facilitate the pathological study of DR and help ophthalmologists make disease prognoses.", "keywords": ["scanning laser ophthalmoscopy", "adaptive optics scanning", "optics scanning laser", "AOSLO images", "AOSLO", "laser ophthalmoscopy", "adaptive optics", "optics scanning", "scanning laser", "crucial for identifying", "identifying MA morphologies", "morphologies and assessing", "assessing the hemodynamics", "hemodynamics inside", "images", "AOSLO-net", "Accurate segmentation", "segmentation", "model", "segmentation from AOSLO"], "paper_title": "AOSLO-net: A Deep Learning-Based Method for Automatic Segmentation of Retinal Microaneurysms From Adaptive Optics Scanning Laser Ophthalmoscopy Images.", "last_updated": "2023/02/04"}, {"id": "0034748191", "domain": "Diabetic retinopathy", "model_name": "Jagan Mohan et al.", "publication_date": "2021/11/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34748191/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34748191", "task": null, "abstract": "Diabetic retinopathy is a microvascular complication of diabetes mellitus that develops over time. Diabetic retinopathy is one of the retinal disorders. Early detection of diabetic retinopathy reduces the chances of permanent vision loss. However, the identification and regular diagnosis of diabetic retinopathy is a time-consuming task and requires expert ophthalmologists and radiologists. In addition, an automatic diabetic retinopathy detection technique is necessary for real-time applications to facilitate and minimize potential human errors. Therefore, we propose an ensemble deep neural network and a novel four-step feature selection technique in this paper. In the first step, the preprocessed entropy images improve the quality of the retinal features. Second, the features\u00a0are extracted using a deep ensemble model include InceptionV3, ResNet101, and Vgg19 from the retinal fundus images. Then, these features are combined to create an ample feature space. To reduce the feature space, we propose four-step feature selection techniques: minimum redundancy, maximum relevance, Chi-Square, ReliefF, and F test for selecting efficient features. Further, appropriate features are chosen from the majority voting techniques to reduce the computational complexity. Finally, the standard machine learning classifier, support vector machines, is used in diabetic retinopathy classification. The proposed method is tested on Kaggle, MESSIDOR-2, and IDRiD databases, available publicly. The proposed algorithm provided an accuracy of 97.78%, a sensitivity of 97.6%, and a specificity of 99.3%, using top 300 features, which are better than other state-of-the-art methods.", "keywords": ["Diabetic retinopathy", "develops over time", "retinopathy", "diabetic retinopathy reduces", "Diabetic", "microvascular complication", "complication of diabetes", "diabetes mellitus", "mellitus that develops", "diabetic retinopathy detection", "automatic diabetic retinopathy", "features", "diabetic retinopathy classification", "feature", "retinopathy detection technique", "four-step feature selection", "feature space", "feature selection", "retinopathy reduces", "four-step feature"], "paper_title": "A novel four-step feature selection technique for diabetic retinopathy grading.", "last_updated": "2023/02/04"}, {"id": "0035430865", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2022/04/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35430865/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35430865", "task": "aipbNdPTIt", "abstract": "Diabetic retinopathy is the leading cause of vision loss in working-age adults. Early screening and diagnosis can help to facilitate subsequent treatment and prevent vision loss. Deep learning has been applied in various fields of medical identification. However, current deep learning-based lesion segmentation techniques rely on a large amount of pixel-level labeled ground truth data, which limits their performance and application. In this work, we present a weakly supervised deep learning framework for eye fundus lesion segmentation in patients with diabetic retinopathy. First, an efficient segmentation algorithm based on grayscale and morphological features is proposed for rapid coarse segmentation of lesions. Then, a deep learning model named Residual-Attention Unet (RAUNet) is proposed for eye fundus lesion segmentation. Finally, a data sample of fundus images with labeled lesions and unlabeled images with coarse segmentation results is jointly used to train RAUNet to broaden the diversity of lesion samples and increase the robustness of the segmentation model. A dataset containing 582 fundus images with labels verified by doctors, including hemorrhage (HE), microaneurysm (MA), hard exudate (EX) and soft exudate (SE), and 903 images without labels was used to evaluate the model. In ablation test, the proposed RAUNet achieved the highest intersection over union (IOU) on the labeled dataset, and the proposed attention and residual modules both improved the IOU of the UNet benchmark. Using both the images labeled by doctors and the proposed coarse segmentation method, the weakly supervised framework based on RAUNet architecture significantly improved the mean segmentation accuracy by over 7% on the lesions. This study demonstrates that combining unlabeled medical images with coarse segmentation results can effectively improve the robustness of the lesion segmentation model and proposes a practical framework for improving the performance of medical image segmentation given limited labeled data samples.", "keywords": ["segmentation", "lesion segmentation", "vision loss", "fundus lesion segmentation", "working-age adults", "coarse segmentation", "prevent vision loss", "lesion", "images", "eye fundus lesion", "Deep learning", "proposed", "fundus lesion", "coarse segmentation results", "Deep", "labeled", "fundus", "lesion segmentation model", "coarse", "lesions"], "paper_title": "Weakly supervised training for eye fundus lesion segmentation in patients with diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0035494508", "domain": "Diabetic retinopathy", "model_name": "Zuo et al.", "publication_date": "2022/04/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35494508/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35494508", "task": null, "abstract": "Diabetic complications have brought a tremendous burden for diabetic patients, but the problem of predicting diabetic complications is still unresolved. Our aim is to explore the relationship between hemoglobin A1C (HbA1c), insulin (INS), and glucose (GLU) and diabetic complications in combination with individual factors and to effectively predict multiple complications of diabetes. This was a real-world study. Data were collected from 40,913 participants with an average age of 48 years from the Department of Endocrinology of Ruijin Hospital in Shanghai. We proposed deep personal multitask prediction of diabetes complication with attentive interactions (DPMP-DC) to predict the five complication models of diabetes, including diabetic retinopathy, diabetic nephropathy, diabetic peripheral neuropathy, diabetic foot disease, and diabetic cardiovascular disease. Our model has an accuracy rate of 88.01% for diabetic retinopathy, 89.58% for diabetic nephropathy, 85.77% for diabetic neuropathy, 80.56% for diabetic foot disease, and 82.48% for diabetic cardiovascular disease. The multitasking accuracy of multiple complications is 84.67%, and the missed diagnosis rate is 9.07%. We put forward the method of interactive integration with individual factors of patients for the first time in diabetic complications, which reflect the differences between individuals. Our multitask model using the hard sharing mechanism provides better prediction than prior single prediction models.", "keywords": ["Diabetic", "Diabetic complications", "predicting diabetic complications", "brought a tremendous", "tremendous burden", "problem of predicting", "diabetic foot disease", "diabetic cardiovascular disease", "complications", "predicting diabetic", "disease", "diabetic retinopathy", "diabetic nephropathy", "diabetic foot", "diabetic cardiovascular", "Hospital in Shanghai", "Department of Endocrinology", "Endocrinology of Ruijin", "Ruijin Hospital", "foot disease"], "paper_title": "Deep Personal Multitask Prediction of Diabetes Complication with Attentive Interactions Predicting Diabetes Complications by Multitask-Learning.", "last_updated": "2023/02/04"}, {"id": "0035936377", "domain": "Diabetic retinopathy", "model_name": "Wang et al.", "publication_date": "2022/07/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35936377/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35936377", "task": null, "abstract": "Diabetes mellitus is a common chronic noncommunicable disease, the main manifestation of which is the long-term high blood sugar level in patients due to metabolic disorders. However, due to excessive reliance on the clinical experience of ophthalmologists, our diagnosis takes a long time, and it is prone to missed diagnosis and misdiagnosis. In recent years, with the development of deep learning, its application in the auxiliary diagnosis of diabetic retinopathy has become possible. How to use the powerful feature extraction ability of deep learning algorithm to realize the mining of massive medical data is of great significance. Therefore, under the action of computer-aided technology, this paper processes and analyzes the retinal images of the fundus through traditional image processing and convolutional neural network-related methods, so as to achieve the role of assisting clinical treatment. Based on the admission records of diabetic patients after data analysis and feature processing, this paper uses an improved convolutional neural network algorithm to establish a model for predicting changes in diabetic conditions. The model can assist doctors to judge the patient's treatment effect by using it based on the case records of inpatient diagnosis and treatment and to predict the risk of readmission of inpatients after discharge. It also can help to judge the effectiveness of the treatment plan. The results of the study show that the model proposed in this paper has a lower probability of misjudging patients with poor recovery as good recovery, and the prediction is more accurate.", "keywords": ["chronic noncommunicable disease", "common chronic noncommunicable", "long-term high blood", "high blood sugar", "blood sugar level", "Diabetes mellitus", "noncommunicable disease", "metabolic disorders", "common chronic", "chronic noncommunicable", "main manifestation", "long-term high", "high blood", "blood sugar", "sugar level", "due to metabolic", "diagnosis", "deep learning", "deep learning algorithm", "patients due"], "paper_title": "Analysis and Recognition of Clinical Features of Diabetes Based on Convolutional Neural Network.", "last_updated": "2023/02/04"}, {"id": "0035587313", "domain": "Diabetic retinopathy", "model_name": "Deepa et al.", "publication_date": "2022/05/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35587313/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35587313", "task": null, "abstract": "Diabetic retinopathy (DR) is a progressive vascular complication that affects people who have diabetes. This retinal abnormality can cause irreversible vision loss or permanent blindness; therefore, it is crucial to undergo frequent eye screening for early recognition and treatment. This paper proposes a feature extraction algorithm using discriminative multi-sized patches, based on deep learning convolutional neural network (CNN) for DR grading. This comprehensive algorithm extracts local and global features for efficient decision-making. Each input image is divided into small-sized patches to extract local-level features and then split into clusters or subsets. Hierarchical clustering by Siamese network with pre-trained CNN is proposed in this paper to select clusters with more discriminative patches. The fine-tuned Xception model of CNN is used to extract the global-level features of larger image patches. Local and global features are combined to improve the overall image-wise classification accuracy. The final support vector machine classifier exhibits 96% of classification accuracy with tenfold cross-validation in classifying DR images.", "keywords": ["progressive vascular complication", "Diabetic retinopathy", "progressive vascular", "vascular complication", "complication that affects", "affects people", "CNN", "features", "patches", "Diabetic", "retinopathy", "diabetes", "progressive", "vascular", "complication", "affects", "people", "local and global", "global features", "global"], "paper_title": "Automated grading of diabetic retinopathy using CNN with hierarchical clustering of image patches by siamese network.", "last_updated": "2023/02/04"}, {"id": "0033490274", "domain": "Diabetic retinopathy", "model_name": "FFU-Net", "publication_date": "2021/01/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33490274/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33490274", "task": "aipbNdPTIt", "abstract": "Diabetic retinopathy is one of the main causes of blindness in human eyes, and lesion segmentation is an important basic work for the diagnosis of diabetic retinopathy. Due to the small lesion areas scattered in fundus images, it is laborious to segment the lesion of diabetic retinopathy effectively with the existing U-Net model. In this paper, we proposed a new lesion segmentation model named FFU-Net (Feature Fusion U-Net) that enhances U-Net from the following points. Firstly, the pooling layer in the network is replaced with a convolutional layer to reduce spatial loss of the fundus image. Then, we integrate multiscale feature fusion (MSFF) block into the encoders which helps the network to learn multiscale features efficiently and enrich the information carried with skip connection and lower-resolution decoder by fusing contextual channel attention (CCA) models. Finally, in order to solve the problems of data imbalance and misclassification, we present a Balanced Focal Loss function. In the experiments on benchmark dataset IDRID, we make an ablation study to verify the effectiveness of each component and compare FFU-Net against several state-of-the-art models. In comparison with baseline U-Net, FFU-Net improves the segmentation performance by 11.97%, 10.68%, and 5.79% on metrics SEN, IOU, and DICE, respectively. The quantitative and qualitative results demonstrate the superiority of our FFU-Net in the task of lesion segmentation of diabetic retinopathy.", "keywords": ["important basic work", "Diabetic retinopathy", "diabetic retinopathy effectively", "human eyes", "blindness in human", "important basic", "basic work", "lesion segmentation", "Feature Fusion U-Net", "lesion segmentation model", "Diabetic", "diagnosis of diabetic", "Feature Fusion", "lesion", "retinopathy", "multiscale feature fusion", "small lesion areas", "lesion areas scattered", "Balanced Focal Loss", "existing U-Net model"], "paper_title": "FFU-Net: Feature Fusion U-Net for Lesion Segmentation of Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0034003898", "domain": "Diabetic retinopathy", "model_name": "Burlina et al.", "publication_date": "2021/06/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34003898/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34003898", "task": "IWqQC1koJA", "abstract": "This study evaluated generative methods to potentially mitigate artificial intelligence (AI) bias when diagnosing diabetic retinopathy (DR) resulting from training data imbalance or domain generalization, which occurs when deep learning systems (DLSs) face concepts at test/inference time they were not initially trained on. The public domain Kaggle EyePACS dataset (88,692 fundi and 44,346 individuals, originally diverse for ethnicity) was modified by adding clinician-annotated labels and constructing an artificial scenario of data imbalance and domain generalization by disallowing training (but not testing) exemplars for images of retinas with DR warranting referral (DR-referable) from darker-skin individuals, who presumably have greater concentration of melanin within uveal melanocytes, on average, contributing to retinal image pigmentation. A traditional/baseline diagnostic DLS was compared against new DLSs that would use training data augmented via generative models for debiasing. Accuracy (95% confidence intervals [CIs]) of the baseline diagnostics DLS for fundus images of lighter-skin individuals was 73.0% (66.9% to 79.2%) versus darker-skin of 60.5% (53.5% to 67.3%), demonstrating bias/disparity (delta = 12.5%; Welch t-test t = 2.670, P = 0.008) in AI performance across protected subpopulations. Using novel generative methods for addressing missing subpopulation training data (DR-referable darker-skin) achieved instead accuracy, for lighter-skin, of 72.0% (65.8% to 78.2%), and for darker-skin, of 71.5% (65.2% to 77.8%), demonstrating closer parity (delta = 0.5%) in accuracy across subpopulations (Welch t-test t = 0.111, P = 0.912). Findings illustrate how data imbalance and domain generalization can lead to disparity of accuracy across subpopulations, and show that novel generative methods of synthetic fundus images may play a role for debiasing AI. New AI methods have possible applications to address potential AI bias in DR diagnostics from fundus pigmentation, and potentially other ophthalmic DLSs too.", "keywords": ["diagnosing diabetic retinopathy", "deep learning systems", "mitigate artificial intelligence", "study evaluated generative", "domain generalization", "data imbalance", "training data imbalance", "training data", "public domain Kaggle", "domain Kaggle EyePACS", "diabetic retinopathy", "learning systems", "face concepts", "concepts at test", "inference time", "potentially mitigate artificial", "study evaluated", "diagnosing diabetic", "occurs when deep", "deep learning"], "paper_title": "Addressing Artificial Intelligence Bias in Retinal Diagnostics.", "last_updated": "2023/02/04"}, {"id": "0035607469", "domain": "Diabetic retinopathy", "model_name": "Khan et al.", "publication_date": "2022/05/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35607469/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35607469", "task": "IWqQC1koJA", "abstract": "The estimated 30 million children and adults are suffering with diabetes across the world. A person with diabetes can recognize several symptoms, and it can also be tested using retina image as diabetes also affects the human eye. The doctor is usually able to detect retinal changes quickly and can help prevent vision loss. Therefore, regular eye examinations are very important. Diabetes is a chronic disease that affects various parts of the human body including the retina. It can also be considered as major cause for blindness in developed countries. This paper deals with classification of retinal image into diabetes or not with the help of deep learning algorithms and architecture. Hence, deep learning is beneficial for classification of medical images specifically such a complex image of human retina. A large number of image data are considered throughout the project on which classification is performed by using binary classifier. On applying certain deep learning algorithms, model results into the training accuracy of 96.68% and validation accuracy of 66.82%. Diabetic retinopathy can be considered as an effective and efficient method for diabetes detection.", "keywords": ["million children", "children and adults", "adults are suffering", "diabetes", "deep learning", "image", "learning", "human", "deep learning algorithms", "deep", "considered", "retina", "estimated", "million", "world", "classification", "learning algorithms", "children", "adults", "suffering"], "paper_title": "Computational Approach for Detection of Diabetes from Ocular Scans.", "last_updated": "2023/02/04"}, {"id": "0033361011", "domain": "Diabetic retinopathy", "model_name": "DeepUWF", "publication_date": "2021/08/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33361011/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33361011", "task": null, "abstract": "The emerging ultra-wide field of view (UWF) fundus color imaging is a powerful tool for fundus screening. However, manual screening is labor-intensive and subjective. Based on 2644 UWF images, a set of early fundus abnormal screening system named DeepUWF is developed. DeepUWF includes an abnormal fundus screening subsystem and a disease diagnosis subsystem for three kinds of fundus diseases (retinal tear & retinal detachment, diabetic retinopathy and pathological myopia). The components in the system are composed of a set of excellent convolutional neural networks and two custom classifiers. However, the contrast of UWF images used in the research is low, which seriously limits the extraction of fine features of UWF images by depth model. Therefore, the high specificity and low sensitivity of prediction results have always been difficult problems in research. In order to solve this problem, six kinds of image preprocessing techniques are adopted, and their effects on the prediction performance of fundus abnormal and three kinds of fundus diseases models are studied. A variety of experimental indicators are used to evaluate the algorithms for validity and reliability. The experimental results show that these preprocessing methods are helpful to improve the learning ability of the networks and achieve good sensitivity and specificity. Without ophthalmologists, DeepUWF has potential application value, which is helpful for fundus health screening and workflow improvement.", "keywords": ["emerging ultra-wide field", "fundus color imaging", "UWF images", "field of view", "fundus", "UWF", "emerging ultra-wide", "ultra-wide field", "color imaging", "powerful tool", "fundus screening", "fundus abnormal screening", "abnormal fundus screening", "fundus color", "screening", "fundus abnormal", "fundus screening subsystem", "fundus diseases", "abnormal screening system", "early fundus abnormal"], "paper_title": "DeepUWF: An Automated Ultra-Wide-Field Fundus Screening System via Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0033934549", "domain": "Diabetic retinopathy", "model_name": "Luo et al.", "publication_date": "2021/05/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33934549/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33934549", "task": "IWqQC1koJA", "abstract": "Deep learning based retinopathy classification with optical coherence tomography (OCT) images has recently attracted great attention. However, existing deep learning methods fail to work well when training and testing datasets are different due to the general issue of domain shift between datasets caused by different collection devices, subjects, imaging parameters, etc. To address this practical and challenging issue, we propose a novel deep domain adaptation (DDA) method to train a model on a labeled dataset and adapt it to an unlabelled dataset (collected under different conditions). It consists of two modules for domain alignment, that is, adversarial learning and entropy minimization. We conduct extensive experiments on three public datasets to evaluate the performance of the proposed method. The results indicate that there are large domain shifts between datasets, resulting a poor performance for conventional deep learning methods. The proposed DDA method can significantly outperform existing methods for retinopathy classification with OCT images. It achieves retinopathy classification accuracies of 0.915, 0.959 and 0.990 under three cross-domain (cross-dataset) scenarios. Moreover, it obtains a comparable performance with human experts on a dataset where no labeled data in this dataset have been used to train the proposed DDA method. We have also visualized the learnt features by using the t-distributed stochastic neighbor embedding (t-SNE) technique. The results demonstrate that the proposed method can learn discriminative features for retinopathy classification.", "keywords": ["optical coherence tomography", "attracted great attention", "recently attracted great", "Deep learning based", "proposed DDA method", "coherence tomography", "great attention", "deep learning methods", "Deep learning", "optical coherence", "recently attracted", "attracted great", "learning based retinopathy", "retinopathy classification", "based retinopathy classification", "proposed DDA", "DDA method", "Deep", "learning", "learning based"], "paper_title": "Cross-domain retinopathy classification with optical coherence tomography images via a novel deep domain adaptation method.", "last_updated": "2023/02/04"}, {"id": "0036311363", "domain": "Diabetic retinopathy", "model_name": "Ren et al.", "publication_date": "2022/10/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36311363/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36311363", "task": "aipbNdPTIt", "abstract": "Diabetic retinopathy is not just the most common complication of diabetes but also the leading cause of adult blindness. Currently, doctors determine the cause of diabetic retinopathy primarily by diagnosing fundus images. Large-scale manual screening is difficult to achieve for retinal health screen. In this paper, we proposed an improved U-net network for segmenting retinal vessels. Firstly, due to the lack of retinal data, pre-processing of the raw data is required. The data processed by grayscale transformation, normalization, CLAHE, gamma transformation. Data augmentation can prevent overfitting in the training process. Secondly, the basic network structure model U-net is built, and the Bi-FPN network is fused based on U-net. Datasets from a public challenge are used to evaluate the performance of the proposed method, which is able to detect vessel SP of 0.8604, SE of 0.9767, ACC of 0.9651, and AUC of 0.9787.", "keywords": ["diabetic retinopathy primarily", "Diabetic retinopathy", "adult blindness", "common complication", "complication of diabetes", "diagnosing fundus images", "retinopathy primarily", "U-net", "Diabetic", "retinopathy", "data", "improved U-net network", "retinal", "blindness", "network", "common", "complication", "diabetes", "leading", "adult"], "paper_title": "An improved U-net based retinal vessel image segmentation method.", "last_updated": "2023/02/04"}, {"id": "0035325369", "domain": "Diabetic retinopathy", "model_name": "Derwin et al.", "publication_date": "2022/03/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35325369/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35325369", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a chronic disease that may cause vision loss in diabetic patients. Microaneurysms which are characterized by small red spots on the retina due to fluid or blood leakage from the weak capillary wall often occur during the early stage of DR, making screening at this stage is essential. In this paper, an automatic screening system for early detection of DR in retinal images is developed using a combined shape and texture features. Due to minimum number of hand-crafted features, the computational burden is much reduced. The proposed hybrid multi-kernel support vector machine classifier is constructed by learning a kernel model formed as a combination of the base kernels. This approach outperforms the recent deep learning techniques in terms of the evaluation metrics. The efficiency of the proposed scheme is experimentally validated on three public datasets - Retinopathy Online Challenge, DIARETdB1, MESSIDOR, and AGAR300 (developed for this study). Studies reveal that the proposed model produced the best results of 0.503 in ROC dataset, 0.481 in DIARETdB1, and 0.464 in the MESSIDOR dataset in terms of FROC score. The AGAR300 database outperforms the existing MA detection algorithm in terms of FROC, AUC, F1 score, precision, sensitivity, and specificity which guarantees the robustness of this system.", "keywords": ["diabetic patients", "chronic disease", "vision loss", "Diabetic retinopathy", "loss in diabetic", "Retinopathy Online Challenge", "Diabetic", "terms of FROC", "small red spots", "weak capillary wall", "early stage", "automatic screening system", "terms", "Retinopathy Online", "proposed", "making screening", "patients", "Online Challenge", "retina due", "due to fluid"], "paper_title": "Hybrid multi-kernel SVM algorithm for detection of microaneurysm in color fundus images.", "last_updated": "2023/02/04"}, {"id": "0030340214", "domain": "Diabetic retinopathy", "model_name": "Zago et al.", "publication_date": "2018/10/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30340214/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30340214", "task": null, "abstract": "Poor-quality retinal images do not allow an accurate medical diagnosis, and it is inconvenient for a patient to return to a medical center to repeat the fundus photography exam. In this paper, a robust automatic system is proposed to assess the quality of retinal images at the moment of the acquisition, aiming at assisting health care professionals during a fundus photography exam. We propose a convolutional neural network (CNN) pretrained on non-medical images for extracting general image features. The weights of the CNN are further adjusted via a fine-tuning procedure, resulting in a performant classifier obtained only with a small quantity of labeled images. The CNN performance was evaluated on two publicly available databases (i.e., DRIMDB and ELSA-Brasil) using two different procedures: intra-database and inter-database cross-validation. The CNN achieved an area under the curve (AUC) of 99.98% on DRIMDB and an AUC of 98.56% on ELSA-Brasil in the inter-database experiment, where training and testing were not performed on the same database. These results show the robustness of the proposed model to various image acquisitions without requiring special adaptation, thus making it a good candidate for use in operational clinical scenarios.", "keywords": ["accurate medical diagnosis", "fundus photography exam", "Poor-quality retinal images", "photography exam", "medical diagnosis", "accurate medical", "medical center", "fundus photography", "patient to return", "center to repeat", "Poor-quality retinal", "retinal images", "CNN", "medical", "images", "repeat the fundus", "exam", "photography", "robust automatic system", "assisting health care"], "paper_title": "Retinal image quality assessment using deep learning.", "last_updated": "2023/02/04"}, {"id": "0034206941", "domain": "Diabetic retinopathy", "model_name": "Liu et al.", "publication_date": "2021/06/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34206941/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34206941", "task": null, "abstract": "Diabetic retinopathy (DR) is a common complication of diabetes mellitus (DM), and it is necessary to diagnose DR in the early stages of treatment. With the rapid development of convolutional neural networks in the field of image processing, deep learning methods have achieved great success in the field of medical image processing. Various medical lesion detection systems have been proposed to detect fundus lesions. At present, in the image classification process of diabetic retinopathy, the fine-grained properties of the diseased image are ignored and most of the retinopathy image data sets have serious uneven distribution problems, which limits the ability of the network to predict the classification of lesions to a large extent. We propose a new non-homologous bilinear pooling convolutional neural network model and combine it with the attention mechanism to further improve the network's ability to extract specific features of the image. The experimental results show that, compared with the most popular fundus image classification models, the network model we proposed can greatly improve the prediction accuracy of the network while maintaining computational efficiency.", "keywords": ["diabetes mellitus", "stages of treatment", "common complication", "complication of diabetes", "early stages", "medical image processing", "image", "image processing", "convolutional neural networks", "Diabetic retinopathy", "network", "neural network model", "network model", "image classification", "retinopathy image data", "image classification models", "convolutional neural", "fundus image classification", "retinopathy image", "retinopathy"], "paper_title": "Diabetic Retinal Grading Using Attention-Based Bilinear Convolutional Neural Network and Complement Cross Entropy.", "last_updated": "2023/02/04"}, {"id": "0035390737", "domain": "Diabetic retinopathy", "model_name": "Qu et al.", "publication_date": "2022/03/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35390737/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35390737", "task": null, "abstract": "Collaborative learning, which enables collaborative and decentralized training of deep neural networks at multiple institutions in a privacy-preserving manner, is rapidly emerging as a valuable technique in healthcare applications. However, its distributed nature often leads to significant heterogeneity in data distributions across institutions. In this paper, we present a novel generative replay strategy to address the challenge of data heterogeneity in collaborative learning methods. Different from traditional methods that directly aggregating the model parameters, we leverage generative adversarial learning to aggregate the knowledge from all the local institutions. Specifically, instead of directly training a model for task performance, we develop a novel dual model architecture: a primary model learns the desired task, and an auxiliary \"generative replay model\" allows aggregating knowledge from the heterogenous clients. The auxiliary model is then broadcasted to the central sever, to regulate the training of primary model with an unbiased target distribution. Experimental results demonstrate the capability of the proposed method in handling heterogeneous data across institutions. On highly heterogeneous data partitions, our model achieves \u223c4.88% improvement in the prediction accuracy on a diabetic retinopathy classification dataset, and \u223c49.8% reduction of mean absolution value on a Bone Age prediction dataset, respectively, compared to the state-of-the art collaborative learning methods.", "keywords": ["deep neural networks", "privacy-preserving manner", "healthcare applications", "collaborative learning methods", "deep neural", "neural networks", "networks at multiple", "rapidly emerging", "valuable technique", "technique in healthcare", "Collaborative learning", "model", "enables collaborative", "multiple institutions", "learning", "learning methods", "Collaborative", "institutions", "decentralized training", "data"], "paper_title": "Handling data heterogeneity with generative replay in collaborative learning for medical imaging.", "last_updated": "2023/02/04"}, {"id": "0034741905", "domain": "Diabetic retinopathy", "model_name": "Sun et al.", "publication_date": "2021/11/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34741905/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34741905", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR), as an important complication of diabetes, is the primary cause of blindness in adults. Automatic DR detection poses a challenge which is crucial for early DR screening. Currently, the vast majority of DR is diagnosed through fundus images, where the microaneurysm (MA) has been widely used as the most distinguishable marker. Research works on automatic DR detection have traditionally utilized manually designed operators, while a few recent researchers have explored deep learning techniques for this topic. But due to issues such as the extremely small size of microaneurysms, low resolution of fundus pictures, and insufficient imaging depth, the DR detection problem is quite challenging and remains unsolved. To address these issues, this research proposes a new deep learning model (Magnified Adaptive Feature Pyramid Network, MAFP-Net) for DR detection, which conducts super-resolution on low quality fundus images and integrates an improved feature pyramid structure while utilizing a standard two-stage detection network as the backbone. Our proposed detection model needs no pre-segmented patches to train the CNN network. When tested on the E-ophtha-MA dataset, the sensitivity value of our method reached as high as 83.5% at false positives per image (FPI) of 8 and the F1 value achieved 0.676, exceeding all those of the state-of-the-art algorithms as well as the human performance of experienced physicians. Similar results were achieved on another public dataset of IDRiD.", "keywords": ["Diabetic retinopathy", "complication of diabetes", "blindness in adults", "important complication", "detection", "Magnified Adaptive Feature", "Adaptive Feature Pyramid", "Feature Pyramid Network", "Feature Pyramid", "fundus images", "Automatic DR detection", "deep learning", "Network", "Magnified Adaptive", "fundus", "two-stage detection network", "improved feature pyramid", "feature pyramid structure", "Diabetic", "retinopathy"], "paper_title": "A Magnified Adaptive Feature Pyramid Network for automatic microaneurysms detection.", "last_updated": "2023/02/04"}, {"id": "0035784186", "domain": "Diabetic retinopathy", "model_name": "FN-OCT", "publication_date": "2022/06/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35784186/", "code_link": "https://github.com/harisiqbal88/plotneuralnet", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "35784186", "task": "IWqQC1koJA", "abstract": "Optical coherence tomography (OCT) is a new type of tomography that has experienced rapid development and potential in recent years. It is playing an increasingly important role in retinopathy diagnoses. At present, due to the uneven distributions of medical resources in various regions, the uneven proficiency levels of doctors in grassroots and remote areas, and the development needs of rare disease diagnosis and precision medicine, artificial intelligence technology based on deep learning can provide fast, accurate, and effective solutions for the recognition and diagnosis of retinal OCT images. To prevent vision damage and blindness caused by the delayed discovery of retinopathy, a fusion network (FN)-based retinal OCT classification algorithm (FN-OCT) is proposed in this paper to improve upon the adaptability and accuracy of traditional classification algorithms. The InceptionV3, Inception-ResNet, and Xception deep learning algorithms are used as base classifiers, a convolutional block attention mechanism (CBAM) is added after each base classifier, and three different fusion strategies are used to merge the prediction results of the base classifiers to output the final prediction results (choroidal neovascularization (CNV), diabetic macular oedema (DME), drusen, normal). The results show that in a classification problem involving the UCSD common retinal OCT dataset (108,312 OCT images from 4,686 patients), compared with that of the InceptionV3 network model, the prediction accuracy of FN-OCT is improved by 5.3% (accuracy = 98.7%, area under the curve (AUC) = 99.1%). The predictive accuracy and AUC achieved on an external dataset for the classification of retinal OCT diseases are 92 and 94.5%, respectively, and gradient-weighted class activation mapping (Grad-CAM) is used as a visualization tool to verify the effectiveness of the proposed FNs. This finding indicates that the developed fusion algorithm can significantly improve the performance of classifiers while providing a powerful tool and theoretical support for assisting with the diagnosis of retinal OCT.", "keywords": ["Optical coherence tomography", "retinal OCT", "experienced rapid development", "retinal OCT classification", "based retinal OCT", "retinal OCT images", "coherence tomography", "OCT", "retinal OCT diseases", "retinal OCT dataset", "OCT classification algorithm", "Optical coherence", "recent years", "experienced rapid", "potential in recent", "common retinal OCT", "OCT images", "type of tomography", "rapid development", "retinal"], "paper_title": "FN-OCT: Disease Detection Algorithm for Retinal Optical Coherence Tomography Based on a Fusion Network.", "last_updated": "2023/02/04"}, {"id": "0035785142", "domain": "Diabetic retinopathy", "model_name": "Bhardwaj et al.", "publication_date": "2022/06/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35785142/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35785142", "task": "IWqQC1koJA", "abstract": "Early diagnosis of retinal diseases such as diabetic retinopathy has had the attention of many researchers. Deep learning through the introduction of convolutional neural networks has become a prominent solution for image-related tasks such as classification and segmentation. Most tasks in image classification are handled by deep CNNs pretrained and evaluated on imagenet dataset. However, these models do not always translate to the best result on other datasets. Devising a neural network manually from scratch based on heuristics may not lead to an optimal model as there are numerous hyperparameters in play. In this paper, we use two nature-inspired swarm algorithms: particle swarm optimization (PSO) and ant colony optimization (ACO) to obtain TDCN models to perform classification of fundus images into severity classes. The power of swarm algorithms is used to search for various combinations of convolutional, pooling, and normalization layers to provide the best model for the task. It is observed that TDCN-PSO outperforms imagenet models and existing literature, while TDCN-ACO achieves faster architecture search. The best TDCN model achieves an accuracy of 90.3%, AUC ROC of 0.956, and a Cohen's kappa score of 0.967. The results were compared with the previous studies to show that the proposed TDCN models exhibit superior performance.", "keywords": ["Early diagnosis", "diagnosis of retinal", "retinal diseases", "diabetic retinopathy", "TDCN models", "TDCN", "models", "classification", "convolutional neural networks", "model", "TDCN model achieves", "Early", "researchers", "swarm", "swarm algorithms", "obtain TDCN models", "diagnosis", "retinal", "diseases", "diabetic"], "paper_title": "Early Diagnosis of Retinal Blood Vessel Damage via Deep Learning-Powered Collective Intelligence Models.", "last_updated": "2023/02/04"}, {"id": "0036405613", "domain": "Diabetic retinopathy", "model_name": "Yang et al.", "publication_date": "2022/11/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36405613/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36405613", "task": null, "abstract": "Artificial intelligence (AI) has been applied in the field of retina. The purpose of this study was to analyze the study trends within AI in retina by reporting on publication trends, to identify journals, countries, authors, international collaborations, and keywords involved in AI in retina. A cross-sectional study. Bibliometric methods were used to evaluate global production and development trends in AI in retina since 2012 using Web of Science Core Collection. A total of 599 publications were retrieved ultimately. We found that AI in retina is a very attractive topic in scientific and medical community. No journal was found to specialize in AI in retina. The USA, China, and India were the three most productive countries. Authors from Austria, Singapore, and England also had worldwide academic influence. China has shown the greatest rapid increase in publication numbers. International collaboration could increase influence in this field. Keywords revealed that diabetic retinopathy, optical coherence tomography on multiple diseases, algorithm were three popular topics in the field. Most of top journals and top publication on AI in retina were mainly focused on engineering and computing, rather than medicine. These results helped clarify the current status and future trends in researches of AI in retina. This study may be useful for clinicians and scientists to have a general overview of this field, and better understand the main actors in this field (including authors, journals, and countries). Researches are supposed to focus on more retinal diseases, multiple modal imaging, and performance of AI models in real-world clinical application. Collaboration among countries and institutions is common in current research of AI in retina.", "keywords": ["retina", "Artificial intelligence", "Science Core Collection", "study", "field", "trends", "Core Collection", "Web of Science", "Science Core", "countries", "study trends", "publication", "journals", "authors", "Artificial", "intelligence", "applied", "China", "publication trends", "cross-sectional study"], "paper_title": "Publication trends of artificial intelligence in retina in 10 years: Where do we stand?", "last_updated": "2023/02/04"}, {"id": "0036374821", "domain": "Diabetic retinopathy", "model_name": "Basha et al.", "publication_date": "2022/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36374821/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36374821", "task": "IWqQC1koJA", "abstract": "In recent years, diabetic retinopathy (DR) needs to be focused with the intention of developing accurate and effective approaches by accomplishing the existing challenges in the traditional models. With this objective, this paper aims to introduce an effective diagnosis system by utilizing retinal fundus images. The implementation of this diagnosis model incorporates 4 stages like (i) preprocessing, (ii) blood vessel segmentation, (iii) feature extraction, as well as (iv) classification. Originally, the median filter as well as contrast limited adaptive histogram equalization (CLAHE) help to preprocess the image. Moreover, the Fuzzy C Mean (FCM) thresholding is applied for blood vessel segmentation, which generates stochastic clustering of pixels to obtain enhanced threshold values. Further, feature extraction is accomplished by utilizing gray-level run-length matrix (GLRM), local, and morphological transformation-based features. Furthermore, a deep learning (DL) model known as convolutional neural network (CNN) is employed for the diagnosis or classification purpose. As a main novelty, this paper introduces an optimal feature selection as well as classification model. Further, the feature selection is done optimally by FireFly Migration Operator-based Monarch Butterfly Optimization (FM-MBO) which hybridized of the monarch butterfly optimization (MBO) and fire fly (FF) algorithms as the entire adopted extracted features attain higher feature length. Moreover, the proposed FM-MBO algorithm helps for optimizing the count of CNN's convolutional neurons to further improve the performance accuracy. At the end, the enhanced outcomes of the adopted diagnostic scheme are validated via a valuable comparative examination in terms of significant performance measures.", "keywords": ["diabetic retinopathy", "recent years", "intention of developing", "developing accurate", "approaches by accomplishing", "accomplishing the existing", "existing challenges", "effective approaches", "effective diagnosis system", "blood vessel segmentation", "Monarch Butterfly Optimization", "traditional models", "feature", "vessel segmentation", "effective diagnosis", "Butterfly Optimization", "retinal fundus images", "accurate and effective", "diagnosis model incorporates", "Monarch Butterfly"], "paper_title": "Optimal Feature Selection for Diagnosing Diabetic Retinopathy Using FireFly Migration Operator-Based Monarch Butterfly Optimization.", "last_updated": "2023/02/04"}, {"id": "0033130554", "domain": "Diabetic retinopathy", "model_name": "Sevgi et al.", "publication_date": "2020/10/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33130554/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33130554", "task": null, "abstract": "To evaluate longitudinal quantitative ischaemic and vasculature parameters, including ischaemic index, vessel area, length and geodesic distance in sickle cell retinopathy (SCR) on ultra-widefield fluorescein angiography (UWFA). Optimal UWFA images from two longitudinal timepoints of 74 eyes from 45 patients with SCR were aligned and a common region of interest was determined. A deep-learning augmented ischaemia and vascular segmentation platform was used for feature extraction. Geodesic distance maps demonstrating the shortest distance within the vascular masks from the centre of the optic disc were created. Ischaemic index, vessel area, vessel length and geodesic distance were measured. Paired t-test and linear mixed effect model analysis were performed. Overall, 25 (44 eyes) patients with HbSS, 14 (19 eyes) with HbSC, 6 (11 eyes) with HbSthal and other genotypes were included. Mean age was 40.1\u00b111.0\u00a0years. Mean time interval between two UWFA studies was 23.0\u00b115.1\u00a0months (range: 3-71.3). Mean panretinal ischaemic index increased from 10.0\u00b17.2% to 10.9\u00b17.3% (p<0.005). Mean rate of change in ischaemic index was 0.5\u00b10.7% per year. Mean vessel area (p=0.020) and geodesic distance (p=0.048) decreased significantly. Multivariate analysis demonstrated baseline ischaemic index and Goldberg stage are correlated with progression. Longitudinal ischaemic index and retinal vascular parameter measurements demonstrate statistically significant progression in SCR. The clinical significance of these relatively small magnitude changes remains unclear but may provide insights into the progression of retinal ischaemia in SCR.", "keywords": ["sickle cell retinopathy", "ultra-widefield fluorescein angiography", "evaluate longitudinal quantitative", "geodesic distance", "cell retinopathy", "fluorescein angiography", "ischaemic index", "sickle cell", "ultra-widefield fluorescein", "UWFA", "longitudinal quantitative ischaemic", "Optimal UWFA images", "including ischaemic index", "vessel area", "ischaemic", "index", "SCR", "geodesic", "distance", "evaluate longitudinal"], "paper_title": "Longitudinal assessment of quantitative ultra-widefield ischaemic and vascular parameters in sickle cell retinopathy.", "last_updated": "2023/02/04"}, {"id": "0032196092", "domain": "Diabetic retinopathy", "model_name": "Balachandar et al.", "publication_date": "2021/03/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32196092/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32196092", "task": null, "abstract": "Sharing patient data across institutions to train generalizable deep learning models is challenging due to regulatory and technical hurdles. Distributed learning, where model weights are shared instead of patient data, presents an attractive alternative. Cyclical weight transfer (CWT) has recently been demonstrated as an effective distributed learning method for medical imaging with homogeneous data across institutions. In this study, we optimize CWT to overcome performance losses from variability in training sample sizes and label distributions across institutions. Optimizations included proportional local training iterations, cyclical learning rate, locally weighted minibatch sampling, and cyclically weighted loss. We evaluated our optimizations on simulated distributed diabetic retinopathy detection and chest radiograph classification. Proportional local training iteration mitigated performance losses from sample size variability, achieving 98.6% of the accuracy attained by centrally hosting in the diabetic retinopathy dataset split with highest sample size variance across institutions. Locally weighted minibatch sampling and cyclically weighted loss both mitigated performance losses from label distribution variability, achieving 98.6% and 99.1%, respectively, of the accuracy attained by centrally hosting in the diabetic retinopathy dataset split with highest label distribution variability across institutions. Our optimizations to CWT improve its capability of handling data variability across institutions. Compared to CWT without optimizations, CWT with optimizations achieved performance significantly closer to performance from centrally hosting. Our work is the first to identify and address challenges of sample size and label distribution variability in simulated distributed deep learning for medical imaging. Future work is needed to address other sources of real-world data variability.", "keywords": ["Sharing patient data", "technical hurdles", "CWT", "train generalizable deep", "variability", "train generalizable", "challenging due", "due to regulatory", "regulatory and technical", "institutions", "data", "Sharing patient", "patient data", "learning", "performance", "label distribution variability", "Optimizations", "distribution variability", "Distributed", "label distribution"], "paper_title": "Accounting for data variability in multi-institutional distributed deep learning for medical imaging.", "last_updated": "2023/02/04"}, {"id": "0035885619", "domain": "Diabetic retinopathy", "model_name": "Khan et al.", "publication_date": "2022/07/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35885619/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35885619", "task": null, "abstract": "While color fundus photos are used in routine clinical practice to diagnose ophthalmic conditions, evidence suggests that ocular imaging contains valuable information regarding the systemic health features of patients. These features can be identified through computer vision techniques including deep learning (DL) artificial intelligence (AI) models. We aim to construct a DL model that can predict systemic features from fundus images and to determine the optimal method of model construction for this task. Data were collected from a cohort of patients undergoing diabetic retinopathy screening between March 2020 and March 2021. Two models were created for each of 12 systemic health features based on the DenseNet201 architecture: one utilizing transfer learning with images from ImageNet and another from 35,126 fundus images. Here, 1277 fundus images were used to train the AI models. Area under the receiver operating characteristics curve (AUROC) scores were used to compare the model performance. Models utilizing the ImageNet transfer learning data were superior to those using retinal images for transfer learning (mean AUROC 0.78 vs. 0.65, p-value < 0.001). Models using ImageNet pretraining were able to predict systemic features including ethnicity (AUROC 0.93), age > 70 (AUROC 0.90), gender (AUROC 0.85), ACE inhibitor (AUROC 0.82), and ARB medication use (AUROC 0.78). We conclude that fundus images contain valuable information about the systemic characteristics of a patient. To optimize DL model performance, we recommend that even domain specific models consider using transfer learning from more generalized image sets to improve accuracy.", "keywords": ["diagnose ophthalmic conditions", "routine clinical practice", "AUROC", "color fundus photos", "ophthalmic conditions", "evidence suggests", "routine clinical", "clinical practice", "practice to diagnose", "diagnose ophthalmic", "suggests that ocular", "ocular imaging", "systemic health features", "features", "systemic", "fundus images", "images", "fundus", "models", "transfer learning"], "paper_title": "Predicting Systemic Health Features from Retinal Fundus Images Using Transfer-Learning-Based Artificial Intelligence Models.", "last_updated": "2023/02/04"}, {"id": "0036246951", "domain": "Diabetic retinopathy", "model_name": "Chen et al.", "publication_date": "2021/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36246951/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36246951", "task": null, "abstract": "Generative adversarial networks (GANs) are deep learning (DL) models that can create and modify realistic-appearing synthetic images, or deepfakes, from real images. The purpose of our study was to evaluate the ability of experts to discern synthesized retinal fundus images from real fundus images and to review the current uses and limitations of GANs in ophthalmology. Development and expert evaluation of a GAN and an informal review of the literature. A total of 4282 image pairs of fundus images and retinal vessel maps acquired from a multicenter ROP screening program. Pix2Pix HD, a high-resolution GAN, was first trained and validated on fundus and vessel map image pairs and subsequently used to generate 880 images from a held-out test set. Fifty synthetic images from this test set and 50 different real images were presented to 4 expert ROP ophthalmologists using a custom online system for evaluation of whether the images were real or synthetic. Literature was reviewed on PubMed and Google Scholars using combinations of the terms <i>ophthalmology</i>, <i>GANs, generative adversarial networks, ophthalmology, images, deepfakes,</i> and <i>synthetic</i>. Ancestor search was performed to broaden results. Expert ability to discern real versus synthetic images was evaluated using percent accuracy. Statistical significance was evaluated using a Fisher exact test, with <i>P</i> values \u2264 0.05 thresholded for significance. The expert majority correctly identified 59% of images as being real or synthetic (<i>P</i>\u00a0= 0.1). Experts 1 to 4 correctly identified 54%, 58%, 49%, and 61% of images (<i>P</i>\u00a0= 0.505, 0.158, 1.000, and 0.043, respectively). These results suggest that the majority of experts could not discern between real and synthetic images. Additionally, we identified 20 implementations of GANs in the ophthalmology literature, with applications in a variety of imaging modalities and ophthalmic diseases. Generative adversarial networks can create synthetic fundus images that are indiscernible from real fundus images by expert ROP ophthalmologists. Synthetic images may improve dataset augmentation for DL, may be used in trainee education, and may have implications for patient privacy.", "keywords": ["images", "fundus images", "synthetic images", "modify realistic-appearing synthetic", "synthetic", "real", "real fundus images", "fundus", "realistic-appearing synthetic images", "deep learning", "GANs", "modify realistic-appearing", "expert ROP", "ROP", "Generative adversarial networks", "expert ROP ophthalmologists", "expert", "Generative adversarial", "synthetic fundus images", "real images"], "paper_title": "Deepfakes in Ophthalmology: Applications and Realism of Synthetic Retinal Images from Generative Adversarial Networks.", "last_updated": "2023/02/04"}, {"id": "0033328056", "domain": "Diabetic retinopathy", "model_name": "Xie et al.", "publication_date": "2020/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33328056/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33328056", "task": null, "abstract": "Deep learning is a novel machine learning technique that has been shown to be as effective as human graders in detecting diabetic retinopathy from fundus photographs. We used a cost-minimisation analysis to evaluate the potential savings of two deep learning approaches as compared with the current human assessment: a semi-automated deep learning model as a triage filter before secondary human assessment; and a fully automated deep learning model without human assessment. In this economic analysis modelling study, using 39\u2008006 consecutive patients with diabetes in a national diabetic retinopathy screening programme in Singapore in 2015, we used a decision tree model and TreeAge Pro to compare the actual cost of screening this cohort with human graders against the simulated cost for semi-automated and fully automated screening models. Model parameters included diabetic retinopathy prevalence rates, diabetic retinopathy screening costs under each screening model, cost of medical consultation, and diagnostic performance (ie, sensitivity and specificity). The primary outcome was total cost for each screening model. Deterministic sensitivity analyses were done to gauge the sensitivity of the results to key model assumptions. From the health system perspective, the semi-automated screening model was the least expensive of the three models, at US$62 per patient per year. The fully automated model was $66 per patient per year, and the human assessment model was $77 per patient per year. The savings to the Singapore health system associated with switching to the semi-automated model are estimated to be $489\u2008000, which is roughly 20% of the current annual screening cost. By 2050, Singapore is projected to have 1 million people with diabetes; at this time, the estimated annual savings would be $15 million. This study provides a strong economic rationale for using deep learning systems as an assistive tool to screen for diabetic retinopathy. Ministry of Health, Singapore.", "keywords": ["Deep learning", "deep learning model", "machine learning technique", "diabetic retinopathy", "diabetic retinopathy screening", "automated deep learning", "model", "detecting diabetic retinopathy", "human assessment", "learning", "semi-automated deep learning", "screening", "deep learning approaches", "human", "Deep", "deep learning systems", "learning model", "fundus photographs", "fully automated deep", "screening model"], "paper_title": "Artificial intelligence for teleophthalmology-based diabetic retinopathy screening in a national programme: an economic analysis modelling study.", "last_updated": "2023/02/04"}, {"id": "0034492064", "domain": "Diabetic retinopathy", "model_name": "HDC-Net", "publication_date": "2021/09/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34492064/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34492064", "task": "aipbNdPTIt", "abstract": "The cardinal symptoms of some ophthalmic diseases observed through exceptional retinal blood vessels, such as retinal vein occlusion, diabetic retinopathy, etc. The advanced deep learning models used to obtain morphological and structural information of blood vessels automatically are conducive to the early treatment and initiative prevention of ophthalmic diseases. In our work, we propose a hierarchical dilation convolutional network (HDC-Net) to extract retinal vessels in a pixel-to-pixel manner. It utilizes the hierarchical dilation convolution (HDC) module to capture the fragile retinal blood vessels usually neglected by other methods. An improved residual dual efficient channel attention (RDECA) module can infer more delicate channel information to reinforce the discriminative capability of the model. The structured Dropblock can help our HDC-Net model to solve the network overfitting effectively. From a holistic perspective, the segmentation results obtained by HDC-Net are superior to other deep learning methods on three acknowledged datasets (DRIVE, CHASE-DB1, STARE), the sensitivity, specificity, accuracy, f1-score and AUC score are {0.8252, 0.9829, 0.9692, 0.8239, 0.9871}, {0.8227, 0.9853, 0.9745, 0.8113, 0.9884}, and {0.8369, 0.9866, 0.9751, 0.8385, 0.9913}, respectively. It surpasses most other advanced retinal vessel segmentation models. Qualitative and quantitative analysis demonstrates that HDC-Net can fulfill the task of retinal vessel segmentation efficiently and accurately.", "keywords": ["ophthalmic diseases observed", "retinal vein occlusion", "retinal blood vessels", "exceptional retinal blood", "blood vessels", "diabetic retinopathy", "ophthalmic diseases", "vein occlusion", "cardinal symptoms", "observed through exceptional", "retinal vessel segmentation", "blood vessels automatically", "retinal blood", "diseases observed", "retinal vessel", "retinal", "exceptional retinal", "retinal vein", "vessels", "fragile retinal blood"], "paper_title": "HDC-Net: A hierarchical dilation convolutional network for retinal vessel segmentation.", "last_updated": "2023/02/04"}, {"id": "0032222424", "domain": "Diabetic retinopathy", "model_name": "NFN\uff0b", "publication_date": "2020/03/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32222424/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32222424", "task": "aipbNdPTIt", "abstract": "In the early diagnosis of diabetic retinopathy, the morphological attributes of blood vessels play an essential role to construct a retinal computer-aided diagnosis system. However, due to the challenges including limited densely annotated data, inter-vessel differences and structured prediction problem, it remains challenging to segment accurately the retinal vessels, particularly the capillaries on color fundus images. To address these issues, in this paper, we propose a novel deep learning-based model called NFN\uff0b\u00a0to effectively extract multi-scale information and make full use of deep feature maps. In NFN\uff0b, the front network converts an image patch into a probabilistic retinal vessel map, and the followed network further refines the map to achieve a better post-processing module, which helps represent the vessel structures implicitly. We employ the inter-network skip connections to unite two identical multi-scale backbones, which enables the useful multi-scale features to be directly transferred from shallow layers to deeper layers. The refined probabilistic retinal vessel maps produced from the augmented images are then averaged to construct the segmentation results. We evaluated this model on the digital retinal images for vessel extraction (DRIVE), structured analysis of the retina (STARE), and the child heart and health study (CHASE) databases. Our results indicate that the elaborated cascaded designs can produce performance gain and the proposed NFN\uff0b\u00a0model, to our best knowledge, achieved the state-of-the-art retinal vessel segmentation accuracy on color fundus images (AUC: 98.30%, 98.75% and 98.94%, respectively).", "keywords": ["computer-aided diagnosis system", "retinal computer-aided diagnosis", "blood vessels play", "diagnosis system", "early diagnosis", "computer-aided diagnosis", "probabilistic retinal vessel", "diabetic retinopathy", "retinal vessel", "morphological attributes", "attributes of blood", "play an essential", "essential role", "retinal vessel map", "retinal", "color fundus images", "retinal computer-aided", "diagnosis of diabetic", "blood vessels", "vessels play"], "paper_title": "NFN\uff0b: A novel network followed network for retinal vessel segmentation.", "last_updated": "2023/02/04"}, {"id": "0033441825", "domain": "Diabetic retinopathy", "model_name": "Mirshahi et al.", "publication_date": "2021/01/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33441825/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33441825", "task": "aipbNdPTIt", "abstract": "The purpose of this study was\u00a0to introduce a new deep learning (DL) model for segmentation of the fovea avascular zone (FAZ) in en face optical coherence tomography angiography (OCTA) and compare the results with those of the device's built-in software and manual measurements in healthy subjects and diabetic patients. In this retrospective study, FAZ borders were delineated in the\u00a0inner retinal slab of 3\u2009\u00d7\u20093 enface OCTA images of 131 eyes of 88 diabetic patients and 32 eyes of 18 healthy subjects. To train a deep convolutional neural network (CNN) model, 126 enface OCTA images (104 eyes with diabetic retinopathy and 22 normal eyes) were used as training/validation dataset. Then, the accuracy of the model was evaluated using a dataset consisting of OCTA images of 10 normal eyes and 27 eyes with diabetic retinopathy. The CNN model was based on Detectron2, an open-source modular object detection library. In addition, automated FAZ measurements were conducted using the device's built-in commercial software, and manual FAZ delineation was performed using ImageJ software. Bland-Altman analysis was used to show 95% limit of agreement (95% LoA) between different methods. The mean dice similarity coefficient of the DL model was 0.94\u2009\u00b1\u20090.04 in the testing dataset. There was excellent agreement between automated, DL model and manual measurements of FAZ in healthy subjects (95% LoA of\u2009-\u20090.005 to 0.026 mm<sup>2</sup> between automated and manual measurement and 0.000 to 0.009 mm<sup>2</sup> between DL and manual FAZ area). In diabetic eyes, the agreement between DL and manual measurements was excellent (95% LoA of\u2009-\u20090.063 to 0.095), however, there was a poor agreement between the automated and manual method (95% LoA of\u2009-\u20090.186 to 0.331). The presence of diabetic macular edema and intraretinal cysts at the fovea were associated with erroneous FAZ measurements by the device's built-in software.\u00a0In conclusion, the DL model showed an excellent accuracy in detection of FAZ border in enfaces OCTA images of both diabetic patients and healthy subjects. The DL and manual measurements outperformed the automated measurements of the built-in software.", "keywords": ["enface OCTA images", "OCTA images", "coherence tomography angiography", "face optical coherence", "optical coherence tomography", "FAZ", "OCTA", "enface OCTA", "diabetic patients", "manual FAZ", "manual measurements", "manual", "fovea avascular zone", "eyes", "diabetic", "model", "FAZ measurements", "healthy subjects", "avascular zone", "tomography angiography"], "paper_title": "Foveal avascular zone segmentation in optical coherence tomography angiography images using a deep learning approach.", "last_updated": "2023/02/04"}, {"id": "0034248661", "domain": "Diabetic retinopathy", "model_name": "diffusionsurrogatedeeplearning", "publication_date": "2021/06/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34248661/", "code_link": "https://github.com/jquetzalcoatl/diffusionsurrogatedeeplearning", "model_type": "CNN", "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "34248661", "task": null, "abstract": "In many mechanistic medical, biological, physical, and engineered spatiotemporal dynamic models the numerical solution of partial differential equations (PDEs), especially for diffusion, fluid flow and mechanical relaxation, can make simulations impractically slow. Biological models of tissues and organs often require the simultaneous calculation of the spatial variation of concentration of dozens of diffusing chemical species. One clinical example where rapid calculation of a diffusing field is of use is the estimation of oxygen gradients in the retina, based on imaging of the retinal vasculature, to guide surgical interventions in diabetic retinopathy. Furthermore, the ability to predict blood perfusion and oxygenation may one day guide clinical interventions in diverse settings, i.e., from stent placement in treating heart disease to BOLD fMRI interpretation in evaluating cognitive function (Xie et al., 2019; Lee et al., 2020). Since the quasi-steady-state solutions required for fast-diffusing chemical species like oxygen are particularly computationally costly, we consider the use of a neural network to provide an approximate solution to the steady-state diffusion equation. Machine learning surrogates, neural networks trained to provide approximate solutions to such complicated numerical problems, can often provide speed-ups of several orders of magnitude compared to direct calculation. Surrogates of PDEs could enable use of larger and more detailed models than are possible with direct calculation and can make including such simulations in real-time or near-real time workflows practical. Creating a surrogate requires running the direct calculation tens of thousands of times to generate training data and then training the neural network, both of which are computationally expensive. Often the practical applications of such models require thousands to millions of replica simulations, for example for parameter identification and uncertainty quantification, each of which gains speed from surrogate use and rapidly recovers the up-front costs of surrogate generation. We use a Convolutional Neural Network to approximate the stationary solution to the diffusion equation in the case of two equal-diameter, circular, constant-value sources located at random positions in a two-dimensional square domain with absorbing boundary conditions. Such a configuration caricatures the chemical concentration field of a fast-diffusing species like oxygen in a tissue with two parallel blood vessels in a cross section perpendicular to the two blood vessels. To improve convergence during training, we apply a training approach that uses roll-back to reject stochastic changes to the network that increase the loss function. The trained neural network approximation is about 1000 times faster than the direct calculation for individual replicas. Because different applications will have different criteria for acceptable approximation accuracy, we discuss a variety of loss functions and accuracy estimators that can help select the best network for a particular application. We briefly discuss some of the issues we encountered with overfitting, mismapping of the field values and the geometrical conditions that lead to large absolute and relative errors in the approximate solution.", "keywords": ["engineered spatiotemporal dynamic", "spatiotemporal dynamic models", "partial differential equations", "simulations impractically slow", "neural network", "direct calculation", "mechanistic medical", "fluid flow", "mechanical relaxation", "impractically slow", "engineered spatiotemporal", "spatiotemporal dynamic", "partial differential", "flow and mechanical", "Convolutional Neural Network", "network", "calculation", "make simulations impractically", "neural", "approximate solution"], "paper_title": "Deep Learning Approaches to Surrogates for Solving the Diffusion Equation for Mechanistic Real-World Simulations.", "last_updated": "2023/02/04"}, {"id": "0034567101", "domain": "Diabetic retinopathy", "model_name": "Ihnaini et al.", "publication_date": "2021/09/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34567101/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34567101", "task": null, "abstract": "The prediction of human diseases precisely is still an uphill battle task for better and timely treatment. A multidisciplinary diabetic disease is a life-threatening disease all over the world. It attacks different vital parts of the human body, like Neuropathy, Retinopathy, Nephropathy, and ultimately Heart. A smart healthcare recommendation system predicts and recommends the diabetic disease accurately using optimal machine learning models with the data fusion technique on healthcare datasets. Various machine learning models and methods have been proposed in the recent past to predict diabetes disease. Still, these systems cannot handle the massive number of multifeatures datasets on diabetes disease properly. A smart healthcare recommendation system is proposed for diabetes disease based on deep machine learning and data fusion perspectives. Using data fusion, we can eliminate the irrelevant burden of system computational capabilities and increase the proposed system's performance to predict and recommend this life-threatening disease more accurately. Finally, the ensemble machine learning model is trained for diabetes prediction. This intelligent recommendation system is evaluated based on a well-known diabetes dataset, and its performance is compared with the most recent developments from the literature. The proposed system achieved 99.6% accuracy, which is higher compared to the existing deep machine learning methods. Therefore, our proposed system is better for multidisciplinary diabetes disease prediction and recommendation. Our proposed system's improved disease diagnosis performance advocates for its employment in the automated diagnostic and recommendation systems for diabetic patients.", "keywords": ["uphill battle task", "machine learning", "disease", "system", "timely treatment", "proposed system", "uphill battle", "battle task", "diabetes disease", "human diseases precisely", "proposed", "diabetes", "machine learning models", "learning", "machine", "recommendation system", "recommendation", "healthcare recommendation system", "deep machine learning", "learning models"], "paper_title": "A Smart Healthcare Recommendation System for Multidisciplinary Diabetes Patients with Data Fusion Based on Deep Ensemble Learning.", "last_updated": "2023/02/04"}, {"id": "0033389425", "domain": "Diabetic retinopathy", "model_name": "Ming et al.", "publication_date": "2021/01/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33389425/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33389425", "task": null, "abstract": "To evaluate the performance of an AI-based diabetic retinopathy (DR) grading model in real-world community clinical setting. Participants with diabetes on record in the chosen community were recruited by health care staffs in a primary clinic of Zhengzhou city, China. Retinal images were prospectively collected during December 2018 and April 2019 based on intent-to-screen principle. A pre-validated AI system based on deep learning algorithm was deployed to screen DR graded according to the International Clinical Diabetic Retinopathy scale. Kappa value of DR severity, the sensitivity, specificity of detecting referable DR (RDR) and any DR were generated based on the standard of the majority manual grading decision of a retina specialist panel. Of the 193 eligible participants, 173 (89.6%) were readable with at least one eye image. Mean [SD] age was 69.3 (9.0) years old. Total of 321 eyes (83.2%) were graded both by AI and the specialist panel. The \u03ba value in eye image grading was 0.715. The sensitivity, specificity and area under curve for detection of RDR were 84.6% (95% CI: 54.6-\u200998.1%), 98.0% (95% CI: 94.3-99.6%) and 0.913 (95% CI: 0.797-1.000), respectively. For detection of any DR, the upper indicators were 90.0% (95% CI: 68.3-98.8), 96.6% (95% CI: 92.1-98.9) and 0.933 (95% CI: 0.933-1.000), respectively. The AI system showed relatively good consistency with ophthalmologist diagnosis in DR grading, high specificity and acceptable sensitivity for identifying RDR and any DR. It is feasible to apply AI-based DR screening in community. Deployed in community real-world clinic setting, AI-based DR screening system showed high specificity and acceptable sensitivity in identifying RDR and any DR. Good DR diagnostic consistency was found between AI and manual grading. These prospective evidences were essential for regulatory approval.", "keywords": ["Clinical Diabetic Retinopathy", "International Clinical Diabetic", "diabetic retinopathy", "evaluate the performance", "AI-based diabetic retinopathy", "Diabetic Retinopathy scale", "RDR", "Clinical Diabetic", "Zhengzhou city", "grading", "International Clinical", "specificity", "grading model", "identifying RDR", "community clinical setting", "community", "sensitivity", "diabetic", "retinopathy", "based"], "paper_title": "Evaluation of a novel artificial intelligence-based screening system for diabetic retinopathy in community of China: a real-world study.", "last_updated": "2023/02/04"}, {"id": "0036256665", "domain": "Diabetic retinopathy", "model_name": "dr_pretraining", "publication_date": "2022/10/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36256665/", "code_link": "https://github.com/vigneshsrinivasan10/dr_pretraining", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "36256665", "task": null, "abstract": "There is an increasing number of medical use cases where classification algorithms based on deep neural networks reach performance levels that are competitive with human medical experts. To alleviate the challenges of small dataset sizes, these systems often rely on pretraining. In this work, we aim to assess the broader implications of these approaches in order to better understand what type of pretraining works reliably (with respect to performance, robustness, learned representation etc.) in practice and what type of pretraining dataset is best suited to achieve good performance in small target dataset size scenarios. Considering diabetic retinopathy grading as an exemplary use case, we compare the impact of different training procedures including recently established self-supervised pretraining methods based on contrastive learning. To this end, we investigate different aspects such as quantitative performance, statistics of the learned feature representations, interpretability and robustness to image distortions. Our results indicate that models initialized from ImageNet pretraining report a significant increase in performance, generalization and robustness to image distortions. In particular, self-supervised models show further benefits to supervised models. Self-supervised models with initialization from ImageNet pretraining not only report higher performance, they also reduce overfitting to large lesions along with improvements in taking into account minute lesions indicative of the progression of the disease. Understanding the effects of pretraining in a broader sense that goes beyond simple performance comparisons is of crucial importance for the broader medical imaging community beyond the use case considered in this work.", "keywords": ["deep neural networks", "neural networks reach", "classification algorithms based", "human medical experts", "networks reach performance", "reach performance levels", "increasing number", "classification algorithms", "deep neural", "neural networks", "networks reach", "competitive with human", "pretraining", "performance", "algorithms based", "based on deep", "small dataset sizes", "medical experts", "reach performance", "performance levels"], "paper_title": "To pretrain or not? A systematic analysis of the benefits of pretraining in diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0028035657", "domain": "Diabetic retinopathy", "model_name": "ElTanboly et al.", "publication_date": "2017/03/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28035657/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28035657", "task": "IWqQC1koJA", "abstract": "Detection (diagnosis) of diabetic retinopathy (DR) in optical coherence tomography (OCT) images for patients with type 2 diabetes, but almost clinically normal retina appearances. The proposed computer-aided diagnostic (CAD) system detects the DR in three steps: (a) localizing and segmenting 12 distinct retinal layers on the OCT image; (b) deriving features of the segmented layers, and (c) learning most discriminative features and classifying each subject as normal or diabetic. To localise and segment the retinal layers, signals (intensities) of the OCT image are described with a joint Markov-Gibbs random field (MGRF) model of intensities and shape descriptors. Each segmented layer is characterized with cumulative probability distribution functions (CDF) of its locally extracted features, such as reflectivity, curvature, and thickness. A multistage deep fusion classification network (DFCN) with a stack of non-negativity-constrained autoencoders (NCAE) is trained to select the most discriminative retinal layers' features and use their CDFs for detecting the DR. A training atlas was built using the OCT scans for 12 normal subjects and their maps of layers hand-drawn by retina experts. Preliminary experiments on 52 clinical OCT scans (26 normal and 26 with early-stage DR, balanced between 40-79\u00a0yr old males and females; 40 training and 12 test subjects) gave the DR detection accuracy, sensitivity, and specificity of 92%; 83%, and 100%, respectively. The 100% accuracy, sensitivity, and specificity have been obtained in the leave-one-out cross-validation test for all the 52 subjects. Both the quantitative and visual assessments confirmed the high accuracy of the proposed computer-assisted diagnostic system for early DR detection using the OCT retinal images.", "keywords": ["optical coherence tomography", "OCT retinal images", "OCT image", "OCT", "coherence tomography", "patients with type", "optical coherence", "normal retina appearances", "clinically normal retina", "diabetic retinopathy", "OCT scans", "OCT retinal", "retinal layers", "distinct retinal layers", "clinically normal", "retina appearances", "layers", "clinical OCT scans", "retinal", "features"], "paper_title": "A computer-aided diagnostic system for detecting diabetic retinopathy in optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0035395214", "domain": "Diabetic retinopathy", "model_name": "Kalur et al.", "publication_date": "2022/04/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35395214/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35395214", "task": null, "abstract": "This study aims to determine the effect of intraretinal (IRF) and subretinal (SRF) fluid on visual outcomes for diabetic macular edema (DME) patients treated with anti-vascular endothelial growth factor (anti-VEGF) in routine clinical practice. Optical coherence tomography scans were analyzed with a deep-learning artificial intelligence software to quantify IRF, SRF, and total retinal fluid (TRF) at baseline and at 3, 6, and 12 months. Predictive variables for best-corrected visual acuity (BCVA) were evaluated with linear mixed-effects regression models. A total of 220 DME eyes of 220 patients from the Cole Eye Institute at Cleveland Clinic. Retrospective, nonrandomized cohort study. BCVA improved from baseline to 12 months (63.36 \u00b1 14.72 to 68.49 \u00b1 13.14 Early Treatment Diabetic Retinopathy Study letters, p < 0.001, respectively). Central subfield thickness improved from baseline to 12 months (411.74 \u00b1 129.7 to 335.94 \u00b1 116.91 mm, p < 0.001, respectively). Injection frequency per patient was 8.25 \u00b1 2.5 injections over 12 months. The linear mixed-effects regression model in the foveal region for TRF, IRF, and SRF volume at the fourth quartile showed BCVA losses of -8.29 letters (range, -10.96 to -5.62 letters, p < 0.001), -7.52 letters (range, -10.3 to -4.74 letters, p < .001), and -6.93 letters (range, -10.54 to -3.41 letters, p < .001), respectively. The highest quartile of TRF, IRF, and SRF volumes led to worse visual outcomes after 12 months of anti-VEGF treatment in patients with DME. Further studies designed to investigate the effect of anti-VEGF treatment on retinal fluid morphology could provide greater insight into individualized DME treatment.", "keywords": ["endothelial growth factor", "routine clinical practice", "anti-vascular endothelial growth", "diabetic macular edema", "macular edema", "growth factor", "clinical practice", "aims to determine", "treated with anti-vascular", "anti-vascular endothelial", "endothelial growth", "routine clinical", "IRF", "SRF", "Cole Eye Institute", "letters", "DME", "months", "diabetic macular", "Diabetic Retinopathy Study"], "paper_title": "Impact of retinal fluid in patients with diabetic macular edema treated with anti-VEGF in routine clinical practice.", "last_updated": "2023/02/04"}, {"id": "0034891706", "domain": "Diabetic retinopathy", "model_name": "Qian et al.", "publication_date": "2021/12/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34891706/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34891706", "task": null, "abstract": "Diabetic retinopathy (DR) is one of the most common eye conditions among diabetic patients. However, vision loss occurs primarily in the late stages of DR, and the symptoms of visual impairment, ranging from mild to severe, can vary greatly, adding to the burden of diagnosis and treatment in clinical practice. Deep learning methods based on retinal images have achieved remarkable success in automatic DR grading, but most of them neglect that the presence of diabetes usually affects both eyes, and ophthalmologists usually compare both eyes concurrently for DR diagnosis, leaving correlations between left and right eyes unexploited. In this study, simulating the diagnostic process, we propose a two-stream binocular network to capture the subtle correlations between left and right eyes, in which, paired images of eyes are fed into two identical subnetworks separately during training. We design a contrastive grading loss to learn binocular correlation for five-class DR detection, which maximizes inter-class dissimilarity while minimizing the intra-class difference. Experimental results on the EyePACS dataset show the superiority of the proposed binocular model, outperforming monocular methods by a large margin.Clinical relevance- Compared to conventional DR grading methods based on monocular images, our approach can provide more accurate predictions and extract graphical patterns from retinal images of both eyes for clinical reference.", "keywords": ["common eye conditions", "Diabetic retinopathy", "diabetic patients", "conditions among diabetic", "eyes", "common eye", "eye conditions", "Diabetic", "images", "retinal images", "methods based", "vision loss occurs", "grading methods based", "loss occurs primarily", "correlations between left", "grading", "methods", "retinopathy", "patients", "learning methods based"], "paper_title": "Two Eyes Are Better Than One: Exploiting Binocular Correlation for Diabetic Retinopathy Severity Grading.", "last_updated": "2023/02/04"}, {"id": "0033264095", "domain": "Diabetic retinopathy", "model_name": "Hua et al.", "publication_date": "2021/07/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33264095/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33264095", "task": null, "abstract": "With the scenario of limited labeled dataset, this paper introduces a deep learning-based approach that leverages Diabetic Retinopathy (DR) severity recognition performance using fundus images combined with wide-field swept-source optical coherence tomography angiography (SS-OCTA). The proposed architecture comprises a backbone convolutional network associated with a Twofold Feature Augmentation mechanism, namely TFA-Net. The former includes multiple convolution blocks extracting representational features at various scales. The latter is constructed in a two-stage manner, i.e., the utilization of weight-sharing convolution kernels and the deployment of a Reverse Cross-Attention (RCA) stream. The proposed model achieves a Quadratic Weighted Kappa rate of 90.2% on the small-sized internal KHUMC dataset. The robustness of the RCA stream is also evaluated by the single-modal Messidor dataset, of which the obtained mean Accuracy (94.8%) and Area Under Receiver Operating Characteristic (99.4%) outperform those of the state-of-the-arts significantly. Utilizing a network strongly regularized at feature space to learn the amalgamation of different modalities is of proven effectiveness. Thanks to the widespread availability of multi-modal retinal imaging for each diabetes patient nowadays, such approach can reduce the heavy reliance on large quantity of labeled visual data. Our TFA-Net is able to coordinate hybrid information of fundus photos and wide-field SS-OCTA for exhaustively exploiting DR-oriented biomarkers. Moreover, the embedded feature-wise augmentation scheme can enrich generalization ability efficiently despite learning from small-scale labeled data.", "keywords": ["leverages Diabetic Retinopathy", "severity recognition performance", "coherence tomography angiography", "swept-source optical coherence", "optical coherence tomography", "Diabetic Retinopathy", "leverages Diabetic", "deep learning-based approach", "fundus images combined", "wide-field swept-source optical", "Twofold Feature Augmentation", "severity recognition", "tomography angiography", "scenario of limited", "paper introduces", "introduces a deep", "deep learning-based", "recognition performance", "images combined", "swept-source optical"], "paper_title": "Convolutional Network With Twofold Feature Augmentation for Diabetic Retinopathy Recognition From Multi-Modal Images.", "last_updated": "2023/02/04"}, {"id": "0031259200", "domain": "Diabetic retinopathy", "model_name": "Kou et al.", "publication_date": "2019/06/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31259200/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31259200", "task": "aipbNdPTIt", "abstract": "Microaneurysms (MAs) play an important role in the diagnosis of clinical diabetic retinopathy at the early stage. Annotation of MAs manually by experts is laborious and so it is essential to develop automatic segmentation methods. Automatic MA segmentation remains a challenging task mainly due to the low local contrast of the image and the small size of MAs. A deep learning-based method called U-Net has become one of the most popular methods for the medical image segmentation task. We propose an architecture for U-Net, named deep recurrent U-Net (DRU-Net), obtained by combining the deep residual model and recurrent convolutional operations into U-Net. In the MA segmentation task, DRU-Net can accumulate effective features much better than the typical U-Net. The proposed method is evaluated on two publicly available datasets: E-Ophtha and IDRiD. Our results show that the proposed DRU-Net achieves the best performance with 0.9999 accuracy value and 0.9943 area under curve (AUC) value on the E-Ophtha dataset. And on the IDRiD dataset, it has achieved 0.987 AUC value (to our knowledge, this is the first result of segmenting MAs on this dataset). Compared with other methods, such as U-Net, FCNN, and ResU-Net, our architecture (DRU-Net) achieves state-of-the-art performance.", "keywords": ["clinical diabetic retinopathy", "play an important", "early stage", "important role", "diagnosis of clinical", "clinical diabetic", "diabetic retinopathy", "U-Net", "MAs", "segmentation task", "segmentation", "automatic segmentation methods", "develop automatic segmentation", "image segmentation task", "DRU-Net", "task", "dataset", "methods", "automatic segmentation", "MAs manually"], "paper_title": "Microaneurysms segmentation with a U-Net based on recurrent residual convolutional neural network.", "last_updated": "2023/02/04"}, {"id": "0032028213", "domain": "Diabetic retinopathy", "model_name": "Quellec et al.", "publication_date": "2020/01/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32028213/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32028213", "task": "IWqQC1koJA", "abstract": "In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.", "keywords": ["diabetic retinopathy", "collected in diabetic", "learning", "large datasets", "conditions", "screening", "automatic detectors", "few-shot learning", "datasets of fundus", "fundus photographs", "deep learning", "train automatic detectors", "rare", "few-shot", "datasets", "rare conditions", "AUC", "networks", "automatic detectors ignore", "decades"], "paper_title": "Automatic detection of rare pathologies in fundus photographs using few-shot learning.", "last_updated": "2023/02/04"}, {"id": "0034804140", "domain": "Diabetic retinopathy", "model_name": "RF-GANs", "publication_date": "2021/11/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34804140/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34804140", "task": null, "abstract": "Diabetic retinopathy (DR) is a diabetic complication affecting the eyes, which is the main cause of blindness in young and middle-aged people. In order to speed up the diagnosis of DR, a mass of deep learning methods have been used for the detection of this disease, but they failed to attain excellent results due to unbalanced training data, i.e., the lack of DR fundus images. To address the problem of data imbalance, this paper proposes a method dubbed retinal fundus images generative adversarial networks (RF-GANs), which is based on generative adversarial network, to synthesize retinal fundus images. RF-GANs is composed of two generation models, RF-GAN1 and RF-GAN2. Firstly, RF-GAN1 is employed to translate retinal fundus images from source domain (the domain of semantic segmentation datasets) to target domain (the domain of EyePACS dataset connected to Kaggle (EyePACS)). Then, we train the semantic segmentation models with the translated images, and employ the trained models to extract the structural and lesion masks (hereafter, we refer to it as Masks) of EyePACS. Finally, we employ RF-GAN2 to synthesize retinal fundus images using the Masks and DR grading labels. This paper verifies the effectiveness of the method: RF-GAN1 can narrow down the domain gap between different datasets to improve the performance of the segmentation models. RF-GAN2 can synthesize realistic retinal fundus images. Adopting the synthesized images for data augmentation, the accuracy and quadratic weighted kappa of the state-of-the-art DR grading model on the testing set of EyePACS increase by 1.53% and 1.70%, respectively.", "keywords": ["diabetic complication affecting", "retinal fundus images", "fundus images", "retinal fundus", "synthesize retinal fundus", "Diabetic retinopathy", "diabetic complication", "affecting the eyes", "middle-aged people", "images", "complication affecting", "blindness in young", "young and middle-aged", "fundus", "fundus images generative", "dubbed retinal fundus", "retinal", "Diabetic", "domain", "translate retinal fundus"], "paper_title": "RF-GANs: A Method to Synthesize Retinal Fundus Images Based on Generative Adversarial Network.", "last_updated": "2023/02/04"}, {"id": "0035336513", "domain": "Diabetic retinopathy", "model_name": "OCTA", "publication_date": "2022/03/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35336513/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35336513", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) refers to the ophthalmological complications of diabetes mellitus. It is primarily a disease of the retinal vasculature that can lead to vision loss. Optical coherence tomography angiography (OCTA) demonstrates the ability to detect the changes in the retinal vascular system, which can help in the early detection of DR. In this paper, we describe a novel framework that can detect DR from OCTA based on capturing the appearance and morphological markers of the retinal vascular system. This new framework consists of the following main steps: (1) extracting retinal vascular system from OCTA images based on using joint Markov-Gibbs Random Field (MGRF) model to model the appearance of OCTA images and (2) estimating the distance map inside the extracted vascular system to be used as imaging markers that describe the morphology of the retinal vascular (RV) system. The OCTA images, extracted vascular system, and the RV-estimated distance map is then composed into a three-dimensional matrix to be used as an input to a convolutional neural network (CNN). The main motivation for using this data representation is that it combines the low-level data as well as high-level processed data to allow the CNN to capture significant features to increase its ability to distinguish DR from the normal retina. This has been applied on multi-scale levels to include the original full dimension images as well as sub-images extracted from the original OCTA images. The proposed approach was tested on in-vivo data using about 91 patients, which were qualitatively graded by retinal experts. In addition, it was quantitatively validated using datasets based on three metrics: sensitivity, specificity, and overall accuracy. Results showed the capability of the proposed approach, outperforming the current deep learning as well as features-based detecting DR approaches.", "keywords": ["retinal vascular system", "vascular system", "retinal vascular", "OCTA images", "extracted vascular system", "OCTA images based", "OCTA", "Diabetic retinopathy", "diabetes mellitus", "ophthalmological complications", "complications of diabetes", "vascular", "retinal", "original OCTA images", "extracting retinal vascular", "system", "extracted vascular", "OCTA based", "images", "original OCTA"], "paper_title": "Automated Diagnosis of Optical Coherence Tomography Angiography (OCTA) Based on Machine Learning Techniques.", "last_updated": "2023/02/04"}, {"id": "0031426516", "domain": "Diabetic retinopathy", "model_name": "CNNs", "publication_date": "2019/08/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31426516/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31426516", "task": null, "abstract": "The fast progress in research and development of multifunctional, distributed sensor networks has brought challenges in processing data from a large number of sensors. Using deep learning methods such as convolutional neural networks (CNN), it is possible to build smarter systems to forecasting future situations as well as precisely classify large amounts of data from sensors. Multi-sensor data from atmospheric pollutants measurements that involves five criteria, with the underlying analytic model unknown, need to be categorized, so do the Diabetic Retinopathy (DR) fundus images dataset. In this work, we created automatic classifiers based on a deep convolutional neural network (CNN) with two models, a simpler feedforward model with dual modules and an Inception Resnet v2 model, and various structural tweaks for classifying the data from the two tasks. For segregating multi-sensor data, we trained a deep CNN-based classifier on an image dataset extracted from the data by a novel image generating method. We created two deepened and one reductive feedforward network for DR phase classification. The validation accuracies and visualization results show that increasing deep CNN structure depth or kernels number in convolutional layers will not indefinitely improve the classification quality and that a more sophisticated model does not necessarily achieve higher performance when training datasets are quantitatively limited, while increasing training image resolution can induce higher classification accuracies for trained CNNs. The methodology aims at providing support for devising classification networks powering intelligent sensors.", "keywords": ["development of multifunctional", "distributed sensor networks", "fast progress", "progress in research", "research and development", "brought challenges", "challenges in processing", "data", "processing data", "distributed sensor", "Diabetic Retinopathy", "Multi-sensor data", "CNN", "Inception Resnet", "classify large amounts", "precisely classify large", "convolutional neural", "deep", "sensors", "model"], "paper_title": "Study of the Application of Deep Convolutional Neural Networks (CNNs) in Processing Sensor Data and Biomedical Images.", "last_updated": "2023/02/04"}, {"id": "0032870389", "domain": "Glaucoma (unspecified)", "model_name": "Maji et al.", "publication_date": "2020/09/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32870389/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32870389", "task": "IWqQC1koJA", "abstract": "Automatic grading of retinal blood vessels from fundus image can be a useful tool for diagnosis, planning and treatment of eye. Automatic diagnosis of retinal images for early detection of glaucoma, stroke, and blindness is emerging in intelligent health care system. The method primarily depends on various abnormal signs, such as area of hard exudates, area of blood vessels, bifurcation points, texture, and entropies. The development of an automated screening system based on vessel width, tortuosity, and vessel branching are also used for grading. However, the automated method that directly can come to a decision by taking the fundus images got less attention. Detecting eye problems based on the tortuosity of the vessel from fundus images is a complicated task for opthalmologists. So automated grading algorithm using deep learning can be most valuable for grading retinal health. The aim of this work is to develop an automatic computer aided diagnosis system to solve the problem. This work approaches to achieve an automatic grading method that is opted using Convolutional Neural Network (CNN) model. In this work we have studied the state-of-the-art machine learning algorithms and proposed an attention network which can grade retinal images. The proposed method is validated on a public dataset EIARG1, which is only publicly available dataset for such task as per our knowledge.", "keywords": ["planning and treatment", "blood vessels", "grading", "Automatic", "images", "retinal blood vessels", "retinal", "fundus images", "diagnosis", "retinal images", "method", "Automatic grading", "vessel", "fundus", "Convolutional Neural Network", "system", "automated", "work", "blood", "planning"], "paper_title": "Automatic Grading of Retinal Blood Vessel in Deep Retinal Image Diagnosis.", "last_updated": "2023/02/04"}, {"id": "0036931202", "domain": "Diabetic retinopathy", "model_name": "Liu et al.", "publication_date": "2023/03/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36931202/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36931202", "task": null, "abstract": "Diabetic retinopathy(DR) is a common early diabetic complication and one of the main causes of blindness. In clinical diagnosis and treatment, regular screening with fundus imaging is an effective way to prevent the development of DR. However, the regular fundus images used in most DR screening work have a small imaging range, narrow field of vision, and can not contain more complete lesion information, which leads to less ideal automatic DR grading results. In order to improve the accuracy of DR grading, we establish a dataset containing 101 ultra-wide-field(UWF) DR fundus images and propose a deep learning(DL) automatic classification method based on a new preprocessing method. The emerging UWF fundus images have the advantages of a large imaging range and wide field of vision and contain more information about the lesions. In data preprocessing, we design a data denoising method for UWF images and use data enhancement methods to improve their contrast and brightness to improve the classification effect. In order to verify the efficiency of our dataset and the effectiveness of our preprocessing method, we design a series of experiments including a variety of DL classification models. The experimental results show that we can achieve high classification accuracy by using only the backbone model. The most basic ResNet50 model reaches an average of classification accuracy(ACA) 0.66, Macro F1 0.6559, and Kappa 0.58. The best-performing Swin-S model reaches ACA 0.72, Macro F1 0.7018, and Kappa 0.65. DR grading using UWF images can achieve higher accuracy and efficiency, which has practical significance and value in clinical applications.", "keywords": ["common early diabetic", "early diabetic complication", "Diabetic retinopathy", "early diabetic", "diabetic complication", "common early", "UWF fundus images", "fundus images", "UWF images", "UWF", "images", "Diabetic", "regular fundus images", "fundus", "classification", "emerging UWF fundus", "UWF fundus", "imaging range", "method", "accuracy"], "paper_title": "A new ultra-wide-field fundus dataset to diabetic retinopathy grading using hybrid preprocessing methods.", "last_updated": "2023/02/04"}, {"id": "0034266030", "domain": "Diabetic retinopathy", "model_name": "MMIB-Net", "publication_date": "2021/12/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34266030/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34266030", "task": "IWqQC1koJA", "abstract": "Multicolor (MC) imaging is an imaging modality that records confocal scanning laser ophthalmoscope (cSLO) fundus images, which can be used for the diabetic retinopathy (DR) detection. By utilizing this imaging technique, multiple modal images can be obtained in a single case. Additional symptomatic features can be obtained if these images are considered during the diagnosis of DR. However, few studies have been carried out to classify MC Images using deep learning methods, let alone using multi modal features for analysis. In this work, we propose a novel model which uses the multimodal information bottleneck network (MMIB-Net) to classify the MC Images for the detection of DR. Our model can extract the features of multiple modalities simultaneously while finding concise feature representations of each modality using the information bottleneck theory. MC Images classification can be achieved by picking up the combined representations and features of all modalities. In our experiments, it is shown that the proposed method can achieve an accurate classification of MC Images. Comparative experiments also demonstrate that the use of multimodality and information bottleneck improves the performance of MC Images classification. To the best of our knowledge, this is the first report of DR identification utilizing the multimodal information bottleneck convolutional neural network in MC Images.", "keywords": ["scanning laser ophthalmoscope", "records confocal scanning", "confocal scanning laser", "images", "laser ophthalmoscope", "diabetic retinopathy", "fundus images", "records confocal", "confocal scanning", "scanning laser", "information bottleneck", "Images classification", "multiple modal images", "imaging", "multimodal information bottleneck", "information", "bottleneck", "features", "modal images", "imaging technique"], "paper_title": "Multicolor image classification using the multimodal information bottleneck network (MMIB-Net) for detecting diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0036766498", "domain": "Diabetic retinopathy", "model_name": "Bajwa et al.", "publication_date": "2023/01/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36766498/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36766498", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) is the most common complication that arises due to diabetes, and it affects the retina. It is the leading cause of blindness globally, and early detection can protect patients from losing sight. However, the early detection of Diabetic Retinopathy is an difficult task that needs clinical experts' interpretation of fundus images. In this study, a deep learning model was trained and validated on a private dataset and tested in real time at the Sindh Institute of Ophthalmology & Visual Sciences (SIOVS). The intelligent model evaluated the quality of the test images. The implemented model classified the test images into DR-Positive and DR-Negative ones. Furthermore, the results were reviewed by clinical experts to assess the model's performance. A total number of 398 patients, including 232 male and 166 female patients, were screened for five weeks. The model achieves 93.72% accuracy, 97.30% sensitivity, and 92.90% specificity on the test data as labelled by clinical experts on Diabetic Retinopathy.", "keywords": ["Diabetic Retinopathy", "due to diabetes", "affects the retina", "common complication", "complication that arises", "arises due", "Retinopathy", "early detection", "Diabetic", "model", "test images", "Visual Sciences", "Institute of Ophthalmology", "Sindh Institute", "test", "clinical experts", "images", "clinical", "patients", "diabetes"], "paper_title": "A Prospective Study on Diabetic Retinopathy Detection Based on Modify Convolutional Neural Network Using Fundus Images at Sindh Institute of Ophthalmology & Visual Sciences.", "last_updated": "2023/02/04"}, {"id": "0032879756", "domain": "Diabetic retinopathy", "model_name": "Wang et al.", "publication_date": "2020/08/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32879756/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32879756", "task": null, "abstract": "This study aimed to develop an automated system with artificial intelligence algorithms to comprehensively identify pathologic retinal cases and make urgent referrals. To build and test the intelligent system, this study obtained 28,664 optical coherence tomography (OCT) images from 2254 patients in the Eye and ENT Hospital of Fudan University (EENT Hospital) and Shanghai Tenth People's Hospital (TENTH Hospital). We applied a deep learning model with an adapted feature pyramid network to detect 15 categories of retinal pathologies from OCT images as common signs of various retinal diseases. Subsequently, the pathologies detected in the OCT images and thickness features extracted from retinal thickness measurements were combined for urgent referral using the random forest tool. The retinal pathologies detection model had a sensitivity of 96.39% and specificity of 98.91% from the EENT Hospital test dataset, whereas those from the TENTH Hospital test dataset were 94.89% and 98.76%, respectively. The urgent referral model achieved accuracies of 98.12% and 98.01% from the EENT Hospital and TENTH Hospital test datasets, respectively. An intelligent system capable of automatically identifying pathologic retinal cases and offering urgent referrals was developed and demonstrated reliable performance with high sensitivity, specificity, and accuracy. This intelligent system has great value and practicability in communities where exist increasing cases of retinal disease and a lack of ophthalmologists.", "keywords": ["artificial intelligence algorithms", "TENTH Hospital test", "Tenth People Hospital", "Shanghai Tenth People", "EENT Hospital", "TENTH Hospital", "EENT Hospital test", "Hospital test dataset", "Hospital", "comprehensively identify pathologic", "Hospital test", "OCT images", "make urgent referrals", "People Hospital", "Fudan University", "aimed to develop", "develop an automated", "artificial intelligence", "intelligence algorithms", "algorithms to comprehensively"], "paper_title": "An Intelligent Optical Coherence Tomography-based System for Pathological Retinal Cases Identification and Urgent Referrals.", "last_updated": "2023/02/04"}, {"id": "0030952891", "domain": "Diabetic retinopathy", "model_name": "keras", "publication_date": "2019/04/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30952891/", "code_link": "https://github.com/fchollet/keras", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "30952891", "task": null, "abstract": "Despite advances in artificial intelligence (AI), its application in medical imaging has been burdened and limited by expert-generated labels. We used images from optical coherence tomography angiography (OCTA), a relatively new imaging modality that measures retinal blood flow, to train an AI algorithm to generate flow maps from standard optical coherence tomography (OCT) images, exceeding the ability and bypassing the need for expert labeling. Deep learning was able to infer flow from single structural OCT images with similar fidelity to OCTA and significantly better than expert clinicians (P\u2009<\u20090.00001). Our model allows generating flow maps from large volumes of previously collected OCT data in existing clinical trials and clinical practice. This finding demonstrates a novel application of AI to medical imaging, whereby subtle regularities between different modalities are used to image the same body part and AI is used to generate detailed inferences of tissue function from structure imaging.", "keywords": ["optical coherence tomography", "artificial intelligence", "expert-generated labels", "advances in artificial", "burdened and limited", "limited by expert-generated", "coherence tomography angiography", "optical coherence", "coherence tomography", "standard optical coherence", "medical imaging", "imaging", "OCT", "flow maps", "structural OCT images", "flow", "generate flow maps", "OCTA", "single structural OCT", "retinal blood flow"], "paper_title": "Generating retinal flow maps from structural optical coherence tomography with artificial intelligence.", "last_updated": "2023/02/04"}, {"id": "0029201938", "domain": "Diabetic retinopathy", "model_name": "Ord\u00f3\u00f1ez et al.", "publication_date": "2017/11/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29201938/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "29201938", "task": "IWqQC1koJA", "abstract": "Convolutional neural networks (CNNs), the state of the art in image classification, have proven to be as effective as an ophthalmologist, when detecting referable diabetic retinopathy. Having a size of [Formula: see text] of the total image, microaneurysms are early lesions in diabetic retinopathy that are difficult to classify. A model that includes two CNNs with different input image sizes, [Formula: see text] and [Formula: see text], was developed. These models were trained using the Kaggle and Messidor datasets and tested independently against the Kaggle dataset, showing a sensitivity [Formula: see text], a specificity [Formula: see text], and an area under the receiver operating characteristics curve [Formula: see text]. Furthermore, by combining these trained models, there was a reduction of false positives for complete images by about 50% and a sensitivity of 96% when tested against the DiaRetDB1 dataset. In addition, a powerful image preprocessing procedure was implemented, improving not only images for annotations, but also decreasing the number of epochs during training. Finally, a feedback method was developed increasing the accuracy of the CNN [Formula: see text] input model.", "keywords": ["Convolutional neural networks", "Formula", "detecting referable diabetic", "referable diabetic retinopathy", "text", "Convolutional neural", "neural networks", "diabetic retinopathy", "detecting referable", "referable diabetic", "image classification", "image", "diabetic", "retinopathy", "Kaggle", "CNNs", "Convolutional", "networks", "classification", "ophthalmologist"], "paper_title": "Classification of images based on small local features: a case applied to microaneurysms in fundus retina images.", "last_updated": "2023/02/04"}, {"id": "0036874499", "domain": "Diabetic retinopathy", "model_name": "Hou et al.", "publication_date": "2023/01/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36874499/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36874499", "task": null, "abstract": "Retina fundus imaging for diagnosing diabetic retinopathy (DR) is an efficient and patient-friendly modality, where many high-resolution images can be easily obtained for accurate diagnosis. With the advancements of deep learning, data-driven models may facilitate the process of high-throughput diagnosis especially in areas with less availability of certified human experts. Many datasets of DR already exist for training learning-based models. However, most are often unbalanced, do not have a large enough sample count, or both. This paper proposes a two-stage pipeline for generating photo-realistic retinal fundus images based on either artificially generated or free-hand drawn semantic lesion maps. The first stage uses a conditional StyleGAN to generate synthetic lesion maps based on a DR severity grade. The second stage then uses GauGAN to convert the synthetic lesion maps into high resolution fundus images. We evaluate the photo-realism of generated images using the Fr\u00e9chet inception distance (FID), and show the efficacy of our pipeline through downstream tasks, such as; dataset augmentation for automatic DR grading and lesion segmentation.", "keywords": ["diagnosing diabetic retinopathy", "Retina fundus imaging", "diabetic retinopathy", "patient-friendly modality", "imaging for diagnosing", "diagnosing diabetic", "efficient and patient-friendly", "easily obtained", "obtained for accurate", "accurate diagnosis", "Retina fundus", "lesion maps", "synthetic lesion maps", "fundus imaging", "high-resolution images", "lesion", "certified human experts", "images", "high-throughput diagnosis", "diagnosis"], "paper_title": "High-fidelity diabetic retina fundus image synthesis from freestyle lesion maps.", "last_updated": "2023/02/04"}, {"id": "0036633529", "domain": "Diabetic retinopathy", "model_name": "CVI", "publication_date": "2023/01/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36633529/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36633529", "task": null, "abstract": "A deep learning model called choroidal vascularity index (CVI)-Net is proposed to automatically segment the choroid layer and its vessels in overall optical coherence tomography (OCT) scans. Clinical parameters are then automatically quantified to determine structural and vascular changes in the choroid with the progression of diabetic retinopathy (DR) severity. The study includes 65 eyes consisting of 34 with proliferative DR (PDR), 17 with nonproliferative DR (NPDR), and 14 healthy controls from two OCT systems. On a dataset of 396 OCT B-scan images with manually annotated ground truths, overall Dice coefficients of 96.6\u2009\u00b1\u20091.5 and 89.1\u2009\u00b1\u20093.1 are obtained by CVI-Net for the choroid layer and vessel segmentation, respectively. The mean CVI values among the normal, NPDR, and PDR groups are consistent with reported outcomes. Statistical results indicate that CVI shows a significant negative correlation with DR severity level, and this correlation is independent of changes in other physiological parameters.", "keywords": ["choroidal vascularity index", "optical coherence tomography", "deep learning model", "learning model called", "model called choroidal", "called choroidal vascularity", "Net is proposed", "vascularity index", "coherence tomography", "deep learning", "learning model", "model called", "called choroidal", "choroidal vascularity", "optical coherence", "choroid layer", "segment the choroid", "automatically segment", "Net", "OCT B-scan images"], "paper_title": "Choroidal vascularity index (CVI)-Net-based automatic assessment of diabetic retinopathy severity using CVI in optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0028268573", "domain": "Diabetic retinopathy", "model_name": "Lahiri et al.", "publication_date": "2017/07/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28268573/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28268573", "task": "aipbNdPTIt", "abstract": "Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.", "keywords": ["computed aided diagnosis", "label-free fundus images", "fundus images entails", "retinal blood vessels", "diabetic retinopathy", "ophthalmic pathologies", "hypertensive disorders", "cardiovascular diseases", "label-free fundus", "entails a pivotal", "pivotal role", "role in computed", "computed aided", "aided diagnosis", "diagnosis of ophthalmic", "disorders and cardiovascular", "blood vessels", "retinal blood", "fundus images", "images entails"], "paper_title": "Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography.", "last_updated": "2023/02/04"}, {"id": "0036531584", "domain": "Glaucoma (unspecified)", "model_name": "ncbi\" aria-label=\"github\">", "publication_date": "2022/08/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36531584/", "code_link": "https://github.com/ncbi\" aria-label=\"github\">", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "36531584", "task": "IWqQC1koJA", "abstract": "Primary open-angle glaucoma (POAG) is one of the leading causes of irreversible blindness in the United States and worldwide. Although deep learning methods have been proposed to diagnose POAG, these methods all used a single image as input. Contrastingly, glaucoma specialists typically compare the follow-up image with the baseline image to diagnose incident glaucoma. To simulate this process, we proposed a Siamese neural network, POAGNet, to detect POAG from optic disc photographs. The POAGNet, an algorithm for glaucoma diagnosis, is developed using optic disc photographs. The POAGNet was trained and evaluated on 2 data sets: (1) 37\u2009339 optic disc photographs from 1636 Ocular Hypertension Treatment Study (OHTS) participants and (2) 3684 optic disc photographs from the Sequential fundus Images for Glaucoma (SIG) data set. Gold standard labels were obtained using reading center grades. We proposed a Siamese network model, POAGNet, to simulate the clinical process of identifying POAG from optic disc photographs. The POAGNet consists of 2 side outputs for deep supervision and uses convolution to measure the similarity between 2 networks. The main outcome measures are the area under the receiver operating characteristic\u00a0curve, accuracy, sensitivity, and specificity. In POAG diagnosis, extensive experiments show that POAGNet performed better than the best state-of-the-art model on the OHTS test set (area under the curve [AUC] 0.9587 versus 0.8750). It also outperformed the baseline models on the SIG test set (AUC 0.7518 versus 0.6434). To assess the transferability of POAGNet, we also validated the impact of cross-data set variability on our model. The model trained on OHTS achieved an AUC of 0.7490 on SIG, comparable to the previous model trained on the same data set. When using the combination of SIG and OHTS for training, our model achieved superior AUC to the single-data model (AUC 0.8165 versus 0.7518). These demonstrate the relative generalizability of POAGNet. By simulating the clinical grading process, POAGNet demonstrated high accuracy in POAG diagnosis. These results highlight the potential of deep learning to assist and enhance clinical POAG diagnosis. The POAGNet is publicly available on https://github.com/bionlplab/poagnet.", "keywords": ["United States", "optic disc photographs", "States and worldwide", "Primary open-angle glaucoma", "disc photographs", "optic disc", "POAG", "POAG diagnosis", "POAGNet", "Primary open-angle", "irreversible blindness", "model", "AUC", "glaucoma", "disc", "photographs", "optic", "United", "States", "Hypertension Treatment Study"], "paper_title": "Primary Open-Angle Glaucoma Diagnosis from Optic Disc Photographs Using a Siamese Network.", "last_updated": "2023/02/04"}, {"id": "0034339778", "domain": "Glaucoma (unspecified)", "model_name": "Xiong et al.", "publication_date": "2021/07/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34339778/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34339778", "task": "IWqQC1koJA", "abstract": "To develop and validate a multimodal artificial intelligence algorithm, FusionNet, using the pattern deviation probability plots from visual field (VF) reports and circular peripapillary OCT scans to detect glaucomatous optic neuropathy (GON). Cross-sectional study. Two thousand four hundred sixty-three pairs of VF and OCT images from 1083 patients. FusionNet based on bimodal input of VF and OCT paired data was developed to detect GON. Visual field data were collected using the Humphrey Field Analyzer (HFA). OCT images were collected from 3 types of devices (DRI-OCT, Cirrus OCT, and Spectralis). Two thousand four hundred sixty-three pairs of VF and OCT images were divided into 4 datasets: 1567 for training (HFA and DRI-OCT), 441 for primary validation (HFA and DRI-OCT), 255 for the internal test (HFA and Cirrus OCT), and 200 for the external test set (HFA and Spectralis). GON was defined as retinal nerve fiber layer thinning with corresponding VF defects. Diagnostic performance of FusionNet compared with that of VFNet (with VF data as input) and OCTNet (with OCT data as input). FusionNet achieved an area under the receiver operating characteristic curve (AUC) of 0.950 (0.931-0.968) and outperformed VFNet (AUC, 0.868 [95% confidence interval (CI), 0.834-0.902]), OCTNet (AUC, 0.809 [95% CI, 0.768-0.850]), and 2 glaucoma specialists (glaucoma specialist 1: AUC, 0.882 [95% CI, 0.847-0.917]; glaucoma specialist 2: AUC, 0.883 [95% CI, 0.849-0.918]) in the primary validation set. In the internal and external test sets, the performances of FusionNet were also superior to VFNet and OCTNet (FusionNet vs VFNet vs OCTNet: internal test set 0.917 vs 0.854 vs 0.811; external test set 0.873 vs 0.772 vs 0.785). No significant difference was found between the 2 glaucoma specialists and FusionNet in the internal and external test sets, except for glaucoma specialist 2 (AUC, 0.858 [95% CI, 0.805-0.912]) in the internal test set. FusionNet, developed using paired VF and OCT data, demonstrated superior performance to both VFNet and OCTNet in detecting GON, suggesting that multimodal machine learning models are valuable in detecting GON.", "keywords": ["peripapillary OCT scans", "circular peripapillary OCT", "artificial intelligence algorithm", "glaucomatous optic neuropathy", "pattern deviation probability", "deviation probability plots", "OCT", "detect glaucomatous optic", "OCT images", "Cirrus OCT", "AUC", "test set", "internal test set", "HFA", "external test set", "peripapillary OCT", "OCT scans", "OCT data", "multimodal artificial intelligence", "GON"], "paper_title": "Multimodal Machine Learning Using Visual Fields and Peripapillary Circular OCT Scans in Detection of Glaucomatous Optic Neuropathy.", "last_updated": "2023/02/04"}, {"id": "0032593978", "domain": "Glaucoma (unspecified)", "model_name": "Hashimoto et al.", "publication_date": "2020/06/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32593978/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32593978", "task": null, "abstract": "To train and validate the prediction performance of the deep learning (DL) model to predict visual field (VF) in central 10\u00b0 from spectral domain optical coherence tomography (SD-OCT). This multicentre, cross-sectional study included paired Humphrey field analyser (HFA) 10-2 VF and SD-OCT measurements from 591 eyes of 347 patients with open-angle glaucoma (OAG) or normal subjects for the training data set. We trained a convolutional neural network (CNN) for predicting VF threshold (TH) sensitivity values from the thickness of the three macular layers: retinal nerve fibre layer, ganglion cell layer+inner plexiform layer and outer segment+retinal pigment epithelium. We implemented pattern-based regularisation on top of CNN to avoid overfitting. Using an external testing data set of 160 eyes of 131 patients with OAG, the prediction performance (absolute error (AE) and R<sup>2</sup> between predicted and actual TH values) was calculated for (1) mean TH in whole VF and (2) each TH of 68 points. For comparison, we trained support vector machine (SVM) and multiple linear regression (MLR). AE of whole VF with CNN was 2.84\u00b12.98 (mean\u00b1SD) dB, significantly smaller than those with SVM (5.65\u00b15.12 dB) and MLR (6.96\u00b15.38 dB) (all, p<0.001). Mean of point-wise mean AE with CNN was 5.47\u00b13.05 dB, significantly smaller than those with SVM (7.96\u00b14.63 dB) and MLR (11.71\u00b14.15 dB) (all, p<0.001). R<sup>2</sup> with CNN was 0.74 for the mean TH of whole VF, and 0.44\u00b10.24 for the overall 68 points. DL model showed considerably accurate prediction of HFA 10-2 VF from SD-OCT.", "keywords": ["optical coherence tomography", "spectral domain optical", "domain optical coherence", "predict visual field", "Humphrey field analyser", "paired Humphrey field", "deep learning", "coherence tomography", "train and validate", "predict visual", "spectral domain", "domain optical", "optical coherence", "CNN", "visual field", "included paired Humphrey", "Humphrey field", "SVM", "MLR", "prediction performance"], "paper_title": "Deep learning model to predict visual field in central 10\u00b0 from optical coherence tomography measurement in glaucoma.", "last_updated": "2023/02/04"}, {"id": "0035091438", "domain": "Glaucoma (unspecified)", "model_name": "Liu et al.", "publication_date": "2022/01/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35091438/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35091438", "task": null, "abstract": "To apply a deep learning model for automatic localisation of the scleral spur (SS) in anterior segment optical coherence tomography (AS-OCT) images and compare the reproducibility of anterior chamber angle (ACA) width between deep learning located SS (DLLSS) and manually plotted SS (MPSS). In this multicentre, cross-sectional study, a test dataset comprising 5166 AS-OCT images from 287 eyes (116 healthy eyes with open angles and 171 eyes with primary angle-closure disease (PACD)) of 287 subjects were recruited from four ophthalmology clinics. Each eye was imaged twice by a swept-source AS-OCT (CASIA2, Tomey, Nagoya, Japan) in the same visit and one eye of each patient was randomly selected for measurements of ACA. The agreement between DLLSS and MPSS was assessed using the Euclidean distance (ED). The angle opening distance (AOD) of 750 \u00b5m (AOD750) and trabecular-iris space area (TISA) of 750 \u00b5m (TISA750) were calculated using the CASIA2 embedded software. The repeatability of ACA width was measured. The mean age was 60.8\u00b112.3 years (range: 30-85 years) for the normal group and 63.4\u00b110.6 years (range: 40-91 years) for the PACD group. The mean difference in ED for SS localisation between DLLSS and MPSS was 66.50\u00b120.54 \u00b5m and 84.78\u00b128.33 \u00b5m for the normal group and the PACD group, respectively. The span of 95% limits of agreement between DLLSS and MPSS was 0.064 mm for AOD750 and 0.034 mm<sup>2</sup> for TISA750. The respective repeatability coefficients of AOD750 and TISA750 were 0.049 mm and 0.026 mm<sup>2</sup> for DLLSS, and 0.058 mm and 0.030 mm<sup>2</sup> for MPSS. DLLSS achieved comparable repeatability compared with MPSS for measurement of ACA.", "keywords": ["deep learning model", "deep learning located", "anterior segment optical", "optical coherence tomography", "segment optical coherence", "deep learning", "anterior chamber angle", "learning model", "learning located", "anterior segment", "anterior chamber", "scleral spur", "coherence tomography", "DLLSS", "MPSS", "model for automatic", "segment optical", "optical coherence", "compare the reproducibility", "manually plotted"], "paper_title": "Reproducibility of deep learning based scleral spur localisation and anterior chamber angle measurements from anterior segment optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0030316669", "domain": "Glaucoma (unspecified)", "model_name": "Asaoka et al.", "publication_date": "2018/10/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30316669/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30316669", "task": "IWqQC1koJA", "abstract": "We sought to construct and evaluate a deep learning (DL) model to diagnose early glaucoma from spectral-domain optical coherence tomography (OCT) images. Artificial intelligence diagnostic tool development, evaluation, and comparison. This multi-institution study included pretraining data of 4316 OCT images (RS3000) from 1371 eyes with open angle glaucoma (OAG) regardless of the stage of glaucoma and 193 normal eyes. Training data included OCT-1000/2000 images from 94 eyes of 94 patients with early OAG (mean deviation >\u00a0-5.0 dB) and 84 eyes of 84 normal subjects. Testing data included OCT-1000/2000 from 114 eyes of 114 patients with early OAG (mean deviation >\u00a0-5.0 dB) and 82 eyes of 82 normal subjects. A DL (convolutional neural network) classifier was trained using a pretraining dataset, followed by a second round of training using an independent training dataset. The DL model input features were the 8\u00a0\u00d7 8 grid macular retinal nerve fiber layer thickness and ganglion cell complex layer thickness from spectral-domain OCT. Diagnostic accuracy was investigated in the testing dataset. For comparison, diagnostic accuracy was also evaluated using the random forests and support vector machine models. The primary outcome measure was the area under the receiver operating characteristic curve (AROC). The AROC with the DL model was 93.7%. The AROC significantly decreased to between 76.6% and 78.8% without the pretraining process. Significantly smaller AROCs were obtained with random forests and support vector machine models (82.0% and 67.4%, respectively). A DL model for glaucoma using spectral-domain OCT offers a substantive increase in diagnostic performance.", "keywords": ["optical coherence tomography", "spectral-domain optical coherence", "deep learning", "coherence tomography", "early OAG", "sought to construct", "construct and evaluate", "evaluate a deep", "optical coherence", "diagnose early glaucoma", "OCT", "OAG", "eyes", "spectral-domain OCT", "normal subjects", "diagnose early", "glaucoma", "model", "normal", "AROC"], "paper_title": "Using Deep Learning and Transfer Learning to Accurately Diagnose Early-Onset Glaucoma From Macular Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0033149191", "domain": "Glaucoma (unspecified)", "model_name": "Seo et al.", "publication_date": "2020/11/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33149191/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33149191", "task": "IWqQC1koJA", "abstract": "We aimed to classify early normal-tension glaucoma (NTG) and glaucoma suspect (GS) using Bruch's membrane opening-minimum rim width (BMO-MRW), peripapillary retinal nerve fiber layer (RNFL), and the color classification of RNFL based on a deep-learning model. Discriminating early-stage glaucoma and GS is challenging and a deep-learning model may be helpful to clinicians. NTG accounts for an average 77% of open-angle glaucoma in Asians. BMO-MRW is a new structural parameter that has advantages in assessing neuroretinal rim tissue more accurately than conventional parameters. A dataset consisted of 229 eyes out of 277 GS and 168 eyes of 285 patients with early NTG. A deep-learning algorithm was developed to discriminate between GS and early NTG using a training set, and its accuracy was validated in the testing dataset using the area under the curve (AUC) of the receiver operating characteristic curve (ROC). The deep neural network model (DNN) achieved highest diagnostic performance, with an AUC of 0.966 (95%confidence interval 0.929-1.000) in classifying either GS or early NTG, while AUCs of 0.927-0.947 were obtained by other machine-learning models. The performance of the DNN model considering all three OCT-based parameters was the highest (AUC 0.966) compared to the combinations of just two parameters. As a single parameter, BMO-MRW (0.959) performed better than RNFL alone (0.914).", "keywords": ["Bruch membrane opening-minimum", "peripapillary retinal nerve", "nerve fiber layer", "retinal nerve fiber", "Bruch membrane", "opening-minimum rim width", "membrane opening-minimum rim", "classify early normal-tension", "peripapillary retinal", "fiber layer", "early NTG", "early normal-tension glaucoma", "aimed to classify", "membrane opening-minimum", "retinal nerve", "nerve fiber", "color classification", "NTG", "glaucoma suspect", "RNFL based"], "paper_title": "Deep learning classification of early normal-tension glaucoma and glaucoma suspects using Bruch's membrane opening-minimum rim width and RNFL.", "last_updated": "2023/02/04"}, {"id": "0035297959", "domain": "Glaucoma (unspecified)", "model_name": "Fan et al.", "publication_date": "2022/04/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35297959/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35297959", "task": "IWqQC1koJA", "abstract": "Automated deep learning (DL) analyses of fundus photographs potentially can reduce the cost and improve the efficiency of reading center assessment of end points in clinical trials. To investigate the diagnostic accuracy of DL algorithms trained on fundus photographs from the Ocular Hypertension Treatment Study (OHTS) to detect primary open-angle glaucoma (POAG). In this diagnostic study, 1636 OHTS participants from 22 sites with a mean (range) follow-up of 10.7 (0-14.3) years. A total of 66\u202f715 photographs from 3272 eyes were used to train and test a ResNet-50 model to detect the OHTS Endpoint Committee POAG determination based on optic disc (287 eyes, 3502 photographs) and/or visual field (198 eyes, 2300 visual fields) changes. Three independent test sets were used to evaluate the generalizability of the model. Areas under the receiver operating characteristic curve (AUROC) and sensitivities at fixed specificities were calculated to compare model performance. Evaluation of false-positive rates was used to determine whether the DL model detected POAG before the OHTS Endpoint Committee POAG determination. A total of 1147 participants were included in the training set (661 [57.6%] female; mean age, 57.2 years; 95% CI, 56.6-57.8), 167 in the validation set (97 [58.1%] female; mean age, 57.1 years; 95% CI, 55.6-58.7), and 322 in the test set (173 [53.7%] female; mean age, 57.2 years; 95% CI, 56.1-58.2). The DL model achieved an AUROC of 0.88 (95% CI, 0.82-0.92) for the OHTS Endpoint Committee determination of optic disc or VF changes. For the OHTS end points based on optic disc changes or visual field changes, AUROCs were 0.91 (95% CI, 0.88-0.94) and 0.86 (95% CI, 0.76-0.93), respectively. False-positive rates (at 90% specificity) were higher in photographs of eyes that later developed POAG by disc or visual field (27.5% [56 of 204]) compared with eyes that did not develop POAG (11.4% [50 of 440]) during follow-up. The diagnostic accuracy of the DL model developed on the optic disc end point applied to 3 independent data sets was lower, with AUROCs ranging from 0.74 (95% CI, 0.70-0.77) to 0.79 (95% CI, 0.78-0.81). The model's high diagnostic accuracy using OHTS photographs suggests that DL has the potential to standardize and automate POAG determination for clinical trials and management. In addition, the higher false-positive rate in early photographs of eyes that later developed POAG suggests that DL models detected POAG in some eyes earlier than the OHTS Endpoint Committee, reflecting the OHTS design that emphasized a high specificity for POAG determination by requiring a clinically significant change from baseline.", "keywords": ["OHTS Endpoint Committee", "Endpoint Committee POAG", "Ocular Hypertension Treatment", "OHTS Endpoint", "Automated deep learning", "Hypertension Treatment Study", "Committee POAG determination", "reading center assessment", "Endpoint Committee", "OHTS", "POAG", "POAG determination", "Committee POAG", "Endpoint Committee determination", "fundus photographs potentially", "Ocular Hypertension", "Hypertension Treatment", "model", "Automated deep", "deep learning"], "paper_title": "Detecting Glaucoma in the Ocular Hypertension Study Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0034793828", "domain": "Glaucoma (unspecified)", "model_name": "oct-schlemm-seg", "publication_date": "2021/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34793828/", "code_link": "https://github.com/kevinchoy/oct-schlemm-seg", "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "34793828", "task": "aipbNdPTIt", "abstract": "The purpose of this study was to develop an automatic deep learning-based approach and corresponding free, open-source software to perform segmentation of the Schlemm's canal (SC) lumen in optical coherence tomography (OCT) scans of living mouse eyes. A novel convolutional neural network (CNN) for semantic segmentation grounded in a U-Net architecture was developed by incorporating a late fusion scheme, multi-scale input image pyramid, dilated residual convolution blocks, and attention-gating. 163 pairs of intensity and speckle variance (SV) OCT B-scans acquired from 32 living mouse eyes were used for training, validation, and testing of this CNN model for segmentation of the SC lumen. The proposed model achieved a mean Dice Similarity Coefficient (DSC) of 0.694\u00a0\u00b1\u00a00.256 and median DSC of 0.791, while manual segmentation performed by a second expert grader achieved a mean and median DSC of 0.713\u00a0\u00b1\u00a00.209 and 0.763, respectively. This work presents the first automatic method for segmentation of the SC lumen in OCT images of living mouse eyes. The performance of the proposed model is comparable to the performance of a second human grader. Open-source automatic software for segmentation of the SC lumen is expected to accelerate experiments for studying treatment efficacy of new drugs affecting intraocular pressure and related diseases such as glaucoma, which present as changes in the SC area.", "keywords": ["optical coherence tomography", "deep learning-based approach", "living mouse eyes", "Schlemm canal", "automatic deep learning-based", "living mouse", "coherence tomography", "mouse eyes", "deep learning-based", "learning-based approach", "optical coherence", "Dice Similarity Coefficient", "OCT B-scans acquired", "segmentation", "perform segmentation", "median DSC", "OCT B-scans", "semantic segmentation grounded", "Schlemm", "OCT"], "paper_title": "Open-source deep learning-based automatic segmentation of mouse Schlemm's canal in optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0033069883", "domain": "Glaucoma (unspecified)", "model_name": "Lazaridis et al.", "publication_date": "2020/10/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33069883/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33069883", "task": null, "abstract": "To establish whether deep learning methods are able to improve the signal-to-noise ratio of time-domain (TD) OCT images to approach that of spectral-domain (SD) OCT images. Method agreement study and progression detection in a randomized, double-masked, placebo-controlled, multicenter trial for open-angle glaucoma (OAG), the United Kingdom Glaucoma Treatment Study (UKGTS). The training and validation cohort comprised 77 stable OAG participants with TD OCT and SD OCT imaging at up to 11 visits within 3 months. The testing cohort comprised 284 newly diagnosed OAG patients with TD OCT images from a cohort of 516 recruited at 10 United Kingdom centers between 2007 and\u00a02010. An ensemble of generative adversarial networks (GANs) was trained on TD OCT and SD OCT image pairs from the training dataset and applied to TD OCT images from the testing dataset. Time-domain OCT images were converted to synthesized SD OCT images and segmented via Bayesian fusion on the output of the GANs. Bland-Altman analysis assessed agreement between TD OCT and synthesized SD OCT average retinal nerve fiber layer thickness (RNFLT) measurements and the SD OCT RNFLT. Analysis of the distribution of the rates of RNFLT change in TD OCT and synthesized SD OCT in the two treatment arms of the UKGTS was compared. A Cox model for predictors of time-to-incident visual field (VF) progression was computed with the TD OCT and the synthesized SD OCT images. The 95% limits of agreement were between TD OCT and SD OCT were 26.64 to -22.95; between synthesized SD OCT and SD OCT were 8.11 to -6.73; and between SD OCT and SD OCT were 4.16 to -4.04. The mean difference in the rate of RNFLT change between UKGTS treatment and placebo arms with TD OCT was 0.24 (P\u00a0= 0.11) and with synthesized SD OCT was 0.43 (P\u00a0= 0.0017). The hazard ratio for RNFLT slope in Cox regression modeling for time to incident VF progression was 1.09 (95% confidence interval [CI], 1.02-1.21; P\u00a0=\u00a00.035) for TD OCT and 1.24 (95% CI, 1.08-1.39; P\u00a0= 0.011) for synthesized SD OCT. Image enhancement significantly improved the agreement of TD OCT RNFLT measurements with SD OCT RNFLT measurements. The difference, and its significance, in rates of RNFLT change in the UKGTS treatment arms was enhanced and RNFLT change became a stronger predictor of VF progression.", "keywords": ["OCT", "OCT images", "OCT RNFLT", "OCT RNFLT measurements", "synthesized SD OCT", "United Kingdom Glaucoma", "Time-domain OCT images", "RNFLT", "United Kingdom", "OCT image pairs", "OCT image", "RNFLT change", "deep learning methods", "Kingdom Glaucoma Treatment", "Glaucoma Treatment Study", "images", "Time-domain OCT", "United Kingdom centers", "synthesized", "OCT imaging"], "paper_title": "OCT Signal Enhancement with Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0036675435", "domain": "Glaucoma (unspecified)", "model_name": "Bunod et al.", "publication_date": "2023/01/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36675435/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36675435", "task": "IWqQC1koJA", "abstract": "Glaucoma and non-arteritic anterior ischemic optic neuropathy (NAION) are optic neuropathies that can both lead to irreversible blindness. Several studies have compared optical coherence tomography angiography (OCTA) findings in glaucoma and NAION in the presence of similar functional and structural damages with contradictory results. The goal of this study was to use a deep learning system to differentiate OCTA in glaucoma and NAION. Sixty eyes with glaucoma (including primary open angle glaucoma, angle-closure glaucoma, normal tension glaucoma, pigmentary glaucoma, pseudoexfoliative glaucoma and juvenile glaucoma), thirty eyes with atrophic NAION and forty control eyes (NC) were included. All patients underwent OCTA imaging and automatic segmentation was used to analyze the macular superficial capillary plexus (SCP) and the radial peripapillary capillary (RPC) plexus. We used the classic convolutional neural network (CNN) architecture of ResNet50. Attribution maps were obtained using the \"Integrated Gradients\" method. The best performances were obtained with the SCP + RPC model achieving a mean area under the receiver operating characteristics curve (ROC AUC) of 0.94 (95% CI 0.92-0.96) for glaucoma, 0.90 (95% CI 0.86-0.94) for NAION and 0.96 (95% CI 0.96-0.97) for NC. This study shows that deep learning architecture can classify NAION, glaucoma and normal OCTA images with a good diagnostic performance and may outperform the specialist assessment.", "keywords": ["ischemic optic neuropathy", "anterior ischemic optic", "non-arteritic anterior ischemic", "Glaucoma", "optic neuropathy", "NAION", "ischemic optic", "optic neuropathies", "irreversible blindness", "non-arteritic anterior", "anterior ischemic", "lead to irreversible", "OCTA", "optic", "glaucoma and NAION", "atrophic NAION", "SCP", "RPC", "eyes", "classify NAION"], "paper_title": "A Deep Learning System Using Optical Coherence Tomography Angiography to Detect Glaucoma and Anterior Ischemic Optic Neuropathy.", "last_updated": "2023/02/04"}, {"id": "0036897658", "domain": "Glaucoma (unspecified)", "model_name": "Eslami et al.", "publication_date": "2023/03/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36897658/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36897658", "task": "IWqQC1koJA", "abstract": "We developed a deep learning-based classifier that can discriminate primary angle-closure suspects, primary angle-closure/primary angle-closure glaucoma, and also control eyes with open-angle with acceptable accuracy. To develop a deep learning (DL) based classifier for differentiating subtypes of primary angle closure disease (PACD), including primary angle-closure suspect (PACS) and primary angle-closure/primary angle-closure glaucoma (PAC/PACG) and also normal control eyes. Anterior segment optical coherence tomography (AS-OCT) images were used for analysis with five different networks including MnasNet, MobileNet, ResNet18, ResNet50, and EfficientNet. The data set was split with randomization performed at the patient level into a training plus validation set (85%), and a test data set (15%). Then 4-fold cross-validation was used to train the model. In each above-mentioned architecture, the networks were trained with original and cropped images. Also, the analyses were carried out for single images and images grouped on the patient level (case-based). Then majority voting was applied to the determination of the final prediction. A total of 1616 images of normal eyes (87 eyes), 1055 images of PACS (66 eyes), and 1076 images of PAC/PACG (66 eyes) eyes were included in the analysis. The mean\u00b1SD age was 51.76\u00b115.15 years and 48.3% were male. MobileNet had the best performance in the model in which both original and cropped images were used. The accuracy of MobileNet for detecting normal, PACS, and PAC/PACG eyes was 0.99\u00b10.00, 0.77\u00b10.02, and 0.77\u00b10.03, respectively. By running MobileNet in a case-based classification approach, the accuracy improved and reached 0.95\u00b10.03, 0.83\u00b10.06, and 0.81\u00b10.05, respectively. For detecting the open angle, PACS, and PAC/PACG, the MobileNet classifier achieved an area under the curve of 1, 0.906, and 0.872, respectively, on the test dataset. The MobileNet-based classifier can detect normal, PACS, and PAC/PACG eyes with acceptable accuracy based on AS-OCT images.", "keywords": ["primary angle-closure glaucoma", "primary angle-closure", "discriminate primary angle-closure", "primary angle-closure suspects", "including primary angle-closure", "angle-closure glaucoma", "angle-closure", "primary", "PACS", "PAC", "angle-closure suspects", "discriminate primary", "images", "PACG", "eyes", "deep learning-based classifier", "PACG eyes", "control eyes", "developed a deep", "deep learning-based"], "paper_title": "A Deep Learning Approach for Classification of the Primary Angle-closure Disease Spectrum Based on Anterior Segment Optical Coherence Tomography.", "last_updated": "2023/02/04"}, {"id": "0031293810", "domain": "Glaucoma (unspecified)", "model_name": "Nishimura et al.", "publication_date": "2019/06/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31293810/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31293810", "task": null, "abstract": "We developed and evaluated an eye dropper bottle sensor system comprising motion sensor with automatic motion waveform analysis using deep learning (DL) to accurately measure adherence of patients with antiglaucoma ophthalmic solution therapy. We enrolled 20 patients with open-angle glaucoma who were treated with either latanoprost ophthalmic solution 0.005% or latanoprost-timolol maleate fixed combination ophthalmic solution in both eyes. An eye dropper bottle sensor was installed at patients' homes, and they were asked to instill the medication and manually record each instillation time for 3 days. Waveform data were automatically collected from the eye dropper bottle sensor and judged as a complete instillation by the DL instillation assessment model. We compared the instillation times captured on the waveform data with those on each patient's record form. In addition, we also calculated instillation movement duration from Waveform data. The developed eye bottle sensor detected all 60 instillation events (100%). Mean difference between patient and eye bottle sensor recorded time was 1 \u00b1 1.22 (range, 0-3) minutes. Additionally, mean instillation movement duration was 16.1 \u00b1 14.4 (range, 4-43) seconds. Two-way ANOVA revealed a significant difference in instillation movement duration among patients (<i>P</i> < 0.001) and across days (<i>P</i> < 0.001). The eye dropper bottle sensor system developed by us can be used for automatic monitoring of instillation adherence in patients with glaucoma. We believe that our eye dropper bottle sensor system will accurately measure adherence of all glaucoma patients as well as help glaucoma treatment.", "keywords": ["eye dropper bottle", "dropper bottle sensor", "dropper bottle", "bottle sensor", "ophthalmic solution therapy", "eye dropper", "antiglaucoma ophthalmic solution", "ophthalmic solution", "motion waveform analysis", "comprising motion sensor", "bottle", "sensor", "system comprising motion", "dropper", "eye", "instillation", "comprising motion", "deep learning", "eye bottle sensor", "bottle sensor system"], "paper_title": "Evaluation of Automatic Monitoring of Instillation Adherence Using Eye Dropper Bottle Sensor and Deep Learning in Patients With Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0036246943", "domain": "Glaucoma (unspecified)", "model_name": "Asaoka et al.", "publication_date": "2021/09/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36246943/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36246943", "task": null, "abstract": "We constructed a multitask learning model (latent space linear regression and deep learning [LSLR-DL]) in which the 2 tasks of cross-sectional predictions (using OCT) of visual field (VF; central 10\u00b0) and longitudinal progression predictions of VF (30\u00b0) were performed jointly via sharing the deep learning (DL) component such that information from both tasks was used in an auxiliary manner (The Association for Computing Machinery's Special Interest Group on Knowledge Discovery and Data Mining [SIGKDD] 2021). The purpose of the current study was to investigate the prediction accuracy preparing an independent validation dataset. Cohort study. Cross-sectional training and testing data sets included the VF (Humphrey Field Analyzer [HFA] 10-2 test) and an OCT measurement (obtained within 6 months) from 591 eyes of 351 healthy people or patients with open-angle glaucoma (OAG) and from 155 eyes of 131 patients with OAG, respectively. Longitudinal training and testing data sets included 7984 VF results (HFA 24-2 test) from 998 eyes of 592 patients with OAG and 1184 VF results (HFA 24-2 test) from 148 eyes of 84 patients with OAG, respectively. Each eye had 8 VF test results (HFA 24-2 test). The OCT sequences within the observation period were used. Root mean square error (RMSE) was used to evaluate the accuracy of LSLR-DL for the cross-sectional prediction of VF (HFA 10-2 test). For the longitudinal prediction, the final (eighth) VF test (HFA 24-2 test) was predicted using a shorter VF series and relevant OCT images, and the RMSE was calculated. For comparison, RMSE values were calculated by applying the DL component (cross-sectional prediction) and the ordinary pointwise linear regression (longitudinal prediction). Root mean square error in the cross-sectional and longitudinal predictions. Using LSLR-DL, the mean RMSE in the cross-sectional prediction was 6.4 dB and was between 4.4 dB (VF tests 1 and 2) and 3.7 dB (VF tests 1-7) in the longitudinal prediction, indicating that LSLR-DL significantly outperformed other methods. The results of this study indicate that LSLR-DL is useful for both the cross-sectional prediction of VF (HFA 10-2 test) and the longitudinal progression prediction of VF (HFA 24-2 test).", "keywords": ["Computing Machinery Special", "Machinery Special Interest", "Special Interest Group", "Association for Computing", "Computing Machinery", "Machinery Special", "Special Interest", "Interest Group", "Group on Knowledge", "Knowledge Discovery", "HFA 24-2 test", "multitask learning model", "HFA 24-2", "deep learning", "HFA", "HFA 10-2 test", "Data Mining", "24-2 test", "learning model", "cross-sectional prediction"], "paper_title": "A Joint Multitask Learning Model for Cross-sectional and Longitudinal Predictions of Visual Field Using OCT.", "last_updated": "2023/02/04"}, {"id": "0034427626", "domain": "Glaucoma (unspecified)", "model_name": "Wang et al.", "publication_date": "2021/10/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34427626/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34427626", "task": null, "abstract": "The purpose of this study was to develop a convolutional neural network (CNN) for automated localization of the scleral spur in ultrasound biomicroscopy (UBM) images of open-angle eyes. UBM images were acquired, and one glaucoma specialist provided reference coordinates of scleral spur locations in all images. A CNN model based on the EfficientNetB3 architecture was developed to detect the scleral spur in each image. The prediction errors and Euclidean distance were used to evaluate localization performance of the CNN model. Trabecular-iris angle 500 (TIA500) and angle-opening distance 500 (AOD500) were measured and analyzed using the scleral spur locations provided by the specialist and predicted by the CNN model. The CNN was developed using a training dataset of 2328 images and tested using an independent dataset of 258 images. The mean absolute prediction errors of CNN model were 48.06 \u00b1 45.40\u00a0\u00b5m for X-coordinates and 30.84 \u00b1 27.03\u00a0\u00b5m for Y-coordinates. The mean absolute intraobserver variability was 47.80 \u00b1 44.45\u00a0\u00b5m for X-coordinates and 29.50 \u00b1 25.77\u00a0\u00b5m for Y-coordinates. The mean Euclidean distance of the CNN was 60.41 \u00b1 49.02\u00a0\u00b5m and the intraobserver mean Euclidean distance was 59.78 \u00b1 47.12\u00a0\u00b5m. The mean absolute error in TIA500 was 1.26 \u00b1 1.38 degrees for all test images and in AOD500 was 0.039 \u00b1 0.051 mm. A CNN can detect the scleral spur on UBM images of open-angle eyes with performance similar to that of a glaucoma specialist. Deep learning algorithms for automating scleral spur localization would facilitate the quantitative assessment of the opening of the angle and the risk in angle closure.", "keywords": ["convolutional neural network", "scleral spur", "CNN model", "CNN", "scleral spur locations", "UBM images", "scleral spur localization", "Euclidean distance", "scleral", "spur", "images", "neural network", "ultrasound biomicroscopy", "UBM", "CNN model based", "develop a convolutional", "convolutional neural", "spur locations", "automating scleral spur", "Euclidean"], "paper_title": "Automatic Localization of the Scleral Spur Using Deep Learning and Ultrasound Biomicroscopy.", "last_updated": "2023/02/04"}, {"id": "0029045329", "domain": "Glaucoma (unspecified)", "model_name": "Muhammad et al.", "publication_date": "2017/12/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29045329/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "29045329", "task": "IWqQC1koJA", "abstract": "Existing summary statistics based upon optical coherence tomographic (OCT) scans and/or visual fields (VFs) are suboptimal for distinguishing between healthy and glaucomatous eyes in the clinic. This study evaluates the extent to which a hybrid deep learning method (HDLM), combined with a single wide-field OCT protocol, can distinguish eyes previously classified as either healthy suspects or mild glaucoma. In total, 102 eyes from 102 patients, with or suspected open-angle glaucoma, had previously been classified by 2 glaucoma experts as either glaucomatous (57 eyes) or healthy/suspects (45 eyes). The HDLM had access only to information from a single, wide-field (9\u00d712\u2009mm) swept-source OCT scan per patient. Convolutional neural networks were used to extract rich features from maps derived from these scans. Random forest classifier was used to train a model based on these features to predict the existence of glaucomatous damage. The algorithm was compared against traditional OCT and VF metrics. The accuracy of the HDLM ranged from 63.7% to 93.1% depending upon the input map. The retinal nerve fiber layer probability map had the best accuracy (93.1%), with 4 false positives, and 3 false negatives. In comparison, the accuracy of the OCT and 24-2 and 10-2 VF metrics ranged from 66.7% to 87.3%. The OCT quadrants analysis had the best accuracy (87.3%) of the metrics, with 4 false positives and 9 false negatives. The HDLM protocol outperforms standard OCT and VF clinical metrics in distinguishing healthy suspect eyes from eyes with early glaucoma. It should be possible to further improve this algorithm and with improvement it might be useful for screening.", "keywords": ["Existing summary statistics", "optical coherence tomographic", "summary statistics based", "OCT", "Existing summary", "coherence tomographic", "visual fields", "eyes", "summary statistics", "optical coherence", "healthy suspect eyes", "HDLM", "single wide-field OCT", "healthy", "eyes previously classified", "glaucoma", "distinguish eyes previously", "wide-field OCT protocol", "false", "swept-source OCT scan"], "paper_title": "Hybrid Deep Learning on Single Wide-field Optical Coherence tomography Scans Accurately Classifies Glaucoma Suspects.", "last_updated": "2023/02/04"}, {"id": "0032920530", "domain": "Glaucoma (unspecified)", "model_name": "Sun et al.", "publication_date": "2020/09/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32920530/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32920530", "task": "IWqQC1koJA", "abstract": "To evaluate, with spectral-domain optical coherence tomography (SD-OCT), the glaucoma-diagnostic ability of a deep-learning classifier. A total of 777 Cirrus high-definition SD-OCT image sets of the retinal nerve fibre layer (RNFL) and ganglion cell-inner plexiform layer (GCIPL) of 315 normal subjects, 219 patients with early-stage primary open-angle glaucoma (POAG) and 243 patients with moderate-to-severe-stage POAG were aggregated. The image sets were divided into a training data set (252 normal, 174 early POAG and 195 moderate-to-severe POAG) and a test data set (63 normal, 45 early POAG and 48 moderate-to-severe POAG). The visual geometry group (VGG16)-based dual-input convolutional neural network (DICNN) was adopted for the glaucoma diagnoses. Unlike other networks, the DICNN structure takes two images (both RNFL and GCIPL) as inputs. The glaucoma-diagnostic ability was computed according to both accuracy and area under the receiver operating characteristic curve (AUC). For the test data set, DICNN could distinguish between patients with glaucoma and normal subjects accurately (accuracy=92.793%, AUC=0.957 (95% CI 0.943 to 0.966), sensitivity=0.896 (95% CI 0.896 to 0.917), specificity=0.952 (95% CI 0.921 to 0.952)). For distinguishing between patients with early-stage glaucoma and normal subjects, DICNN's diagnostic ability (accuracy=85.185%, AUC=0.869 (95% CI 0.825 to 0.879), sensitivity=0.921 (95% CI 0.813 to 0.905), specificity=0.756 (95% CI 0.610 to 0.790)]) was higher than convolutional neural network algorithms that trained with RNFL or GCIPL separately. The deep-learning algorithm using SD-OCT can distinguish normal subjects not only from established patients with glaucoma but also from patients with early-stage glaucoma. The deep-learning model with DICNN, as trained by both RNFL and GCIPL thickness map data, showed a high diagnostic ability for discriminatingpatients with early-stage glaucoma from normal subjects.", "keywords": ["optical coherence tomography", "spectral-domain optical coherence", "POAG", "early POAG", "normal subjects", "coherence tomography", "Cirrus high-definition SD-OCT", "spectral-domain optical", "optical coherence", "normal", "DICNN", "glaucoma", "RNFL", "patients", "AUC", "GCIPL", "data set", "patients with early-stage", "early-stage glaucoma", "subjects"], "paper_title": "Dual-input convolutional neural network for glaucoma diagnosis using spectral-domain optical coherence tomography.", "last_updated": "2023/02/04"}, {"id": "0034812893", "domain": "Glaucoma (unspecified)", "model_name": "Hashimoto et al.", "publication_date": "2022/01/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34812893/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34812893", "task": null, "abstract": "To investigate whether a correction based on a Humphrey field analyzer (HFA) 24-2/30-2 visual field (VF) can improve the prediction performance of a deep learning model to predict the HFA 10-2 VF test from macular optical coherence tomography (OCT) measurements. This is a multicenter, cross-sectional study. The training dataset comprised 493 eyes of 285 subjects (407, open-angle glaucoma [OAG]; 86, normative) who underwent HFA 10-2 testing and macular OCT. The independent testing dataset comprised 104 OAG eyes of 82 subjects who had undergone HFA 10-2 test, HFA 24-2/30-2 test, and macular OCT. A convolutional neural network (CNN) DL model was trained to predict threshold sensitivity (TH) values in HFA 10-2 from retinal thickness measured by macular OCT. The predicted TH values was modified by pattern-based regularization (PBR) and corrected with HFA 24-2/30-2. Absolute error (AE) of mean TH values and mean absolute error (MAE) of TH values were compared between the CNN-PBR alone model and the CNN-PBR corrected with HFA 24-2/30-2. AE of mean TH values was lower in the CNN-PBR with HFA 24-2/30-2 correction than in the CNN-PBR alone (1.9dB vs. 2.6dB; P = 0.006). MAE of TH values was lower in the CNN-PBR with correction compared to the CNN-PBR alone (4.2dB vs. 5.3 dB; P < 0.001). The inferior temporal quadrant showed lower prediction errors compared with other quadrants. The performance of a DL model to predict 10-2 VF from macular OCT was improved by the correction with HFA 24-2/30-2. This model can reduce the burden of additional HFA 10-2 by making the best use of routinely performed HFA 24-2/30-2 and macular OCT.", "keywords": ["Humphrey field analyzer", "HFA 24-2", "HFA 10-2", "HFA", "Humphrey field", "macular OCT", "optical coherence tomography", "30-2 visual field", "macular optical coherence", "OCT", "24-2", "HFA 10-2 test", "deep learning model", "field analyzer", "HFA 10-2 testing", "visual field", "performed HFA 24-2", "macular", "30-2", "underwent HFA 10-2"], "paper_title": "Predicting 10-2 Visual Field From Optical Coherence Tomography in Glaucoma Using Deep Learning Corrected With 24-2/30-2 Visual Field.", "last_updated": "2023/02/04"}, {"id": "0032149291", "domain": "Glaucoma (unspecified)", "model_name": "Myer et al.", "publication_date": "2021/07/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32149291/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32149291", "task": null, "abstract": "Pseudoexfoliation (PEX) is a known cause of secondary open angle glaucoma. PEX glaucoma is associated with structural and metabolic changes in the eye. Despite similarities, PEX and primary open angle glaucoma (POAG) may have differences in the composition of metabolites. We analyzed the metabolites of the aqueous humor (AH) of PEX subjects sequentially first using nuclear magnetic resonance (1H NMR: HSQC and TOCSY), and subsequently with liquid chromatography tandem mass spectrometry (LC-MS/MS) implementing isotopic ratio outlier analysis (IROA) quantification. The findings were compared with previous results for POAG and control subjects analyzed using identical sequential steps. We found significant differences in metabolites between the three conditions. Principle component analysis (PCA) and partial least squares discriminant analysis (PLS-DA) indicated clear grouping based on the metabolomes of the three conditions. We used machine learning algorithms and a percentage set of the data to train, and utilized a different or larger dataset to test whether a trained model can correctly classify the test dataset as PEX, POAG or control. Three different algorithms: linear support vector machines (SVM), deep learning, and a neural network were used for prediction. They all accurately classified the test datasets based on the AH metabolome of the sample. We next compared the AH metabolome with known AH and TM proteomes and genomes in order to understand metabolic pathways that may contribute to alterations in the AH metabolome in PEX. We found potential protein/gene pathways associated with observed significant metabolite changes in PEX.", "keywords": ["open angle glaucoma", "secondary open angle", "PEX", "angle glaucoma", "open angle", "PEX glaucoma", "secondary open", "primary open angle", "POAG", "glaucoma", "PEX subjects sequentially", "HSQC and TOCSY", "PEX subjects", "analysis", "metabolites", "angle", "metabolome", "Pseudoexfoliation", "open", "test"], "paper_title": "Aqueous humor metabolite profile of pseudoexfoliation glaucoma is distinctive.", "last_updated": "2023/02/04"}, {"id": "0034798322", "domain": "Glaucoma (unspecified)", "model_name": "Lin et al.", "publication_date": "2021/11/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34798322/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34798322", "task": null, "abstract": "Accurate identification of iridocorneal structures on gonioscopy is difficult to master, and errors can lead to grave surgical complications. This study aimed to develop and train convolutional neural networks (CNNs) to accurately identify the trabecular meshwork (TM) in gonioscopic videos in real time for eventual clinical integrations. Cross-sectional study. Adult patients with open angle were identified in academic glaucoma clinics in both Taipei, Taiwan, and Irvine, California. Neural Encoder-Decoder CNNs (U-nets) were trained to predict a curve marking the TM using an expert-annotated data set of 378 gonioscopy images. The model was trained and evaluated with stratified cross-validation grouped by patients to ensure uncorrelated training and testing sets, as well as on a separate test set and 3 intraoperative gonioscopic videos of ab interno trabeculotomy with Trabectome (totaling 90 seconds long, 30 frames per second). We also evaluated our model's performance by comparing its accuracy against ophthalmologists. Successful development of real-time-capable CNNs that are accurate in predicting and marking the TM's position in video frames of gonioscopic views. Models were evaluated in comparison with human expert annotations of static images and video data. The best CNN model produced test set predictions with a median deviation of 0.8% of the video frame's height (15.25 \u03bcm) from the human experts' annotations. This error is less than the average vertical height of the TM. The worst test frame prediction of this model had an average deviation of 4% of the frame height (76.28 \u03bcm), which is still considered a successful prediction. When challenged with unseen images, the CNN model scored greater than 2 standard deviations above the mean performance of the surveyed general ophthalmologists. Our CNN model can identify the TM in gonioscopy videos in real time with remarkable accuracy, allowing it to be used in connection with a video camera intraoperatively. This model can have applications in surgical training, automated screenings, and intraoperative guidance. The dataset developed in this study is one of the first publicly available gonioscopy image banks (https://lin.hs.uci.edu/research), which may encourage future investigations in this topic.", "keywords": ["grave surgical complications", "difficult to master", "identification of iridocorneal", "iridocorneal structures", "lead to grave", "CNN model", "model", "CNNs", "CNN", "video", "Accurate identification", "study", "gonioscopy", "surgical complications", "gonioscopic videos", "grave surgical", "videos", "set", "frame", "gonioscopic"], "paper_title": "Accurate Identification of the Trabecular Meshwork under Gonioscopic View in Real Time Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0032735906", "domain": "Glaucoma (unspecified)", "model_name": "Medeiros et al.", "publication_date": "2020/07/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32735906/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32735906", "task": "0GPcU42tzV", "abstract": "To investigate whether predictions of retinal nerve fiber layer (RNFL) thickness obtained from a deep learning model applied to fundus photographs can detect progressive glaucomatous changes over time. Retrospective cohort study. Eighty-six thousand one hundred twenty-three pairs of color fundus photographs and spectral-domain (SD) OCT images collected during 21\u2009232 visits from 8831 eyes of 5529 patients with glaucoma or glaucoma suspects. A deep learning convolutional neural network was trained to assess fundus photographs and to predict SD OCT global RNFL thickness measurements. The model then was tested on an independent sample of eyes that had longitudinal follow-up with both fundus photography and SD OCT. The ability to detect eyes that had statistically significant slopes of SD OCT change was assessed by receiver operating characteristic (ROC) curves. The repeatability of RNFL thickness predictions was investigated by measurements obtained from multiple photographs that had been acquired during the same day. The relationship between change in predicted RNFL thickness from photographs and change in SD OCT RNFL thickness over time. The test sample consisted of 33\u2009466 pairs of fundus photographs and SD OCT images collected during 7125 visits from 1147 eyes of 717 patients. Eyes in the test sample were followed up for an average of 5.3 \u00b1 3.3 years, with an average of 6.2 \u00b1 3.8 visits. A significant correlation was found between change over time in predicted and observed RNFL thickness (r\u00a0= 0.76; 95% confidence interval [CI], 0.70-0.80; P < 0.001). Retinal nerve fiber layer predictions showed an ROC curve area of 0.86 (95% CI, 0.83-0.88) to discriminate progressors from nonprogressors. For detecting fast progressors (slope faster than 2 \u03bcm/year), the ROC curve area was 0.96 (95% CI, 0.94-0.98), with a sensitivity of 97% for 80% specificity and 85% for 90% specificity. For photographs obtained at the same visit, the intraclass correlation coefficient was 0.946 (95% CI, 0.940-0.952), with a coefficient of variation of 3.2% (95% CI, 3.1%-3.3%). A deep learning model was able to obtain objective and quantitative estimates of RNFL thickness that correlated well with SD OCT measurements and potentially could be used to monitor for glaucomatous changes over time.", "keywords": ["RNFL thickness", "OCT RNFL thickness", "OCT", "RNFL", "OCT global RNFL", "fundus photographs", "OCT RNFL", "OCT images collected", "photographs", "thickness", "OCT images", "deep learning", "deep learning model", "learning model applied", "RNFL thickness measurements", "RNFL thickness predictions", "fundus", "predicted RNFL thickness", "global RNFL thickness", "eyes"], "paper_title": "Detection of Progressive Glaucomatous Optic Nerve Damage on Fundus Photographs with Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0032791238", "domain": "Glaucoma (unspecified)", "model_name": "Xu et al.", "publication_date": "2020/08/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32791238/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32791238", "task": null, "abstract": "To investigate whether OCT measurements can improve visual field (VF) trend analyses in glaucoma patients using the deeply regularized latent-space linear regression (DLLR) model. Retrospective cohort study. Training and testing datasets included 7984 VF results from 998 eyes of 592 patients and 1184 VF results from 148 eyes of 84 patients with open-angle glaucoma, respectively. Each eye underwent a series of 8 VF tests with the Humphrey Field Analyzer OCT series obtained within the same observation period. Using pointwise linear regression (PLR), the threshold values of a patient's eighth VF results were predicted using values from shorter VF series (first to second VF tests [VF1-2], first to third VF tests, . . . , to first to seventh VF tests [VF1-7]), and the root mean square error (RMSE) was calculated. With DLLR, OCT measurements (macular retinal nerve fiber layer thickness, the thickness of macular ganglion cell layer and inner plexiform layer, and the thickness of the outer segment and retinal pigment epithelium) that were obtained within the period of shorter VF series were incorporated into the model to predict the eighth VF. Prediction accuracy of VF trend analyses. The mean \u00b1 standard deviation RMSE resulting from PLR averaged 27.48 \u00b1 16.14 dB for VF1-2 and 3.98 \u00b1 2.25 dB for VF1-7. Significantly (P < 0.001) smaller RMSEs were obtained from DLLR: 4.57 \u00b1 2.71 dB (VF1-2) and 3.65 \u00b1 2.27 dB (VF1-7). It is useful to include OCT measurements when predicting future VF progression in glaucoma patients, especially with short VF series.", "keywords": ["deeply regularized latent-space", "improve visual field", "regularized latent-space linear", "Field Analyzer OCT", "Humphrey Field Analyzer", "latent-space linear regression", "improve visual", "deeply regularized", "regularized latent-space", "OCT measurements", "Analyzer OCT series", "visual field", "OCT", "glaucoma patients", "OCT series obtained", "linear regression", "Analyzer OCT", "series", "latent-space linear", "DLLR"], "paper_title": "Improving Visual Field Trend Analysis with OCT and Deeply Regularized Latent-Space Linear Regression.", "last_updated": "2023/02/04"}, {"id": "0032387432", "domain": "Glaucoma (unspecified)", "model_name": "Xu et al.", "publication_date": "2020/05/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32387432/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32387432", "task": null, "abstract": "To predict the visual field (VF) of glaucoma patients within the central 10\u00b0 from optical coherence tomography (OCT) measurements using deep learning and tensor regression. Cross-sectional study. Humphrey 10-2 VFs and OCT measurements were carried out in 505 eyes of 304 glaucoma patients and 86 eyes of 43 normal subjects. VF sensitivity at each test point was predicted from OCT-measured thicknesses of macular ganglion cell layer\u00a0+ inner plexiform layer, retinal nerve fiber layer, and outer segment\u00a0+ retinal pigment epithelium. Two convolutional neural network (CNN) models were generated: (1) CNN-PR, which simply connects the output of the CNN to each VF test point; and (2) CNN-TR, which connects the output of the CNN to each VF test point using tensor regression. Prediction performance was assessed using 5-fold cross-validation through the root mean squared error (RMSE). For comparison, RMSE values were also calculated using multiple linear regression (MLR) and support vector regression (SVR). In addition, the absolute prediction error for predicting mean sensitivity in the whole VF was analyzed. RMSE with the CNN-TR model averaged 6.32 \u00b1 3.76 (mean \u00b1 standard deviation) dB. Significantly (P < .05) larger RMSEs were obtained with other models: CNN-PR (6.76 \u00b1 3.86 dB), SVR (7.18 \u00b1 3.87 dB), and MLR (8.56 \u00b1 3.69 dB). The absolute mean prediction error for the whole VF was 2.72 \u00b1 2.60 dB with the CNN-TR model. The Humphrey 10-2 VF can be predicted from OCT-measured retinal layer thicknesses using deep learning and tensor regression.", "keywords": ["optical coherence tomography", "visual field", "coherence tomography", "predict the visual", "optical coherence", "tensor regression", "glaucoma patients", "regression", "OCT measurements", "test point", "CNN", "OCT", "RMSE", "test", "tensor", "layer", "point", "SVR", "Humphrey 10-2", "CNN-TR model"], "paper_title": "Predicting the Glaucomatous Central 10-Degree Visual Field From Optical Coherence Tomography Using Deep Learning and Tensor Regression.", "last_updated": "2023/02/04"}, {"id": "0032398783", "domain": "Glaucoma (unspecified)", "model_name": "Asaoka et al.", "publication_date": "2020/05/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32398783/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32398783", "task": null, "abstract": "The aim of the study was to investigate the usefulness of processing visual field (VF) using a variational autoencoder (VAE). The training data consisted of 82,433 VFs from 16,836 eyes. Testing dataset 1 consisted of test-retest VFs from 104 eyes with open angle glaucoma. Testing dataset 2 was series of 10 VFs from 638 eyes with open angle glaucoma. A VAE model to reconstruct VF was developed using the training dataset. VFs in the testing dataset 1 were then reconstructed using the trained VAE and the mean total deviation (mTD) was calculated (mTD<sub>VAE</sub>). In testing dataset 2, the mTD value of the tenth VF was predicted using shorter series of VFs. A similar calculation was carried out using a weighted linear regression where the weights were equal to the absolute difference between mTD and mTD<sub>VAE</sub>. In testing dataset 1, there was a significant relationship between the difference between mTD and mTD<sub>VAE</sub> from the first VF and the difference between mTD in the first and second VFs. In testing dataset 2, mean squared prediction errors with the weighted mTD trend analysis were significantly smaller than those form the unweighted mTD trend analysis.", "keywords": ["processing visual field", "Testing dataset", "dataset", "mTD", "VAE", "visual field", "variational autoencoder", "Testing", "investigate the usefulness", "usefulness of processing", "processing visual", "VFs", "open angle glaucoma", "eyes", "angle glaucoma", "open angle", "eyes with open", "training dataset", "difference", "mTD trend"], "paper_title": "The usefulness of the Deep Learning method of variational autoencoder to reduce measurement noise in glaucomatous visual fields.", "last_updated": "2023/02/04"}, {"id": "0032313133", "domain": "Glaucoma (unspecified)", "model_name": "Asano et al.", "publication_date": "2020/04/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32313133/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32313133", "task": null, "abstract": "The aim of the current study is to identify possible new Ocular Response Analyzer (ORA) waveform parameters related to changes of retinal structure/deformation, as measured by the peripapillary retinal arteries angle (PRAA), using a generative deep learning method of variational autoencoder (VAE). Fifty-four eyes of 52 subjects were enrolled. The PRAA was calculated from fundus photographs and was used to train a VAE model. By analyzing the ORA waveform reconstructed (noise filtered) using VAE, a novel ORA waveform parameter (Monot1-2), was introduced, representing the change in monotonicity between the first and second applanation peak of the waveform. The variables mostly related to the PRAA were identified from a set of 41 variables including age, axial length (AL), keratometry, ORA corneal hysteresis, ORA corneal resistant factor, 35 well established ORA waveform parameters, and Monot1-2, using a model selection method based on the second-order bias-corrected Akaike information criterion. The optimal model for PRAA was the AL and six ORA waveform parameters, including Monot1-2. This optimal model was significantly better than the model without Monot1-2 (p\u2009=\u20090.0031, ANOVA). The current study suggested the value of a generative deep learning approach in discovering new useful parameters that may have clinical relevance.", "keywords": ["Ocular Response Analyzer", "Response Analyzer", "Ocular Response", "retinal arteries angle", "peripapillary retinal arteries", "ORA waveform parameters", "ORA waveform", "retinal structure", "ORA", "peripapillary retinal", "retinal arteries", "arteries angle", "variational autoencoder", "waveform parameters", "ORA waveform reconstructed", "established ORA waveform", "waveform", "PRAA", "ORA corneal", "waveform parameters related"], "paper_title": "Visualizing the dynamic change of Ocular Response Analyzer waveform using Variational Autoencoder in association with the peripapillary retinal arteries angle.", "last_updated": "2023/02/04"}, {"id": "0036220350", "domain": "Glaucoma (unspecified)", "model_name": "Lin et al.", "publication_date": "2022/10/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36220350/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36220350", "task": "0GPcU42tzV", "abstract": "To determine the relationship between baseline retinal-vessel calibers computed by a deep-learning system and the risk of normal tension glaucoma (NTG) progression. Prospective cohort study. Three hundred and ninety eyes from 197 patients with NTG were followed up for at least 24 months. Retinal-vessel calibers (central retinal arteriolar equivalent [CRAE] and central retinal venular equivalent [CRVE]) were computed from fundus photographs at baseline using a previously validated deep-learning system. Retinal nerve fiber layer (RNFL) thickness and visual field (VF) were evaluated semiannually. The Cox proportional-hazards model was used to evaluate the relationship of baseline retinal-vessel calibers to the risk of glaucoma progression. Over a mean follow-up period of 34.36 \u00b1 5.88 months, 69 NTG eyes (17.69%) developed progressive RNFL thinning and 22 eyes (5.64%) developed VF deterioration. In the multivariable Cox regression analysis adjusting for age, gender, intraocular pressure, mean ocular perfusion pressure, systolic blood pressure, axial length, standard automated perimetry mean deviation, and RNFL thickness, narrower baseline CRAE (hazard ratio per SD decrease [95% confidence interval], 1.36 [1.01-1.82]) and CRVE (1.35 [1.01-1.80]) were associated with progressive RNFL thinning and narrower baseline CRAE (1.98 [1.17-3.35]) was associated with VF deterioration. In this study, each SD decrease in the baseline CRAE or CRVE was associated with a more than 30% increase in the risk of progressive RNFL thinning and a more than 90% increase in the risk of VF deterioration during the follow-up period. Baseline attenuation of retinal vasculature in NTG eyes was associated with subsequent glaucoma progression. High-throughput deep-learning-based retinal vasculature analysis demonstrated its clinical utility for NTG risk assessment.", "keywords": ["progressive RNFL thinning", "normal tension glaucoma", "RNFL thinning", "RNFL", "progressive RNFL", "normal tension", "baseline CRAE", "NTG", "baseline", "retinal-vessel calibers", "CRAE", "baseline retinal-vessel calibers", "risk", "narrower baseline CRAE", "retinal", "NTG eyes", "deep-learning system", "CRVE", "baseline retinal-vessel", "determine the relationship"], "paper_title": "Risk of Normal Tension Glaucoma Progression From Automated Baseline Retinal-Vessel Caliber Analysis: A Prospective Cohort Study.", "last_updated": "2023/02/04"}, {"id": "0032672618", "domain": "Glaucoma (unspecified)", "model_name": "Asaoka et al.", "publication_date": "2020/01/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32672618/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32672618", "task": null, "abstract": "To investigate whether processing visual field (VF) measurements using a variational autoencoder (VAE) improves the structure-function relationship in glaucoma. Cross-sectional study. The training data consisted of 82\u2009433 VF measurements from 16\u2009836 eyes. The testing dataset consisted of 117 eyes of 75 patients with open-angle glaucoma. A VAE model to reconstruct the threshold of VF was developed using the training dataset. OCT and VF (Humphrey Field Analyzer 24-2, Swedish interactive threshold algorithm standard) measurements were carried out for all eyes in the testing dataset. Visual fields in the testing dataset then were reconstructed using the trained VAE. The structure-function relationship between the circumpapillary retinal nerve fiber layer (cpRNFL) thickness and VF sensitivity was investigated in each of twelve 30\u00b0 segments of the optic disc (3 nasal sectors were merged). Similarly, the structure-function relationship was investigated using the VAE-reconstructed VF. Structure-function relationship. The corrected Akaike information criterion values with threshold were found to be smaller than the threshold reconstructed with the VAE (threshold<sub>VAE</sub>) in 9 of 10 sectors. A significant relationship was found between threshold and cpRNFL thickness in 6 of 10 sectors, whereas it was significant in 9 of 10 sectors with threshold<sub>VAE</sub>. Applying VAE to VF data results in an improved structure-function relationship.", "keywords": ["structure-function relationship", "VAE", "variational autoencoder", "investigate whether processing", "processing visual field", "relationship", "threshold", "structure-function", "testing dataset", "Humphrey Field Analyzer", "dataset", "processing visual", "measurements", "testing", "sectors", "improves the structure-function", "eyes", "testing dataset consisted", "Humphrey Field", "Field Analyzer"], "paper_title": "Improving the Structure-Function Relationship in Glaucomatous Visual Fields by Using a Deep Learning-Based Noise Reduction Approach.", "last_updated": "2023/02/04"}, {"id": "0030794201", "domain": "Glaucoma (unspecified)", "model_name": "Fu et al.", "publication_date": "2019/02/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30794201/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30794201", "task": "IWqQC1koJA", "abstract": "Irreversible visual impairment is often caused by primary angle-closure glaucoma, which could be detected via anterior segment optical coherence tomography (AS-OCT). In this paper, an automated system based on deep learning is presented for angle-closure detection in AS-OCT images. Our system learns a discriminative representation from training data that captures subtle visual cues not modeled by handcrafted features. A multilevel deep network is proposed to formulate this learning, which utilizes three particular AS-OCT regions based on clinical priors: 1) the global anterior segment structure; 2) local iris region; and 3) anterior chamber angle (ACA) patch. In our method, a sliding window-based detector is designed to localize the ACA region, which addresses ACA detection as a regression task. Then, three parallel subnetworks are applied to extract AS-OCT representations for the global image and at clinically relevant local regions. Finally, the extracted deep features of these subnetworks are concatenated into one fully connected layer to predict the angle-closure detection result. In the experiments, our system is shown to surpass previous detection methods and other deep learning systems on two clinical AS-OCT datasets.", "keywords": ["optical coherence tomography", "primary angle-closure glaucoma", "Irreversible visual impairment", "segment optical coherence", "coherence tomography", "caused by primary", "optical coherence", "anterior segment optical", "angle-closure glaucoma", "Irreversible visual", "primary angle-closure", "visual impairment", "segment optical", "AS-OCT", "ACA", "angle-closure", "anterior segment", "deep", "detection", "anterior"], "paper_title": "Angle-Closure Detection in Anterior Segment OCT Based on Multilevel Deep Network.", "last_updated": "2023/02/04"}, {"id": "0034615666", "domain": "Glaucoma (unspecified)", "model_name": "Shen et al.", "publication_date": "2021/10/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34615666/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34615666", "task": "IWqQC1koJA", "abstract": "To identify biometric parameters that explain misclassifications by a deep learning classifier for detecting gonioscopic angle closure in anterior segment optical coherence tomography (AS-OCT) images. Chinese American Eye Study (CHES) participants underwent gonioscopy and AS-OCT of each angle quadrant. A subset of CHES AS-OCT images were analysed using a deep learning classifier to detect positive angle closure based on manual gonioscopy by a reference human examiner. Parameter measurements were compared between four prediction classes: true positives (TPs), true negatives (TNs), false positives (FPs) and false negatives (FN). Logistic regression models were developed to differentiate between true and false predictions. Performance was assessed using area under the receiver operating curve (AUC) and classifier accuracy metrics. 584 images from 127 participants were analysed, yielding 271 TPs, 224 TNs, 77 FPs and 12 FNs. Parameter measurements differed (p<0.001) between prediction classes among anterior segment parameters, including iris curvature (IC) and lens vault (LV), and angle parameters, including angle opening distance (AOD). FP resembled TP more than FN and TN in terms of anterior segment parameters (steeper IC and higher LV), but resembled TN more than TP and FN in terms of angle parameters (wider AOD). Models for detecting FP (AUC=0.752) and FN (AUC=0.838) improved classifier accuracy from 84.8% to 89.0%. Misclassifications by an OCT-based deep learning classifier for detecting gonioscopic angle closure are explained by disagreement between anterior segment and angle parameters. This finding could be used to improve classifier performance and highlights differences between gonioscopic and AS-OCT definitions of angle closure.", "keywords": ["American Eye Study", "optical coherence tomography", "Chinese American Eye", "deep learning classifier", "identify biometric parameters", "segment optical coherence", "Eye Study", "angle", "deep learning", "CHES AS-OCT images", "American Eye", "coherence tomography", "identify biometric", "optical coherence", "angle closure", "learning classifier", "parameters", "angle parameters", "anterior segment", "classifier"], "paper_title": "Anterior segment biometric measurements explain misclassifications by a deep learning classifier for detecting gonioscopic angle closure.", "last_updated": "2023/02/04"}, {"id": "0035305492", "domain": "Glaucoma (unspecified)", "model_name": "Yoo et al.", "publication_date": "2022/03/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35305492/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35305492", "task": "IWqQC1koJA", "abstract": "Patients with angle-closure glaucoma (ACG) are asymptomatic until they experience a painful attack. Shallow anterior chamber depth (ACD) is considered a significant risk factor for ACG. We propose a deep learning approach to detect shallow ACD using fundus photographs and to identify the hidden features of shallow ACD. This retrospective study assigned healthy subjects to the training (n\u00a0=\u00a01188 eyes) and test (n\u00a0=\u00a0594) datasets (prospective validation design). We used a deep learning approach to estimate ACD and build a classification model to identify eyes with a shallow ACD. The proposed method, including subtraction of the input and output images of CycleGAN and a thresholding algorithm, was adopted to visualize the characteristic features of fundus photographs with a shallow ACD. The deep learning model integrating fundus photographs and clinical variables achieved areas under the receiver operating characteristic curve of 0.978 (95% confidence interval [CI], 0.963-0.988) for an ACD \u2264 2.60\u00a0mm and 0.895 (95% CI, 0.868-0.919) for an ACD \u2264 2.80\u00a0mm, and outperformed the regression model using only clinical variables. However, the difference between shallow and deep ACD classes on fundus photographs was difficult to be detected with the naked eye. We were unable to identify the features of shallow ACD using the Grad-CAM. The CycleGAN-based feature images showed that area around the macula and optic disk significantly contributed to the classification of fundus photographs with a shallow ACD. We demonstrated the feasibility of a novel deep learning model to detect a shallow ACD as a screening tool for ACG using fundus photographs. The CycleGAN-based feature map showed the hidden characteristic features of shallow ACD that were previously undetectable by conventional techniques and ophthalmologists. This framework will facilitate the early detection of shallow ACD to prevent overlooking the risks associated with ACG.", "keywords": ["shallow ACD", "ACD", "fundus photographs", "Shallow", "detect shallow ACD", "deep learning", "Patients with angle-closure", "angle-closure glaucoma", "painful attack", "photographs", "experience a painful", "fundus", "ACG", "deep learning model", "deep learning approach", "deep", "deep ACD", "deep ACD classes", "learning", "features"], "paper_title": "A deep learning approach for detection of shallow anterior chamber depth based on the hidden features of fundus photographs.", "last_updated": "2023/02/04"}, {"id": "0033505774", "domain": "Glaucoma (unspecified)", "model_name": "Wanichwecharungruang et al.", "publication_date": "2021/01/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33505774/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33505774", "task": null, "abstract": "The purpose of this study was to evaluate the diagnostic performance of deep learning (DL) anterior segment optical coherence tomography (AS-OCT) as a plateau iris prediction model. We used a cross-sectional study of the development and validation of the DL system. We conducted a collaboration between a referral eye center and an informative technology department. The study enrolled 179 eyes from 142 patients with primary angle closure disease (PACD). All patients had remaining appositional angle after iridotomy. Each eye was scanned in four quadrants for both AS-OCT and ultrasound biomicroscopy (UBM). A DL algorithm for plateau iris prediction of AS-OCT was developed from training datasets and was validated in test sets. Sensitivity, specificity, and area under the receiver operating characteristics curve (AUC-ROC) of the DL for predicting plateau iris were evaluated, using UBM as a reference standard. Total paired images of AS-OCT and UBM were from 716 quadrants. Plateau iris was observed with UBM in 276 (38.5%) quadrants. Trainings dataset with data augmentation were used to develop an algorithm from 2500 images, and the test set was validated from 160 images. AUC-ROC was 0.95 (95% confidence interval [CI] = 0.91 to 0.99), sensitivity was 87.9%, and specificity was 97.6%. DL revealed a high performance in predicting plateau iris on the noncontact AS-OCT images. This work could potentially assist clinicians in more practically detecting this nonpupillary block mechanism of PACD.", "keywords": ["anterior segment optical", "optical coherence tomography", "segment optical coherence", "deep learning", "anterior segment", "coherence tomography", "plateau iris", "evaluate the diagnostic", "segment optical", "optical coherence", "iris prediction model", "plateau", "plateau iris prediction", "iris", "UBM", "AS-OCT", "prediction model", "predicting plateau iris", "study", "iris prediction"], "paper_title": "Deep Learning for Anterior Segment Optical Coherence Tomography to Predict the Presence of Plateau Iris.", "last_updated": "2023/02/04"}, {"id": "0035951641", "domain": "Glaucoma (unspecified)", "model_name": "pytorch-pretrained-vit", "publication_date": "2022/08/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35951641/", "code_link": "https://github.com/lukemelas/pytorch-pretrained-vit", "model_type": "transformer", "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "35951641", "task": null, "abstract": "Anterior segment optical coherence tomography (AS-OCT) is a non-contact, rapid, and high-resolution in vivo modality for imaging of the eyeball's anterior segment structures. Because progressive anterior segment deformation is a hallmark of certain eye diseases such as angle-closure glaucoma, identification of AS-OCT structural changes over time is fundamental to their diagnosis and monitoring. Detection of pathologic damage, however, relies on the ability to differentiate it from normal, age-related structural changes. This proposed large-scale, retrospective cross-sectional study will determine whether demographic characteristics including age can be predicted from deep learning analysis of AS-OCT images; it will also assess the importance of specific anterior segment areas of the eyeball to the prediction. We plan to extract, from SUPREME\u00ae, a clinical data warehouse (CDW) of Seoul National University Hospital (SNUH; Seoul, South Korea), a list of patients (at least 2,000) who underwent AS-OCT imaging between 2008 and 2020. AS-OCT images as well as demographic characteristics including age, gender, height, weight and body mass index (BMI) will be collected from electronic medical records (EMRs). The dataset of horizontal AS-OCT images will be split into training (80%), validation (10%), and test (10%) datasets, and a Vision Transformer (ViT) model will be built to predict demographics. Gradient-weighted Class Activation Mapping (Grad-CAM) will be used to visualize the regions of AS-OCT images that contributed to the model's decisions. The accuracy, sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC) will be applied to evaluate the model performance. This paper presents a study protocol for prediction of demographic characteristics from AS-OCT images of the eyeball using a deep learning model. The results of this study will aid clinicians in understanding and identifying age-related structural changes and other demographics-based structural differences. Registration ID with open science framework: 10.17605/OSF.IO/FQ46X.", "keywords": ["optical coherence tomography", "segment optical coherence", "anterior segment structures", "Anterior segment optical", "Anterior segment", "eyeball anterior segment", "AS-OCT images", "progressive anterior segment", "anterior segment deformation", "coherence tomography", "optical coherence", "high-resolution in vivo", "vivo modality", "AS-OCT", "anterior segment areas", "segment structures", "specific anterior segment", "segment optical", "Seoul National University", "National University Hospital"], "paper_title": "Predicting demographic characteristics from anterior segment OCT images with deep learning: A study protocol.", "last_updated": "2023/02/04"}, {"id": "0036040250", "domain": "Glaucoma (unspecified)", "model_name": "Shon et al.", "publication_date": "2022/09/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36040250/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36040250", "task": null, "abstract": "To develop a variational autoencoder (VAE) suitable for analysis of the latent structure of anterior segment optical coherence tomography (AS-OCT) images and to investigate possibilities of latent structure analysis of the AS-OCT images. We retrospectively collected clinical data and AS-OCT images from 2111 eyes of 1261 participants from the ongoing Asan Glaucoma Progression Study. A specifically modified VAE was used to extract six symmetrical and one asymmetrical latent variable. A total of 1692 eyes of 1007 patients were used for training the model. Conventional measurements and latent variables were compared between 74 primary angle closure (PAC) and 51 primary angle closure glaucoma (PACG) eyes from validation set (419 eyes of 254 patients) that were not used for training. Among the symmetrical latent variables, the first three and the last demonstrated easily recognized features, anterior chamber area in \u03b71, curvature of the cornea in \u03b72, the pupil size in \u03b73 and corneal thickness in \u03b76, whereas \u03b74 and \u03b75 were more complex aggregating complex interactions of multiple structures. Compared with PAC eyes, there was no difference in any of the conventional measurements in PACG eyes. However, values of \u03b74 were significantly different between the two groups, being smaller in the PACG group (P = 0.015). VAE is a useful framework for analysis of the latent structure of AS-OCT. Latent structure analysis could be useful in capturing features not readily evident with conventional measures. This study suggested that a deep learning-based latent space model can be applied for the analysis of AS-OCT images to find latent characteristics of the anterior segment of the eye.", "keywords": ["optical coherence tomography", "Asan Glaucoma Progression", "segment optical coherence", "AS-OCT images", "Glaucoma Progression Study", "latent", "latent structure", "variational autoencoder", "coherence tomography", "latent structure analysis", "ongoing Asan Glaucoma", "develop a variational", "optical coherence", "investigate possibilities", "eyes", "images", "VAE", "AS-OCT", "Asan Glaucoma", "Glaucoma Progression"], "paper_title": "Development of Cumulative Order-Preserving Image Transformation Based Variational Autoencoder for Anterior Segment Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0035882767", "domain": "Glaucoma (unspecified)", "model_name": "Ye et al.", "publication_date": "2022/07/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35882767/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35882767", "task": null, "abstract": "The aim of this study was to investigate the feasibility of generating synthesized ultrasound biomicroscopy (UBM) images from swept-source anterior segment optical coherent tomography (SS-ASOCT) images using a cycle-consistent generative adversarial network framework (CycleGAN) for iridociliary assessment on a cohort presenting for primary angle-closure screening. The CycleGAN architecture was adopted to synthesize high-resolution UBM images trained on the SS-ASOCT dataset from the department of ophthalmology, Xinhua Hospital. The performance of the CycleGAN model was further tested in two separate datasets using synthetic UBM images from two different ASOCT modalities (in-distribution and out-of-distribution). We compared the ability of glaucoma specialists to assess the image quality of real and synthetic images. UBM measurements, including anterior chamber, iridociliary parameters, were compared between real and synthetic UBM images. Intra-class correlation coefficients, coefficients of variation, and Bland-Altman plots were used to assess the level of agreement. The Fr\u00e9chet Inception Distance (FID) was measured to evaluate the quality of the synthetic images. The whole trained dataset included anterior chamber angle images, of which 4037 were obtained by SS-ASOCT and 2206 were obtained by UBM. The image quality of real versus synthetic SS-ASOCT images was similar as assessed by two glaucoma specialists. The Bland-Altman analysis also suggested high consistency between measurements of real and synthetic UBM images. In addition, there was fair to excellent agreement between real and synthetic UBM measurements for the in-distribution dataset (ICC range 0.48-0.97) and the out-of-distribution dataset (ICC range 0.52-0.86). The FID was 21.3 and 24.1 for the synthetic UBM images from the in-distribution and out-of-distribution datasets, respectively. We developed a CycleGAN model to translate UBM images from non-contact SS-ASOCT images. The CycleGAN synthetic UBM images showed fair to excellent reproducibility when compared with real UBM images. Our results suggest that the CycleGAN technique is a promising tool to evaluate the iridociliary and anterior chamber in an alternative non-contact method.", "keywords": ["synthetic UBM images", "UBM images", "synthetic UBM", "UBM", "synthesized ultrasound biomicroscopy", "optical coherent tomography", "adversarial network framework", "primary angle-closure screening", "images", "generating synthesized ultrasound", "segment optical coherent", "cycle-consistent generative adversarial", "generative adversarial network", "real UBM images", "CycleGAN synthetic UBM", "high-resolution UBM images", "synthetic", "UBM images trained", "swept-source anterior segment", "anterior segment optical"], "paper_title": "Generating Synthesized Ultrasound Biomicroscopy Images from Anterior Segment Optical Coherent Tomography Images by Generative Adversarial Networks for Iridociliary Assessment.", "last_updated": "2023/02/04"}, {"id": "0036229338", "domain": "Cataract", "model_name": "Z\u00e9boulon et al.", "publication_date": "2022/10/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36229338/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36229338", "task": "IWqQC1koJA", "abstract": "The diagnosis of cataract is mostly clinical and there is a lack of objective and specific tool to detect and grade it automatically. The goal of this study was to develop and validate a deep learning model to detect and localize cataract on Swept Source Optical Coherance Tomography (SS-OCT) images. We trained a convolutional network to detect cataract at the pixel level from 504 SS-OCT images of clear lens and cataract patients. The model was then validated on 1326 different images of 114 patients. The output of the model is a map repreenting the probability of cataract for each pixel of the image. We calculated the Cataract Fraction (CF), defined as the number of pixel classified as \"cataract\" divided by the number of pixel representing the lens for each image. Receiver Operating Characteristic Curves were plotted. Area Under the Curve (ROC AUC) sensitivity and specitivity to detect cataract were calculated. In the validsation set, mean CF was 0.024\u00a0\u00b1\u00a00.077 and 0.479\u00a0\u00b1\u00a00.230 (p\u00a0<\u00a00.001). ROC AUC was 0.98 with an optimal CF threshold of 0.14. Using that threshold, sensitivity and specificity to detect cataract were 94.4% and 94.7%, respectively. We developed an automatic detection tool for cataract on SS-OCT images. Probability maps of cataract on the images provide an additional tool to help the physician in its diagnosis and surgical planning.", "keywords": ["Optical Coherance Tomography", "Swept Source Optical", "Source Optical Coherance", "detect cataract", "cataract", "Coherance Tomography", "Swept Source", "Source Optical", "Optical Coherance", "grade it automatically", "detect", "lack of objective", "objective and specific", "ROC AUC", "images", "Operating Characteristic Curves", "SS-OCT images", "pixel", "SS-OCT", "Cataract Fraction"], "paper_title": "Development and validation of a pixel wise deep learning model to detect cataract on swept-source optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0036143048", "domain": "Cataract", "model_name": "Kiuchi et al.", "publication_date": "2022/09/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36143048/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36143048", "task": null, "abstract": "An artificial intelligence-based system was implemented for preoperative safety management in cataract surgery, including facial recognition, laterality (right and left eye) confirmation, and intraocular lens (IOL) parameter verification. A deep-learning model was constructed with a face identification development kit for facial recognition, the You Only Look Once Version 3 (YOLOv3) algorithm for laterality confirmation, and the Visual Geometry Group-16 (VGG-16) for IOL parameter verification. In 171 patients who were undergoing phacoemulsification and IOL implantation, a mobile device (iPad mini, Apple Inc.) camera was used to capture patients' faces, location of surgical drape aperture, and IOL parameter descriptions on the packages, which were then checked with the information stored in the referral database. The authentication rates on the first attempt and after repeated attempts were 92.0% and 96.3% for facial recognition, 82.5% and 98.2% for laterality confirmation, and 67.4% and 88.9% for IOL parameter verification, respectively. After authentication, both the false rejection rate and the false acceptance rate were 0% for all three parameters. An artificial intelligence-based system for preoperative safety management was implemented in real cataract surgery with a passable authentication rate and very high accuracy.", "keywords": ["IOL parameter verification", "including facial recognition", "IOL parameter", "facial recognition", "parameter verification", "IOL parameter descriptions", "laterality confirmation", "IOL", "left eye", "intraocular lens", "including facial", "Visual Geometry", "artificial intelligence-based system", "preoperative safety management", "recognition", "verification", "parameter", "confirmation", "Apple Inc.", "IOL implantation"], "paper_title": "Deep Learning-Based System for Preoperative Safety Management in Cataract Surgery.", "last_updated": "2023/02/04"}, {"id": "0035128815", "domain": "Cataract", "model_name": "Langenbucher et al.", "publication_date": "2022/02/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35128815/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "35128815", "task": null, "abstract": "The prediction of anatomical axial intraocular lens position (ALP) is one of the major challenges in cataract surgery. The purpose of this study was to develop and test prediction algorithms for ALP based on deep learning strategies. We evaluated a large data set of 1345 biometric measurements from the IOLMaster 700 before and after cataract surgery. The target parameter was the intraocular lens (IOL) equator plane at half the distance between anterior and posterior apex. The relevant input parameters from preoperative biometry were extracted using a principal component analysis. A selection of neural network algorithms was tested using a 5-fold cross-validation procedure to avoid overfitting. The results were then compared with a traditional multilinear regression in terms of root mean squared prediction error (RMSE). Corneal radius of curvature, axial length, anterior chamber depth, corneal thickness, lens thickness and patient age were identified as effective predictive parameters, whereas pupil size, horizontal corneal diameter and Chang-Waring chord did not enhance the model. From the tested algorithms, the Gaussian prediction regression and the Support Vector Machine algorithms performed best (RMSE\u2009=\u20090.2805 and 0.2731\u2009mm), outperforming the multilinear prediction model (0.3379\u2009mm). The mean absolute prediction error yielded 0.1998, 0.1948 and 0.2415\u2009mm for the respective models. Modern prediction techniques may have the potential to outperform traditional multilinear regression techniques as they can deal easily with nonlinearities between input and output parameters. However, in all cases a cross-validation is mandatory to avoid overfitting and misinterpretation of the results.", "keywords": ["intraocular lens position", "major challenges", "anatomical axial intraocular", "cataract surgery", "ALP based", "prediction", "ALP", "lens position", "intraocular lens", "axial intraocular lens", "anatomical axial", "Support Vector Machine", "traditional multilinear regression", "Vector Machine algorithms", "algorithms", "test prediction algorithms", "challenges in cataract", "deep learning strategies", "Gaussian prediction regression", "lens"], "paper_title": "Prediction of the axial lens position after cataract surgery using deep learning algorithms and multilinear regression.", "last_updated": "2023/02/04"}, {"id": "0035165304", "domain": "Cataract", "model_name": "Touma et al.", "publication_date": "2022/02/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35165304/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "35165304", "task": "IWqQC1koJA", "abstract": "This study assessed the performance of automated machine learning (AutoML) in classifying cataract surgery phases from surgical videos. Two ophthalmology trainees without coding experience designed a deep learning model in Google Cloud AutoML Video Classification for the classification of 10 different cataract surgery phases. We used two open-access publicly available datasets (total of 122 surgeries) for model training, validation and testing. External validation was performed on 10 surgeries issued from another dataset. The AutoML model demonstrated excellent discriminating performance, even outperforming bespoke deep learning models handcrafter by experts. The area under the precision-recall curve was 0.855. At the 0.5 confidence threshold cut-off, the overall performance metrics were as follows: sensitivity (81.0%), recall (77.1%), accuracy (96.0%) and F1 score (0.79). The per-segment metrics varied across the surgical phases: precision 66.7-100%, recall 46.2-100% and specificity 94.1-100%. Hydrodissection and phacoemulsification were the most accurately predicted phases (100 and 92.31% correct predictions, respectively). During external validation, the average precision was 54.2% (0.00-90.0%), the recall was 61.1% (0.00-100%) and specificity was 96.2% (91.0-99.0%). In conclusion, a code-free AutoML model can accurately classify cataract surgery phases from videos with an accuracy comparable or better than models developed by experts.", "keywords": ["cataract surgery phases", "automated machine learning", "classifying cataract surgery", "Google Cloud AutoML", "cataract surgery", "Cloud AutoML Video", "AutoML Video Classification", "surgery phases", "study assessed", "automated machine", "Google Cloud", "machine learning", "classifying cataract", "classify cataract surgery", "Video Classification", "deep learning", "deep learning model", "surgery", "phases", "learning"], "paper_title": "Development of a code-free machine learning model for the classification of cataract surgery phases.", "last_updated": "2023/02/04"}, {"id": "0030951163", "domain": "Cataract", "model_name": "Yu et al.", "publication_date": "2019/04/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30951163/", "code_link": null, "model_type": "RNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "30951163", "task": null, "abstract": "Competence in cataract surgery is a public health necessity, and videos of cataract surgery are routinely available to educators and trainees but currently are of limited use in training. Machine learning and deep learning techniques can yield tools that efficiently segment videos of cataract surgery into constituent phases for subsequent automated skill assessment and feedback. To evaluate machine learning and deep learning algorithms for automated phase classification of manually presegmented phases in videos of cataract surgery. This was a cross-sectional study using a data set of videos from a convenience sample of 100 cataract procedures performed by faculty and trainee surgeons in an ophthalmology residency program from July 2011 to December 2017. Demographic characteristics for surgeons and patients were not captured. Ten standard labels in the procedure and 14 instruments used during surgery were manually annotated, which served as the ground truth. Five algorithms with different input data: (1) a support vector machine input with cross-sectional instrument label data; (2) a recurrent neural network (RNN) input with a time series of instrument labels; (3) a convolutional neural network (CNN) input with cross-sectional image data; (4) a CNN-RNN input with a time series of images; and (5) a CNN-RNN input with time series of images and instrument labels. Each algorithm was evaluated with 5-fold cross-validation. Accuracy, area under the receiver operating characteristic curve, sensitivity, specificity, and precision. Unweighted accuracy for the 5 algorithms ranged between 0.915 and 0.959. Area under the receiver operating characteristic curve for the 5 algorithms ranged between 0.712 and 0.773, with small differences among them. The area under the receiver operating characteristic curve for the image-only CNN-RNN (0.752) was significantly greater than that of the CNN with cross-sectional image data (0.712) (difference, -0.040; 95% CI, -0.049 to -0.033) and the CNN-RNN with images and instrument labels (0.737) (difference, 0.016; 95% CI, 0.014 to 0.018). While specificity was uniformly high for all phases with all 5 algorithms (range, 0.877 to 0.999), sensitivity ranged between 0.005 (95% CI, 0.000 to 0.015) for the support vector machine for wound closure (corneal hydration) and 0.974 (95% CI, 0.957 to 0.991) for the RNN for main incision. Precision ranged between 0.283 and 0.963. Time series modeling of instrument labels and video images using deep learning techniques may yield potentially useful tools for the automated detection of phases in cataract surgery procedures.", "keywords": ["public health necessity", "cataract surgery", "cataract", "deep learning", "surgery", "health necessity", "deep learning techniques", "public health", "cataract surgery procedures", "instrument labels", "learning", "input", "Machine learning", "time series", "receiver operating characteristic", "videos", "videos of cataract", "instrument", "operating characteristic curve", "deep learning algorithms"], "paper_title": "Assessment of Automated Identification of Phases in Videos of Cataract Surgery Using Machine Learning and Deep Learning Techniques.", "last_updated": "2023/02/04"}, {"id": "0035343651", "domain": "Cataract", "model_name": "Schwarzenbacher et al.", "publication_date": "2022/03/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35343651/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "35343651", "task": "aipbNdPTIt", "abstract": "To develop and validate a deep learning model to automatically segment three structures using an anterior segment optical coherence tomography (AS-OCT): The intraocular lens (IOL), the retrolental space (IOL to the posterior lens capsule) and Berger's space (BS; posterior capsule to the anterior hyaloid membrane). An artificial intelligence (AI) approach based on a deep learning model to automatically segment the IOL, the retrolental space, and BS in AS-OCT, was trained using annotations from an experienced clinician. The training, validation and test set consisted of 92 cross-sectional OCT slices, acquired in 47 visits from 41 eyes. Annotations from a second experienced clinician in the test set were additionally evaluated to conduct an inter-reader variability analysis. The AI model achieved a Precision/Recall/Dice score of 0.97/0.90/0.93 for IOL, 0.54/0.65/0.55 for retrolental space, and 0.72/0.58/0.59 for BS. For inter-reader variability, Precision/Recall/Dice values were 0.98/0.98/0.98 for IOL, 0.74/0.59/0.62 for retrolental space, and 0.58/0.57/0.57 for BS. No statistical differences were observed between the automated algorithm and the inter-reader variability for BS segmentation. The deep learning model allows for fully automatic segmentation of all investigated structures, achieving human-level performance in BS segmentation. We, therefore, expect promising applications of the algorithm with particular interest in BS in automated big data analysis and real-time intra-operative support in ophthalmology, particularly in conjunction with primary posterior capsulotomy in femtosecond laser-assisted cataract surgery.", "keywords": ["anterior hyaloid membrane", "deep learning model", "optical coherence tomography", "anterior segment optical", "segment optical coherence", "posterior lens capsule", "Berger space", "retrolental space", "deep learning", "lens capsule", "learning model", "automatically segment", "intraocular lens", "anterior hyaloid", "coherence tomography", "hyaloid membrane", "anterior segment", "develop and validate", "optical coherence", "IOL"], "paper_title": "Automatic segmentation of intraocular lens, the retrolental space and Berger's space using deep learning.", "last_updated": "2023/02/04"}, {"id": "0032271836", "domain": "Cataract", "model_name": "Yoo et al.", "publication_date": "2020/04/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32271836/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "32271836", "task": null, "abstract": "Wrong-site surgeries can occur due to the absence of an appropriate surgical time-out. However, during a time-out, surgical participants are unable to review the patient's charts due to their aseptic hands. To improve the conditions in surgical time-outs, we introduce a deep learning-based smart speaker to confirm the surgical information prior to cataract surgeries. This pilot study utilized the publicly available audio vocabulary dataset and recorded audio data published by the authors. The audio clips of the target words, such as left, right, cataract, phacoemulsification, and intraocular lens, were selected to determine and confirm surgical information in the time-out speech. A deep convolutional neural network model was trained and implemented in the smart speaker that was developed using a mini development board and commercial speakerphone. To validate our model in the consecutive speeches during time-outs, we generated 200 time-out speeches for cataract surgeries by randomly selecting the surgical statuses of the surgical participants. After the training process, the deep learning model achieved an accuracy of 96.3% for the validation dataset of short-word audio clips. Our deep learning-based smart speaker achieved an accuracy of 93.5% for the 200 time-out speeches. The surgical and procedural accuracy was 100%. Additionally, on validating the deep learning model by using web-generated time-out speeches and video clips for general surgery, the model exhibited a robust and good performance. In this pilot study, the proposed deep learning-based smart speaker was able to successfully confirm the surgical information during the time-out speech. Future studies should focus on collecting real-world time-out data and automatically connecting the device to electronic health records. Adopting smart speaker-assisted time-out phases will improve the patients' safety during cataract surgeries, particularly in relation to wrong-site surgeries.", "keywords": ["surgical", "time-out", "deep learning-based smart", "learning-based smart speaker", "surgical information", "smart speaker", "cataract surgeries", "deep", "occur due", "time-out speeches", "deep learning-based", "surgeries", "learning-based smart", "smart", "surgical participants", "confirm surgical information", "deep learning model", "model", "cataract", "surgical time-out"], "paper_title": "Deep learning-based smart speaker to confirm surgical sites for cataract surgeries: A pilot study.", "last_updated": "2023/02/04"}, {"id": "0034039562", "domain": "Cataract", "model_name": "Wang et al.", "publication_date": "2021/05/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34039562/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34039562", "task": null, "abstract": "To investigate the feasibility and accuracy of using machine learning (ML) techniques on self-reported questionnaire data to predict the 10-year risk of cataract surgery, and to identify meaningful predictors of cataract surgery in middle-aged and older Australians. Baseline information regarding demographic, socioeconomic, medical history and family history, lifestyle, dietary and self-rated health status were collected as risk factors. Cataract surgery events were confirmed by the Medicare Benefits Schedule Claims dataset. Three ML algorithms (random forests [RF], gradient boosting machine and deep learning) and one traditional regression algorithm (logistic model) were compared on the accuracy of their predictions for the risk of cataract surgery. The performance was assessed using 10-fold cross-validation. The main outcome measures were areas under the receiver operating characteristic curves (AUCs). In total, 207\u2009573 participants, aged 45 years and above without a history of cataract surgery at baseline, were recruited from the 45 and Up Study. The performance of gradient boosting machine (AUC 0.790, 95%\u2009CI 0.785 to 0.795), RF (AUC 0.785, 95%\u2009CI 0.780 to 0.790) and deep learning (AUC 0.781, 95%\u2009CI 0.775 to 61 0.786) were robust and outperformed the traditional logistic regression method (AUC 0.767, 95% CI 0.762 to 0.773, all p<0.05). Age, self-rated eye vision and health insurance were consistently identified as important predictors in all models. The study demonstrated that ML modelling was able to reasonably accurately predict the 10-year risk of cataract surgery based on questionnaire data alone and was marginally superior to the conventional logistic model.", "keywords": ["cataract surgery", "older Australians", "identify meaningful predictors", "Medicare Benefits Schedule", "Benefits Schedule Claims", "techniques on self-reported", "cataract", "investigate the feasibility", "identify meaningful", "middle-aged and older", "risk of cataract", "AUC", "Cataract surgery events", "surgery", "Schedule Claims dataset", "Australians", "cataract surgery based", "self-reported questionnaire data", "risk", "Medicare Benefits"], "paper_title": "Predicting the 10-year risk of cataract surgery using machine learning techniques on questionnaire data: findings from the 45 and Up Study.", "last_updated": "2023/02/04"}, {"id": "0036947046", "domain": "Cataract", "model_name": "Yeh et al.", "publication_date": "2023/03/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36947046/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36947046", "task": null, "abstract": "The purpose of this study was to build a deep-learning model that automatically analyzes cataract surgical videos for the locations of surgical landmarks, and to derive skill-related motion metrics. The locations of the pupil, limbus, and 8 classes of surgical instruments were identified by a 2-step algorithm: (1) mask segmentation and (2) landmark identification from the masks. To perform mask segmentation, we trained the YOLACT model on 1156 frames sampled from 268 videos and the public Cataract Dataset for Image Segmentation (CaDIS) dataset. Landmark identification was performed by fitting ellipses or lines to the contours of the masks and deriving locations of interest, including surgical tooltips and the pupil center. Landmark identification was evaluated by the distance between the predicted and true positions in 5853 frames of 10 phacoemulsification video clips. We derived the total path length, maximal speed, and covered area using the tip positions and examined the correlation with human-rated surgical performance. The mean average precision score and intersection-over-union for mask detection were 0.78 and 0.82. The average distance between the predicted and true positions of the pupil center, phaco tip, and second instrument tip was 5.8, 9.1, and 17.1 pixels. The total path length and covered areas of these landmarks were negatively correlated with surgical performance. We developed a deep-learning method to localize key anatomical portions of the eye and cataract surgical tools, which can be used to automatically derive metrics correlated with surgical skill. Our system could form the basis of an automated feedback system that helps cataract surgeons evaluate their performance.", "keywords": ["derive skill-related motion", "skill-related motion metrics", "analyzes cataract surgical", "skill-related motion", "surgical", "mask segmentation", "landmark identification", "automatically analyzes cataract", "cataract surgical videos", "perform mask segmentation", "cataract surgical", "Image Segmentation", "analyzes cataract", "surgical performance", "locations", "segmentation", "cataract", "public Cataract Dataset", "pupil center", "YOLACT model"], "paper_title": "PhacoTrainer: Deep Learning for Cataract Surgical Videos to Track Surgical Tools.", "last_updated": "2023/02/04"}, {"id": "0034003903", "domain": "Cataract", "model_name": "Obana et al.", "publication_date": "2021/06/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34003903/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34003903", "task": null, "abstract": "Measurements of macular pigment optical density (MPOD) by the autofluorescence technique yield underestimations of actual values in eyes with cataract. We applied deep learning (DL) to correct this error. MPOD was measured by SPECTRALIS (Heidelberg Engineering, Heidelberg, Germany) in 197 eyes before and after cataract surgery. The nominal MPOD values (= preoperative value) were corrected by three methods: the regression equation (RE) method, subjective classification (SC) method (described in our previous study), and DL method. The errors between the corrected and true values (= postoperative value) were calculated for local MPODs at 0.25\u00b0, 0.5\u00b0, 1\u00b0, and 2\u00b0 eccentricities and macular pigment optical volume (MPOV) within 9\u00b0 eccentricity. The mean error for MPODs at four eccentricities was 32% without any correction, 15% with correction by RE, 16% with correction by SC, and 14% with correction by DL. The mean error for MPOV was 21% without correction and 14%, 10%, and 10%, respectively, with correction by the same methods. The errors with any correction were significantly lower than those without correction (P < 0.001, linear mixed model with Tukey's test). The errors with DL correction were significantly lower than those with RE correction in MPOD at 1\u00b0 eccentricity and MPOV (P < 0.001) and were equivalent to those with SC correction. The objective method using DL was useful to correct MPOD values measured in aged people. MPOD can be obtained with small errors in eyes with cataract using DL.", "keywords": ["autofluorescence technique yield", "technique yield underestimations", "correction", "MPOD", "autofluorescence technique", "technique yield", "yield underestimations", "underestimations of actual", "Heidelberg Engineering", "pigment optical density", "macular pigment optical", "MPOV", "errors", "Heidelberg", "optical density", "eyes", "MPODs", "macular pigment", "pigment optical", "cataract"], "paper_title": "Correction for the Influence of Cataract on Macular Pigment Measurement by Autofluorescence Technique Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0034124042", "domain": "Cataract", "model_name": "Wei et al.", "publication_date": "2021/05/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34124042/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34124042", "task": null, "abstract": "Due to complicated and variable fundus status of highly myopic eyes, their visual benefit from cataract surgery remains hard to be determined preoperatively. We therefore aimed to develop an optical coherence tomography (OCT)-based deep learning algorithms to predict the postoperative visual acuity of highly myopic eyes after cataract surgery. The internal dataset consisted of 1,415 highly myopic eyes having cataract surgeries in our hospital. Another external dataset consisted of 161 highly myopic eyes from Heping Eye Hospital. Preoperative macular OCT images were set as the only feature. The best corrected visual acuity (BCVA) at 4 weeks after surgery was set as the ground truth. Five different deep learning algorithms, namely ResNet-18, ResNet-34, ResNet-50, ResNet-101, and Inception-v3, were used to develop the model aiming at predicting the postoperative BCVA, and an ensemble learning was further developed. The model was further evaluated in the internal and external test datasets. The ensemble learning showed the lowest mean absolute error (MAE) of 0.1566 logMAR and the lowest root mean square error (RMSE) of 0.2433 logMAR in the validation dataset. Promising outcomes in the internal and external test datasets were revealed with MAEs of 0.1524 and 0.1602 logMAR and RMSEs of 0.2612 and 0.2020 logMAR, respectively. Considerable sensitivity and precision were achieved in the BCVA < 0.30 logMAR group, with 90.32 and 75.34% in the internal test dataset and 81.75 and 89.60% in the external test dataset, respectively. The percentages of the prediction errors within \u00b1 0.30 logMAR were 89.01% in the internal and 88.82% in the external test dataset. Promising prediction outcomes of postoperative BCVA were achieved by the novel OCT-trained deep learning model, which will be helpful for the surgical planning of highly myopic cataract patients.", "keywords": ["highly myopic eyes", "highly myopic", "variable fundus status", "myopic eyes", "surgery remains hard", "highly myopic cataract", "cataract surgery remains", "Heping Eye Hospital", "external test datasets", "external test", "highly", "myopic", "Due to complicated", "determined preoperatively", "cataract surgery", "complicated and variable", "variable fundus", "fundus status", "remains hard", "test dataset"], "paper_title": "An Optical Coherence Tomography-Based Deep Learning Algorithm for Visual Acuity Prediction of Highly Myopic Eyes After Cataract Surgery.", "last_updated": "2023/02/04"}, {"id": "0018361999", "domain": "Cataract", "model_name": "Fontana et al.", "publication_date": "2008/04/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/18361999/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "18361999", "task": null, "abstract": "", "keywords": [], "paper_title": "Experimental model for learning and practicing big-bubble deep anterior lamellar keratoplasty.", "last_updated": "2023/02/04"}, {"id": "0031481391", "domain": "Cataract", "model_name": "Ting et al.", "publication_date": "2019/09/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31481391/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "31481391", "task": null, "abstract": "", "keywords": [], "paper_title": "Artificial intelligence-assisted telemedicine platform for cataract screening and management: a potential model of care for global eye health.", "last_updated": "2023/02/04"}, {"id": "0035528343", "domain": "Glaucoma (unspecified)", "model_name": "Khan et al.", "publication_date": "2022/04/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35528343/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35528343", "task": null, "abstract": "It can be challenging for doctors to identify eye disorders early enough using fundus pictures. Diagnosing ocular illnesses by hand is time-consuming, error-prone, and complicated. Therefore, an automated ocular disease detection system with computer-aided tools is necessary to detect various eye disorders using fundus pictures. Such a system is now possible as a consequence of deep learning algorithms that have improved image classification capabilities. A deep-learning-based approach to targeted ocular detection is presented in this study. For this study, we used state-of-the-art image classification algorithms, such as VGG-19, to classify the ODIR dataset, which contains 5000 images of eight different classes of the fundus. These classes represent different ocular diseases. However, the dataset within these classes is highly unbalanced. To resolve this issue, the work suggested converting this multiclass classification problem into a binary classification problem and taking the same number of images for both classifications. Then, the binary classifications were trained with VGG-19. The accuracy of the VGG-19 model was 98.13% for the normal (N) versus pathological myopia (M) class; the model reached an accuracy of 94.03% for normal (N) versus cataract (C), and the model provided an accuracy of 90.94% for normal (N) versus glaucoma (G). All of the other models also improve the accuracy when the data is balanced.", "keywords": ["eye disorders early", "identify eye disorders", "challenging for doctors", "doctors to identify", "fundus pictures", "eye disorders", "identify eye", "disorders early", "ocular", "classification", "fundus", "accuracy", "image classification", "pictures", "Diagnosing ocular illnesses", "normal", "eye", "disorders", "classes", "versus"], "paper_title": "Deep Learning for Ocular Disease Recognition: An Inner-Class Balance.", "last_updated": "2023/02/04"}, {"id": "0034621980", "domain": "Cataract", "model_name": "Shin et al.", "publication_date": "2021/04/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34621980/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34621980", "task": "aipbNdPTIt", "abstract": "The overarching goal of this work is to demonstrate the feasibility of using optical coherence tomography (OCT) to guide a robotic system to extract lens fragments from <i>ex vivo</i> pig eyes. A convolutional neural network (CNN) was developed to semantically segment four intraocular structures (lens material, capsule, cornea, and iris) from OCT images. The neural network was trained on images from ten pig eyes, validated on images from eight different eyes, and tested on images from another ten eyes. This segmentation algorithm was incorporated into the Intraocular Robotic Interventional Surgical System (IRISS) to realize semi-automated detection and extraction of lens material. To demonstrate the system, the semi-automated detection and extraction task was performed on seven separate <i>ex vivo</i> pig eyes. The developed neural network exhibited 78.20% for the validation set and 83.89% for the test set in mean intersection over union metrics. Successful implementation and efficacy of the developed method were confirmed by comparing the preoperative and postoperative OCT volume scans from the seven experiments.", "keywords": ["optical coherence tomography", "extract lens fragments", "pig eyes", "coherence tomography", "Intraocular Robotic Interventional", "Robotic Interventional Surgical", "overarching goal", "optical coherence", "neural network", "OCT images", "eyes", "ten pig eyes", "Interventional Surgical System", "lens material", "extract lens", "lens fragments", "OCT", "images", "convolutional neural network", "demonstrate the feasibility"], "paper_title": "Semi-Automated Extraction of Lens Fragments via a Surgical Robot Using Semantic Segmentation of OCT Images with Deep Learning - Experimental Results in <i>ex vivo</i> Animal Model.", "last_updated": "2023/02/04"}, {"id": "0034726283", "domain": "Cataract", "model_name": "Langenbucher et al.", "publication_date": "2021/11/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34726283/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34726283", "task": null, "abstract": "The corneal back surface is known to add some against the rule astigmatism, with implications in cataract surgery with toric lens implantation. This study aimed to set up and validate a deep learning algorithm to predict corneal back surface power from the corneal front surface power and biometric measures. This study was based on a large dataset of IOLMaster 700\u00a0measurements from two clinical centres. N\u00a0=\u00a019,553\u00a0measurements of 19,553 eyes with valid corneal front (CFSPM) and back surface power (CBSPM) data and other biometric measures. After a vector decomposition of CFSPM and CBSPM into equivalent power and projections of astigmatism to the 0\u00b0/90\u00b0 and 45\u00b0/135\u00b0 axes, a multi-output feedforward neural network was derived to predict vector components of CBSPM from CFSPM and other measurements. The predictions were compared with a multivariate linear regression model based on CFSPM components only. After pre-conditioning, a network with two hidden layers each having 12 neurons was derived. The dataset was split into training (70%), validation (15%) and test (15%) subsets. The prediction error (predicted corneal back surface power CBSPP - CBSPM) of the network after training and crossvalidation showed no systematic offset, narrower distributions for CBSPP - CBSPM and no trend error of CBSPP - CBSPM vs. CBSPM for any of the vector components. The multivariate linear model also showed no systematic offset, but broader distributions of the prediction error components and a systematic trend of all vector components vs. CFSPM components. The neural network approach based on CFSPM vector components and other biometric measures outperforms the multivariate linear model in predicting corneal back surface power vector components. Modern biometers can supply all parameters required for this algorithm, enabling reliable predictions for corneal back surface data where direct corneal back surface data are unavailable.", "keywords": ["corneal back surface", "back surface power", "back surface", "toric lens implantation", "corneal back", "surface power", "CBSPM", "CFSPM", "surface", "back", "corneal", "back surface data", "components", "lens implantation", "power", "implications in cataract", "cataract surgery", "surgery with toric", "toric lens", "surface power CBSPP"], "paper_title": "Prediction of corneal back surface power - Deep learning algorithm versus multivariate regression.", "last_updated": "2023/02/04"}, {"id": "0033864969", "domain": "Cataract", "model_name": "Grammatikopoulou et al.", "publication_date": "2021/03/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33864969/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33864969", "task": "aipbNdPTIt", "abstract": "Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labelled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos complementing the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts-semantic-segmentation2020.grand-challenge.org/.", "keywords": ["main sensory cue", "cue for surgeons", "wealth of information", "main sensory", "sensory cue", "semantic segmentation", "surgical procedures", "semantic", "Video feedback", "segmentation", "surgical", "dataset", "Deep learning", "computer assisted interventions", "advanced semantic segmentation", "semantic segmentation techniques", "surgeons", "information about surgical", "CAI", "feedback"], "paper_title": "CaDIS: Cataract dataset for surgical RGB-image segmentation.", "last_updated": "2023/02/04"}, {"id": "0035343267", "domain": "Cataract", "model_name": "Reddy et al.", "publication_date": "2022/03/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35343267/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "35343267", "task": "0GPcU42tzV", "abstract": "To investigate a method to identification of early progression of keratoconus using deep learning neural networks. Retrospective evaluation of medical records of patients with progressive keratoconus and had more than one followup visits. Images extracted from the single scheimplug analyzer for analysis were captured during the patient visits. The baseline progression of keratoconus is detected by a change in flat or steep K of \u22651.0D which is labeled as keratometric progression (KP) and progression detected by image based deep learning convolutional neural network (CNN) models, is labeled as latent progression (LP). Patient data consisted of model data (385 eyes of 351patients) to train and test the learning models and prediction data (1331 eyes of 828 patients) to determine the LP based on the learning models. The LP prediction model was able to identify progression at a mean of 11.1 months earlier than KP (p\u2009<\u20090.001). LP prediction model was able to identify progression earlier than KP irrespective of age category, gender, the severity of keratoconus, presenting visual acuity, astigmatism, and spherical equivalent (P\u2009<\u20090.001). When compared to the first visit the corrected distance visual acuity was more stable in 71% of the eyes at LP prediction visit compared to 50% at KP visit (p\u2009<\u20090.001). Through this study, we propose a possible solution to address the shortcomings noted in the current approaches of detecting progression relying only on KP. Avoiding bias towards feature selection from tomography images as done in the current study aids in identifying very subtle changes on the images between visits.", "keywords": ["investigate a method", "method to identification", "identification of early", "progression", "learning neural networks", "keratoconus", "deep learning", "learning models", "deep learning neural", "learning", "prediction model", "early progression", "prediction", "based deep learning", "models", "model", "convolutional neural network", "Images", "identify progression", "neural networks"], "paper_title": "KEDOP: Keratoconus early detection of progression using tomography images.", "last_updated": "2023/02/04"}, {"id": "0036451164", "domain": "Cataract", "model_name": "Fang et al.", "publication_date": "2022/11/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36451164/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36451164", "task": null, "abstract": "Surgical video phase recognition is an essential technique in computer-assisted surgical systems for monitoring surgical procedures, which can assist surgeons in standardizing procedures and enhancing postsurgical assessment and indexing. However, the high similarity between the phases and temporal variations of cataract videos still poses the greatest challenge for video phase recognition. In this paper, we introduce a global-local multi-stage temporal convolutional network (GL-MSTCN) to explore the subtle differences between high similarity surgical phases and mitigate the temporal variations of surgical videos. The presented work consists of a triple-stream network (i.e., pupil stream, instrument stream, and video frame stream) and a multi-stage temporal convolutional network. The triple-stream network first detects the pupil and surgical instruments regions in the frame separately and then obtains the fine-grained semantic features of the video frames. The proposed multi-stage temporal convolutional network improves the surgical phase recognition performance by capturing longer time series features through dilated convolutional layers with varying receptive fields. Our method is thoroughly validated on the CSVideo dataset with 32 cataract surgery videos and the public Cataract101 dataset with 101 cataract surgery videos, outperforming state-of-the-art approaches with 95.8% and 96.5% accuracy, respectively. The experimental results show that the use of global and local feature information can effectively enhance the model to explore fine-grained features and mitigate temporal and spatial variations, thus improving the surgical phase recognition performance of the proposed GL-MSTCN.", "keywords": ["enhancing postsurgical assessment", "monitoring surgical procedures", "video phase recognition", "temporal convolutional network", "surgical phase recognition", "computer-assisted surgical systems", "multi-stage temporal convolutional", "phase recognition", "phase recognition performance", "Surgical video phase", "standardizing procedures", "video phase", "temporal convolutional", "similarity surgical phases", "assessment and indexing", "surgical phase", "multi-stage temporal", "Surgical", "essential technique", "technique in computer-assisted"], "paper_title": "Global-local multi-stage temporal convolutional network for cataract surgery phase recognition.", "last_updated": "2023/02/04"}, {"id": "0031059460", "domain": "Cataract", "model_name": "Xu et al.", "publication_date": "2019/05/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31059460/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "31059460", "task": null, "abstract": "Cataract is one of the most serious eye diseases leading to blindness. Early detection and treatment can reduce the rate of blindness in cataract patients. However, the professional knowledge of ophthalmologists is necessary for the clinical cataract detection. Therefore, the potential costs may make it difficult for the widespread use of cataract detection to prevent blindness. Artificial intelligence assisted diagnosis based on medical images has attracted more and more attention of researchers. Many studies have focused on the use of pre-defined feature sets for cataract classification, but the predefined feature sets may be incomplete or redundant. On account of the aforementioned issues, some studies have proposed deep learning methods to automatically extract image features, but all based on global features and none has analyzed the layer-by-layer transformation process of the middle-tier features. This paper uses convolutional neural networks (CNN) to learn useful features directly from input data, and deconvolution network method is employed to investigate how CNN characterizes cataract layer-by-layer. We found that compared to the global feature set, the detail vascular information, which is lost after multi-layer convolution calculation also plays an important role in cataract grading task. And this finding fits with the morphological definition of fundus image. Through the finding, we gained insights into the design of hybrid global-local feature representation model to improve the recognition performance of automatic cataract grading.", "keywords": ["eye diseases leading", "eye diseases", "diseases leading", "Cataract", "cataract detection", "blindness", "clinical cataract detection", "feature", "features", "feature sets", "cataract grading", "detection", "CNN characterizes cataract", "leading to blindness", "cataract patients", "Early detection", "CNN", "global feature set", "eye", "diseases"], "paper_title": "A Hybrid Global-Local Representation CNN Model for Automatic Cataract Grading.", "last_updated": "2023/02/04"}, {"id": "0034244880", "domain": "Cataract", "model_name": "Chen et al.", "publication_date": "2021/07/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34244880/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34244880", "task": "IWqQC1koJA", "abstract": "The purpose of this study was to detect the presence of retinitis pigmentosa (RP) based on color fundus photographs using a deep learning model. A total of 1670 color fundus photographs from the Taiwan inherited retinal degeneration project and National Taiwan University Hospital were acquired and preprocessed. The fundus photographs were labeled RP or normal and divided into training and validation datasets (n\u2009=\u20091284) and a test dataset (n\u2009=\u2009386). Three transfer learning models based on pre-trained Inception V3, Inception Resnet V2, and Xception deep learning architectures, respectively, were developed to classify the presence of RP on fundus images. The model sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve were compared. The results from the best transfer learning model were compared with the reading results of two general ophthalmologists, one retinal specialist, and one specialist in retina and inherited retinal degenerations. A total of 935 RP and 324 normal images were used to train the models. The test dataset consisted of 193 RP and 193 normal images. Among the three transfer learning models evaluated, the Xception model had the best performance, achieving an AUROC of 96.74%. Gradient-weighted class activation mapping indicated that the contrast between the periphery and the macula on fundus photographs was an important feature in detecting RP. False-positive results were mostly obtained in cases of high myopia with highly tessellated retina, and false-negative results were mostly obtained in cases of unclear media, such as cataract, that led to a decrease in the contrast between the peripheral retina and the macula. Our model demonstrated the highest accuracy of 96.00%, which was comparable with the average results of 81.50%, of the other four ophthalmologists. Moreover, the accuracy was obtained at the same level of sensitivity (95.71%), as compared to an inherited retinal disease specialist. RP is an important disease, but its early and precise diagnosis is challenging. We developed and evaluated a transfer-learning-based model to detect RP from color fundus photographs. The results of this study validate the utility of deep learning in automating the identification of RP from fundus photographs.", "keywords": ["National Taiwan University", "Taiwan University Hospital", "color fundus photographs", "fundus photographs", "color fundus", "transfer learning models", "fundus", "National Taiwan", "Taiwan University", "Taiwan inherited retinal", "University Hospital", "deep learning", "retinitis pigmentosa", "transfer learning", "learning", "Xception deep learning", "learning models based", "deep learning model", "model", "learning model"], "paper_title": "Artificial Intelligence-Assisted Early Detection of Retinitis Pigmentosa - the Most Common Inherited Retinal Degeneration.", "last_updated": "2023/02/04"}, {"id": "0034752858", "domain": "Cataract", "model_name": "GLA-Net", "publication_date": "2021/11/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34752858/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34752858", "task": "IWqQC1koJA", "abstract": "Cataracts are the most crucial cause of blindness among all ophthalmic diseases. Convenient and cost-effective early cataract screening is urgently needed to reduce the risks of visual loss. To date, many studies have investigated automatic cataract classification based on fundus images. However, existing methods mainly rely on global image information while ignoring various local and subtle features. Notably, these local features are highly helpful for the identification of cataracts with different severities. To avoid this disadvantage, we introduce a deep learning technique to learn multilevel feature representations of the fundus image simultaneously. Specifically, a global-local attention network (GLA-Net) is proposed to handle the cataract classification task, which consists of two levels of subnets: the global-level attention subnet pays attention to the global structure information of the fundus image, while the local-level attention subnet focuses on the local discriminative features of the specific regions. These two types of subnets extract retinal features at different attention levels, which are then combined for final cataract classification. Our GLA-Net achieves the best performance in all metrics (90.65% detection accuracy, 83.47% grading accuracy, and 81.11% classification accuracy of grades 1 and 2). The experimental results on a real clinical dataset show that the combination of global-level and local-level attention models is effective for cataract screening and provides significant potential for other medical tasks.", "keywords": ["ophthalmic diseases", "cataract classification", "cataract", "attention", "fundus image", "classification", "features", "cataract screening", "cost-effective early cataract", "image", "fundus", "attention subnet", "cataract classification task", "early cataract screening", "local", "automatic cataract classification", "cataract classification based", "diseases", "local-level attention", "Cataracts"], "paper_title": "GLA-Net: A global-local attention network for automatic cataract classification.", "last_updated": "2023/02/04"}, {"id": "0036187623", "domain": "Cataract", "model_name": "Zhang et al.", "publication_date": "2022/09/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36187623/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36187623", "task": null, "abstract": "This study aimed to develop a deep learning model to generate a postoperative corneal axial curvature map of femtosecond laser arcuate keratotomy (FLAK) based on corneal tomography using a pix2pix conditional generative adversarial network (pix2pix cGAN) for surgical planning. A total of 451 eyes of 318 nonconsecutive patients were subjected to FLAK for corneal astigmatism correction during cataract surgery. Paired or single anterior penetrating FLAKs were performed at an 8.0-mm optical zone with a depth of 90% using a femtosecond laser (LenSx laser, Alcon Laboratories, Inc.). Corneal tomography images were acquired from Oculus Pentacam HR (Optikger\u00e4te GmbH, Wetzlar, Germany) before and 3 months after the surgery. The raw data required for analysis consisted of the anterior corneal curvature for a range of \u00b1 3.5 mm around the corneal apex in 0.1-mm steps, which the pseudo-color corneal curvature map synthesized was based on. The deep learning model used was a pix2pix conditional generative adversarial network. The prediction accuracy of synthetic postoperative corneal astigmatism in zones of different diameters centered on the corneal apex was assessed using vector analysis. The synthetic postoperative corneal axial curvature maps were compared with the real postoperative corneal axial curvature maps using the structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR). A total of 386 pairs of preoperative and postoperative corneal tomography data were included in the training set, whereas 65 preoperative data were retrospectively included in the test set. The correlation coefficient between synthetic and real postoperative astigmatism (difference vector) in the 3-mm zone was 0.89, and that between surgically induced astigmatism (SIA) was 0.93. The mean absolute errors of SIA for real and synthetic postoperative corneal axial curvature maps in the 1-, 3-, and 5-mm zone were 0.20 \u00b1 0.25, 0.12 \u00b1 0.17, and 0.09 \u00b1 0.13 diopters, respectively. The average SSIM and PSNR of the 3-mm zone were 0.86 \u00b1 0.04 and 18.24 \u00b1 5.78, respectively. Our results showed that the application of pix2pix cGAN can synthesize plausible postoperative corneal tomography for FLAK, showing the possibility of using GAN to predict corneal tomography, with the potential of applying artificial intelligence to construct surgical planning models.", "keywords": ["corneal axial curvature", "postoperative corneal axial", "postoperative corneal", "laser arcuate keratotomy", "corneal", "synthetic postoperative corneal", "corneal tomography", "postoperative corneal tomography", "axial curvature maps", "corneal axial", "femtosecond laser arcuate", "axial curvature", "postoperative", "corneal curvature map", "corneal curvature", "arcuate keratotomy", "curvature maps", "study aimed", "aimed to develop", "curvature"], "paper_title": "Prediction of corneal astigmatism based on corneal tomography after femtosecond laser arcuate keratotomy using a pix2pix conditional generative adversarial network.", "last_updated": "2023/02/04"}, {"id": "0036233565", "domain": "Cataract", "model_name": "Jing et al.", "publication_date": "2022/09/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36233565/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36233565", "task": null, "abstract": "This study aimed to evaluate the change patterns in corneal intrinsic aberrations and nerve density after cataract surgery in dry eye disease. The preoperative, 1- and 3-month postoperative dry eye-related parameters were obtained by the Oculus keratograph and the ocular surface disease index questionnaire. The corneal intrinsic aberrations were measured using the Pentacam HR system. In vivo confocal microscopy was performed to observe the vortical and peripheral corneal nerves. An artificial intelligence technique run by the deep learning model generated the corneal nerve parameters. Corneal aberrations on the anterior and total corneal surfaces were significantly increased at 1 month compared with the baseline (p < 0.05) but gradually returned to the baseline by 3 months (p > 0.05). However, the change in posterior corneal aberration lasted up to 3 months (p < 0.05). There was a significant decrease in the corneal vortical nerve maximum length and average density after the operation (p < 0.05), and this damage lasted approximately 3 months. The corneal vortical nerve maximum length and average density were negatively correlated with the anterior corneal surface aberrations before and 1 month after the operation (correlation coefficients, CC = \u22120.26, \u22120.25, \u22120.28; all p < 0.05). Corneal vortex provided a unique site to observe long-term corneal nerve injury related to eye dryness. The continuous damage to the corneal vortical nerve may be due to the continuous dry eye state.", "keywords": ["corneal", "corneal vortical nerve", "corneal intrinsic aberrations", "corneal vortical", "study aimed", "aimed to evaluate", "cataract surgery", "corneal intrinsic", "nerve", "vortical nerve", "corneal nerve", "dry eye disease", "intrinsic aberrations", "vortical nerve maximum", "corneal nerve parameters", "dry eye", "corneal surface aberrations", "dry", "aberrations", "vortical"], "paper_title": "Change Patterns in Corneal Intrinsic Aberrations and Nerve Density after Cataract Surgery in Patients with Dry Eye Disease.", "last_updated": "2023/02/04"}, {"id": "0031222558", "domain": "Cataract", "model_name": "Filice et al.", "publication_date": "2020/10/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31222558/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "31222558", "task": null, "abstract": "Exposure of the lenses to direct ionizing radiation during computed tomography (CT) examinations predisposes patients to cataract formation and should be avoided when possible. Avoiding such exposure requires positioning and other maneuvers by technologists that can be challenging. Continuous feedback has been shown to sustain quality improvement and can remind and encourage technologists to comply with these methods. Previously, for use cases such as this, cumbersome manual techniques were required for such feedback. Modern deep learning methods utilizing convolutional neural networks (CNNs) can be used to develop models that can detect lenses in CT examinations. These models can then be used to facilitate automatic and continuous feedback to sustain technologist performance for this task, thus contributing to higher quality patient care. This continuous evaluation for quality purposes also surfaces other operational or process-based challenges that can be addressed. Given high-performance characteristics, these models could also be used for other tasks such as population health research.", "keywords": ["direct ionizing radiation", "computed tomography", "direct ionizing", "ionizing radiation", "radiation during computed", "cataract formation", "examinations predisposes patients", "exposure requires positioning", "examinations predisposes", "predisposes patients", "exposure requires", "Continuous feedback", "Exposure", "feedback", "tomography", "Continuous", "models", "lenses to direct", "quality", "direct"], "paper_title": "Lens Identification to Prevent Radiation-Induced Cataracts Using Convolutional Neural Networks.", "last_updated": "2023/02/04"}, {"id": "0029184668", "domain": "Cataract", "model_name": "ncbi\" aria-label=\"github\">", "publication_date": "2017/09/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29184668/", "code_link": "https://github.com/ncbi\" aria-label=\"github\">", "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "3kPkWQNPZH", "pmid": "29184668", "task": "IWqQC1koJA", "abstract": "Computer-assisted interventions (CAI) aim to increase the effectiveness, precision and repeatability of procedures to improve surgical outcomes. The presence and motion of surgical tools is a key information input for CAI surgical phase recognition algorithms. Vision-based tool detection and recognition approaches are an attractive solution and can be designed to take advantage of the powerful deep learning paradigm that is rapidly advancing image recognition and classification. The challenge for such algorithms is the availability and quality of labelled data used for training. In this Letter, surgical simulation is used to train tool detection and segmentation based on deep convolutional neural networks and generative adversarial networks. The authors experiment with two network architectures for image segmentation in tool classes commonly encountered during cataract surgery. A commercially-available simulator is used to create a simulated cataract dataset for training models prior to performing transfer learning on real surgical data. To the best of authors' knowledge, this is the first attempt to train deep learning models for surgical instrument detection on simulated data while demonstrating promising results to generalise on real data. Results indicate that simulated data does have some potential for training advanced classification methods for CAI systems.", "keywords": ["improve surgical outcomes", "Computer-assisted interventions", "aim to increase", "increase the effectiveness", "precision and repeatability", "repeatability of procedures", "procedures to improve", "CAI", "CAI surgical phase", "surgical", "CAI surgical", "surgical outcomes", "data", "CAI systems", "improve surgical", "input for CAI", "tool detection", "recognition", "tool", "detection"], "paper_title": "Can surgical simulation be used to train detection and classification of neural networks?", "last_updated": "2023/02/04"}, {"id": "0034850585", "domain": "Cataract", "model_name": "Langenbucher et al.", "publication_date": "2021/12/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34850585/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34850585", "task": null, "abstract": "The angles alpha and kappa are widely discussed for centring refractive procedures, but they cannot be determined with ophthalmic instruments. The purpose of this study is to investigate the Chang-Waring chord (position of the Purkinje reflex PI relative to the corneal centre) derived from an optical biometer before and after cataract surgery and to study the changes resulting from cataract surgery. The analysis was based on a large dataset of 1587 complete sets of preoperative and postoperative IOMaster 700 biometry measurements from two clinical centres, each containing: valid data for pupil and corneal centre position, the position of the Purkinje reflex PI originated from a coaxial fixation target, keratometry (K), axial length (AL), anterior chamber depth (ACD), lens thickness (LT), central corneal thickness CCT, and horizontal corneal diameter W2W. The Chang-Waring chord CW was derived from pupil centre and Purkinje reflex PI analysed preoperatively and postoperatively, and a multilinear regression model together with a feedforward neural network algorithm was set up to predict postoperative CW chord from preoperative CW chord, K and biometric distances of the eye. The Y component of CW chord shows a slight shift in the inferior direction in both left and right eyes, before and after cataract surgery. The X component shows some shift in the temporal direction, which is more pronounced preoperatively and slightly reduced postoperatively but with a larger variation. The change in CW chord from preoperative to postoperative shows a slight shift in the superior and nasal directions. Our algorithms for prediction of postoperative CW chord using preoperative CW chord, keratometry and biometry as input data performed with a multilinear regression and a feedforward neural network approach were able to reduce the variance, but could not properly predict the postoperative CW chord X and Y components. The CW chord as the position of the Purkinje reflex PI with respect to the pupil centre can be directly measured with any biometer, topographer or tomographer with a coaxial fixation light. The mean Y component does not differ between right and left eyes or preoperatively and postoperatively, but the mean temporal shift of the X component preoperatively is slightly reduced postoperatively, but with a larger scatter of the values.", "keywords": ["centring refractive procedures", "Purkinje reflex", "chord", "corneal centre position", "Purkinje", "refractive procedures", "ophthalmic instruments", "angles alpha", "alpha and kappa", "kappa are widely", "widely discussed", "discussed for centring", "centring refractive", "determined with ophthalmic", "corneal thickness CCT", "corneal centre", "cataract surgery", "Chang-Waring chord", "reflex", "pupil centre"], "paper_title": "Prediction of CW chord as a measure for the eye's orientation axis after cataract surgery from preoperative IOLMaster 700 measurement data.", "last_updated": "2023/02/04"}, {"id": "0028801700", "domain": "Glaucoma (unspecified)", "model_name": "Aslan et al.", "publication_date": "2017/08/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28801700/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "28801700", "task": null, "abstract": "To evaluate the learning curve of non-penetrating glaucoma surgery (NPGS). The study included 32 eyes of 27 patients' (20 male and 7 female) with medically uncontrolled glaucoma. Non-penetrating glaucoma surgeries performed by trainees under control of an experienced surgeon between 2005 and 2007 at our tertiary referral hospital were evaluated. Residents were separated into two groups. Humanistic training model applied to the one in the first group, he studied with experimental models before performing NPGS. Two residents in the second group performed NPGS after a conventional training model. Surgeries of the residents were recorded on video and intraoperative parameters were scored by the experienced surgeon at the end of the study. Postoperative intraocular pressure, absolute and total success rates were analyzed. In the first group 19 eyes of 16 patients and in the second group 13 eyes of 11 patients had been operated by residents. Intraoperative parameters and complication rates were not statistically significant between groups (p\u00a0>\u00a00.05, Chi-square). The duration of surgery was 32.7\u00a0\u00b1\u00a05.6\u00a0min in the first group and 45\u00a0\u00b1\u00a03.8\u00a0min in the second group. The difference was statistically significant (p\u00a0<\u00a00.001, Student's t test). Absolute and total success was 68.8 and 93.8% in the first group and 62.5 and 87.5% in the second group, respectively. The difference was not statistically significant. Humanistic and conventional training models under control of an experienced surgeon are safe and effective for senior residents who manage phacoemulsification surgery in routine cataract cases. Senior residents can practice these surgical techniques with reasonable complication rates.", "keywords": ["group", "evaluate the learning", "learning curve", "non-penetrating glaucoma", "Residents", "group performed NPGS", "NPGS", "Non-penetrating glaucoma surgeries", "experienced surgeon", "glaucoma", "medically uncontrolled glaucoma", "training model", "statistically significant", "eyes", "training", "experienced", "surgeon", "glaucoma surgeries performed", "rates", "performed NPGS"], "paper_title": "Evaluation of the learning curve of non-penetrating glaucoma surgery.", "last_updated": "2023/02/04"}, {"id": "0034848137", "domain": "Cataract", "model_name": "Cabeza-Gil et al.", "publication_date": "2021/11/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34848137/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34848137", "task": null, "abstract": "In this work, the mechanical behaviour of hydrophilic and hydrophobic acrylates has been characterised by depth sensing indentation. Time-dependent behaviour has been studied using load-relaxation tests. Experiments have been simulated with a finite element software using a visco-hyperelastic material model. The parameters of this model have been determined using deep learning techniques. The developed material models have been used to mechanically simulate a standard compression test of a prototype intraocular lens.", "keywords": ["depth sensing indentation", "sensing indentation", "hydrophilic and hydrophobic", "hydrophobic acrylates", "characterised by depth", "depth sensing", "mechanical behaviour", "Time-dependent behaviour", "behaviour of hydrophilic", "work", "indentation", "behaviour", "mechanical", "hydrophilic", "hydrophobic", "acrylates", "characterised", "depth", "sensing", "visco-hyperelastic material model"], "paper_title": "Mechanical characterisation of hydrophobic and hydrophilic acrylates used in intraocular lenses through depth sensing indentation.", "last_updated": "2023/02/04"}, {"id": "0030977091", "domain": "Cataract", "model_name": "Kim et al.", "publication_date": "2019/04/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30977091/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "30977091", "task": null, "abstract": "Objective assessment of intraoperative technical skill is necessary for technology to improve patient care through surgical training. Our objective in this study was to develop and validate deep learning techniques for technical skill assessment using videos of the surgical field. We used a data set of 99 videos of capsulorhexis, a critical step in cataract surgery. One expert surgeon annotated each video for technical skill using a standard structured rating scale, the International Council of Ophthalmology's Ophthalmology Surgical Competency Assessment Rubric:phacoemulsification (ICO-OSCAR:phaco). Using two capsulorhexis indices in this scale (commencement of flap and follow-through, formation and completion), we specified an expert performance when at least one of the indices was 5 and the other index was at least 4, and novice otherwise. In addition, we used scores for capsulorhexis commencement and capsulorhexis formation as separate ground truths (Likert scale of 2 to 5; analyzed as 2/3, 4 and 5). We crowdsourced annotations of instrument tips. We separately modeled instrument trajectories and optical flow using temporal convolutional neural networks to predict a skill class (expert/novice) and score on each item for capsulorhexis in ICO-OSCAR:phaco. We evaluated the algorithms in a five-fold cross-validation and computed accuracy and area under the receiver operating characteristics curve (AUC). The accuracy and AUC were 0.848 and 0.863 for instrument tip velocities, and 0.634 and 0.803 for optical flow fields, respectively. Deep neural networks effectively model surgical technical skill in capsulorhexis given structured representation of intraoperative data such as optical flow fields extracted from video or crowdsourced tool localization information.", "keywords": ["improve patient care", "technical skill", "technical skill assessment", "Competency Assessment Rubric", "Surgical Competency Assessment", "Ophthalmology Ophthalmology Surgical", "Ophthalmology Surgical Competency", "surgical technical skill", "technology to improve", "improve patient", "patient care", "surgical training", "Objective assessment", "skill", "technical", "intraoperative technical skill", "capsulorhexis", "Assessment Rubric", "surgical", "assessment"], "paper_title": "Objective assessment of intraoperative technical skill in capsulorhexis using videos of cataract surgery.", "last_updated": "2023/02/04"}, {"id": "0033037510", "domain": "Cataract", "model_name": "Cabeza-Gil et al.", "publication_date": "2020/10/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33037510/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33037510", "task": null, "abstract": "In order to increase the probability of having a successful cataract post-surgery, the customisation of the haptic design of the intraocular lens (IOL) according to the characteristics of the patient is recommended. In this study, we present two prediction models based on deep neural networks (DNNs). One is capable of predicting the biomechanical stability of any C-loop IOL, whereas the other can predict the haptic design that fits a desired biomechanical response, enabling the selection of the optimal IOL as a function of the IOL diameter compression. The data used to feed the networks has been obtained from a validated finite element model in which multitude of geometries are tested according to the ISO 11979-3 compression test, a standard for the mechanical properties of the IOLs. The biomechanical response model provides a very high accurate response (Pearson's r = 0.995), whilst the IOL haptic design model shows that several IOL designs can provide the same biomechanical response (Pearson's r = 0.992). This study might help manufacturers and ophthalmologists both analyse any IOL design and select the best IOL for each patient. In order to facilitate its application, a graphical user interface (GUI) was created to show the potential of deep learning methods in cataract surgery.", "keywords": ["successful cataract post-surgery", "IOL haptic design", "IOL", "haptic design", "intraocular lens", "increase the probability", "haptic design model", "biomechanical response", "C-loop IOL", "IOL haptic", "IOL designs", "IOL diameter compression", "biomechanical response model", "design", "design model shows", "order to increase", "cataract post-surgery", "biomechanical", "successful cataract", "response"], "paper_title": "Customised Selection of the Haptic Design in C-Loop Intraocular Lenses Based on Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0036623332", "domain": "Cataract", "model_name": "Fang et al.", "publication_date": "2022/12/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36623332/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36623332", "task": "aipbNdPTIt", "abstract": "The lens is one of the important refractive media in the eyeball. Abnormality of the nucleus or cortex in the lens can lead to ocular disorders such as cataracts and presbyopia. To achieve an accurate diagnosis, segmentation of these ocular structures from anterior segment optical coherence tomography (AS-OCT) is essential. However, weak-contrast boundaries of the object in the images present a challenge for accurate segmentation. The state-of-the-art (SOTA) methods, such as U-Net, treat segmentation as a binary classification of pixels, which cannot handle pixels on weak-contrast boundaries well. In this paper, we propose to incorporate shape prior into a deep learning framework for accurate nucleus and cortex segmentation. Specifically, we propose to learn a level set function, whose zero-level set represents the object boundary, through a convolutional neural network. Moreover, we design a novel shape-based loss function, where the shape prior knowledge can be naturally embedded into the learning procedure, leading to improvement in performance. We collect a high-quality AS-OCT image dataset with precise annotations to train our model. Abundant experiments are conducted to verify the effectiveness of the proposed framework and the novel shape-based loss. The mean Intersection over Unions (MIoUs) of the proposed method for lens nucleus and cortex segmentation are 0.946 and 0.957, and the mean Euclidean Distance (MED) measure, which can reflect the accuracy of the segmentation boundary, are 6.746 and 2.045 pixels. In addition, the proposed shape-based loss improves the SOTA models on the nucleus and cortex segmentation tasks by an average of 0.0156 and 0.0078 in the MIoU metric and 1.394 and 0.134 pixels in the MED metric. We transform the segmentation from a classification task to a regression task by making the model learn the level set function, and embed shape information in deep learning by designing loss functions. This allows the proposed method to be more efficient in the segmentation of the object with weak-contrast boundaries. We propose to incorporate shape priors into a deep learning framework for accurate nucleus and cortex segmentation from AS-OCT images. Specifically, we propose to learn a level set function, where the zero-level set represents the boundary of the target. Meanwhile, we design a novel shape-based loss function in which additional convex shape prior can be embedded in the learning process, leading to an improvement in performance. The IOUs for nucleus and cortex segmentation are 0.946 and 0.957, while the MED that reflects the accuracy of the boundary are 6.746 and 2.045 pixels. The proposed shape-based loss improves the SOTA model for nucleus and cortex segmentation by an average of 0.0156 and 0.0078 in IOU, and 1.394 and 0.134 pixels in MED. We transform segmentation from classification to regression by making the model learn a level set function, resulting in improved performance at the boundary with weak contrast.", "keywords": ["important refractive media", "cortex segmentation", "segmentation", "nucleus and cortex", "level set function", "shape-based loss", "cortex", "nucleus", "set function", "shape-based loss function", "proposed shape-based loss", "level set", "loss", "important refractive", "refractive media", "set", "function", "pixels", "MED", "shape-based"], "paper_title": "Lens structure segmentation from AS-OCT images via shape-based learning.", "last_updated": "2023/02/04"}, {"id": "0035325999", "domain": "Glaucoma (unspecified)", "model_name": "Ramesh et al.", "publication_date": "2022/04/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35325999/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35325999", "task": "IWqQC1koJA", "abstract": "For diagnosing glaucomatous damage, we have employed a novel convolutional neural network (CNN) from TrueColor confocal fundus images to conquer the black box dilemma in artificial intelligence (AI). This neural network with CNN architecture with human-in-the-loop (HITL) data annotation helps not only in diagnosing glaucoma but also in predicting and locating detailed signs in the glaucomatous fundus, such as splinter hemorrhages, glaucomatous optic atrophy, vertical glaucomatous cupping, peripapillary atrophy, and retinal nerve fiber layer (RNFL) defect. The training was done on a well-curated private dataset of 1,400 high-resolution confocal fundus images, out of which 1,120 images (80%) were used exclusively for training and 280 images (20%) were used exclusively for testing. A custom trained You Only Look Once version 5 (YOLOv5)-based object detection methodology was used to identify the underlying conditions precisely. Twenty-six predefined medical conditions were annotated by a team of humans (comprising two glaucoma specialists and two optometrists) by using the Microsoft Visual Object Tagging Tool (VoTT) tool. The 280 testing images were split into three groups (90,100, and 90 images) for three test runs done once every 15 days. Test results showed consistent increments in the accuracy, from 94.44% to 98.89%, in predicting the glaucoma diagnosis along with the detailed signs of the glaucomatous fundus. Utilizing human intelligence in AI for detecting glaucomatous fundus images by using HITL machine learning has never been reported in the literature before. This AI model not only has good sensitivity and specificity in accurate glaucoma predictions but is also an explainable AI, thus overcoming the black box dilemma.", "keywords": ["convolutional neural network", "TrueColor confocal fundus", "confocal fundus images", "diagnosing glaucomatous damage", "neural network", "glaucomatous fundus", "fundus images", "confocal fundus", "fundus", "glaucomatous fundus images", "convolutional neural", "high-resolution confocal fundus", "images", "CNN architecture", "Microsoft Visual Object", "Visual Object Tagging", "glaucomatous", "Object Tagging Tool", "TrueColor confocal", "CNN"], "paper_title": "Utilizing human intelligence in artificial intelligence for detecting glaucomatous fundus images using human-in-the-loop machine learning.", "last_updated": "2023/02/04"}, {"id": "0036944845", "domain": "Cataract", "model_name": "TRandAugment", "publication_date": "2023/03/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36944845/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36944845", "task": null, "abstract": "Automatic recognition of surgical activities from intraoperative surgical videos is crucial for developing intelligent support systems for computer-assisted interventions. Current state-of-the-art recognition methods are based on deep learning where data augmentation has shown the potential to improve the generalization of these methods. This has spurred work on automated and simplified augmentation strategies for image classification and object detection on datasets of still images. Extending such augmentation methods to videos is not straightforward, as the temporal dimension needs to be considered. Furthermore, surgical videos pose additional challenges as they are composed of multiple, interconnected, and long-duration activities. This work proposes a new simplified augmentation method, called TRandAugment, specifically designed for long surgical videos, that treats each video as an assemble of temporal segments and applies consistent but random transformations to each segment. The proposed augmentation method is used to train an end-to-end spatiotemporal model consisting of a CNN (ResNet50) followed by a TCN. The effectiveness of the proposed method is demonstrated on two surgical video datasets, namely Bypass40 and CATARACTS, and two tasks, surgical phase and step recognition. TRandAugment adds a performance boost of 1-6% over previous state-of-the-art methods, that uses manually designed augmentations. This work presents a simplified and automated augmentation method for long surgical videos. The proposed method has been validated on different datasets and tasks indicating the importance of devising temporal augmentation methods for long surgical videos.", "keywords": ["developing intelligent support", "intelligent support systems", "surgical videos", "surgical", "computer-assisted interventions", "crucial for developing", "developing intelligent", "intelligent support", "support systems", "systems for computer-assisted", "augmentation", "long surgical videos", "videos", "intraoperative surgical videos", "methods", "method", "Automatic recognition", "long surgical", "augmentation method", "intraoperative surgical"], "paper_title": "TRandAugment: temporal random augmentation strategy for surgical activity recognition from videos.", "last_updated": "2023/02/04"}, {"id": "0031015713", "domain": "Disc hemorrhage", "model_name": "Poplin et al.", "publication_date": "2018/02/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31015713/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "31015713", "task": null, "abstract": "Traditionally, medical discoveries are made by observing associations, making hypotheses from them and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can often be difficult because of the wide variety of features, patterns, colours, values and shapes that are present in real data. Here, we show that deep learning can extract new knowledge from retinal fundus images. Using deep-learning models trained on data from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients, we predicted cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as age (mean absolute error within 3.26 years), gender (area under the receiver operating characteristic curve (AUC)\u2009=\u20090.97), smoking status (AUC\u2009=\u20090.71), systolic blood pressure (mean absolute error within 11.23\u2009mmHg) and major adverse cardiac events (AUC\u2009=\u20090.70). We also show that the trained deep-learning models used anatomical features, such as the optic disc or blood vessels, to generate each prediction.", "keywords": ["making hypotheses", "discoveries are made", "designing and running", "running experiments", "experiments to test", "medical discoveries", "AUC", "test the hypotheses", "hypotheses", "observing associations", "medical images", "images", "made by observing", "absolute error", "Traditionally", "retinal fundus images", "quantifying associations", "deep-learning models", "medical", "observing"], "paper_title": "Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning.", "last_updated": "2023/02/04"}, {"id": "0035051149", "domain": "Disc hemorrhage", "model_name": "Liu et al.", "publication_date": "2022/04/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35051149/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "35051149", "task": null, "abstract": "Segmentation of spinal structures is important in medical imaging analysis, which facilitates surgeons to plan a preoperative trajectory for the transforaminal approach. However, manual segmentation of spinal structures is time-consuming, and studies have not explored automatic segmentation of spinal structures at the L5/S1 level. This study sought to develop a new method based on a deep learning algorithm for automatic segmentation of spinal structures. The resulting algorithm may be used to rapidly generate a precise 3D lumbosacral intervertebral foramen model to assist physicians in planning an ideal trajectory in L5/S1 lumbar transforaminal radiofrequency ablation (LTRFA). This was an observational study for developing a new technique on spinal structures segmentation. The study was carried out at the department of radiology and spine surgery at our hospital. A total of 100 L5/S1 level data samples from 100 study patients were used in this study. Masks of vertebral bone structures (VBSs) and intervertebral discs (IVDs) for all data samples were segmented manually by a skilled surgeon and served as the \"ground truth.\" After data preprocessing, a 3D-UNet model based on deep learning was used for automated segmentation of lumbar spine structures at L5/S1 level magnetic resonance imaging (MRI). Segmentation performances and morphometric measurement were used for 3D lumbosacral intervertebral foramen (LIVF) reconstruction\u00a0 generated by either manual segmentation and automatic segmentation. The 3D-UNet model showed high performance in automatic segmentation of lumbar spinal structures (VBSs and IVDs). The corresponding mean Dice similarity coefficient (DSC) of 5-fold cross-validation scores for L5 vertebrae, IVDs, S1 vertebrae, and all L5/S1 level spinal structures were 93.46 \u00b1 2.93%, 90.39 \u00b1 6.22%, 93.32 \u00b1 1.51%, and 92.39 \u00b1 2.82%, respectively. Notably, the analysis showed no associated difference in morphometric measurements between the manual and automatic segmentation at the L5/S1 level. Semantic segmentation of multiple spinal structures (such as VBSs, IVDs, blood vessels, muscles, and ligaments) was simultaneously not integrated into the deep-learning method in this study. In addition, large clinical experiments are needed to evaluate the clinical efficacy of the model. The 3D-UNet model developed in this study based on deep learning can effectively and simultaneously segment VBSs and IVDs at L5/S1 level formMR images, thereby enabling rapid and accurate 3D reconstruction of LIVF models. The method can be used to segment VBSs and IVDs of spinal structures on MR images within near-human expert performance; therefore, it is reliable for reconstructing LIVF for L5/S1 LTRFA.", "keywords": ["spinal structures", "Segmentation", "structures", "spinal", "automatic segmentation", "study", "important in medical", "plan a preoperative", "level", "spinal structures segmentation", "IVDs", "Segmentation of spinal", "automatic", "level spinal structures", "VBSs", "model", "transforaminal approach", "LIVF", "lumbar spinal structures", "preoperative trajectory"], "paper_title": "Computerized Characterization of Spinal Structures on MRI and Clinical Significance of 3D Reconstruction of Lumbosacral Intervertebral Foramen.", "last_updated": "2023/02/04"}, {"id": "0029948436", "domain": "Disc hemorrhage", "model_name": "Jang et al.", "publication_date": "2019/11/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29948436/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "29948436", "task": "IWqQC1koJA", "abstract": "In this paper, we aimed to understand and analyze the outputs of a convolutional neural network model that classifies the laterality of fundus images. Our model not only automatizes the classification process, which results in reducing the labors of clinicians, but also highlights the key regions in the image and evaluates the uncertainty for the decision with proper analytic tools. Our model was trained and tested with 25,911 fundus images (43.4% of macula-centered images and 28.3% each of superior and nasal retinal fundus images). Also, activation maps were generated to mark important regions in the image for the classification. Then, uncertainties were quantified to support explanations as to why certain images were incorrectly classified under the proposed model. Our model achieved a mean training accuracy of 99%, which is comparable to the performance of clinicians. Strong activations were detected at the location of optic disc and retinal blood vessels around the disc, which matches to the regions that clinicians attend when deciding the laterality. Uncertainty analysis discovered that misclassified images tend to accompany with high prediction uncertainties and are likely ungradable. We believe that visualization of informative regions and the estimation of uncertainty, along with presentation of the prediction result, would enhance the interpretability of neural network models in a way that clinicians can be benefitted from using the automatic classification system.", "keywords": ["fundus images", "aimed to understand", "understand and analyze", "analyze the outputs", "retinal fundus images", "images", "convolutional neural network", "model", "fundus", "classifies the laterality", "neural network model", "convolutional neural", "regions", "clinicians", "nasal retinal fundus", "classification", "image", "neural network", "proper analytic tools", "retinal fundus"], "paper_title": "Laterality Classification of Fundus Images Using Interpretable Deep Neural Network.", "last_updated": "2023/02/04"}, {"id": "0036766509", "domain": "Disc hemorrhage", "model_name": "Abd El-Ghany et al.", "publication_date": "2023/01/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36766509/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "36766509", "task": "IWqQC1koJA", "abstract": "The immune system's overproduction of white blood cells (WBCs) results in the most common blood cancer, leukemia. It accounts for about 25% of childhood cancers and is one of the primary causes of death worldwide. The most well-known type of leukemia found in the human bone marrow is acute lymphoblastic leukemia (ALL). It is a disease that affects the bone marrow and kills white blood cells. Better treatment and a higher likelihood of survival can be helped by early and precise cancer detection. As a result, doctors can use computer-aided diagnostic (CAD) models to detect early leukemia effectively. In this research, we proposed a classification model based on the EfficientNet-B3 convolutional neural network (CNN) model to distinguish ALL as an automated model that automatically changes the learning rate (LR). We set up a custom LR that compared the loss value and training accuracy at the beginning of each epoch. We evaluated the proposed model on the C-NMC_Leukemia dataset. The dataset was pre-processed with normalization and balancing. The proposed model was evaluated and compared with recent classifiers. The proposed model's average precision, recall, specificity, accuracy, and Disc similarity coefficient (DSC) were 98.29%, 97.83%, 97.82%, 98.31%, and 98.05%, respectively. Moreover, the proposed model was used to examine microscopic images of the blood to identify the malaria parasite. Our proposed model's average precision, recall, specificity, accuracy, and DSC were 97.69%, 97.68%, 97.67%, 97.68%, and 97.68%, respectively. Therefore, the evaluation of the proposed model showed that it is an unrivaled perceptive outcome with tuning as opposed to other ongoing existing models.", "keywords": ["immune system overproduction", "proposed model", "common blood cancer", "white blood cells", "model", "immune system", "system overproduction", "proposed", "proposed model average", "leukemia", "blood cells", "white blood", "common blood", "blood", "model average precision", "blood cancer", "bone marrow", "model average", "cancer", "kills white blood"], "paper_title": "Computer-Aided Diagnosis System for Blood Diseases Using EfficientNet-B3 Based on a Dynamic Learning Algorithm.", "last_updated": "2023/02/04"}, {"id": "0033871400", "domain": "Uveitis", "model_name": "Lee et al.", "publication_date": "2022/01/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33871400/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "33871400", "task": null, "abstract": "Development of an automated method to quantify the count of vitreous hyperreflective foci (vHF) and intensity of vitreous haze in patients with uveitis by optical coherence tomography. A method based on deep learning to automatically segment the vHF, vitreous, and retinal pigment epithelium (RPE) in optical coherence tomography was developed using 1,058 scans from 88 optical coherence tomography volumes of 33 patients with intermediate, posterior or panuveitis. Based on segmented images, the vHF count and the relative intensity of vitreous to RPE (VIT/RPE-relative intensity) were quantified. Dice coefficient and intraclass correlation coefficient were calculated between ground truth and the trained network. The segmented area of vHF, vitreous, and RPE by the deep learning-based model showed good agreement with the clinicians' results, yielding a Dice coefficient of 0.69, 0.99, and 0.88, respectively. The intraclass correlation coefficient of the vHF count and the VIT/RPE-relative intensity per scan was 0.99 and 1.00, respectively. In eyes of test set, changes in vHF and VIT/RPE-relative intensity during treatment did not show similar patterns. Automated segmentation of the vHF, vitreous, and RPE in optical coherence tomography images of patients with uveitis was accomplished by a deep learning approach. The vHF count and VIT/RPE-relative intensity could be quantified with high reliability.", "keywords": ["optical coherence tomography", "optical coherence", "coherence tomography", "vitreous hyperreflective foci", "coherence tomography volumes", "vHF", "RPE-relative intensity", "hyperreflective foci", "coherence tomography images", "vHF count", "coherence", "vitreous", "VIT", "RPE", "optical", "tomography", "intensity", "vitreous hyperreflective", "vitreous haze", "RPE-relative"], "paper_title": "AUTOMATED QUANTIFICATION OF VITREOUS HYPERREFLECTIVE FOCI AND VITREOUS HAZE USING OPTICAL COHERENCE TOMOGRAPHY IN PATIENTS WITH UVEITIS.", "last_updated": "2023/02/04"}, {"id": "0034222252", "domain": "Uveitis", "model_name": "Zhang et al.", "publication_date": "2021/06/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34222252/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "34222252", "task": "IWqQC1koJA", "abstract": "Fuchs' uveitis syndrome (FUS) is one of the most under- or misdiagnosed uveitis entities. Many undiagnosed FUS patients are unnecessarily overtreated with anti-inflammatory drugs, which may lead to serious complications. To offer assistance for ophthalmologists in the screening and diagnosis of FUS, we developed seven deep convolutional neural networks (DCNNs) to detect FUS using slit-lamp images. We also proposed a new optimized model with a mixed \"attention\" module to improve test accuracy. In the same independent set, we compared the performance between these DCNNs and ophthalmologists in detecting FUS. Seven different network models, including Xception, Resnet50, SE-Resnet50, ResNext50, SE-ResNext50, ST-ResNext50, and SET-ResNext50, were used to predict FUS automatically with the area under the receiver operating characteristic curves (AUCs) that ranged from 0.951 to 0.977. Our proposed SET-ResNext50 model (accuracy = 0.930; Precision = 0.918; Recall = 0.923; F1 measure = 0.920) with an AUC of 0.977 consistently outperformed the other networks and outperformed general ophthalmologists by a large margin. Heat-map visualizations of the SET-ResNext50 were provided to identify the target areas in the slit-lamp images. In conclusion, we confirmed that a trained classification method based on DCNNs achieved high effectiveness in distinguishing FUS from other forms of anterior uveitis. The performance of the DCNNs was better than that of general ophthalmologists and could be of value in the diagnosis of FUS.", "keywords": ["Fuchs' uveitis syndrome", "misdiagnosed uveitis entities", "FUS", "undiagnosed FUS patients", "Fuchs' uveitis", "uveitis syndrome", "uveitis entities", "misdiagnosed uveitis", "ophthalmologists", "undiagnosed FUS", "FUS patients", "DCNNs", "slit-lamp images", "detect FUS", "general ophthalmologists", "uveitis", "Fuchs'", "syndrome", "entities", "detecting FUS"], "paper_title": "Detection of Fuchs' Uveitis Syndrome From Slit-Lamp Images Using Deep Convolutional Neural Networks in a Chinese Population.", "last_updated": "2023/02/04"}, {"id": "0035802368", "domain": "Diabetic retinopathy", "model_name": "Thi et al.", "publication_date": "2022/07/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35802368/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35802368", "task": null, "abstract": "To develop a machine-learning image processing model for three-dimensional (3D) reconstruction of vitreous anatomy visualized with swept-source optical coherence tomography (SS-OCT). Healthy subjects were imaged with SS-OCT. Scans of sufficient quality were transferred into the Fiji is just ImageJ image processing toolkit, and proportions of the resulting stacks were adjusted to form cubic voxels. Image-averaging and Trainable Weka Segmentation using Sobel and variance edge detection and directional membrane projections filters were used to enhance and interpret the signals from vitreous gel, liquid spaces within the vitreous, and interfaces between the former. Two classes were defined: \"Septa\" and \"Other.\" Pixels were selected and added to each class to train the classifier. Results were generated as a probability map. Thresholding was performed to remove pixels that were classified with low confidence. Volume rendering was performed with TomViz. Forty-seven eyes of 34 healthy subjects were imaged with SS-OCT. Thirty-four cube scans from 25 subjects were of sufficient quality for volume rendering. Clinically relevant vitreous features including the premacular bursa, area of Martegiani, and prevascular vitreous fissures and cisterns, as well as varying degrees of vitreous degeneration were visualized in 3D. A machine-learning model for 3D vitreous reconstruction of SS-OCT cube scans was developed. The resultant high-resolution 3D movies illustrated vitreous anatomy in a manner like triamcinolone-assisted vitrectomy or postmortem dye injection. This machine learning model now allows for comprehensive examination of the vitreous structure beyond the vitreoretinal interface in 3D with potential applications for common disease states such as the vitreomacular traction and Macular Hole spectrum of diseases or proliferative diabetic retinopathy.", "keywords": ["optical coherence tomography", "swept-source optical coherence", "coherence tomography", "swept-source optical", "optical coherence", "Trainable Weka Segmentation", "vitreous", "image processing", "image processing toolkit", "machine-learning image processing", "ImageJ image processing", "image processing model", "Trainable Weka", "Weka Segmentation", "Segmentation using Sobel", "SS-OCT", "develop a machine-learning", "Healthy subjects", "subjects were imaged", "subjects"], "paper_title": "A Pixel-Based Machine-Learning Model For Three-Dimensional Reconstruction of Vitreous Anatomy.", "last_updated": "2023/02/04"}, {"id": "0032890546", "domain": "Macular degeneration", "model_name": "Ajana et al.", "publication_date": "2020/09/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32890546/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32890546", "task": "0GPcU42tzV", "abstract": "Current prediction models for advanced age-related macular degeneration (AMD) are based on a restrictive set of risk factors. The objective of this study was to develop a comprehensive prediction model applying a machine learning algorithm allowing selection of the most predictive risk factors automatically. Two population-based cohort studies. The Rotterdam Study I (RS-I; training set) included 3838 participants 55 years of age or older, with a median follow-up period of 10.8 years, and 108 incident cases of advanced AMD. The Antioxydants, Lipids Essentiels, Nutrition et Maladies Oculaires (ALIENOR) study (test set) included 362 participants 73 years of age or older, with a median follow-up period of 6.5 years, and 33 incident cases of advanced AMD. The prediction model used the bootstrap least absolute shrinkage and selection operator (LASSO) method for survival analysis to select the best predictors of incident advanced AMD in the training set. Predictive performance of the model was assessed using the area under the receiver operating characteristic curve (AUC). Incident advanced AMD (atrophic, neovascular, or both), based on standardized interpretation of retinal photographs. The prediction model retained (1) age, (2) a combination of phenotypic predictors (based on the presence of intermediate drusen, hyperpigmentation in one or both eyes, and Age-Related Eye Disease Study simplified score), (3) a summary genetic risk score based on 49 single nucleotide polymorphisms, (4) smoking, (5) diet quality, (6) education, and (7) pulse pressure. The cross-validated AUC estimation in RS-I was 0.92 (95% confidence interval [CI], 0.88-0.97) at 5 years, 0.92 (95% CI, 0.90-0.95) at 10 years, and 0.91 (95% CI, 0.88-0.94) at 15 years. In ALIENOR, the AUC reached 0.92 at 5 years (95% CI, 0.87-0.98). In terms of calibration, the model tended to underestimate the cumulative incidence of advanced AMD for the high-risk groups, especially in ALIENOR. This prediction model reached high discrimination abilities, paving the way toward making precision medicine for AMD patients a reality in the near future.", "keywords": ["advanced AMD", "incident advanced AMD", "age-related macular degeneration", "AMD", "Current prediction models", "years", "prediction model", "risk factors", "macular degeneration", "risk factors automatically", "advanced age-related macular", "advanced", "predictive risk factors", "model", "incident advanced", "prediction", "Current prediction", "study", "Eye Disease Study", "comprehensive prediction model"], "paper_title": "Predicting Progression to Advanced Age-Related Macular Degeneration from Clinical, Genetic, and Lifestyle Factors Using\u00a0Machine Learning.", "last_updated": "2023/02/04"}, {"id": "0032838591", "domain": "Macular degeneration", "model_name": "Mart\u00ednez-Velasco et al.", "publication_date": "2020/08/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32838591/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32838591", "task": null, "abstract": "<i>CFH</i> and <i>HTRA1</i> are pivotal genes driving increased risk for age-related macular degeneration (AMD) among several populations. Here, we performed a hospital-based case-control study to evaluate the effects of three single nucleotide polymorphisms (SNPs) among Hispanics from Mexico. 122 cases and 249 controls were genotyped using Taqman probes. Experienced ophthalmologists diagnosed AMD following the American Association of Ophthalmology guidelines. We studied <i>CFH</i> (rs1329428, rs203687) and <i>HTRA1</i> (rs11200638) SNPs thoroughly by logistic regression models (assuming different modes of inheritance) and machine learning-based methods (ML). <i>HTRA1</i> rs11200638 is the most significant polymorphism associated with AMD in our studied population. In a multivariate regression model adjusted for clinically and statistically meaningful covariates, the A/G and A/A genotypes increased the odds of disease by a factor of 2.32 and 7.81, respectively (<i>P</i> <\u00a0.05) suggesting a multiplicative effect of the polymorphic A allele. Furthermore, this observation remains statistically meaningful in the allelic, dominant, and recessive models, and ML algorithms. When stratifying by phenotype, this polymorphism was significantly associated with increased odds for geographic atrophy (GA) in a recessive mode of inheritance (12.4, <i>p</i> <\u00a0.05). In sum, this work supports a strong association between <i>HTRA1</i> genetic variants and AMD in Hispanics from Mexico, especially with GA. Moreover, ML was able to replicate the results of conventional biostatistics methods unbiasedly.", "keywords": ["age-related macular degeneration", "pivotal genes driving", "genes driving increased", "driving increased risk", "macular degeneration", "pivotal genes", "genes driving", "risk for age-related", "age-related macular", "Hispanics from Mexico", "AMD", "driving increased", "increased risk", "CFH", "hospital-based case-control study", "ophthalmologists diagnosed AMD", "single nucleotide polymorphisms", "Mexico", "American Association", "diagnosed AMD"], "paper_title": "Assessment of <i>CFH</i> and <i>HTRA1</i> polymorphisms in age-related macular degeneration using classic and machine-learning approaches.", "last_updated": "2023/02/04"}, {"id": "0033746023", "domain": "Diabetic retinopathy", "model_name": "Sagkriotis et al.", "publication_date": "2021/03/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33746023/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33746023", "task": null, "abstract": "The effectiveness of intravitreal anti-vascular endothelial growth factor agents is usually lower in real world settings compared with randomized clinical trials (RCTs), often limiting the use of real-world evidence (RWE) in regulatory and healthcare decisions. The current analysis aimed to develop and validate an algorithm to explain the difference in outcomes between RWE studies and RCTs in patients with neovascular age-related macular degeneration. The algorithm was developed using ranibizumab real world data (RWD) from the US and validated on Australian and UK RWD. A decision model was developed using machine learning principles, in which the model learns how to partition the most influential factors (out of 59 variables) so that they maximally relate to the change in visual acuity (VA) over 12\u00a0months. The algorithm identified baseline VA <73 Early Treatment Diabetic Retinopathy Study letters, presence of baseline subretinal fluid, and administration of three loading doses by Day 90 from drug initiation as the characteristics with the greatest impact on VA at month 12. When applying the different criteria, RWE outcomes became similar to those obtained in known RCTs. Machine learning techniques can be used to classify real world cohorts and identify subsets of patients who benefit to the same extent as that reported in RCTs. This methodology may support the translation of clinical trial findings to treatment performance in the clinical practice setting.", "keywords": ["intravitreal anti-vascular endothelial", "anti-vascular endothelial growth", "endothelial growth factor", "growth factor agents", "real-world evidence", "world settings compared", "effectiveness of intravitreal", "intravitreal anti-vascular", "anti-vascular endothelial", "endothelial growth", "compared with randomized", "regulatory and healthcare", "real world", "randomized clinical trials", "real world settings", "Diabetic Retinopathy Study", "RWE", "real world data", "Early Treatment Diabetic", "ranibizumab real world"], "paper_title": "Application of machine learning methods to bridge the gap between non-interventional studies and randomized controlled trials in ophthalmic patients with neovascular age-related macular degeneration.", "last_updated": "2023/02/04"}, {"id": "0034422023", "domain": "Macular degeneration", "model_name": "Du et al.", "publication_date": "2021/08/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34422023/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34422023", "task": null, "abstract": "Age-related macular degeneration (AMD) is the most common cause of irreversible vision loss in the developed world which affects the quality of life for millions of elderly individuals worldwide. Genome-wide association studies (GWAS) have identified genetic variants at 34 loci contributing to AMD. To better understand the disease pathogenesis and identify causal genes for AMD, we applied random walk (RW) and support vector machine (SVM) to identify AMD-related genes based on gene interaction relationship and significance of genes. Our model achieved 0.927 of area under the curve (AUC), and 65 novel genes have been identified as AMD-related genes. To verify our results, a statistics method called summary data-based Mendelian randomization (SMR) has been implemented to integrate GWAS data and transcriptome data to verify AMD susceptibility-related genes. We found 45 genes are related to AMD by SMR. Among these genes, 37 genes overlap with those found by SVM-RW. Finally, we revealed the biological process of genetic mutations leading to changes in gene expression leading to AMD. Our results reveal the genetic pathogenic factors and related mechanisms of AMD.", "keywords": ["Age-related macular degeneration", "elderly individuals worldwide", "irreversible vision loss", "AMD", "Age-related macular", "macular degeneration", "individuals worldwide", "irreversible vision", "vision loss", "developed world", "world which affects", "affects the quality", "quality of life", "life for millions", "millions of elderly", "elderly individuals", "genes", "GWAS", "SMR", "verify AMD"], "paper_title": "Genetic Mechanism Revealed of Age-Related Macular Degeneration Based on Fusion of Statistics and Machine Learning Method.", "last_updated": "2023/02/04"}, {"id": "0025623470", "domain": "Macular degeneration", "model_name": "Fraccaro et al.", "publication_date": "2015/01/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25623470/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "25623470", "task": "IWqQC1koJA", "abstract": "To investigate machine learning methods, ranging from simpler interpretable techniques to complex (non-linear) \"black-box\" approaches, for automated diagnosis of Age-related Macular Degeneration (AMD). Data from healthy subjects and patients diagnosed with AMD or other retinal diseases were collected during routine visits via an Electronic Health Record (EHR) system. Patients' attributes included demographics and, for each eye, presence/absence of major AMD-related clinical signs (soft drusen, retinal pigment epitelium, defects/pigment mottling, depigmentation area, subretinal haemorrhage, subretinal fluid, macula thickness, macular scar, subretinal fibrosis). Interpretable techniques known as white box methods including logistic regression and decision trees as well as less interpreitable techniques known as black box methods, such as support vector machines (SVM), random forests and AdaBoost, were used to develop models (trained and validated on unseen data) to diagnose AMD. The gold standard was confirmed diagnosis of AMD by physicians. Sensitivity, specificity and area under the receiver operating characteristic (AUC) were used to assess performance. Study population included 487 patients (912 eyes). In terms of AUC, random forests, logistic regression and adaboost showed a mean performance of (0.92), followed by SVM and decision trees (0.90). All machine learning models identified soft drusen and age as the most discriminating variables in clinicians' decision pathways to diagnose AMD. Both black-box and white box methods performed well in identifying diagnoses of AMD and their decision pathways. Machine learning models developed through the proposed approach, relying on clinical signs identified by retinal specialists, could be embedded into EHR to provide physicians with real time (interpretable) support.", "keywords": ["Age-related Macular Degeneration", "Electronic Health Record", "Macular Degeneration", "Age-related Macular", "diagnosis of Age-related", "ranging from simpler", "simpler interpretable techniques", "AMD", "Health Record", "Electronic Health", "investigate machine learning", "Degeneration", "diagnose AMD", "Age-related", "box methods", "interpretable techniques", "machine learning", "automated diagnosis", "simpler interpretable", "white box methods"], "paper_title": "Combining macula clinical signs and patient characteristics for age-related macular degeneration diagnosis: a machine learning approach.", "last_updated": "2023/02/04"}, {"id": "0034727162", "domain": "Macular degeneration", "model_name": "Arslan et al.", "publication_date": "2022/01/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34727162/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34727162", "task": "0GPcU42tzV", "abstract": "The purpose of this study was to identify a taxonomy of epistemic uncertainties that affect results for geographic atrophy (GA) assessment and progression. An important source of variability is called \"epistemic uncertainty,\" which is due to incomplete system knowledge (i.e. limitations in measurement devices, artifacts, and human subjective evaluation, including annotation errors). In this study, different epistemic uncertainties affecting the analysis of GA were identified and organized into a taxonomy. The uncertainties were discussed and analyzed, and an example was provided in the case of model structure uncertainty by characterizing progression of GA by mathematical modelling and machine learning. It was hypothesized that GA growth follows a logistic (sigmoidal) function. Using case studies, the GA growth data were used to test the sigmoidal hypothesis. Epistemic uncertainties were identified, including measurement error (imperfect outcomes from measuring tools), subjective judgment (grading affected by grader's vision and experience), model input uncertainties (data corruption or entry errors), and model structure uncertainties (elucidating the right progression pattern). Using GA growth data from case studies, it was demonstrated that GA growth can be represented by a sigmoidal function, where growth eventually approaches an upper limit. Epistemic uncertainties contribute to errors in study results and are reducible if identified and addressed. By prior identification of epistemic uncertainties, it is possible to (a) quantify uncertainty not accounted for by natural statistical variability, and (b) reduce the presence of these uncertainties in future studies. Lowering epistemic uncertainty will reduce experimental error, improve consistency and reproducibility, and increase confidence in diagnostics.", "keywords": ["epistemic uncertainties", "geographic atrophy", "uncertainties", "epistemic", "identify a taxonomy", "growth", "epistemic uncertainty", "uncertainty", "study", "affect results", "growth data", "case studies", "studies", "progression", "identified", "epistemic uncertainties affecting", "case", "data", "model", "taxonomy"], "paper_title": "Progression of Geographic Atrophy: Epistemic Uncertainties Affecting Mathematical Models and Machine Learning.", "last_updated": "2023/02/04"}, {"id": "0028658477", "domain": "Macular degeneration", "model_name": "Bogunovic et al.", "publication_date": "2017/07/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28658477/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "28658477", "task": "0GPcU42tzV", "abstract": "To develop a data-driven interpretable predictive model of incoming drusen regression as a sign of disease activity and identify optical coherence tomography (OCT) biomarkers associated with its risk in intermediate age-related macular degeneration (AMD). Patients with AMD were observed every 3 months, using Spectralis OCT imaging, for a minimum duration of 12 months and up to a period of 60 months. Segmentation of drusen and the overlying layers was obtained using a graph-theoretic method, and the hyperreflective foci were segmented using a voxel classification method. Automated image analysis steps were then applied to identify and characterize individual drusen at baseline, and their development was monitored at every follow-up visit. Finally, a machine learning method based on a sparse Cox proportional hazard regression was developed to estimate a risk score and predict the incoming regression of individual drusen. The predictive model was trained and evaluated on a longitudinal dataset of 61 eyes from 38 patients using cross-validation. The mean follow-up time was 37.8 \u00b1 13.8 months. A total of 944 drusen were identified at baseline, out of which 249 (26%) regressed during follow-up. The prediction performance was evaluated as area under the curve (AUC) for different time periods. Prediction within the first 2 years achieved an AUC of 0.75. The predictive model proposed in this study represents a promising step toward image-guided prediction of AMD progression. Machine learning is expected to accelerate and contribute to the development of new therapeutics that delay the progression of AMD.", "keywords": ["optical coherence tomography", "age-related macular degeneration", "intermediate age-related macular", "Spectralis OCT imaging", "identify optical coherence", "data-driven interpretable predictive", "Spectralis OCT", "coherence tomography", "macular degeneration", "develop a data-driven", "data-driven interpretable", "sign of disease", "disease activity", "optical coherence", "intermediate age-related", "age-related macular", "interpretable predictive model", "OCT imaging", "OCT", "AMD"], "paper_title": "Machine Learning of the Progression of Intermediate Age-Related Macular Degeneration Based on OCT Imaging.", "last_updated": "2023/02/04"}, {"id": "0035272558", "domain": "Diabetic retinopathy", "model_name": "Song et al.", "publication_date": "2022/03/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35272558/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35272558", "task": null, "abstract": "The objective is to validate an automated artificial intelligence model in detecting and quantifying fluid in diabetic macular edema (DME) and retinal vein occlusion (RVO) optical coherence tomography images. DME (<i>n</i> = 100) and RVO (<i>n</i> = 100) images of adult patients were reviewed. The performance of machine-learning (ML) computational image analysis algorithm was evaluated against consensus manual grading. Main outcomes were accuracy and sensitivity for detection and Pearson's correlation coefficients for quantification. The ML algorithm had a high accuracy and sensitivity in both DME (intraretinal fluid [IRF]: 0.92, 0.97; subretinal fluid [SRF]: 0.93, 1.00) and RVO (IRF: 0.94, 0.99; SRF: 0.93, 1.00). It had moderate-high correlation in quantifying fluid in DME (total retinal fluid: 0.88; IRF: 0.88; SRF: 0.97) and RVO (total retinal fluid: 0.83; IRF: 0.76; SRF: 0.64). The ML algorithm is highly accurate and sensitive in detecting fluid in DME and RVO optical coherence tomography images and effectively quantifies IRF and SRF in both disease states, particularly in images with low to moderate fluid burden. <b>[<i>Ophthalmic Surg Lasers Imaging Retina</i>. 2022;53:123-131.]</b>.", "keywords": ["diabetic macular edema", "automated artificial intelligence", "artificial intelligence model", "retinal vein occlusion", "macular edema", "vein occlusion", "RVO", "validate an automated", "automated artificial", "artificial intelligence", "intelligence model", "diabetic macular", "DME", "fluid", "coherence tomography images", "IRF", "SRF", "RVO optical coherence", "total retinal fluid", "images"], "paper_title": "Performance of a Machine-Learning Computational Image Analysis Algorithm in Retinal Fluid Quantification for Patients With Diabetic Macular Edema and Retinal Vein Occlusions.", "last_updated": "2023/02/04"}, {"id": "0034982004", "domain": "Macular degeneration", "model_name": "Sarici et al.", "publication_date": "2022/01/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34982004/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "34982004", "task": "0GPcU42tzV", "abstract": "To evaluate the utility of spectral-domain optical coherence tomography biomarkers to predict the development of subfoveal geographic atrophy (sfGA). This was a retrospective cohort analysis including 137 individuals with dry age-related macular degeneration without sfGA with 5 years of follow-up. Multiple spectral-domain optical coherence tomography quantitative metrics were generated, including ellipsoid zone (EZ) integrity and subretinal pigment epithelium (sub-RPE) compartment features. Reduced mean EZ-RPE central subfield thickness and increased sub-RPE compartment thickness were significantly different between sfGA convertors and nonconvertors at baseline in both 2-year and 5-year sfGA risk assessment. Longitudinal change assessment showed a significantly higher degradation of EZ integrity in sfGA convertors. The predictive performance of a machine learning classification model based on 5-year and 2-year risk conversion to sfGA demonstrated an area under the receiver operating characteristic curve of 0.92 \u00b1 0.06 and 0.96 \u00b1 0.04, respectively. Quantitative outer retinal and sub-RPE feature assessment using a machine learning-enabled retinal segmentation platform provides multiple parameters that are associated with progression to sfGA. <b>[<i>Ophthalmic Surg Lasers Imaging</i>. 2022;53:31-39.]</b>.", "keywords": ["subfoveal geographic atrophy", "spectral-domain optical coherence", "optical coherence tomography", "coherence tomography biomarkers", "geographic atrophy", "evaluate the utility", "biomarkers to predict", "predict the development", "development of subfoveal", "subfoveal geographic", "optical coherence", "spectral-domain optical", "coherence tomography", "sfGA", "tomography biomarkers", "Multiple spectral-domain optical", "coherence tomography quantitative", "Ophthalmic Surg Lasers", "Surg Lasers Imaging", "optical"], "paper_title": "Risk Classification for Progression to Subfoveal Geographic Atrophy in Dry Age-Related Macular Degeneration Using Machine Learning-Enabled Outer Retinal Feature Extraction.", "last_updated": "2023/02/04"}, {"id": "0028018716", "domain": "Macular degeneration", "model_name": "AMD", "publication_date": "2016/11/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28018716/", "code_link": "https://github.com/ncbi\" aria-label=\"github\">", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "28018716", "task": "IWqQC1koJA", "abstract": "Non-lethal macular diseases greatly impact patients' life quality, and will cause vision loss at the late stages. Visual inspection of the optical coherence tomography (OCT) images by the experienced clinicians is the main diagnosis technique. We proposed a computer-aided diagnosis (CAD) model to discriminate age-related macular degeneration (AMD), diabetic macular edema (DME) and healthy macula. The linear configuration pattern (LCP) based features of the OCT images were screened by the Correlation-based Feature Subset (CFS) selection algorithm. And the best model based on the sequential minimal optimization (SMO) algorithm achieved 99.3% in the overall accuracy for the three classes of samples.", "keywords": ["patients' life quality", "diseases greatly impact", "greatly impact patients'", "impact patients' life", "Non-lethal macular diseases", "macular diseases greatly", "life quality", "late stages", "diseases greatly", "greatly impact", "impact patients'", "patients' life", "vision loss", "Non-lethal macular", "Correlation-based Feature Subset", "macular diseases", "main diagnosis technique", "optical coherence tomography", "diabetic macular edema", "age-related macular degeneration"], "paper_title": "Machine learning based detection of age-related macular degeneration (AMD) and diabetic macular edema (DME) from optical coherence tomography (OCT) images.", "last_updated": "2023/02/04"}, {"id": "0031047298", "domain": "Macular degeneration", "model_name": "Schmidt-Erfurth et al.", "publication_date": "2017/05/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31047298/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31047298", "task": null, "abstract": "To evaluate the potential of machine learning to predict best-corrected visual acuity (BCVA) outcomes from structural and functional assessments during the initiation phase in patients receiving standardized ranibizumab therapy for neovascular age-related macular degeneration (AMD). Post hoc analysis of a randomized, prospective clinical trial. Data of 614 evaluable patients receiving intravitreal ranibizumab monthly or pro re nata according to protocol-specified criteria in the HARBOR trial. Monthly spectral-domain (SD) OCT volume scans were processed by validated, fully automated computational image analysis. This system performs spatially resolved 3-dimensional segmentation of retinal layers, intraretinal cystoid fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachments (PED). The extracted quantitative OCT biomarkers and BCVA measurements at baseline and months 1, 2, and 3 were used to predict BCVA at 12 months using random forest machine learning. This approach was also used to correlate OCT morphology to BCVA at baseline (structure-function correlation). Accuracy (R<sup>2</sup>) of the prediction models; ranking of input variables. Computational image analysis enabled fully automated quantitative characterization of neovascular lesions in a large-scale clinical SD-OCT data set. At baseline, OCT features and BCVA were correlated with R<sup>2</sup>\u00a0=\u00a00.21. The most relevant biomarker for BCVA was the horizontal extension of IRF in the foveal region, whereas SRF and PED ranked low. In predicting functional outcomes, the model's accuracy increased in a linear fashion with each month. If only the baseline visit was considered, the accuracy was R<sup>2</sup>\u00a0= 0.34. At month 3, final visual acuity outcomes could be predicted with an accuracy of R<sup>2</sup>\u00a0= 0.70. The strongest predictive factor for functional outcomes at 1 year was consistently the individual BCVA level during the initiation phase. In this large-scale study based on a wide spectrum of morphologic and functional features, baseline BCVA correlated modestly with baseline SD-OCT, whereas functional outcomes were determined by BCVA levels during the initiation phase with a minor influence of fluid-related features. This finding suggests a re-evaluation of current diagnostic imaging features and a search for novel imaging approaches, where machine learning is a promising approach.", "keywords": ["age-related macular degeneration", "standardized ranibizumab therapy", "BCVA", "neovascular age-related macular", "patients receiving standardized", "receiving standardized ranibizumab", "macular degeneration", "evaluate the potential", "age-related macular", "AMD", "patients receiving", "OCT", "receiving standardized", "baseline", "standardized ranibizumab", "ranibizumab therapy", "predict best-corrected visual", "machine learning", "functional", "outcomes"], "paper_title": "Machine Learning to Analyze the Prognostic Value of Current Imaging Biomarkers in Neovascular Age-Related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0033971352", "domain": "Diabetic retinopathy", "model_name": "Gallardo et al.", "publication_date": "2021/05/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33971352/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33971352", "task": null, "abstract": "To assess the potential of machine learning to predict low and high treatment demand in real life in patients with neovascular age-related macular degeneration (nAMD), retinal vein occlusion (RVO), and diabetic macular edema (DME) treated according to a treat-and-extend regimen (TER). Retrospective cohort study. Three hundred seventy-seven eyes (340 patients) with nAMD and 333 eyes (285 patients) with RVO or DME treated with anti-vascular endothelial growth factor agents (VEGF) according to a predefined TER from 2014 through\u00a02018. Eyes were grouped by disease into low, moderate, and high treatment demands, defined by the average treatment interval (low, \u226510 weeks; high, \u22645 weeks; moderate, remaining eyes). Two random forest models were trained to predict the probability of the long-term treatment demand of a new patient. Both models use morphological features automatically extracted from the OCT volumes at baseline and after 2 consecutive visits, as well as patient demographic information. Evaluation of the models included a 10-fold cross-validation ensuring that no patient was present in both the training set (nAMD, approximately 339; RVO and DME, approximately 300) and test set (nAMD, approximately 38; RVO and DME, approximately 33). Mean area under the receiver operating characteristic curve (AUC) of both models; contribution to the prediction and statistical significance of the input features. Based on the first 3 visits, it was possible to predict low and high treatment demand in nAMD eyes and in RVO and DME eyes with similar accuracy. The distribution of low, high, and moderate demanders was 127, 42, and 208, respectively, for nAMD and 61, 50, and 222, respectively, for RVO and DME. The nAMD-trained models yielded mean AUCs of 0.79 and 0.79 over the 10-fold crossovers for low and high demand, respectively. Models for RVO and DME showed similar results, with a mean AUC of 0.76 and 0.78 for low and high demand, respectively. Even more importantly, this study revealed that it is possible to predict low demand reasonably well at the first visit, before the first injection. Machine learning classifiers can predict treatment demand and may assist in establishing patient-specific treatment plans in the near future.", "keywords": ["age-related macular degeneration", "diabetic macular edema", "neovascular age-related macular", "retinal vein occlusion", "RVO and DME", "high treatment demand", "RVO", "DME", "treatment demand", "macular degeneration", "macular edema", "age-related macular", "diabetic macular", "high treatment", "low", "treatment", "high", "retinal vein", "vein occlusion", "demand"], "paper_title": "Machine Learning Can Predict Anti-VEGF Treatment Demand in a Treat-and-Extend Regimen for Patients with Neovascular AMD, DME, and RVO Associated Macular Edema.", "last_updated": "2023/02/04"}, {"id": "0026884750", "domain": "Macular degeneration", "model_name": "Zou et al.", "publication_date": "2016/01/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26884750/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "26884750", "task": "IWqQC1koJA", "abstract": "This paper brings forth a learning-based visual saliency model method for detecting diagnostic diabetic macular edema (DME) regions of interest (RoIs) in retinal image. The method introduces the cognitive process of visual selection of relevant regions that arises during an ophthalmologist's image examination. To record the process, we collected eye-tracking data of 10 ophthalmologists on 100 images and used this database as training and testing examples. Based on analysis, two properties (Feature Property and Position Property) can be derived and combined by a simple intersection operation to obtain a saliency map. The Feature Property is implemented by support vector machine (SVM) technique using the diagnosis as supervisor; Position Property is implemented by statistical analysis of training samples. This technique is able to learn the preferences of ophthalmologist visual behavior while simultaneously considering feature uniqueness. The method was evaluated using three popular saliency model evaluation scores (AUC, EMD, and SS) and three quality measurements (classical sensitivity, specificity, and Youden's J statistic). The proposed method outperforms 8 state-of-the-art saliency models and 3 salient region detection approaches devised for natural images. Furthermore, our model successfully detects the DME RoIs in retinal image without sophisticated image processing such as region segmentation.", "keywords": ["diabetic macular edema", "detecting diagnostic diabetic", "diagnostic diabetic macular", "macular edema", "Position Property", "paper brings", "detecting diagnostic", "diagnostic diabetic", "diabetic macular", "Feature Property", "learning-based visual saliency", "Property", "learning-based visual", "image", "Feature", "saliency", "method", "visual", "retinal image", "Property is implemented"], "paper_title": "Learning-Based Visual Saliency Model for Detecting Diabetic Macular Edema in Retinal Image.", "last_updated": "2023/02/04"}, {"id": "0022364233", "domain": "Macular degeneration", "model_name": "Newman et al.", "publication_date": "2012/02/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/22364233/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "22364233", "task": null, "abstract": "Please see related commentary: http://www.biomedcentral.com/1741-7015/10/21/abstract Age-related macular degeneration (AMD) is a leading cause of blindness that affects the central region of the retinal pigmented epithelium (RPE), choroid, and neural retina. Initially characterized by an accumulation of sub-RPE deposits, AMD leads to progressive retinal degeneration, and in advanced cases, irreversible vision loss. Although genetic analysis, animal models, and cell culture systems have yielded important insights into AMD, the molecular pathways underlying AMD's onset and progression remain poorly delineated. We sought to better understand the molecular underpinnings of this devastating disease by performing the first comparative transcriptome analysis of AMD and normal human donor eyes. RPE-choroid and retina tissue samples were obtained from a common cohort of 31 normal, 26 AMD, and 11 potential pre-AMD human donor eyes. Transcriptome profiles were generated for macular and extramacular regions, and statistical and bioinformatic methods were employed to identify disease-associated gene signatures and functionally enriched protein association networks. Selected genes of high significance were validated using an independent donor cohort. We identified over 50 annotated genes enriched in cell-mediated immune responses that are globally over-expressed in RPE-choroid AMD phenotypes. Using a machine learning model and a second donor cohort, we show that the top 20 global genes are predictive of AMD clinical diagnosis. We also discovered functionally enriched gene sets in the RPE-choroid that delineate the advanced AMD phenotypes, neovascular AMD and geographic atrophy. Moreover, we identified a graded increase of transcript levels in the retina related to wound response, complement cascade, and neurogenesis that strongly correlates with decreased levels of phototransduction transcripts and increased AMD severity. Based on our findings, we assembled protein-protein interactomes that highlight functional networks likely to be involved in AMD pathogenesis. We discovered new global biomarkers and gene expression signatures of AMD. These results are consistent with a model whereby cell-based inflammatory responses represent a central feature of AMD etiology, and depending on genetics, environment, or stochastic factors, may give rise to the advanced AMD phenotypes characterized by angiogenesis and/or cell death. Genes regulating these immunological activities, along with numerous other genes identified here, represent promising new targets for AMD-directed therapeutics and diagnostics.", "keywords": ["Age-related macular degeneration", "retinal pigmented epithelium", "AMD", "Age-related macular", "AMD phenotypes", "advanced AMD phenotypes", "pigmented epithelium", "progressive retinal degeneration", "blindness that affects", "advanced AMD", "RPE", "retinal pigmented", "Age-related", "retinal degeneration", "human donor eyes", "RPE-choroid AMD phenotypes", "genes", "AMD phenotypes characterized", "donor", "AMD leads"], "paper_title": "Systems-level analysis of age-related macular degeneration reveals global biomarkers and phenotype-specific functional networks.", "last_updated": "2023/02/04"}, {"id": "0029101008", "domain": "Macular degeneration", "model_name": "Aslam et al.", "publication_date": "2017/10/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29101008/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "29101008", "task": null, "abstract": "To develop a neural network for the estimation of visual acuity from optical coherence tomography (OCT) images of patients with neovascular age-related macular degeneration (AMD) and to demonstrate its use to model the impact of specific controlled OCT changes on vision. Artificial intelligence (neural network) study. We assessed 1400 OCT scans of patients with neovascular AMD. Fifteen physical features for each eligible OCT, as well as patient age, were used as input data and corresponding recorded visual acuity as the target data to train, validate, and test a supervised neural network. We then applied this network to model the impact on acuity of defined OCT changes in subretinal fluid, subretinal hyperreflective material, and loss of external limiting membrane (ELM) integrity. A total of 1210 eligible OCT scans were analyzed, resulting in 1210 data points, which were each 16-dimensional. A 10-layer feed-forward neural network with 1 hidden layer of 10 neurons was trained to predict acuity and demonstrated a root mean square error of 8.2 letters for predicted compared to actual visual acuity and a mean regression coefficient of 0.85. A virtual model using this network demonstrated the relationship of visual acuity to specific, programmed changes in OCT characteristics. When ELM is intact, there is a shallow decline in acuity with increasing subretinal fluid but a much steeper decline with equivalent increasing subretinal hyperreflective material. When ELM is not intact, all visual acuities are reduced. Increasing subretinal hyperreflective material or subretinal fluid in this circumstance reduces vision further still, but with a smaller gradient than when ELM is intact. The supervised machine learning neural network developed is able to generate an estimated visual acuity value from OCT images in a population of patients with AMD. These findings should be of clinical and research interest in macular degeneration, for example in estimating visual prognosis or highlighting the importance of developing treatments targeting more visually destructive pathologies.", "keywords": ["optical coherence tomography", "OCT", "neural network", "specific controlled OCT", "visual acuity", "coherence tomography", "controlled OCT", "acuity", "network", "optical coherence", "visual", "AMD", "ELM", "OCT scans", "eligible OCT", "subretinal", "subretinal hyperreflective material", "neural", "neovascular AMD", "neovascular age-related macular"], "paper_title": "Use of a Neural Net to Model the Impact of Optical Coherence Tomography Abnormalities on Vision in Age-related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0035547561", "domain": "Diabetic retinopathy", "model_name": "medical-image-classification-algorithm-based-on-n-gram-model.git", "publication_date": "2022/05/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35547561/", "code_link": "https://github.com/wanggf618/medical-image-classification-algorithm-based-on-n-gram-model.git", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "35547561", "task": "IWqQC1koJA", "abstract": "Imbalanced classes and dimensional disasters are critical challenges in medical image classification. As a classical machine learning model, the <i>n</i>-gram model has shown excellent performance in addressing this issue in text classification. In this study, we proposed an algorithm to classify medical images by extracting their <i>n</i>-gram semantic features. This algorithm first converts an image classification problem to a text classification problem by building an <i>n</i>-gram corpus for an image. After that, the algorithm was based on the <i>n</i>-gram model to classify images. The algorithm was evaluated by two independent public datasets. The first experiment is to diagnose benign and malignant thyroid nodules. The best area under the curve (AUC) is 0.989. The second experiment is to diagnose the type of fundus lesion. The best result is that it correctly identified 86.667% of patients with dry age-related macular degeneration (AMD), 93.333% of patients with diabetic macular edema (DME), and 93.333% of normal individuals.", "keywords": ["medical image classification", "Imbalanced classes", "classes and dimensional", "dimensional disasters", "disasters are critical", "critical challenges", "image classification", "text classification", "image classification problem", "classification", "classification problem", "gram model", "text classification problem", "gram", "classify medical images", "algorithm", "image", "medical image", "gram semantic features", "model"], "paper_title": "A Novel <i>N</i>-Gram-Based Image Classification Model and Its Applications in Diagnosing Thyroid Nodule and Retinal OCT Images.", "last_updated": "2023/02/04"}, {"id": "0035751184", "domain": "Glaucoma (unspecified)", "model_name": "Glaret Subin et al.", "publication_date": "2022/05/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35751184/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35751184", "task": "IWqQC1koJA", "abstract": "World health organization (WHO) reports around 2.2 billion people in the world as visually challenged which is mostly due to the age-related eye diseases such as age-related macular degeneration (AMD), cataract, diabetic retinopathy (DR) and glaucoma. These diseases lead to blindness if not diagnosed at an early stage. This paper focuses on the identification of the age-related eye diseases at an early stage using retinal fundus images taken from online dataset and pre-processed using maximum entropy transformation. The pre-processed images were fed to a convolution neural network (CNN), which was optimized using a flower pollination optimization algorithm (FPOA) for feature extraction. Hyperparameters were optimized using FPOA for training the CNN. This increased the speed and the accuracy of the network. The CNN output was fed to a Multiclass Support Vector Machine (MSVM) classifier for the classification of the type of disease. The proposed CNN-based multiple disease detection (CNN-MDD) was tested with the online dataset, namelyOcular Disease Intelligent Recognition (ODIR). The proposed model performance was analysed with the other optimized models which yielded the best performance in terms of precision, accuracy, specificity, recall, and F1 score of 98.30%, 95.27%, 95.21%, and 93.3%, respectively. The proposed method assisted automatic detection of the type of disease. Overall, this approach can be of great assistance to the medical professionals concerned in the treatment of eye diseases.", "keywords": ["World health organization", "age-related macular degeneration", "age-related eye diseases", "World health", "diabetic retinopathy", "health organization", "billion people", "macular degeneration", "eye diseases", "visually challenged", "age-related eye", "age-related macular", "world as visually", "early stage", "AMD", "Disease Intelligent Recognition", "World", "Support Vector Machine", "Multiclass Support Vector", "CNN"], "paper_title": "Optimized convolution neural network based multiple eye disease detection.", "last_updated": "2023/02/04"}, {"id": "0034216803", "domain": "Glaucoma (unspecified)", "model_name": "Ma et al.", "publication_date": "2021/06/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34216803/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34216803", "task": null, "abstract": "Aging is a major risk factor for various eye diseases, such as cataract, glaucoma, and age-related macular degeneration. Age-related changes are observed in almost all structures of the human eye. Considerable individual variations exist within a group of similarly aged individuals, indicating the need for more informative biomarkers for assessing the aging of the eyes. The morphology of the ocular anterior segment has been reported to vary across age groups, focusing on only a few corneal parameters, such as keratometry and thickness of the cornea, which could not provide accurate estimation of age. Thus, the association between eye aging and the morphology of the anterior segment remains elusive. In this study, we aimed to develop a predictive model of age based on a large number of anterior segment morphology-related features, measured via the high-resolution ocular anterior segment analysis system (Pentacam). This approach allows for an integrated assessment of age-related changes in corneal morphology, and the identification of important morphological features associated with different eye aging patterns. Three machine learning methods (neural networks, Lasso regression and extreme gradient boosting) were employed to build predictive models using 276 anterior segment features of 63,753 participants from 10 ophthalmic centers in 10 different cities of China. The best performing age prediction model achieved a median absolute error of 2.80\u00a0years and a mean absolute error of 3.89\u00a0years in the validation set. An external cohort of 100 volunteers was used to test the performance of the prediction model. The developed neural network model achieved a median absolute error of 3.03\u00a0years and a mean absolute error of 3.40\u00a0years in the external cohort. In summary, our study revealed that the anterior segment morphology of the human eye may be an informative and non-invasive indicator of eye aging. This could prompt doctors to focus on age-related medical interventions on ocular health.", "keywords": ["major risk factor", "age-related macular degeneration", "anterior segment", "ocular anterior segment", "eye aging", "macular degeneration", "major risk", "risk factor", "anterior", "eye", "anterior segment morphology", "absolute error", "segment", "eye diseases", "anterior segment features", "Aging", "ocular anterior", "median absolute error", "age-related macular", "human eye"], "paper_title": "Predictive models of aging of the human eye based on ocular anterior segment morphology.", "last_updated": "2023/02/04"}, {"id": "0024548898", "domain": "Diabetic retinopathy", "model_name": "Akram et al.", "publication_date": "2014/01/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24548898/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "24548898", "task": "IWqQC1koJA", "abstract": "Medical systems based on state of the art image processing and pattern recognition techniques are very common now a day. These systems are of prime interest to provide basic health care facilities to patients and support to doctors. Diabetic macular edema is one of the retinal abnormalities in which diabetic patient suffers from severe vision loss due to affected macula. It affects the central vision of the person and causes total blindness in severe cases. In this article, we propose an intelligent system for detection and grading of macular edema to assist the ophthalmologists in early and automated detection of the disease. The proposed system consists of a novel method for accurate detection of macula using a detailed feature set and Gaussian mixtures model based classifier. We also present a new hybrid classifier as an ensemble of Gaussian mixture model and support vector machine for improved exudate detection even in the presence of other bright lesions which eventually leads to reliable classification of input retinal image in different stages of macular edema. The statistical analysis and comparative evaluation of proposed system with existing methods are performed on publicly available standard retinal image databases. The proposed system has achieved average value of 97.3%, 95.9% and 96.8% for sensitivity, specificity and accuracy respectively on both databases.", "keywords": ["pattern recognition techniques", "art image processing", "processing and pattern", "pattern recognition", "recognition techniques", "Medical systems based", "macular edema", "proposed system", "Medical systems", "art image", "image processing", "Diabetic macular edema", "system", "detection", "Gaussian mixtures model", "macular", "provide basic health", "basic health care", "health care facilities", "edema"], "paper_title": "Automated detection of exudates and macula for grading of diabetic macular edema.", "last_updated": "2023/02/04"}, {"id": "0028433753", "domain": "Diabetic retinopathy", "model_name": "Gerendas et al.", "publication_date": "2017/05/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28433753/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28433753", "task": null, "abstract": "In this pilot study, we evaluated the potential of computational image analysis of optical coherence tomography (OCT) data to determine the prognosis of patients with diabetic macular edema (DME). Spectral-domain OCT scans with fully automated retinal layer segmentation and segmentation of intraretinal cystoid fluid (IRC) and subretinal fluid of 629 patients receiving anti-vascular endothelial growth factor therapy for DME in a randomized prospective clinical trial were analyzed. The results were used to define 312 potentially predictive features at three timepoints (baseline, weeks 12 and 24) for best-corrected visual acuity (BCVA) at baseline and after one year used in a random forest prediction path. Preliminarily, IRC in the outer nuclear layer in the 3-mm area around the fovea seemed to have the greatest predictive value for BCVA at baseline, and IRC and the total retinal thickness in the 3-mm area at weeks 12 and 24 for BCVA after one year. The overall model accuracy was R<sup>2</sup>=0.21/0.23 (p<0.001). The outcomes of this pilot analysis highlight the great potential of the proposed machine-learning approach for large-scale image data analysis in DME and other retinal diseases.", "keywords": ["optical coherence tomography", "diabetic macular edema", "Spectral-domain OCT scans", "coherence tomography", "macular edema", "optical coherence", "determine the prognosis", "diabetic macular", "computational image analysis", "Spectral-domain OCT", "patients receiving anti-vascular", "OCT scans", "DME", "pilot study", "OCT", "potential of computational", "retinal layer segmentation", "computational image", "BCVA", "intraretinal cystoid fluid"], "paper_title": "Computational image analysis for prognosis determination in DME.", "last_updated": "2023/02/04"}, {"id": "0029971444", "domain": "Macular degeneration", "model_name": "Schmidt-Erfurth et al.", "publication_date": "2019/03/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29971444/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "29971444", "task": null, "abstract": "While millions of individuals show early age-related macular degeneration (AMD) signs, yet have excellent vision, the risk of progression to advanced AMD with legal blindness is highly variable. We suggest means of artificial intelligence to individually predict AMD progression. In eyes with intermediate AMD, progression to the neovascular type with choroidal neovascularization (CNV) or the dry type with geographic atrophy (GA) was diagnosed based on standardized monthly optical coherence tomography (OCT) images by independent graders. We obtained automated volumetric segmentation of outer neurosensory layers and retinal pigment epithelium, drusen, and hyperreflective foci by spectral domain-OCT image analysis. Using imaging, demographic, and genetic input features, we developed and validated a machine learning-based predictive model assessing the risk of conversion to advanced AMD. Of a total of 495 eyes, 159 eyes (32%) had converted to advanced AMD within 2 years, 114 eyes progressed to CNV, and 45 to GA. Our predictive model differentiated converting versus nonconverting eyes with a performance of 0.68 and 0.80 for CNV and GA, respectively. The most critical quantitative features for progression were outer retinal thickness, hyperreflective foci, and drusen area. The features for conversion showed pathognomonic patterns that were distinctly different for the neovascular and the atrophic pathways. Predictive hallmarks for CNV were mostly drusen-centric, while GA markers were associated with neurosensory retina and age. Artificial intelligence with automated analysis of imaging biomarkers allows personalized prediction of AMD progression. Moreover, pathways of progression may be specific in respect to the neovascular/atrophic type.", "keywords": ["age-related macular degeneration", "individuals show early", "show early age-related", "early age-related macular", "advanced AMD", "AMD", "macular degeneration", "excellent vision", "highly variable", "millions of individuals", "individuals show", "show early", "early age-related", "age-related macular", "legal blindness", "blindness is highly", "AMD progression", "predict AMD progression", "progression", "individually predict AMD"], "paper_title": "Prediction of Individual Disease Conversion in Early AMD Using Artificial Intelligence.", "last_updated": "2023/02/04"}, {"id": "0032321392", "domain": "Macular degeneration", "model_name": "Thakur et al.", "publication_date": "2021/01/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32321392/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32321392", "task": "IWqQC1koJA", "abstract": "The diagnosis and prognosis of pathological conditions, such as age-related macular degeneration (AMD) and cancer still need improvement. AMD is primarily caused due to the dysfunction of retinal pigment epithelium (RPE), whereas endothelial cells (ECs) play one of the major roles in angiogenesis; an important process which occurs in malignant progression of cancer. Several reports suggested the augmented release of nano-vesicles under pathological conditions, including from RPE as well as cancer-associated ECs, which take part in various biological processes, including intercellular communication in disease progression. Importantly, these nano-vesicles are around 30-1000 nm and carry the fingerprint of their initiating parent cells (IPCs). Therefore, these nano-vesicles could be utilized as the diagnostic tool for AMD and cancer, respectively. However, the analysis of nano-vesicles for biomarker study is confounded by their extensive heterogeneous nature. To confront this challenge, we utilized artificial intelligence (AI) based machine learning (ML) algorithms such as support vector machine (SVM) and decision tree model on the dataset of nano-vesicles from RPE and ECs cell lines with low dimensionality. Overall, Gaussian SVM demonstrated the highest prediction accuracy of the IPCs of nano-vesicles, among all the chosen SVM classifiers. Additionally, the bagged tree showed the highest prediction among the chosen decision tree-based classifiers. Therefore, the overall bagged tree showed the best performance for the prediction of IPCs of nanovesicles, suggesting the applicability of AI-based prediction approach in diagnosis and prognosis of pathological conditions, including non-invasive liquid biopsy via various biofluids-derived nano-vesicles.", "keywords": ["age-related macular degeneration", "pathological conditions", "macular degeneration", "age-related macular", "nano-vesicles", "AMD", "RPE", "conditions", "cancer", "pathological", "SVM", "prediction", "ECs", "prognosis of pathological", "Gaussian SVM demonstrated", "bagged tree showed", "Gaussian SVM", "retinal pigment epithelium", "chosen SVM classifiers", "primarily caused due"], "paper_title": "Detection of Disease-Specific Parent Cells Via Distinct Population of Nano-Vesicles by Machine Learning.", "last_updated": "2023/02/04"}, {"id": "0032816791", "domain": "Diabetic retinopathy", "model_name": "Prasanna et al.", "publication_date": "2020/08/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32816791/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32816791", "task": null, "abstract": "To evaluate the potential of radiomics-based ultra-widefield fluorescein angiography (UWFA)-derived imaging biomarkers in retinal vascular disease for predicting therapeutic durability of intravitreal aflibercept injection (IAI). The Peripheral and Macular Retinal Vascular Perfusion and Leakage Dynamics in Diabetic Macular Edema and Retinal Venous Occlusions During Intravitreal Aflibercept Injection (IAI) Treatment for Retinal Edema (PERMEATE) study prospectively evaluated quantitative UWFA dynamics in diabetic macular oedema or macular oedema secondary to retinal vascular occlusion. 27 treatment-na\u00efve eyes were treated with 2 mg IAI q4 weeks for the first 6\u00a0months, and then administered q8 weeks. Morphological and graph-based attributes were used to model the spatial distribution of leakage areas, while tortuosity measures were used to model the vessel network disorder. Eyes were grouped based on functional tolerance of the first 8-week treatment interval challenge. 'Non-rebounders' (N=15) maintained/improved best-corrected visual acuity (BCVA) following the 8-week challenge. 'Rebounders' (N=12) exhibited worsened BVCA. The image biomarkers were used with a machine learning classifier to preliminarily evaluate their ability to predict BCVA stability. Two new UWFA image-derived biomarkers were identified and extracted. The cross-validated area under the receiver operating characteristic curve (AUC) was 0.77\u00b10.14 using baseline leakage distribution features and 0.73\u00b10.10 for the UWFA baseline tortuosity measures. Additionally, the change in vascular tortuosity between month 4 and baseline yielded an AUC of 0.73\u00b10.08. Three baseline clinical features of letter score, macular volume and central subfield thickness yielded a corresponding AUC of 0.42\u00b10.09. Two computer-extracted UWFA radiomics-based descriptors were identified as potential biomarkers for predicting treatment durability and tolerance of longer treatment intervals. Conventional treatment parameters were not significantly different between these same groups.", "keywords": ["intravitreal aflibercept injection", "retinal vascular disease", "Retinal Vascular Perfusion", "Retinal Venous Occlusions", "retinal vascular occlusion", "ultra-widefield fluorescein angiography", "retinal vascular", "aflibercept injection", "Macular Retinal Vascular", "intravitreal aflibercept", "Diabetic Macular Edema", "Retinal Edema", "retinal", "derived imaging biomarkers", "Dynamics in Diabetic", "Retinal Venous", "radiomics-based ultra-widefield fluorescein", "fluorescein angiography", "derived imaging", "IAI"], "paper_title": "Radiomics-based assessment of ultra-widefield leakage patterns and vessel network architecture in the PERMEATE study: insights into treatment durability.", "last_updated": "2023/02/04"}, {"id": "0031212307", "domain": "Macular degeneration", "model_name": "Sumaroka et al.", "publication_date": "2019/12/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31212307/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "31212307", "task": null, "abstract": "To use supervised machine learning to predict visual function from retinal structure in retinitis pigmentosa (RP) and apply these estimates to CEP290- and NPHP5-associated Leber congenital amaurosis (LCA) to determine the potential for functional improvement. Patients with RP (n = 20) and LCA due to CEP290 (n = 12) or NPHP5 (n = 6) mutations were studied. A patient with CEP290 mutations but mild retinal degeneration was included. RP patients had cone-mediated macular function. A machine learning technique was used to associate perimetric sensitivities to local structure in RP patients. Models trained on RP data were applied to predict visual function in LCA. The RP and LCA patients had comparable retinal structure. RP patients had peak sensitivity at the fovea surrounded by decreasing sensitivity. Machine learning could successfully predict perimetry results from segmented or unsegmented optical coherence tomography (OCT) input. Application of machine learning predictions to LCA within the residual macular island of photoreceptor structure showed differences between predicted and measured sensitivities defining treatment potential. In patients with retained vision, the treatment potential was 4.6 \u00b1 2.9 dB at the fovea but 16.4 \u00b1 4.4 dB at the parafovea. In patients with limited or no vision, the treatment potential was 17.6 \u00b1 9.4 dB. Cone vision improvement potential in LCA due to CEP290 or NPHP5 mutations is predictable from retinal structure using a machine learning approach. This should allow individual prediction of the maximal efficacy in clinical trials and guide decisions about dosing. Similar strategies can be used in other retinal degenerations to estimate the extent and location of treatment potential.", "keywords": ["Leber congenital amaurosis", "Leber congenital", "LCA", "Patients", "retinitis pigmentosa", "congenital amaurosis", "machine learning", "Leber", "potential", "machine", "LCA due", "supervised machine learning", "retinal", "treatment potential", "structure", "learning", "retinal structure", "LCA patients", "mutations", "treatment"], "paper_title": "Treatment Potential for Macular Cone Vision in Leber Congenital Amaurosis Due to CEP290 or NPHP5 Mutations: Predictions From Artificial Intelligence.", "last_updated": "2023/02/04"}, {"id": "0030337078", "domain": "Diabetic retinopathy", "model_name": "Chakravarty et al.", "publication_date": "2018/09/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30337078/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30337078", "task": "aipbNdPTIt", "abstract": "Accurate segmentation of the intra-retinal tissue layers in Optical Coherence Tomography (OCT) images plays an important role in the diagnosis and treatment of ocular diseases such as Age-Related Macular Degeneration (AMD) and Diabetic Macular Edema (DME). The existing energy minimization based methods employ multiple, manually handcrafted cost terms and often fail in the presence of pathologies. In this work, we eliminate the need to handcraft the energy by learning it from training images in an end-to-end manner. Our method can be easily adapted to pathologies by re-training it on an appropriate dataset. We propose a Conditional Random Field (CRF) framework for the joint multi-layer segmentation of OCT B-scans. The appearance of each retinal layer and boundary is modeled by two convolutional filter banks and the shape priors are modeled using Gaussian distributions. The total CRF energy is linearly parameterized to allow a joint, end-to-end training by employing the Structured Support Vector Machine formulation. The proposed method outperformed three benchmark algorithms on four public datasets. The NORMAL-1 and NORMAL-2 datasets contain healthy OCT B-scans while the AMD-1 and DME-1 dataset contain B-scans of AMD and DME cases respectively. The proposed method achieved an average unsigned boundary localization error (U-BLE) of 1.52\u00a0pixels on NORMAL-1, 1.11\u00a0pixels on NORMAL-2 and 2.04 pixels on the combined NORMAL-1 and DME-1 dataset across the eight layer boundaries, outperforming the three benchmark methods in each case. The Dice coefficient was 0.87 on NORMAL-1, 0.89 on NORMAL-2 and 0.84 on the combined NORMAL-1 and DME-1 dataset across the seven retinal layers. On the combined NORMAL-1 and AMD-1 dataset, we achieved an average U-BLE of 1.86 pixels on the ILM, inner and outer RPE boundaries and a Dice of 0.98 for the ILM-RPE<sub>in</sub> region and 0.81 for the RPE layer. We have proposed a supervised CRF based method to jointly segment multiple tissue layers in OCT images. It can aid the ophthalmologists in the quantitative analysis of structural changes in the retinal tissue layers for clinical practice and large-scale clinical studies.", "keywords": ["Diabetic Macular Edema", "Optical Coherence Tomography", "Age-Related Macular Degeneration", "Macular Degeneration", "Macular Edema", "Diabetic Macular", "Coherence Tomography", "Optical Coherence", "Age-Related Macular", "Macular", "OCT B-scans", "plays an important", "important role", "diagnosis and treatment", "treatment of ocular", "ocular diseases", "intra-retinal tissue layers", "Conditional Random Field", "OCT", "Tomography"], "paper_title": "A supervised joint multi-layer segmentation framework for retinal optical coherence tomography images using conditional random field.", "last_updated": "2023/02/04"}, {"id": "0029131696", "domain": "Macular degeneration", "model_name": "Westborg et al.", "publication_date": "2017/11/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29131696/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "29131696", "task": null, "abstract": "To investigate risk factors for treatment discontinuation for neovascular age-related macular degeneration (nAMD). Data from the Swedish Macula Register and the Sk\u00e5ne Healthcare Register are reported on the treatment received by 932 nAMD patients diagnosed 2013-2015. Treatment discontinuation is defined as having a termination visit or lacking a control or treatment visit during the period of 10-14\u00a0months after the diagnostic visit. The risk of treatment discontinuation during the first year is estimated using a Poisson model and a classification tree. 503 eyes (50.9%) discontinued the treatment within the first year. Patients with visual acuity below 60 ETDRS letters (20/60 Snellen) at baseline, serious comorbidities, or treated at the university hospital have a 42% (95% CI 25-61%, P\u00a0<\u00a00.001), 27% (95% CI 13-43%, P\u00a0=\u00a00.001) and 30% (95% CI 15-46%, P\u00a0<\u00a00.001) increased risk to discontinue treatment compared with similar patients. Patients on ranibizumab therapy have a 45% (95% CI 28-63%, P\u00a0<\u00a00.001) increased risk for treatment discontinuation during year 1 compared with patients on aflibercept therapy. The classification tree also shows that patients on ranibizumab therapy and those with low VA at baseline are at a higher risk of terminating treatment. Almost half of the patients starting anti-VEGF therapy discontinue treatment during the first year. Patients with risk factors may require additional support to continue with the treatment. Aflibercept therapy could be an alternative to patients at risk of treatment discontinuation.", "keywords": ["Swedish Macula Register", "Sk\u00e5ne Healthcare Register", "age-related macular degeneration", "neovascular age-related macular", "treatment", "treatment discontinuation", "Macula Register", "Healthcare Register", "Swedish Macula", "Sk\u00e5ne Healthcare", "patients", "macular degeneration", "neovascular age-related", "age-related macular", "risk", "discontinuation", "Register", "therapy", "investigate risk factors", "year"], "paper_title": "Risk Factors for Discontinuation of Treatment for Neovascular Age-Related Macular Degeneration.", "last_updated": "2023/02/04"}, {"id": "0027576376", "domain": "Macular degeneration", "model_name": "gedi", "publication_date": "2016/08/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/27576376/", "code_link": "https://github.com/bhsai/gedi", "model_type": "GLM", "verified": false, "model_task": null, "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "27576376", "task": null, "abstract": "Genome-wide association studies provide important insights to the genetic component of disease risks. However, an existing challenge is how to incorporate collective effects of interactions beyond the level of independent single nucleotide polymorphism (SNP) tests. While methods considering each SNP pair separately have provided insights, a large portion of expected heritability may reside in higher-order interaction effects. We describe an inference approach (discrete discriminant analysis; DDA) designed to probe collective interactions while treating both genotypes and phenotypes as random variables. The genotype distributions in case and control groups are modeled separately based on empirical allele frequency and covariance data, whose differences yield disease risk parameters. We compared pairwise tests and collective inference methods, the latter based both on DDA and logistic regression. Analyses using simulated data demonstrated that significantly higher sensitivity and specificity can be achieved with collective inference in comparison to pairwise tests, and with DDA in comparison to logistic regression. Using age-related macular degeneration (AMD) data, we demonstrated two possible applications of DDA. In the first application, a genome-wide SNP set is reduced into a small number (\u223c100) of variants via filtering and SNP pairs with significant interactions are identified. We found that interactions between SNPs with highest AMD association were epigenetically active in the liver, adipocytes, and mesenchymal stem cells. In the other application, multiple groups of SNPs were formed from the genome-wide data and their relative strengths of association were compared using cross-validation. This analysis allowed us to discover novel collections of loci for which interactions between SNPs play significant roles in their disease association. In particular, we considered pathway-based groups of SNPs containing up to \u223c10, 000 variants in each group. In addition to pathways related to complement activation, our collective inference pointed to pathway groups involved in phospholipid synthesis, oxidative stress, and apoptosis, consistent with the AMD pathogenesis mechanism where the dysfunction of retinal pigment epithelium cells plays central roles. The simultaneous inference of collective interaction effects within a set of SNPs has the potential to reveal novel aspects of disease association.", "keywords": ["studies provide important", "provide important insights", "association studies provide", "studies provide", "provide important", "genetic component", "SNPs", "collective", "DDA", "collective inference", "SNP", "important insights", "inference", "interactions", "association", "Genome-wide association studies", "AMD", "disease", "interaction effects", "association studies"], "paper_title": "Genotype distribution-based inference of collective effects in genome-wide association studies: insights to age-related macular degeneration disease mechanism.", "last_updated": "2023/02/04"}, {"id": "0023281790", "domain": "Macular degeneration", "model_name": "Han et al.", "publication_date": "2012/12/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/23281790/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "23281790", "task": null, "abstract": "Detecting epistatic interactions plays a significant role in improving pathogenesis, prevention, diagnosis, and treatment of complex human diseases. Applying machine learning or statistical methods to epistatic interaction detection will encounter some common problems, e.g., very limited number of samples, an extremely high search space, a large number of false positives, and ways to measure the association between disease markers and the phenotype. To address the problems of computational methods in epistatic interaction detection, we propose a score-based Bayesian network structure learning method, EpiBN, to detect epistatic interactions. We apply the proposed method to both simulated datasets and three real disease datasets. Experimental results on simulation data show that our method outperforms some other commonly-used methods in terms of power and sample-efficiency, and is especially suitable for detecting epistatic interactions with weak or no marginal effects. Furthermore, our method is scalable to real disease data. We propose a Bayesian network-based method, EpiBN, to detect epistatic interactions. In EpiBN, we develop a new scoring function, which can reflect higher-order epistatic interactions by estimating the model complexity from data, and apply a fast Branch-and-Bound algorithm to learn the structure of a two-layer Bayesian network containing only one target node. To make our method scalable to real data, we propose the use of a Markov chain Monte Carlo (MCMC) method to perform the screening process. Applications of the proposed method to some real GWAS (genome-wide association studies) datasets may provide helpful insights into understanding the genetic basis of Age-related Macular Degeneration, late-onset Alzheimer's disease, and autism.", "keywords": ["epistatic interactions", "epistatic interactions plays", "epistatic interaction detection", "complex human diseases", "Detecting epistatic interactions", "epistatic", "improving pathogenesis", "method", "plays a significant", "significant role", "role in improving", "treatment of complex", "complex human", "interactions", "interactions plays", "interaction detection", "Detecting epistatic", "detect epistatic", "Bayesian", "Bayesian network"], "paper_title": "Genetic studies of complex human diseases: characterizing SNP-disease associations using Bayesian networks.", "last_updated": "2023/02/04"}, {"id": "0029986868", "domain": "Macular degeneration", "model_name": "Cohen-Tayar et al.", "publication_date": "2018/08/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29986868/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "29986868", "task": null, "abstract": "The synchronized differentiation of neuronal and vascular tissues is crucial for normal organ development and function, although there is limited information about the mechanisms regulating the coordinated development of these tissues. The choroid vasculature of the eye serves as the main blood supply to the metabolically active photoreceptors, and develops together with the retinal pigmented epithelium (RPE). Here, we describe a novel regulatory relationship between the RPE transcription factors Pax6 and Sox9 that controls the timing of RPE differentiation and the adjacent choroid maturation. We used a novel machine learning algorithm tool to analyze high resolution imaging of the choroid in <i>Pax6</i> and <i>Sox9</i> conditional mutant mice. Additional unbiased transcriptomic analyses in mutant mice and RPE cells generated from human embryonic stem cells, as well as chromatin immunoprecipitation and high-throughput analyses, revealed secreted factors that are regulated by Pax6 and Sox9. These factors might be involved in choroid development and in the pathogenesis of the common blinding disease: age-related macular degeneration (AMD).", "keywords": ["normal organ development", "vascular tissues", "neuronal and vascular", "crucial for normal", "normal organ", "limited information", "mechanisms regulating", "regulating the coordinated", "tissues is crucial", "synchronized differentiation", "RPE", "RPE transcription factors", "RPE differentiation", "organ development", "coordinated development", "tissues", "choroid", "metabolically active photoreceptors", "retinal pigmented epithelium", "RPE cells generated"], "paper_title": "Pax6 regulation of <i>Sox9</i> in the mouse retinal pigmented epithelium controls its timely differentiation and choroid vasculature development.", "last_updated": "2023/02/04"}, {"id": "0036817116", "domain": "Macular degeneration", "model_name": "standardiser", "publication_date": "2023/02/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36817116/", "code_link": "https://github.com/flatkinson/standardiser", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "36817116", "task": null, "abstract": "Uncontrolled angiogenesis is a common denominator underlying many deadly and debilitating diseases such as myocardial infarction, chronic wounds, cancer, and age-related macular degeneration. As the current range of FDA-approved angiogenesis-based medicines are far from meeting clinical demands, the vast reserve of natural products from traditional Chinese medicine (TCM) offers an alternative source for developing pro-angiogenic or anti-angiogenic modulators. Here, we investigated 100 traditional Chinese medicine-derived individual metabolites which had reported gene expression in MCF7 cell lines in the Gene Expression Omnibus (GSE85871). We extracted literature angiogenic activities for 51 individual metabolites, and subsequently analysed their predicted targets and differentially expressed genes to understand their mechanisms of action. The angiogenesis phenotype was used to generate decision trees for rationalising the poly-pharmacology of known angiogenesis modulators such as ferulic acid and curculigoside and validated by an <i>in vitro</i> endothelial tube formation assay and a zebrafish model of angiogenesis. Moreover, using an <i>in silico</i> model we prospectively examined the angiogenesis-modulating activities of the remaining 49 individual metabolites. <i>In vitro</i>, tetrahydropalmatine and 1 beta-hydroxyalantolactone stimulated, while cinobufotalin and isoalantolactone inhibited endothelial tube formation. <i>In vivo</i>, ginsenosides Rb3 and Rc, 1 beta-hydroxyalantolactone and surprisingly cinobufotalin, restored angiogenesis against PTK787-induced impairment in zebrafish. In the absence of PTK787, deoxycholic acid and ursodeoxycholic acid did not affect angiogenesis. Despite some limitations, these results suggest further refinements of <i>in silico</i> prediction combined with biological assessment will be a valuable platform for accelerating the research and development of natural products from traditional Chinese medicine and understanding their mechanisms of action, and also for other traditional medicines for the prevention and treatment of angiogenic diseases.", "keywords": ["age-related macular degeneration", "common denominator underlying", "traditional Chinese medicine", "traditional Chinese", "Gene Expression Omnibus", "chronic wounds", "myocardial infarction", "macular degeneration", "traditional Chinese medicine-derived", "common denominator", "denominator underlying", "underlying many deadly", "deadly and debilitating", "age-related macular", "Chinese medicine", "Chinese medicine-derived individual", "gene expression", "individual metabolites", "Chinese", "reported gene expression"], "paper_title": "<i>In silico</i> prediction and biological assessment of novel angiogenesis modulators from traditional Chinese medicine.", "last_updated": "2023/02/04"}, {"id": "0032750923", "domain": "Macular degeneration", "model_name": "MS-CAM", "publication_date": "2020/12/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32750923/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32750923", "task": "aipbNdPTIt", "abstract": "As one of the most critical characteristics in advanced stage of non-exudative Age-related Macular Degeneration (AMD), Geographic Atrophy (GA) is one of the significant causes of sustained visual acuity loss. Automatic localization of retinal regions affected by GA is a fundamental step for clinical diagnosis. In this paper, we present a novel weakly supervised model for GA segmentation in Spectral-Domain Optical Coherence Tomography (SD-OCT) images. A novel Multi-Scale Class Activation Map (MS-CAM) is proposed to highlight the discriminatory significance regions in localization and detail descriptions. To extract available multi-scale features, we design a Scaling and UpSampling (SUS) module to balance the information content between features of different scales. To capture more discriminative features, an Attentional Fully Connected (AFC) module is proposed by introducing the attention mechanism into the fully connected operations to enhance the significant informative features and suppress less useful ones. Based on the location cues, the final GA region prediction is obtained by the projection segmentation of MS-CAM. The experimental results on two independent datasets demonstrate that the proposed weakly supervised model outperforms the conventional GA segmentation methods and can produce similar or superior accuracy comparing with fully supervised approaches. The source code has been released and is available on GitHub: https://github.com/ jizexuan/Multi-Scale-Class-Activation-Map-Tensorflow.", "keywords": ["Age-related Macular Degeneration", "non-exudative Age-related Macular", "Geographic Atrophy", "visual acuity loss", "Macular Degeneration", "sustained visual acuity", "Age-related Macular", "non-exudative Age-related", "Optical Coherence Tomography", "acuity loss", "critical characteristics", "characteristics in advanced", "advanced stage", "stage of non-exudative", "sustained visual", "visual acuity", "Class Activation Map", "Spectral-Domain Optical Coherence", "Multi-Scale Class Activation", "AMD"], "paper_title": "MS-CAM: Multi-Scale Class Activation Maps for Weakly-Supervised Segmentation of Geographic Atrophy Lesions in SD-OCT Images.", "last_updated": "2023/02/04"}, {"id": "0032873827", "domain": "Macular degeneration", "model_name": "cellmagicwand", "publication_date": "2020/09/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32873827/", "code_link": "https://github.com/fitzlab/cellmagicwand", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "cBTqdRN4gQ", "pmid": "32873827", "task": null, "abstract": "Transplantation of retinal pigment epithelial (RPE) sheets derived from human induced pluripotent cells (hiPSC) is a promising cell therapy for RPE degeneration, such as in age-related macular degeneration. Current RPE replacement therapies, however, face major challenges. They require a tedious manual process of selecting differentiated RPE from hiPSC-derived cells, and despite wide variation in quality of RPE sheets, there exists no efficient process for distinguishing functional RPE sheets from those unsuitable for transplantation. To overcome these issues, we developed methods for the generation of RPE sheets from hiPSC, and image-based evaluation. We found that stepwise treatment with six signaling pathway inhibitors along with nicotinamide increased RPE differentiation efficiency (RPE6iN), enabling the RPE sheet generation at high purity without manual selection. Machine learning models were developed based on cellular morphological features of F-actin-labeled RPE images for predicting transepithelial electrical resistance values, an indicator of RPE sheet function. Our model was effective at identifying low-quality RPE sheets for elimination, even when using label-free images. The RPE6iN-based RPE sheet generation combined with the non-destructive image-based prediction offers a comprehensive new solution for the large-scale production of pure RPE sheets with lot-to-lot variations and should facilitate the further development of RPE replacement therapies.", "keywords": ["age-related macular degeneration", "retinal pigment epithelial", "RPE sheets", "human induced pluripotent", "RPE", "RPE sheet generation", "RPE sheet", "induced pluripotent cells", "promising cell therapy", "RPE degeneration", "RPE replacement therapies", "macular degeneration", "functional RPE sheets", "pigment epithelial", "RPE sheet function", "RPE replacement", "retinal pigment", "derived from human", "human induced", "induced pluripotent"], "paper_title": "Reproducible production and image-based quality evaluation of retinal pigment epithelium sheets from human induced pluripotent stem cells.", "last_updated": "2023/02/04"}, {"id": "0028475051", "domain": "Macular degeneration", "model_name": "Vogl et al.", "publication_date": "2017/05/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28475051/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "28475051", "task": null, "abstract": "Prediction of treatment responses from available data is key to optimizing personalized treatment. Retinal diseases are treated over long periods and patients' response patterns differ substantially, ranging from a complete response to a recurrence of the disease and need for re-treatment at different intervals. Linking observable variables in high-dimensional observations to outcome is challenging. In this paper, we present and evaluate two different data-driven machine learning approaches operating in a high-dimensional feature space: sparse logistic regression and random forests-based extra trees (ET). Both identify spatio-temporal signatures based on retinal thickness features measured in longitudinal spectral-domain optical coherence tomography (OCT) imaging data and predict individual patient outcome using these quantitative characteristics. We demonstrate on a data set of monthly SD-OCT scans of 155 patients with central retinal vein occlusion (CRVO) and 92 patients with branch retinal vein occlusion (BRVO) followed over one year that we can predict from initial three observations if the treated disease will recur within the covered interval. ET predicts the outcome on fivefold cross-validation with an area under the receiver operating characteristic curve (AuC) of 0.83 for BRVO and 0.76 for CRVO. Logistic regression achieved an AuC of 0.78 and 0.79, respectively. At the same time, the methods identified stable predictive signatures in the longitudinal imaging data that are the basis for accurate prediction. Furthermore, our results show that taking spatio-temporal features into account improves accuracy compared with features extracted at a single time-point. Our results demonstrate the feasibility of mining longitudinal data for predictive signatures, and building predictive models based on observed data.", "keywords": ["optimizing personalized treatment", "personalized treatment", "key to optimizing", "optimizing personalized", "treatment responses", "treatment", "data", "patients' response patterns", "response patterns differ", "retinal vein occlusion", "Retinal", "response", "patients' response", "response patterns", "complete response", "CRVO", "patterns differ substantially", "vein occlusion", "retinal vein", "features"], "paper_title": "Predicting Macular Edema Recurrence from Spatio-Temporal Signatures in Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0036846105", "domain": "Macular degeneration", "model_name": "Chew et al.", "publication_date": "2022/12/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36846105/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "36846105", "task": "IWqQC1koJA", "abstract": "To develop a severity classification for macular telangiectasia type 2 (MacTel) disease using multimodal imaging. An algorithm was used on data from a prospective natural history study of MacTel for classification development. A total of 1733 participants enrolled in an international natural history study of MacTel. The Classification and Regression Trees (CART), a predictive nonparametric algorithm used in machine learning, analyzed the features of the multimodal imaging important for the development of a classification, including reading center gradings of the following digital images: stereoscopic color and red-free fundus photographs, fluorescein angiographic images, fundus autofluorescence images, and spectral-domain (SD)-OCT images. Regression models that used least square method created a decision tree using features of the ocular images into different categories of disease severity. The primary target of interest for the algorithm development by CART was the change in best-corrected visual acuity (BCVA) at baseline for the right and left eyes. These analyses using the algorithm were repeated for the BCVA obtained at the last study visit of the natural history study for the right and left eyes. The CART analyses demonstrated 3 important features from the multimodal imaging for the classification: OCT hyper-reflectivity, pigment, and ellipsoid zone loss. By combining these 3 features (as absent, present, noncentral involvement, and central involvement of the macula), a 7-step scale was created, ranging from excellent to poor visual acuity. At grade 0, 3 features are not present. At the most severe grade, pigment and exudative neovascularization are present. To further validate the classification, using the Generalized Estimating Equation regression models, analyses for the annual relative risk of progression over a period of 5 years for vision loss and for progression along the scale were performed. This analysis using the data from current imaging modalities in participants followed in the MacTel natural history study informed a classification for MacTel disease severity featuring variables from SD-OCT. This classification is designed to provide better communications to other clinicians, researchers, and patients. Proprietary or commercial disclosure may be found after the references.", "keywords": ["macular telangiectasia type", "natural history study", "history study", "natural history", "MacTel natural history", "classification", "telangiectasia type", "macular telangiectasia", "MacTel", "multimodal imaging", "MacTel disease severity", "study", "prospective natural history", "images", "history", "features", "natural", "CART", "Estimating Equation regression", "international natural history"], "paper_title": "Macular Telangiectasia Type 2: A Classification System Using MultiModal Imaging MacTel Project Report Number 10.", "last_updated": "2023/02/04"}, {"id": "0019208169", "domain": "Macular degeneration", "model_name": "Jiang et al.", "publication_date": "2009/01/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/19208169/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "19208169", "task": "IWqQC1koJA", "abstract": "The key roles of epistatic interactions between multiple genetic variants in the pathogenesis of complex diseases notwithstanding, the detection of such interactions remains a great challenge in genome-wide association studies. Although some existing multi-locus approaches have shown their successes in small-scale case-control data, the \"combination explosion\" course prohibits their applications to genome-wide analysis. It is therefore indispensable to develop new methods that are able to reduce the search space for epistatic interactions from an astronomic number of all possible combinations of genetic variants to a manageable set of candidates. We studied case-control data from the viewpoint of binary classification. More precisely, we treated single nucleotide polymorphism (SNP) markers as categorical features and adopted the random forest to discriminate cases against controls. On the basis of the gini importance given by the random forest, we designed a sliding window sequential forward feature selection (SWSFS) algorithm to select a small set of candidate SNPs that could minimize the classification error and then statistically tested up to three-way interactions of the candidates. We compared this approach with three existing methods on three simulated disease models and showed that our approach is comparable to, sometimes more powerful than, the other methods. We applied our approach to a genome-wide case-control dataset for Age-related Macular Degeneration (AMD) and successfully identified two SNPs that were reported to be associated with this disease. Besides existing pure statistical approaches, we demonstrated the feasibility of incorporating machine learning methods into genome-wide case-control studies. The gini importance offers yet another measure for the associations between SNPs and complex diseases, thereby complementing existing statistical measures to facilitate the identification of epistatic interactions and the understanding of epistasis in the pathogenesis of complex diseases.", "keywords": ["multiple genetic variants", "complex diseases notwithstanding", "key roles", "remains a great", "great challenge", "epistatic interactions", "genetic variants", "multiple genetic", "complex diseases", "interactions remains", "interactions", "diseases notwithstanding", "Age-related Macular Degeneration", "genome-wide case-control", "case-control data", "roles of epistatic", "genome-wide case-control studies", "genome-wide", "small-scale case-control data", "case-control"], "paper_title": "A random forest approach to the detection of epistatic interactions in case-control studies.", "last_updated": "2023/02/04"}, {"id": "0032715858", "domain": "Macular degeneration", "model_name": "DeBenedictis et al.", "publication_date": "2020/07/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32715858/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "32715858", "task": null, "abstract": "Sorsby Fundus Dystrophy is an inherited macular degeneration caused by pathogenic variants in the <i>TIMP3</i> gene. Clinical exam findings typically drusen -like deposits beneath the RPE or reticular pseudo drusen deposits above the RPE with a majority of patients developing choroidal neovascularization. Case report of two members of a family that present with atypical clinical exam findings. Protein modeling of the novel Y137CTIMP3 variant was performed and compared with other known variants. In this study we describe a father and son initially diagnosed with retinitis pigmentosa of unknown genetic origin. More recent genetic testing of the patients, identified a novel c.410A>G; p.Tyr137Cys variant of uncertain clinical significance in the Tissue Inhibitor of Metalloproteinase-3 (<i>TIMP3</i>) gene. The atypical clinical findings led us to compare the theoretical molecular effects of this variant on the TIMP3 protein structure and interactions with other proteins using homology modeling and machine learning predictions. It is important to consider mutations in TIMP3 in atypical cases of Retinitis Pigmentosa particularly in the absence of known variants.", "keywords": ["Sorsby Fundus Dystrophy", "Fundus Dystrophy", "inherited macular degeneration", "macular degeneration caused", "Sorsby Fundus", "Clinical exam findings", "inherited macular", "macular degeneration", "degeneration caused", "caused by pathogenic", "Fundus", "Dystrophy", "exam findings", "Clinical exam", "exam findings typically", "beneath the RPE", "RPE", "retinitis pigmentosa", "pathogenic variants", "findings typically drusen"], "paper_title": "A novel <i>TIMP3</i> mutation associated with a retinitis pigmentosa-like phenotype.", "last_updated": "2023/02/04"}, {"id": "0026737478", "domain": "Glaucoma (unspecified)", "model_name": "Chen et al.", "publication_date": "2016/10/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26737478/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "26737478", "task": "IWqQC1koJA", "abstract": "In this paper, we present a multiple ocular diseases detection scheme based on joint sparse multi-task learning. Glaucoma, Pathological Myopia (PM), and Age-related Macular Degeneration (AMD) are three major causes of vision impairment and blindness worldwide. The proposed joint sparse multitask learning framework aims to reconstruct a test fundus image with multiple features from as few training subjects as possible. The linear version of this problem could be casted into a multi-task joint covariate selection model, which can be very efficiently optimized via kernelizable accelerated proximal gradient method. Extensive experiments are conducted in order to validate the proposed framework on the SiMES dataset. From the Area Under Curve (AUC) results in multiple ocular diseases classification, our method is shown to outperform the state-of-the-art algorithms.", "keywords": ["detection scheme based", "Age-related Macular Degeneration", "diseases detection scheme", "detection scheme", "scheme based", "Pathological Myopia", "ocular diseases detection", "Macular Degeneration", "Age-related Macular", "multiple ocular diseases", "sparse multi-task learning", "joint sparse", "joint sparse multi-task", "diseases detection", "multiple ocular", "proposed joint sparse", "ocular diseases", "multiple", "joint", "paper"], "paper_title": "Multiple ocular diseases detection based on joint sparse multi-task learning.", "last_updated": "2023/02/04"}, {"id": "0025495116", "domain": "Macular degeneration", "model_name": "Langenk\u00e4mper et al.", "publication_date": "2014/12/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25495116/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "cBTqdRN4gQ", "pmid": "25495116", "task": "IWqQC1koJA", "abstract": "With the advent of low cost, fast sequencing technologies metagenomic analyses are made possible. The large data volumes gathered by these techniques and the unpredictable diversity captured in them are still, however, a challenge for computational biology. In this paper we address the problem of rapid taxonomic assignment with small and adaptive data models (< 5 MB) and present the accelerated k-mer explorer (AKE). Acceleration in AKE's taxonomic assignments is achieved by a special machine learning architecture, which is well suited to model data collections that are intrinsically hierarchical. We report classification accuracy reasonably well for ranks down to order, observed on a study on real world data (Acid Mine Drainage, Cow Rumen). We show that the execution time of this approach is orders of magnitude shorter than competitive approaches and that accuracy is comparable. The tool is presented to the public as a web application (url: https://ani.cebitec.uni-bielefeld.de/ake/ , username: bmc, password: bmcbioinfo).", "keywords": ["fast sequencing technologies", "sequencing technologies metagenomic", "technologies metagenomic analyses", "low cost", "fast sequencing", "advent of low", "sequencing technologies", "technologies metagenomic", "metagenomic analyses", "analyses are made", "Acid Mine Drainage", "AKE taxonomic assignments", "unpredictable diversity captured", "large data volumes", "data volumes gathered", "data", "Cow Rumen", "AKE taxonomic", "Acid Mine", "Mine Drainage"], "paper_title": "AKE - the Accelerated k-mer Exploration web-tool for rapid taxonomic classification and visualization.", "last_updated": "2023/02/04"}, {"id": "0022255738", "domain": "Diabetic retinopathy", "model_name": "Bernardes et al.", "publication_date": "2012/08/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/22255738/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "22255738", "task": null, "abstract": "Optical coherence tomography is becoming one of the most important imaging modalities in the area of ophthalmology because of being noninvasive and by allowing to visualize the human retina structure in detail. It was recently proposed that OCT data embeds functional information from the human retina. Specifically, it was proposed that blood-retinal barrier status information is present within OCT data. Following this rationale, in this work we illustrate (based on support vector machines) the possibility to discriminate between eyes from healthy volunteers, eyes from type 2 diabetic patients with no signs of diabetic retinopathy (ETDRS level 10 eyes) and eyes diagnosed with diabetic macular edema, thus confirming the presence within OCT data of information on the BRB status.", "keywords": ["Optical coherence tomography", "important imaging modalities", "human retina structure", "OCT data", "human retina", "Optical coherence", "structure in detail", "coherence tomography", "important imaging", "imaging modalities", "area of ophthalmology", "allowing to visualize", "OCT data embeds", "visualize the human", "retina structure", "OCT", "data embeds functional", "human", "data", "retina"], "paper_title": "Optical coherence tomography: Health information embedded on OCT signal statistics.", "last_updated": "2023/02/04"}, {"id": "0034777253", "domain": "Diabetic retinopathy", "model_name": "Sun et al.", "publication_date": "2021/10/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34777253/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34777253", "task": "0GPcU42tzV", "abstract": "Diabetic retinopathy (DR), the main retinal vascular complication of DM, is the leading cause of visual impairment and blindness among working-age people worldwide. The aim of this study was to investigate the difference of plasma metabolic profiles in patients with DR to better understand the mechanism of this disease and disease progression. We used ultrahigh-performance liquid Q-Exactive mass spectrometry and multivariate statistical analyses to conduct a comprehensive analysis of plasma metabolites in a population with DR and proliferative DR (PDR). A risk score based on the level of the selected metabolite was established and evaluated using the least absolute shrinkage and selection operator regularization logistic regression (LASSO-LR) based machine learning model. 22 differentially expressed metabolites which belonged to different metabolic pathway were identified and confirmed to be associated with the occurrence of DR. A risk score based on the level of the selected metabolite pseudouridine was established and evaluated to strongly associated with the occurrence of DR. Four circulating plasma metabolites (pseudouridine, glutamate, leucylleucine and N-acetyltryptophan) were identified to be differentially expressed between patients with PDR and other patients, and a risk score formula based on these plasma metabolites was developed and assessed to be significantly related to PDR. Our work highlights the possible use of the risk score assessment based on the plasma metabolites not only reveal in the early diagnosis of DR and PDR but also assist in enhancing current therapeutic strategies in the clinic.", "keywords": ["working-age people worldwide", "main retinal vascular", "retinal vascular complication", "plasma metabolites", "risk score based", "Diabetic retinopathy", "people worldwide", "risk score", "main retinal", "retinal vascular", "vascular complication", "visual impairment", "impairment and blindness", "blindness among working-age", "working-age people", "metabolites", "plasma", "PDR", "based", "score based"], "paper_title": "Plasma Metabolomics Reveals Metabolic Profiling For Diabetic Retinopathy and Disease Progression.", "last_updated": "2023/02/04"}, {"id": "0030367589", "domain": "Diabetic retinopathy", "model_name": "Tsao et al.", "publication_date": "2018/08/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30367589/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30367589", "task": null, "abstract": "The risk factors of diabetic retinopathy (DR) were investigated extensively in the past studies, but it remains unknown which risk factors were more associated with the DR than others. If we can detect the DR related risk factors more accurately, we can then exercise early prevention strategies for diabetic retinopathy in the most high-risk population. The purpose of this study is to build a prediction model for the DR in type 2 diabetes mellitus using data mining techniques including the support vector machines, decision trees, artificial neural networks, and logistic regressions. Experimental results demonstrated that prediction performance by support vector machines performed better than the other machine learning algorithms and achieved 79.5% and 0.839 in accuracy and area under the receiver operating characteristic curve using percentage split (i.e., data set divided into 80% as trainning and 20% as test), respectively. Evaluated by three-way data split scheme (i.e., data set divided into 60% as training, 20% as validation, and 20% as independent test), our method obtained slightly lower performance compared to percentage split, which suggested that three-way data split is a better way to evaluate the real performance and prevent overestimation. Moreover, we incorporated approaches proposed in previous studies to evaluate our data set and our prediction performance outperformed the other previous studies in most evaluation measures. This lends support to our assumption that appropriate machine learning algorithms combined with discriminative clinical features can effectively detect diabetic retinopathy. Our method identifies use of insulin and duration of diabetes as novel interpretable features to assist with clinical decisions in identifying the high-risk populations for diabetic retinopathy. If duration of DM increases by 1\u00a0year, the odds ratio to have DMR is increased by 9.3%. The odds ratio to have DR is increased by 3.561 times for patients who use insulin compared to patients who do not use insulin. Our results can be used to facilitate development of clinical decision support systems for clinical practice in the future.", "keywords": ["risk factors", "related risk factors", "diabetic retinopathy", "investigated extensively", "remains unknown", "data set divided", "support vector machines", "data set", "three-way data split", "data", "risk", "factors", "machine learning algorithms", "data split", "diabetic", "past studies", "vector machines", "retinopathy", "detect diabetic retinopathy", "split"], "paper_title": "Predicting diabetic retinopathy and identifying interpretable biomedical features using machine learning algorithms.", "last_updated": "2023/02/04"}, {"id": "0036046759", "domain": "Diabetic retinopathy", "model_name": "Liu et al.", "publication_date": "2022/08/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36046759/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36046759", "task": null, "abstract": "The common cause of blindness in people with type 2 diabetes (T2D) is diabetic retinopathy (DR). Early fundus examinations have been shown to prevent vision loss, but routine ophthalmic screenings for patients with diabetes present significant financial and material challenges to existing health-care systems. The purpose of this study is to build a DR prediction model based on the extreme learning machine (ELM) and to compare the performance with the DR prediction models based on support machine vector (SVM), K proximity (KNN), random forest (RF) and artificial neural network (ANN). From January 1, 2020 to November 31, 2021, data were collected from electronic inpatient medical records at Lu'an Hospital of Anhui Medical University in China. An extreme learning machine (ELM) algorithm was used to develop a prediction model based on demographic data and blood testing and urine test results. Several metrics were used to evaluate the model's performance: (1) classification accuracy (ACC), (2) sensitivity, (3) specificity, (4) Precision,(5) Negative predictive value (NPV), (6) Training time and (7) area under the receiver operating characteristic (ROC) curve (AUC). In terms of ACC, Sensitivity, Specificity, Precision, NPV and AUC, DR prediction model based on SVM and ELM is better than DR prediction model based on ANN, KNN and RF. The prediction model for diabetic retinopathy based on elm is the best among them in terms of ACC, Precision, Specificity, Training time and AUC, with 84.45%, 83.93%, 93.16%,1.24s, and 88.34%, respectively. The DR prediction model based on SVM is the best in terms of sensitivity and NPV, which are, respectively, 70.82% and 85.60%. According to the findings of this study, the model based on the extreme learning machine presents an outstanding performance in predicting diabetic retinopathy thus providing technological assistance for screening of diabetic retinopathy.", "keywords": ["prediction model based", "model based", "prediction model", "based", "model", "extreme learning machine", "prediction", "people with type", "blindness in people", "Anhui Medical University", "extreme learning", "based on SVM", "learning machine", "ELM", "Precision", "diabetic retinopathy", "ACC", "AUC", "specificity", "Training time"], "paper_title": "Construction of Predictive Model for Type 2 Diabetic Retinopathy Based on Extreme Learning Machine.", "last_updated": "2023/02/04"}, {"id": "0035931671", "domain": "Diabetic retinopathy", "model_name": "Li et al.", "publication_date": "2022/08/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35931671/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35931671", "task": "IWqQC1koJA", "abstract": "Early identification of diabetic retinopathy (DR) is key to prioritizing therapy and preventing permanent blindness. This study aims to propose a machine learning model for DR early diagnosis using metabolomics and clinical indicators. From 2017 to 2018, 950 participants were enrolled from two affiliated hospitals of Wenzhou Medical University and Anhui Medical University. A total of 69 matched blocks including healthy volunteers, type 2 diabetes, and DR patients were obtained from a propensity score matching-based metabolomics study. UPLC-ESI-MS/MS system was utilized for serum metabolic fingerprint data. CART decision trees (DT) were used to identify the potential biomarkers. Finally, the nomogram model was developed using the multivariable conditional logistic regression models. The calibration curve, Hosmer-Lemeshow test, receiver operating characteristic curve, and decision curve analysis were applied to evaluate the performance of this predictive model. The mean age of enrolled subjects was 56.7 years with a standard deviation of 9.2, and 61.4% were males. Based on the DT model, 2-pyrrolidone completely separated healthy controls from diabetic patients, and thiamine triphosphate (ThTP) might be a principal metabolite for DR detection. The developed nomogram model (including diabetes duration, systolic blood pressure and ThTP) shows an excellent quality of classification, with AUCs (95% CI) of 0.99 (0.97-1.00) and 0.99 (0.95-1.00) in training and testing sets, respectively. Furthermore, the predictive model also has a reasonable degree of calibration. The nomogram presents an accurate and favorable prediction for DR detection. Further research with larger study populations is needed to confirm our findings.", "keywords": ["preventing permanent blindness", "Wenzhou Medical University", "Anhui Medical University", "Medical University", "permanent blindness", "key to prioritizing", "prioritizing therapy", "therapy and preventing", "preventing permanent", "Early identification", "Wenzhou Medical", "Anhui Medical", "model", "diabetic retinopathy", "Medical", "University", "early diagnosis", "Early", "University and Anhui", "study"], "paper_title": "Interpretable machine learning-derived nomogram model for early detection of diabetic retinopathy in type 2 diabetes mellitus: a widely targeted metabolomics study.", "last_updated": "2023/02/04"}, {"id": "0035979449", "domain": "Diabetic retinopathy", "model_name": "Albadr et al.", "publication_date": "2022/08/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35979449/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35979449", "task": "IWqQC1koJA", "abstract": "Many works have employed Machine Learning (ML) techniques in the detection of Diabetic Retinopathy (DR), a disease that affects the human eye. However, the accuracy of most DR detection methods still need improvement. Gray Wolf Optimization-Extreme Learning Machine (GWO-ELM) is one of the most popular ML algorithms, and can be considered as an accurate algorithm in the process of classification, but has not been used in solving DR detection. Therefore, this work aims to apply the GWO-ELM classifier and employ one of the most popular features extractions, Histogram of Oriented Gradients-Principal Component Analysis (HOG-PCA), to increase the accuracy of DR detection system. Although the HOG-PCA has been tested in many image processing domains including medical domains, it has not yet been tested in DR. The GWO-ELM can prevent overfitting, solve multi and binary classifications problems, and it performs like a kernel-based Support Vector Machine with a Neural Network structure, whilst the HOG-PCA has the ability to extract the most relevant features with low dimensionality. Therefore, the combination of the GWO-ELM classifier and HOG-PCA features might produce an effective technique for DR classification and features extraction. The proposed GWO-ELM is evaluated based on two different datasets, namely APTOS-2019 and Indian Diabetic Retinopathy Image Dataset (IDRiD), in both binary and multi-class classification. The experiment results have shown an excellent performance of the proposed GWO-ELM model where it achieved an accuracy of 96.21% for multi-class and 99.47% for binary using APTOS-2019 dataset as well as 96.15% for multi-class and 99.04% for binary using IDRiD dataset. This demonstrates that the combination of the GWO-ELM and HOG-PCA is an effective classifier for detecting DR and might be applicable in solving other image data types.", "keywords": ["employed Machine Learning", "Optimization-Extreme Learning Machine", "Wolf Optimization-Extreme Learning", "Support Vector Machine", "Machine Learning", "Learning Machine", "Indian Diabetic Retinopathy", "human eye", "Diabetic Retinopathy Image", "Diabetic Retinopathy", "disease that affects", "affects the human", "employed Machine", "GWO-ELM", "Retinopathy Image Dataset", "Machine", "detection", "HOG-PCA", "Vector Machine", "GWO-ELM classifier"], "paper_title": "Gray wolf optimization-extreme learning machine approach for diabetic retinopathy detection.", "last_updated": "2023/02/04"}, {"id": "0033286339", "domain": "Diabetic retinopathy", "model_name": "Ali et al.", "publication_date": "2020/05/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33286339/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33286339", "task": "aipbNdPTIt", "abstract": "The object of this study was to demonstrate the ability of machine learning (ML) methods for the segmentation and classification of diabetic retinopathy (DR). Two-dimensional (2D) retinal fundus (RF) images were used. The datasets of DR-that is, the mild, moderate, non-proliferative, proliferative, and normal human eye ones-were acquired from 500 patients at Bahawal Victoria Hospital (BVH), Bahawalpur, Pakistan. Five hundred RF datasets (sized 256 \u00d7 256) for each DR stage and a total of 2500 (500 \u00d7 5) datasets of the five DR stages were acquired. This research introduces the novel clustering-based automated region growing framework. For texture analysis, four types of features-histogram (H), wavelet (W), co-occurrence matrix (COM) and run-length matrix (RLM)-were extracted, and various ML classifiers were employed, achieving 77.67%, 80%, 89.87%, and 96.33% classification accuracies, respectively. To improve classification accuracy, a fused hybrid-feature dataset was generated by applying the data fusion approach. From each image, 245 pieces of hybrid feature data (H, W, COM, and RLM) were observed, while 13 optimized features were selected after applying four different feature selection techniques, namely Fisher, correlation-based feature selection, mutual information, and probability of error plus average correlation. Five ML classifiers named sequential minimal optimization (SMO), logistic (Lg), multi-layer perceptron (MLP), logistic model tree (LMT), and simple logistic (SLg) were deployed on selected optimized features (using 10-fold cross-validation), and they showed considerably high classification accuracies of 98.53%, 99%, 99.66%, 99.73%, and 99.73%, respectively.", "keywords": ["Bahawal Victoria Hospital", "machine learning", "diabetic retinopathy", "demonstrate the ability", "ability of machine", "Victoria Hospital", "Bahawal Victoria", "classification", "datasets", "patients at Bahawal", "classification accuracies", "RLM", "learning", "methods", "retinopathy", "feature", "optimized features", "feature selection", "retinal fundus", "object"], "paper_title": "Machine Learning Based Automated Segmentation and Hybrid Feature Analysis for Diabetic Retinopathy Classification Using Fundus Image.", "last_updated": "2023/02/04"}, {"id": "0034070287", "domain": "Diabetic retinopathy", "model_name": "Shen et al.", "publication_date": "2021/05/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34070287/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34070287", "task": null, "abstract": "(1) Background: Diabetic retinopathy, one of the most serious complications of diabetes, is the primary cause of blindness in developed countries. Therefore, the prediction of diabetic retinopathy has a positive impact on its early detection and treatment. The prediction of diabetic retinopathy based on high-dimensional and small-sample-structured datasets (such as biochemical data and physical data) was the problem to be solved in this study. (2) Methods: This study proposed the XGB-Stacking model with the foundation of XGBoost and stacking. First, a wrapped feature selection algorithm, XGBIBS (Improved Backward Search Based on XGBoost), was used to reduce data feature redundancy and improve the effect of a single ensemble learning classifier. Second, in view of the slight limitation of a single classifier, a stacking model fusion method, Sel-Stacking (Select-Stacking), which keeps Label-Proba as the input matrix of meta-classifier and determines the optimal combination of learners by a global search, was used in the XGB-Stacking model. (3) Results: XGBIBS greatly improved the prediction accuracy and the feature reduction rate of a single classifier. Compared to a single classifier, the accuracy of the Sel-Stacking model was improved to varying degrees. Experiments proved that the prediction model of XGB-Stacking based on the XGBIBS algorithm and the Sel-Stacking method made effective predictions on diabetes retinopathy. (4) Conclusion: The XGB-Stacking prediction model of diabetic retinopathy based on biochemical and physical data had outstanding performance. This is highly significant to improve the screening efficiency of diabetes retinopathy and reduce the cost of diagnosis.", "keywords": ["diabetic retinopathy based", "Diabetic retinopathy", "developed countries", "blindness in developed", "Diabetic", "single classifier", "retinopathy", "retinopathy based", "Backward Search Based", "Background", "model", "prediction", "Improved Backward Search", "prediction model", "based", "XGB-Stacking prediction model", "classifier", "diabetes retinopathy", "single", "XGB-Stacking model"], "paper_title": "Diabetic Retinopathy Prediction by Ensemble Learning Based on Biochemical and Physical Data.", "last_updated": "2023/02/04"}, {"id": "0033577010", "domain": "Diabetic retinopathy", "model_name": "Alabdulwahhab et al.", "publication_date": "2021/06/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33577010/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33577010", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) is a highly threatening microvascular complication of diabetes mellitus. Diabetic patients must be screened annually for DR; however, it is practically not viable due to the high volume of patients, lack of resources, economic burden, and cost of the screening procedure. The use of machine learning (ML) classifiers in medical science is an emerging frontier and can help in assisted diagnosis. The few available proposed models perform best when used in similar population cohorts and their external validation has been questioned. Therefore, the purpose of our research is to classify the DR using different ML methods on Saudi diabetic data, propose the best method based on accuracy and identify the most discriminative interpretable features using the socio-demographic and clinical information. This cross-sectional study was conducted among 327 diabetic patients in Almajmaah, Saudi Arabia. Socio-demographic and clinical data were collected using a systematic random sampling technique. For DR classification, ML algorithm including, linear discriminant analysis, support vector machine, K nearest neighbor, random forest and its variate ranger random forest classifiers were used through cross-validation resampling procedure. In classifying DR, ranger random forest outperforms the other methods by accurately classifying 86% of the DR patients on the test data. HbA1c (p<0.001) and duration of diabetes (p<0.001) were the most influential risk factor that best discriminated the DR patients. Other influential risk factors were the body mass index (p<0.001), age-onset (p<0.001), age (p<0.001), systolic blood pressure (p<0.05), and the use of medication (p<0.05) that significantly discriminated the DR patients. Based on the present study findings, integrating ophthalmology and ML can transform diagnosing the disease pattern that can help generate a compelling clinical effect. ML can be used as an added tool for clinical decision-making and must not be the sole substitute for a clinician. We will work to examine the classification performance of multi-class data using more sophisticated ML methods.", "keywords": ["highly threatening microvascular", "threatening microvascular complication", "Diabetic Retinopathy", "highly threatening", "threatening microvascular", "microvascular complication", "Saudi diabetic data", "patients", "Retinopathy", "Diabetic patients", "random forest", "ranger random forest", "Diabetic", "diabetes mellitus", "complication of diabetes", "random", "Saudi diabetic", "clinical", "data", "Saudi Arabia"], "paper_title": "Automated detection of diabetic retinopathy using machine learning classifiers.", "last_updated": "2023/02/04"}, {"id": "0026958235", "domain": "Diabetic retinopathy", "model_name": "Ogunyemi et al.", "publication_date": "2015/11/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26958235/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "26958235", "task": "IWqQC1koJA", "abstract": "Annual eye examinations are recommended for diabetic patients in order to detect diabetic retinopathy and other eye conditions that arise from diabetes. Medically underserved urban communities in the US have annual screening rates that are much lower than the national average and could benefit from informatics approaches to identify unscreened patients most at risk of developing retinopathy. Using clinical data from urban safety net clinics as well as public health data from the CDC's National Health and Nutrition Examination Survey, we examined different machine learning approaches for predicting retinopathy from clinical or public health data. All datasets utilized exhibited a class imbalance. Classifiers learned on the clinical data were modestly predictive of retinopathy with the best model having an AUC of 0.72, sensitivity of 69.2% and specificity of 55.9%. Classifiers learned on public health data were not predictive of retinopathy. Successful approaches to detecting latent retinopathy using machine learning could help safety net and other clinics identify unscreened patients who are most at risk of developing retinopathy and the use of ensemble classifiers on clinical data shows promise for this purpose.", "keywords": ["Annual eye examinations", "detect diabetic retinopathy", "Nutrition Examination Survey", "arise from diabetes", "public health data", "detect diabetic", "eye conditions", "CDC National Health", "order to detect", "conditions that arise", "eye examinations", "health data", "retinopathy", "Annual eye", "data", "diabetic patients", "public health", "Examination Survey", "identify unscreened patients", "Nutrition Examination"], "paper_title": "Machine Learning Approaches for Detecting Diabetic Retinopathy from Clinical and Public Health Records.", "last_updated": "2023/02/04"}, {"id": "0034763798", "domain": "Diabetic retinopathy", "model_name": "Abbasi et al.", "publication_date": "2021/09/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34763798/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34763798", "task": "IWqQC1koJA", "abstract": "Over the last decade, advances in Machine Learning and Artificial Intelligence have highlighted their potential as a diagnostic tool in the healthcare domain. Despite the widespread availability of medical images, their usefulness is severely hampered by a lack of access to labeled data. For example, while Convolutional Neural Networks (CNNs) have emerged as an essential analytical tool in image processing, their impact is curtailed by training limitations due to insufficient labeled data availability. Transfer Learning enables models developed for one task to be reused for a second task. Knowledge distillation enables transferring knowledge from a pre-trained model to another. However, it suffers from limitations, and the two models' constraints need to be architecturally similar. Knowledge distillation addresses some of the shortcomings of transfer learning by generalizing a complex model to a lighter model. However, some parts of the knowledge may not be distilled by knowledge distillation sufficiently. In this paper, a novel knowledge distillation approach using transfer learning is proposed. The proposed approach transfers the complete knowledge of a model to a new smaller one. Unlabeled data are used in an unsupervised manner to transfer the new smaller model's maximum amount of knowledge. The proposed method can be beneficial in medical image analysis, where labeled data are typically scarce. The proposed approach is evaluated in classifying images for diagnosing Diabetic Retinopathy on two publicly available datasets, including Messidor and EyePACS. Simulation results demonstrate that the approach effectively transfers knowledge from a complex model to a lighter one. Furthermore, experimental results illustrate that different small models' performance is improved significantly using unlabeled data and knowledge distillation.", "keywords": ["Artificial Intelligence", "advances in Machine", "Intelligence have highlighted", "Machine Learning", "Convolutional Neural Networks", "Knowledge", "healthcare domain", "highlighted their potential", "Knowledge distillation", "Machine", "Artificial", "Intelligence", "Transfer Learning", "model", "Neural Networks", "labeled data", "Learning", "Learning and Artificial", "Convolutional Neural", "distillation"], "paper_title": "Classification of diabetic retinopathy using unlabeled data and knowledge distillation.", "last_updated": "2023/02/04"}, {"id": "0035279423", "domain": "Diabetic retinopathy", "model_name": "Cao et al.", "publication_date": "2022/03/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35279423/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35279423", "task": null, "abstract": "Early detection and treatment of diabetic retinopathy (DR) can significantly reduce the risk of vision loss in patients. In essence, we are faced with two challenges: (i) how to simultaneously achieve domain adaptation from the different domains and (ii) how to build an interpretable multi-instance learning (MIL) on the target domain in an end-to-end framework. In this paper, we address these issues and propose a unified weakly-supervised domain adaptation framework, which consists of three components: domain adaptation, instance progressive discriminator and multi-instance learning with attention. The method models the relationship between the patches and images in the target domain with a multi-instance learning scheme and an attention mechanism. Meanwhile, it incorporates all available information from both source and target domains for a jointly learning strategy. We validate the performance of the proposed framework for DR grading on the Messidor dataset and the large-scale Eyepacs dataset. The experimental results demonstrate that it achieves an average accuracy of 0.949 (95% CI 0.931-0.958)/0.764 (95% CI 0.755-0.772) and an average AUC value of 0.958 (95% CI 0.945-0.962)/0.749 (95% CI 0.732-0.761) for binary-class/multi-class classification tasks on the Messidor dataset. Moreover, the proposed method achieves an accuracy of 0.887 and a quadratic weighted kappa score value of 0.860 on the Eyepacs dataset, outperforming the state-of-the-art approaches. Comprehensive experiments confirm the effectiveness of the approach in terms of both grading performance and interpretability. The source code is available at https://github.com/HouQingshan/WAD-Net.", "keywords": ["domain adaptation", "Early detection", "diabetic retinopathy", "loss in patients", "domain adaptation framework", "detection and treatment", "treatment of diabetic", "significantly reduce", "reduce the risk", "risk of vision", "vision loss", "multi-instance learning", "achieve domain adaptation", "target domain", "weakly-supervised domain adaptation", "interpretable multi-instance learning", "domain", "Messidor dataset", "simultaneously achieve domain", "Eyepacs dataset"], "paper_title": "Collaborative learning of weakly-supervised domain adaptation for diabetic retinopathy grading on retinal images.", "last_updated": "2023/02/04"}, {"id": "0034945799", "domain": "Diabetic retinopathy", "model_name": "Maeda-Guti\u00e9rrez et al.", "publication_date": "2021/12/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34945799/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34945799", "task": null, "abstract": "One of the main microvascular complications presented in the Mexican population is diabetic retinopathy which affects 27.50% of individuals with type 2 diabetes. Therefore, the purpose of this study is to construct a predictive model to find out the risk factors of this complication. The dataset contained a total of 298 subjects, including clinical and paraclinical features. An analysis was constructed using machine learning techniques including Boruta as a feature selection method, and random forest as classification algorithm. The model was evaluated through a statistical test based on sensitivity, specificity, area under the curve (AUC), and receiving operating characteristic (ROC) curve. The results present significant values obtained by the model obtaining 69% of AUC. Moreover, a risk evaluation was incorporated to evaluate the impact of the predictors. The proposed method identifies creatinine, lipid treatment, glomerular filtration rate, waist hip ratio, total cholesterol, and high density lipoprotein as risk factors in Mexican subjects. The odds ratio increases by 3.5916 times for control patients which have high levels of cholesterol. It is possible to conclude that this proposed methodology is a preliminary computer-aided diagnosis tool for clinical decision-helping to identify the diagnosis of DR.", "keywords": ["main microvascular complications", "microvascular complications presented", "retinopathy which affects", "individuals with type", "main microvascular", "population is diabetic", "diabetic retinopathy", "microvascular complications", "complications presented", "Mexican population", "techniques including Boruta", "risk factors", "Mexican subjects", "diabetes", "model", "AUC", "including Boruta", "Mexican", "affects", "type"], "paper_title": "Risk-Profile and Feature Selection Comparison in Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0029124453", "domain": "Disc hemorrhage", "model_name": "S K et al.", "publication_date": "2017/11/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29124453/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "29124453", "task": "IWqQC1koJA", "abstract": "The main complication of diabetes is Diabetic retinopathy (DR), retinal vascular disease and it leads to the blindness. Regular screening for early DR disease detection is considered as an intensive labor and resource oriented task. Therefore, automatic detection of DR diseases is performed only by using the computational technique is the great solution. An automatic method is more reliable to determine the presence of an abnormality in Fundus images (FI) but, the classification process is poorly performed. Recently, few research works have been designed for analyzing texture discrimination capacity in FI to distinguish the healthy images. However, the feature extraction (FE) process was not performed well, due to the high dimensionality. Therefore, to identify retinal features for DR disease diagnosis and early detection using Machine Learning and Ensemble Classification method, called, Machine Learning Bagging Ensemble Classifier (ML-BEC) is designed. The ML-BEC method comprises of two stages. The first stage in ML-BEC method comprises extraction of the candidate objects from Retinal Images (RI). The candidate objects or the features for DR disease diagnosis include blood vessels, optic nerve, neural tissue, neuroretinal rim, optic disc size, thickness and variance. These features are initially extracted by applying Machine Learning technique called, t-distributed Stochastic Neighbor Embedding (t-SNE). Besides, t-SNE generates a probability distribution across high-dimensional images where the images are separated into similar and dissimilar pairs. Then, t-SNE describes a similar probability distribution across the points in the low-dimensional map. This lessens the Kullback-Leibler divergence among two distributions regarding the locations of the points on the map. The second stage comprises of application of ensemble classifiers to the extracted features for providing accurate analysis of digital FI using machine learning. In this stage, an automatic detection of DR screening system using Bagging Ensemble Classifier (BEC) is investigated. With the help of voting the process in ML-BEC, bagging minimizes the error due to variance of the base classifier. With the publicly available retinal image databases, our classifier is trained with 25% of RI. Results show that the ensemble classifier can achieve better classification accuracy (CA) than single classification models. Empirical experiments suggest that the machine learning-based ensemble classifier is efficient for further reducing DR classification time (CT).", "keywords": ["Diabetic retinopathy", "diabetes is Diabetic", "Machine Learning", "retinal vascular disease", "Bagging Ensemble Classifier", "Ensemble Classifier", "Machine Learning Bagging", "main complication", "complication of diabetes", "Learning Bagging Ensemble", "Ensemble", "Classifier", "vascular disease", "Machine", "disease", "Bagging Ensemble", "Diabetic", "ML-BEC method comprises", "images", "Machine Learning technique"], "paper_title": "A Machine Learning Ensemble Classifier for Early Prediction of Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0028993124", "domain": "Diabetic retinopathy", "model_name": "Saleh et al.", "publication_date": "2017/10/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28993124/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28993124", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy is one of the most common comorbidities of diabetes. Unfortunately, the recommended annual screening of the eye fundus of diabetic patients is too resource-consuming. Therefore, it is necessary to develop tools that may help doctors to determine the risk of each patient to attain this condition, so that patients with a low risk may be screened less frequently and the use of resources can be improved. This paper explores the use of two kinds of ensemble classifiers learned from data: fuzzy random forest and dominance-based rough set balanced rule ensemble. These classifiers use a small set of attributes which represent main risk factors to determine whether a patient is in risk of developing diabetic retinopathy. The levels of specificity and sensitivity obtained in the presented study are over 80%. This study is thus a first successful step towards the construction of a personalized decision support system that could help physicians in daily clinical practice.", "keywords": ["comorbidities of diabetes", "common comorbidities", "Diabetic retinopathy", "developing diabetic retinopathy", "Diabetic", "diabetic patients", "risk", "recommended annual screening", "diabetes", "patients", "patient", "common", "comorbidities", "developing diabetic", "retinopathy", "recommended annual", "annual screening", "eye fundus", "determine", "ensemble classifiers learned"], "paper_title": "Learning ensemble classifiers for diabetic retinopathy assessment.", "last_updated": "2023/02/04"}, {"id": "0035655800", "domain": "Diabetic retinopathy", "model_name": "Zhao et al.", "publication_date": "2022/05/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35655800/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35655800", "task": null, "abstract": "To construct and validate prediction models for the risk of diabetic retinopathy (DR) in patients with type 2 diabetes mellitus. Patients with type 2 diabetes mellitus hospitalized over the period between January 2010 and September 2018 were retrospectively collected. Eighteen baseline demographic and clinical characteristics were used as predictors to train five machine-learning models. The model that showed favorable predictive efficacy was evaluated at annual follow-ups. Multi-point data of the patients in the test set were utilized to further evaluate the model's performance. We also assessed the relative prognostic importance of the selected risk factors for DR outcomes. Of 7943 collected patients, 1692 (21.30%) developed DR during follow-up. Among the five models, the XGBoost model achieved the highest predictive performance with an AUC, accuracy, sensitivity, and specificity of 0.803, 88.9%, 74.0%, and 81.1%, respectively. The XGBoost model's AUCs in the different follow-up periods were 0.834 to 0.966. In addition to the classical risk factors of DR, serum uric acid (SUA), low-density lipoprotein cholesterol (LDL-C), total cholesterol (TC), estimated glomerular filtration rate (eGFR), and triglyceride (TG) were also identified to be important and strong predictors for the disease. Compared with the clinical diagnosis method of DR, the XGBoost model achieved an average of 2.895 years prior to the first diagnosis. The proposed model achieved high performance in predicting the risk of DR among patients with type 2 diabetes mellitus at each time point. This study established the potential of the XGBoost model to facilitate clinicians in identifying high-risk patients and making type 2 diabetes management-related decisions.", "keywords": ["validate prediction models", "diabetes mellitus", "XGBoost model", "diabetic retinopathy", "construct and validate", "validate prediction", "XGBoost model achieved", "model", "diabetes mellitus hospitalized", "patients", "model achieved", "prediction models", "XGBoost model AUCs", "mellitus", "type", "diabetes", "XGBoost", "risk", "patients with type", "models"], "paper_title": "Using Machine Learning Techniques to Develop Risk Prediction Models for the Risk of Incident Diabetic Retinopathy Among Patients With Type 2 Diabetes Mellitus: A Cohort Study.", "last_updated": "2023/02/04"}, {"id": "0025192577", "domain": "Diabetic retinopathy", "model_name": "DREAM", "publication_date": "2015/04/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25192577/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "25192577", "task": null, "abstract": "This paper presents a computer-aided screening system (DREAM) that analyzes fundus images with varying illumination and fields of view, and generates a severity grade for diabetic retinopathy (DR) using machine learning. Classifiers such as the Gaussian Mixture model (GMM), k-nearest neighbor (kNN), support vector machine (SVM), and AdaBoost are analyzed for classifying retinopathy lesions from nonlesions. GMM and kNN classifiers are found to be the best classifiers for bright and red lesion classification, respectively. A main contribution of this paper is the reduction in the number of features used for lesion classification by feature ranking using Adaboost where 30 top features are selected out of 78. A novel two-step hierarchical classification approach is proposed where the nonlesions or false positives are rejected in the first step. In the second step, the bright lesions are classified as hard exudates and cotton wool spots, and the red lesions are classified as hemorrhages and micro-aneurysms. This lesion classification problem deals with unbalanced datasets and SVM or combination classifiers derived from SVM using the Dempster-Shafer theory are found to incur more classification error than the GMM and kNN classifiers due to the data imbalance. The DR severity grading system is tested on 1200 images from the publicly available MESSIDOR dataset. The DREAM system achieves 100% sensitivity, 53.16% specificity, and 0.904 AUC, compared to the best reported 96% sensitivity, 51% specificity, and 0.875 AUC, for classifying images as with or without DR. The feature reduction further reduces the average computation time for DR severity per image from 59.54 to 3.46 s.", "keywords": ["analyzes fundus images", "Gaussian Mixture model", "computer-aided screening system", "fields of view", "presents a computer-aided", "computer-aided screening", "analyzes fundus", "varying illumination", "illumination and fields", "grade for diabetic", "lesion classification", "Gaussian Mixture", "machine learning", "classifying retinopathy lesions", "diabetic retinopathy", "kNN classifiers", "GMM", "Classifiers", "support vector machine", "red lesion classification"], "paper_title": "DREAM: diabetic retinopathy analysis using machine learning.", "last_updated": "2023/02/04"}, {"id": "0029548646", "domain": "Diabetic retinopathy", "model_name": "Krause et al.", "publication_date": "2018/03/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29548646/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "29548646", "task": null, "abstract": "Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading. Retrospective analysis. Retinal fundus images from DR screening programs. Images were each graded by the algorithm, U.S. board-certified ophthalmologists, and retinal specialists. The adjudicated consensus of the retinal specialists served as the reference standard. For agreement between different graders as well as between the graders and the algorithm, we measured the (quadratic-weighted) kappa score. To compare the performance of different forms of manual grading and the algorithm for various DR severity cutoffs (e.g., mild or worse DR, moderate or worse DR), we measured area under the curve (AUC), sensitivity, and specificity. Of the 193 discrepancies between adjudication by retinal specialists and majority decision of ophthalmologists, the most common were missing microaneurysm (MAs) (36%), artifacts (20%), and misclassified hemorrhages (16%). Relative to the reference standard, the kappa for individual retinal specialists, ophthalmologists, and algorithm ranged from 0.82 to 0.91, 0.80 to 0.84, and 0.84, respectively. For moderate or worse DR, the majority decision of ophthalmologists had a sensitivity of 0.838 and specificity of 0.981. The algorithm had a sensitivity of 0.971, specificity of 0.923, and AUC of 0.986. For mild or worse DR, the algorithm had a sensitivity of 0.970, specificity of 0.917, and AUC of 0.986. By using a small number of adjudicated consensus grades as a tuning dataset and higher-resolution images as input, the algorithm improved in AUC from 0.934 to 0.986 for moderate or worse DR. Adjudication reduces the errors in DR grading. A small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. Board-Certified ophthalmologists and retinal specialists.", "keywords": ["algorithm", "retinal specialists", "diabetic retinopathy", "Retinal", "AUC", "worse", "moderate or worse", "improved automated algorithm", "specialists", "ophthalmologists", "automated algorithm", "majority decision", "sensitivity", "specificity", "grading", "grading based", "algorithm performance", "graders", "images", "mild or worse"], "paper_title": "Grader Variability and the Importance of Reference Standards for Evaluating Machine Learning Models for Diabetic Retinopathy.", "last_updated": "2023/02/04"}, {"id": "0036930723", "domain": "Diabetic retinopathy", "model_name": "Tarasewicz et al.", "publication_date": "2023/03/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36930723/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36930723", "task": null, "abstract": "Although diabetic retinopathy is a leading cause of blindness worldwide, diabetes-related blindness can be prevented through effective screening, detection, and treatment of disease. The study goal was to develop risk stratification algorithms for the onset of retinal complications of diabetes, including proliferative diabetic retinopathy, referable retinopathy, and macular edema. Retrospective cohort analysis of patients from the Kaiser Permanente Northern California Diabetes Registry who had no evidence of diabetic retinopathy at a baseline diabetic retinopathy screening during 2008-2020 was performed. Machine learning and logistic regression prediction models for onset of proliferative diabetic retinopathy, diabetic macular edema, and referable retinopathy detected through routine screening were trained and internally validated. Model performance was assessed using area under the curve (AUC) metrics. The study cohort (N = 276,794) was 51.9% male and 42.1% White. Mean (\u00b1SD) age at baseline was 60.0 (\u00b113.1) years. A machine learning XGBoost algorithm was effective in identifying patients who developed proliferative diabetic retinopathy (AUC 0.86; 95% CI, 0.86-0.87), diabetic macular edema (AUC 0.76; 95% CI, 0.75-0.77), and referable retinopathy (AUC 0.78; 95% CI, 0.78-0.79). Similar results were found using a simpler nine-covariate logistic regression model: proliferative diabetic retinopathy (AUC 0.82; 95% CI, 0.80-0.83), diabetic macular edema (AUC 0.73; 95% CI, 0.72-0.74), and referable retinopathy (AUC 0.75; 95% CI, 0.75-0.76). Relatively simple logistic regression models using nine readily available clinical variables can be used to rank order patients for onset of diabetic eye disease and thereby more efficiently prioritize and target screening for at risk patients.", "keywords": ["diabetic retinopathy", "AUC", "diabetic macular edema", "proliferative diabetic retinopathy", "diabetic", "Kaiser Permanente Northern", "Permanente Northern California", "retinopathy", "Northern California Diabetes", "California Diabetes Registry", "macular edema", "proliferative diabetic", "blindness worldwide", "diabetes-related blindness", "referable retinopathy", "diabetic macular", "edema", "Kaiser Permanente", "Permanente Northern", "Northern California"], "paper_title": "Development and Validation of a Diabetic Retinopathy Risk Stratification Algorithm.", "last_updated": "2023/02/04"}, {"id": "0033633139", "domain": "Diabetic retinopathy", "model_name": "Sharafeldeen et al.", "publication_date": "2021/02/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33633139/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "33633139", "task": "IWqQC1koJA", "abstract": "This study proposes a novel computer assisted diagnostic (CAD) system for early diagnosis of diabetic retinopathy (DR) using optical coherence tomography (OCT) B-scans. The CAD system is based on fusing novel OCT markers that describe both the morphology/anatomy and the reflectivity of retinal layers to improve DR diagnosis. This system separates retinal layers automatically using a segmentation approach based on an adaptive appearance and their prior shape information. High-order morphological and novel reflectivity markers are extracted from individual segmented layers. Namely, the morphological markers are layer thickness and tortuosity while the reflectivity markers are the 1st-order reflectivity of the layer in addition to local and global high-order reflectivity based on Markov-Gibbs random field (MGRF) and gray-level co-occurrence matrix (GLCM), respectively. The extracted image-derived markers are represented using cumulative distribution function (CDF) descriptors. The constructed CDFs are then described using their statistical measures, i.e., the 10th through 90th percentiles with a 10% increment. For individual layer classification, each extracted descriptor of a given layer is fed to a support vector machine (SVM) classifier with a linear kernel. The results of the four classifiers are then fused using a backpropagation neural network (BNN) to diagnose each retinal layer. For global subject diagnosis, classification outputs (probabilities) of the twelve layers are fused using another BNN to make the final diagnosis of the B-scan. This system is validated and tested on 130 patients, with two scans for both eyes (i.e. 260 OCT images), with a balanced number of normal and DR subjects using different validation metrics: 2-folds, 4-folds, 10-folds, and leave-one-subject-out (LOSO) cross-validation approaches. The performance of the proposed system was evaluated using sensitivity, specificity, F1-score, and accuracy metrics. The system's performance after the fusion of these different markers showed better performance compared with individual markers and other machine learning fusion methods. Namely, it achieved [Formula: see text], [Formula: see text], [Formula: see text], and [Formula: see text], respectively, using the LOSO cross-validation technique. The reported results, based on the integration of morphology and reflectivity markers and by using state-of-the-art machine learning classifications, demonstrate the ability of the proposed system to diagnose the DR early.", "keywords": ["computer assisted diagnostic", "optical coherence tomography", "reflectivity markers", "markers", "assisted diagnostic", "diabetic retinopathy", "coherence tomography", "OCT markers", "study proposes", "computer assisted", "optical coherence", "OCT", "CAD system", "reflectivity", "system", "diagnosis", "CAD", "Formula", "retinal layers", "layer"], "paper_title": "Precise higher-order reflectivity and morphology models for early diagnosis of diabetic retinopathy using OCT images.", "last_updated": "2023/02/04"}, {"id": "0030237145", "domain": "Diabetic retinopathy", "model_name": "Cao et al.", "publication_date": "2018/08/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30237145/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30237145", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is one of the most serious complications of diabetes. Early detection and treatment of DR are key public health interventions that can significantly reduce the risk of vision loss. How to effectively screen and diagnose the retinal fundus image in order to identify retinopathy in time is a major challenge. In the traditional DR screening system, the accuracy of micro-aneurysm (MA) and hemorrhagic (H) lesion detection determines the final screening performance. The detection method produced a large number of false positive samples for guaranteeing high sensitivity, and the classification model was not effective in removing false positives since the suspicious lesions lack label information. In order to solve the problem of supervised learning in the diagnosis of DR, we formulate weakly supervised multi-class DR grading as a multi-class multi-instance problem where each image (bag) is labeled as healthy or abnormal and consists of unlabeled candidate lesion regions (instances). Specifically, we proposed a multi-kernel multi-instance learning method based on graph kernel. Moreover, we develop an under-sampling from instance level and over-sampling from bag level to improve the performance of the multi-instance learning in the diagnosis of DR. Through empirical evaluation and comparison with different baselinemethods and the state-of-the-art methods on data from Messidor, we illustrate that the proposed method reports favorable results, with an overall classification accuracy of 0.916 and an AUC of 0.957. The experiments results demonstrate that the proposed multi-kernel multi-instance learning framework with bi-level re-sampling can solve the problem in the imbalanced and weakly supervised data for grading diabetic retinopathy, and it improves the diagnosis performance over several state-of-the-art competing methods.", "keywords": ["complications of diabetes", "multi-instance learning", "multi-kernel multi-instance learning", "multi-instance learning method", "Diabetic retinopathy", "learning", "multi-instance", "retinopathy", "multi-kernel multi-instance", "key public health", "public health interventions", "grading diabetic retinopathy", "detection", "diagnosis", "method", "proposed multi-kernel multi-instance", "lesion detection determines", "multi-class multi-instance problem", "weakly supervised", "proposed"], "paper_title": "Efficient multi-kernel multi-instance learning using weakly supervised and imbalanced data for diabetic retinopathy diagnosis.", "last_updated": "2023/02/04"}, {"id": "0034880311", "domain": "Diabetic retinopathy", "model_name": "Al-Mukhtar et al.", "publication_date": "2021/12/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34880311/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34880311", "task": "IWqQC1koJA", "abstract": "Vision loss happens due to diabetic retinopathy (DR) in severe stages. Thus, an automatic detection method applied to diagnose DR in an earlier phase may help medical doctors to make better decisions. DR is considered one of the main risks, leading to blindness. Computer-Aided Diagnosis systems play an essential role in detecting features in fundus images. Fundus images may include blood vessels, exudates, micro-aneurysm, hemorrhages, and neovascularization. In this paper, our model combines automatic detection for the diabetic retinopathy classification with localization methods depending on weakly-supervised learning. The model has four stages; in stage one, various preprocessing techniques are applied to smooth the data set. In stage two, the network had gotten deeply to the optic disk segment for eliminating any exudate's false prediction because the exudates had the same color pixel as the optic disk. In stage three, the network is fed through training data to classify each label. Finally, the layers of the convolution neural network are re-edited, and used to localize the impact of DR on the patient's eye. The framework tackles the matching technique between two essential concepts where the classification problem depends on the supervised learning method. While the localization problem was obtained by the weakly supervised method. An additional layer known as weakly supervised sensitive heat map (WSSH) was added to detect the ROI of the lesion at a test accuracy of 98.65%, while comparing with Class Activation Map that involved weakly supervised technology achieved 0.954. The main purpose is to learn a representation that collect the central localization of discriminative features in a retina image. CNN-WSSH model is able to highlight decisive features in a single forward pass for getting the best detection of lesions.", "keywords": ["Vision loss", "loss happens due", "severe stages", "weakly supervised", "diabetic retinopathy", "fundus images", "automatic detection", "automatic detection method", "supervised", "detection method applied", "stage", "diabetic retinopathy classification", "Class Activation Map", "weakly supervised method", "detection", "method", "model", "due to diabetic", "network", "supervised learning method"], "paper_title": "Weakly Supervised Sensitive Heatmap framework to classify and localize diabetic retinopathy lesions.", "last_updated": "2023/02/04"}, {"id": "0036846513", "domain": "Diabetic retinopathy", "model_name": "Zaizar-Fregoso et al.", "publication_date": "2023/02/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36846513/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36846513", "task": null, "abstract": "Diabetes mellitus is a disease with no cure that can cause complications and even death. Moreover, over time, it will lead to chronic complications. Predictive models have been used to identify people with a tendency to develop diabetes mellitus. At the same time, there is limited information regarding the chronic complications of patients with diabetes. Our study is aimed at creating a machine-learning model that will be able to identify the risk factors of a diabetic patient developing chronic complications such as amputations, myocardial infarction, stroke, nephropathy, and retinopathy. The design is a national nested case-control study with 63,776 patients and 215 predictors with four years of data. Using an XGBoost model, the prediction of chronic complications has an AUC of 84%, and the model has identified the risk factors for chronic complications in patients with diabetes. According to the analysis, the most crucial risk factors based on SHAP values (Shapley additive explanations) are continued management, metformin treatment, age between 68 and 104 years, nutrition consultation, and treatment adherence. But we highlight two exciting findings. The first is a reaffirmation that high blood pressure figures across patients with diabetes without hypertension become a significant risk factor at diastolic > 70\u2009mmHg (OR: 1.095, 95% CI: 1.078-1.113) or systolic > 120\u2009mmHg (OR: 1.147, 95% CI: 1.124-1.171). Furthermore, people with diabetes with a BMI > 32 (overall obesity) (OR: 0.816, 95% CI: 0.8-0.833) have a statistically significant protective factor, which the paradox of obesity may explain. In conclusion, the results we have obtained show that artificial intelligence is a powerful and feasible tool to use for this type of study. However, we suggest that more studies be conducted to verify and elaborate upon our findings.", "keywords": ["chronic complications", "complications", "Diabetes", "Diabetes mellitus", "chronic", "risk factors", "risk", "patients", "develop diabetes mellitus", "patients with diabetes", "factors", "developing chronic complications", "model", "mellitus", "study", "death", "time", "disease", "cure", "patient developing chronic"], "paper_title": "Using Artificial Intelligence to Develop a Multivariate Model with a Machine Learning Model to Predict Complications in Mexican Diabetic Patients without Arterial Hypertension (National Nested Case-Control Study): Metformin and Elevated Normal Blood Pressure Are Risk Factors, and Obesity Is Protective.", "last_updated": "2023/02/04"}, {"id": "0032908275", "domain": "Diabetic retinopathy", "model_name": "Norgeot et al.", "publication_date": "2020/10/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32908275/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32908275", "task": null, "abstract": "", "keywords": [], "paper_title": "Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist.", "last_updated": "2023/02/04"}, {"id": "0026221613", "domain": "Diabetic retinopathy", "model_name": "Torok et al.", "publication_date": "2015/06/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26221613/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "26221613", "task": null, "abstract": "Background. It is estimated that 347 million people suffer from diabetes mellitus (DM), and almost 5 million are blind due to diabetic retinopathy (DR). The progression of DR can be slowed down with early diagnosis and treatment. Therefore our aim was to develop a novel automated method for DR screening. Methods. 52 patients with diabetes mellitus were enrolled into the project. Of all patients, 39 had signs of DR. Digital retina images and tear fluid samples were taken from each eye. The results from the tear fluid proteomics analysis and from digital microaneurysm (MA) detection on fundus images were used as the input of a machine learning system. Results. MA detection method alone resulted in 0.84 sensitivity and 0.81 specificity. Using the proteomics data for analysis 0.87 sensitivity and 0.68 specificity values were achieved. The combined data analysis integrated the features of the proteomics data along with the number of detected MAs in the associated image and achieved sensitivity/specificity values of 0.93/0.78. Conclusions. As the two different types of data represent independent and complementary information on the outcome, the combined model resulted in a reliable screening method that is comparable to the requirements of DR screening programs applied in clinical routine.", "keywords": ["million people suffer", "diabetes mellitus", "data", "specificity", "million", "million people", "Background", "method", "proteomics", "proteomics data", "sensitivity", "screening", "analysis", "tear fluid", "diabetes", "mellitus", "diabetic retinopathy", "people suffer", "blind due", "due to diabetic"], "paper_title": "Combined Methods for Diabetic Retinopathy Screening, Using Retina Photographs and Tear Fluid Proteomics Biomarkers.", "last_updated": "2023/02/04"}, {"id": "0036993028", "domain": "Diabetic retinopathy", "model_name": "Firdous et al.", "publication_date": "2022/12/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36993028/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36993028", "task": null, "abstract": "Diabetes mellitus (DM) is a chronic condition that can lead to a variety of consequences. Diabetes is a condition that is caused by factors such as age, lack of exercise, sedentary lifestyle, family history of diabetes, high blood pressure, depression and stress, poor food, and so on. Diabetics are at a higher risk of developing diseases such as heart disease, nerve damage (diabetic neuropathy), eye problems (diabetic retinopathy), kidney disease (diabetic nephropathy), stroke, and so on. According to the International Diabetes Federation, 382 million people worldwide suffer from diabetes. By 2035, this number will have risen to 592 million. Every day, a large number of people become victims, and many are ignorant whether they have it or not. It primarily affects individuals between the ages of 25 and 74 years. If diabetes is left untreated and undiagnosed, it can lead to a slew of complications. The emergence of machine learning approaches, on the other hand, solves this crucial issue. The aim was to study the DM and analyze how machine learning algorithms are used to identify the diabetes mellitus at an early stage, which is one of the most serious metabolic disorders in the world today. Data was obtained from databases such as Pubmed, IEEE xplore, and INSPEC,and from other secondary sources and primary sources in which methods based on machine learning approaches used in healthcare to predict diabetes at an early stage are reported. After surveying various research papers, it was found that machine learning classification algorithms like Support Vector Machine (SVM), K-Nearest Neighbor (KNN), and Random Forest (RF) etc shows the best accuracy for predicting diabetes at an early stage. Early detection of diabetes is critical for effective therapy. Many people have no idea whether or not they have it. The full assessment of Machine learning approaches for early diabetes prediction and how to apply a variety of supervised and unsupervised machine learning algorithms to the dataset to achieve the best accuracy are addressed in this paper.. Furthermore, the work will be expanded and refined to create a more precise and general predictive model for diabetes risk prediction at an early stage. Different metrics can be used to assess performance and for accurate diabetic diagnosis.", "keywords": ["machine learning", "Diabetes", "machine learning approaches", "machine", "machine learning algorithms", "International Diabetes Federation", "learning", "chronic condition", "early stage", "early", "learning approaches", "Support Vector Machine", "variety of consequences", "stage", "learning algorithms", "condition", "diabetic", "Diabetes mellitus", "Diabetes Federation", "International Diabetes"], "paper_title": "A survey on diabetes risk prediction using machine learning approaches.", "last_updated": "2023/02/04"}, {"id": "0036516580", "domain": "Diabetic retinopathy", "model_name": "Han et al.", "publication_date": "2022/12/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36516580/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36516580", "task": null, "abstract": "Diabetic retinopathy (DR) is the primary cause of blindness in adults. Incorporating machine learning into DR grading can improve the accuracy of medical diagnosis. However, problems, such as severe data imbalance, persists. Existing studies on DR grading ignore the correlation between its labels. In this study, a category weighted network (CWN) was proposed to achieve data balance at the model level. In the CWN, a reference for weight settings is provided by calculating the category gradient norm and reducing the experimental overhead. We proposed to use relation weighted labels instead of the one-hot label to investigate the distance relationship between labels. Experiments revealed that the proposed CWN achieved excellent performance on various DR datasets. Furthermore, relation weighted labels exhibit broad applicability and can improve other methods using one-hot labels. The proposed method achieved kappa scores of 0.9431 and 0.9226 and accuracy of 90.94% and 86.12% on DDR and APTOS datasets, respectively.", "keywords": ["Diabetic retinopathy", "blindness in adults", "labels", "CWN", "proposed", "relation weighted labels", "Incorporating machine learning", "weighted labels", "proposed CWN achieved", "weighted", "Diabetic", "retinopathy", "adults", "proposed CWN", "relation weighted", "primary", "blindness", "grading", "DDR and APTOS", "Incorporating machine"], "paper_title": "Category weighted network and relation weighted label for diabetic retinopathy screening.", "last_updated": "2023/02/04"}, {"id": "0034946438", "domain": "Diabetic retinopathy", "model_name": "diabetes-complications-prediction", "publication_date": "2021/12/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34946438/", "code_link": "https://github.com/yazanjian/diabetes-complications-prediction", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "34946438", "task": null, "abstract": "Diabetes mellitus (DM) is a chronic disease that is considered to be life-threatening. It can affect any part of the body over time, resulting in serious complications such as nephropathy, neuropathy, and retinopathy. In this work, several supervised classification algorithms were applied for building different models to predict and classify eight diabetes complications. The complications include metabolic syndrome, dyslipidemia, neuropathy, nephropathy, diabetic foot, hypertension, obesity, and retinopathy. For this study, a dataset collected by the Rashid Center for Diabetes and Research (RCDR) located in Ajman, UAE, was utilized. The dataset consists of 884 records with 79 features. Some essential preprocessing steps were applied to handle the missing values and unbalanced data problems. Furthermore, feature selection was performed to select the top five and ten features for each complication. The final number of records used to train and build the binary classifiers for each complication was as follows: 428-metabolic syndrome, 836-dyslipidemia, 223-neuropathy, 233-nephropathy, 240-diabetic foot, 586-hypertension, 498-obesity, 228-retinopathy. Repeated stratified k-fold cross-validation (with k = 10 and a total of 10 repetitions) was employed for a better estimation of the performance. Accuracy and F1-score were used to evaluate the models' performance reaching a maximum of 97.8% and 97.7% for accuracy and F1-scores, respectively. Moreover, by comparing the performance achieved using different attributes' sets, it was found that by using a selected number of features, we can still build adequate classifiers.", "keywords": ["chronic disease", "Diabetes mellitus", "Diabetes", "complications", "Rashid Center", "diabetes complications", "neuropathy", "mellitus", "life-threatening", "retinopathy", "features", "performance", "nephropathy", "chronic", "disease", "considered", "body over time", "located in Ajman", "affect any part", "syndrome"], "paper_title": "A Machine Learning Approach to Predicting Diabetes Complications.", "last_updated": "2023/02/04"}, {"id": "0030409338", "domain": "Diabetic retinopathy", "model_name": "SCREEN-DR", "publication_date": "2018/10/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30409338/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "30409338", "task": null, "abstract": "Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss. Screening programs, based on retinal imaging techniques, are fundamental to detect the disease since the initial stages are asymptomatic. Most of these examinations reflect negative cases and many have poor image quality, representing an important inefficiency factor. The SCREEN-DR project aims to tackle this limitation, by researching and developing computer-aided methods for diabetic retinopathy detection. This article presents a multidisciplinary collaborative platform that was created to meet the needs of physicians and researchers, aiming at the creation of machine learning algorithms to facilitate the screening process. Our proposal is a collaborative platform for textual and visual annotation of image datasets. The architecture and layout were optimized for annotating DR images by gathering feedback from several physicians during the design and conceptualization of the platform. It allows the aggregation and indexing of imagiology studies from diverse sources, and supports the creation and annotation of phenotype-specific datasets to feed artificial intelligence algorithms. The platform makes use of an anonymization pipeline and role-based access control for securing personal data. The SCREEN-DR platform has been deployed in the production environment of the SCREEN-DR project at http://demo.dicoogle.com/screen-dr, and the source code of the project is publicly available. We provide a description of the platform's interface and use cases it supports. At the time of publication, four physicians have created a total of 1826 annotations for 701 distinct images, and the annotated data has been used for training classification models.", "keywords": ["prevalent microvascular complication", "irreversible visual loss", "prevalent microvascular", "microvascular complication", "complication of diabetes", "diabetes mellitus", "lead to irreversible", "Diabetic retinopathy", "platform", "visual loss", "irreversible visual", "diabetic retinopathy detection", "collaborative platform", "retinal imaging techniques", "SCREEN-DR project", "SCREEN-DR", "project", "multidisciplinary collaborative platform", "SCREEN-DR platform", "physicians"], "paper_title": "SCREEN-DR: Collaborative platform for diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0025974230", "domain": "Diabetic retinopathy", "model_name": "Krishnamoorthy et al.", "publication_date": "2015/05/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25974230/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "25974230", "task": "IWqQC1koJA", "abstract": "Retinal fundus images are widely used in diagnosing and providing treatment for several eye diseases. Prior works using retinal fundus images detected the presence of exudation with the aid of publicly available dataset using extensive segmentation process. Though it was proved to be computationally efficient, it failed to create a diabetic retinopathy feature selection system for transparently diagnosing the disease state. Also the diagnosis of diseases did not employ machine learning methods to categorize candidate fundus images into true positive and true negative ratio. Several candidate fundus images did not include more detailed feature selection technique for diabetic retinopathy. To apply machine learning methods and classify the candidate fundus images on the basis of sliding window a method called, Diabetic Fundus Image Recuperation (DFIR) is designed in this paper. The initial phase of DFIR method select the feature of optic cup in digital retinal fundus images based on Sliding Window Approach. With this, the disease state for diabetic retinopathy is assessed. The feature selection in DFIR method uses collection of sliding windows to obtain the features based on the histogram value. The histogram based feature selection with the aid of Group Sparsity Non-overlapping function provides more detailed information of features. Using Support Vector Model in the second phase, the DFIR method based on Spiral Basis Function effectively ranks the diabetic retinopathy diseases. The ranking of disease level for each candidate set provides a much promising result for developing practically automated diabetic retinopathy diagnosis system. Experimental work on digital fundus images using the DFIR method performs research on the factors such as sensitivity, specificity rate, ranking efficiency and feature selection time.", "keywords": ["Retinal fundus images", "fundus images", "candidate fundus images", "Retinal fundus", "DFIR method", "fundus", "Diabetic Fundus Image", "Fundus Image Recuperation", "diabetic retinopathy", "candidate fundus", "images", "feature selection", "DFIR", "fundus images based", "fundus images detected", "diabetic", "providing treatment", "feature", "digital fundus images", "digital retinal fundus"], "paper_title": "A novel image recuperation approach for diagnosing and ranking retinopathy disease level using diabetic fundus image.", "last_updated": "2023/02/04"}, {"id": "0035678993", "domain": "Diabetic retinopathy", "model_name": "Hardas et al.", "publication_date": "2022/06/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35678993/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "35678993", "task": "IWqQC1koJA", "abstract": "Diabetic Retinopathy (DR) is one of the leading causes of blindness in all age groups. Inadequate blood supply to the retina, retinal vascular exudation, and intraocular hemorrhage cause DR. Despite recent advances in the diagnosis and treatment of DR, this complication remains a challenging task for physicians and patients. Hence, a comprehensive and automated technique for DR screening is necessary, which will give early detection of this disease. The proposed work focuses on 16 class classification method using Support Vector Machine (SVM) that predict abnormalities individually or in combination based on the selected class. Our proposed work comprises Gaussian mixture model (GMM), K-means, Maximum a Posteriori (MAP) algorithm, Principal Component Analysis (PCA), Grey level co-occurrence matrix (GLCM), and SVM for disease diagnosis using DR. The proposed method provides an accuracy of 77.3% on DIARETDB1 dataset. We expect this low computational cost will be helpful in the medicine and diagnosis of DR.", "keywords": ["Diabetic Retinopathy", "age groups", "Support Vector Machine", "Principal Component Analysis", "Retinopathy", "retinal vascular exudation", "Inadequate blood supply", "Vector Machine", "Maximum a Posteriori", "proposed work", "Diabetic", "groups", "Support Vector", "Principal Component", "Component Analysis", "proposed", "leading", "blindness", "age", "Inadequate blood"], "paper_title": "Retinal fundus image classification for diabetic retinopathy using SVM predictions.", "last_updated": "2023/02/04"}, {"id": "0036231678", "domain": "Diabetic retinopathy", "model_name": "diabetes-classification-dataset", "publication_date": "2022/09/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36231678/", "code_link": "https://github.com/kamruleee51/diabetes-classification-dataset", "model_type": "random forest", "verified": false, "model_task": null, "code_available": true, "disease_class": "NC0kSKEoVp", "pmid": "36231678", "task": null, "abstract": "Diabetes is one of the most rapidly spreading diseases in the world, resulting in an array of significant complications, including cardiovascular disease, kidney failure, diabetic retinopathy, and neuropathy, among others, which contribute to an increase in morbidity and mortality rate. If diabetes is diagnosed at an early stage, its severity and underlying risk factors can be significantly reduced. However, there is a shortage of labeled data and the occurrence of outliers or data missingness in clinical datasets that are reliable and effective for diabetes prediction, making it a challenging endeavor. Therefore, we introduce a newly labeled diabetes dataset from a South Asian nation (Bangladesh). In addition, we suggest an automated classification pipeline that includes a weighted ensemble of machine learning (ML) classifiers: Naive Bayes (NB), Random Forest (RF), Decision Tree (DT), XGBoost (XGB), and LightGBM (LGB). Grid search hyperparameter optimization is employed to tune the critical hyperparameters of these ML models. Furthermore, missing value imputation, feature selection, and K-fold cross-validation are included in the framework design. A statistical analysis of variance (ANOVA) test reveals that the performance of diabetes prediction significantly improves when the proposed weighted ensemble (DT + RF + XGB + LGB) is executed with the introduced preprocessing, with the highest accuracy of 0.735 and an area under the ROC curve (AUC) of 0.832. In conjunction with the suggested ensemble model, our statistical imputation and RF-based feature selection techniques produced the best results for early diabetes prediction. Moreover, the presented new dataset will contribute to developing and implementing robust ML models for diabetes prediction utilizing population-level data.", "keywords": ["including cardiovascular disease", "rapidly spreading diseases", "spreading diseases", "cardiovascular disease", "kidney failure", "diabetic retinopathy", "significant complications", "including cardiovascular", "mortality rate", "rapidly spreading", "array of significant", "increase in morbidity", "morbidity and mortality", "diabetes prediction", "Diabetes", "labeled diabetes dataset", "South Asian nation", "diabetes prediction significantly", "early diabetes prediction", "prediction"], "paper_title": "Early Prediction of Diabetes Using an Ensemble of Machine Learning Models.", "last_updated": "2023/02/04"}, {"id": "0036286352", "domain": "Diabetic retinopathy", "model_name": "Badawi et al.", "publication_date": "2022/09/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36286352/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36286352", "task": null, "abstract": "Hypertensive retinopathy severity classification is proportionally related to tortuosity severity grading. No tortuosity severity scale enables a computer-aided system to classify the tortuosity severity of a retinal image. This work aimed to introduce a machine learning model that can identify the severity of a retinal image automatically and hence contribute to developing a hypertensive retinopathy or diabetic retinopathy automated grading system. First, the tortuosity is quantified using fourteen tortuosity measurement formulas for the retinal images of the AV-Classification dataset to create the tortuosity feature set. Secondly, a manual labeling is performed and reviewed by two ophthalmologists to construct a tortuosity severity ground truth grading for each image in the AV classification dataset. Finally, the feature set is used to train and validate the machine learning models (J48 decision tree, ensemble rotation forest, and distributed random forest). The best performance learned model is used as the tortuosity severity classifier to identify the tortuosity severity (normal, mild, moderate, and severe) for any given retinal image. The distributed random forest model has reported the highest accuracy (99.4%) compared to the J48 Decision tree model and the rotation forest model with minimal least root mean square error (0.0000192) and the least mean average error (0.0000182). The proposed tortuosity severity grading matched the ophthalmologist's judgment. Moreover, detecting the tortuosity severity of the retinal vessels', optimizing vessel segmentation, the vessel segment extraction, and the created feature set have increased the accuracy of the automatic tortuosity severity detection model.", "keywords": ["tortuosity severity", "tortuosity severity grading", "tortuosity", "severity", "Hypertensive retinopathy severity", "retinal image", "tortuosity severity scale", "retinopathy severity classification", "proportionally related", "severity grading", "Hypertensive retinopathy", "retinal", "tortuosity severity ground", "model", "tortuosity severity classifier", "proposed tortuosity severity", "automatic tortuosity severity", "tortuosity severity detection", "tortuosity feature set", "image"], "paper_title": "Four Severity Levels for Grading the Tortuosity of a Retinal Fundus Image.", "last_updated": "2023/02/04"}, {"id": "0036006638", "domain": "Diabetic retinopathy", "model_name": "Lee et al.", "publication_date": "2022/08/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36006638/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36006638", "task": null, "abstract": "We sought to analyze the visual outcome and systemic prognostic factors for diabetic vitrectomy and predicted outcomes using these factors. This was a multicenter electronic medical records (EMRs) review study of 1504 eyes with type 2 diabetes that underwent vitrectomy for proliferative diabetic retinopathy at 6 university hospitals. Demographics, laboratory results, intra-operative findings, and visual acuity (VA) values were analyzed and correlated with visual outcomes at 1\u00a0year after the vitrectomy. Prediction models for visual outcomes were obtained using machine learning. At 1 year, VA was 1.0 logarithm of minimal angle resolution (logMAR) or greater (poor visual outcome group) in 456 eyes (30%). Baseline visual acuity, duration of diabetes treatment, tractional membrane, silicone oil tamponade, smoking, and vitreous hemorrhage correlated with logMAR VA at 1 year (r = 0.450, -0.159, 0.221, 0.280, 0.067, and -0.105; all P \u2264 0.036). An ensemble decision tree model trained using all variables generated accuracy, specificity, F1 score (the harmonic means of which precision and sensitivity), and receiver-operating characteristic curve area under curve values of 0.77, 0.66, 0.85, and 0.84 for the prediction of poor visual outcomes at 1 year after vitrectomy. Visual outcome after diabetic vitrectomy is associated with pre- and intra-operative findings and systemic factors. Poor visual outcome after diabetic vitrectomy was predictable using clinical factors. Intensive care in patients who are predicted to result in poor vision may limit vision loss resulting from type 2 diabetes. This study demonstrates that a real world EMR big data could predict outcome after diabetic vitrectomy using clinical factors.", "keywords": ["visual", "visual outcome", "sought to analyze", "poor visual outcome", "vitrectomy", "diabetic vitrectomy", "outcome", "poor visual", "diabetic", "year", "outcomes", "visual acuity", "factors", "systemic prognostic factors", "poor", "systemic prognostic", "prognostic factors", "visual outcome group", "outcome after diabetic", "diabetes"], "paper_title": "Prediction of Visual Outcomes After Diabetic Vitrectomy Using Clinical Factors From Common Data Warehouse.", "last_updated": "2023/02/04"}, {"id": "0031259001", "domain": "Diabetic retinopathy", "model_name": "Ogunyemi et al.", "publication_date": "2019/05/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31259001/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31259001", "task": null, "abstract": "<b>Introduction:</b> Timely diabetic retinopathy detection remains a problem in medically underserved settings in the US; diabetic patients in these locales have limited access to eye specialists. Teleretinal screening programs have been introduced to address this problem. <b>Methods:</b> Using data on ethnicity, gender, age, hemoglobin A1C, insulin dependence, time since last eye examination, subjective diabetes control, and years with diabetes from 27,116 diabetic patients participating in a Los Angeles County teleretinal screening program, we compared different machine learning methods for predicting retinopathy. The dataset exhibited a class imbalance. <b>Results:</b> Six classifiers learned on the data were predictive of retinopathy. The best model had an AUC of 0.754, sensitivity of 58% and specificity of 80%. <b>Discussion:</b> Successfully detecting retinopathy from diabetic patients' routinely collected clinical data could help clinicians in medically underserved areas identify unscreened diabetic patients who are at risk of developing retinopathy. This work is a step towards that goal.", "keywords": ["Timely diabetic retinopathy", "retinopathy detection remains", "Timely diabetic", "Los Angeles County", "detection remains", "locales have limited", "limited access", "Angeles County teleretinal", "County teleretinal screening", "Introduction", "medically underserved settings", "diabetic retinopathy detection", "diabetic patients", "Timely", "eye specialists", "Teleretinal screening programs", "Teleretinal screening", "diabetic", "Los Angeles", "Angeles County"], "paper_title": "Predictive Models for Diabetic Retinopathy from Non-Image Teleretinal Screening Data.", "last_updated": "2023/02/04"}, {"id": "0027782018", "domain": "Diabetic retinopathy", "model_name": "John et al.", "publication_date": "2018/03/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/27782018/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "27782018", "task": null, "abstract": "Diabetic retinopathy (DR) is regarded as a major cause of preventable blindness, which can be detected and treated if the cases are identified by screening. Screening for DR is therefore being practiced in developed countries, and tele screening has been a prominent model of delivery of eye care for screening DR. Our study has been designed to provide inputs on the suitability of a computer-assisted DR screening solution, for use in a larger prospective study. Computer-assisted screening technology for grading diabetic retinopathy from fundus images by a set of machine learning algorithms. The preliminary recommendations from a pilot study of a system built using the public datasets and retrospective images, showed a good sensitivity and specificity. The machine learning algorithms has to be validated on a larger dataset of a population level study.", "keywords": ["preventable blindness", "detected and treated", "cases are identified", "screening", "Diabetic retinopathy", "machine learning algorithms", "grading diabetic retinopathy", "study", "machine learning", "learning algorithms", "blindness", "Computer-assisted screening technology", "Diabetic", "retinopathy", "regarded", "major", "preventable", "detected", "treated", "cases"], "paper_title": "Assessment of Computer-Assisted Screening Technology for Diabetic Retinopathy Screening in India - Preliminary Results and Recommendations from a Pilot Study.", "last_updated": "2023/02/04"}, {"id": "0036409801", "domain": "Diabetic retinopathy", "model_name": "TMM-Nets", "publication_date": "2022/11/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36409801/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36409801", "task": "IWqQC1koJA", "abstract": "Rare diseases, which are severely underrepresented in basic and clinical research, can particularly benefit from machine learning techniques. However, current learning-based approaches usually focus on either mono-modal image data or matched multi-modal data, whereas the diagnosis of rare diseases necessitates the aggregation of unstructured and unmatched multi-modal image data due to their rare and diverse nature. In this study, we therefore propose diagnosis-guided multi-to-mono modal generation networks (TMM-Nets) along with training and testing procedures. TMM-Nets can transfer data from multiple sources to a single modality for diagnostic data structurization. To demonstrate their potential in the context of rare diseases, TMM-Nets were deployed to diagnose the lupus retinopathy (LR-SLE), leveraging unmatched regular and ultra-wide-field fundus images for transfer learning. The TMM-Nets encoded the transfer learning from diabetic retinopathy to LR-SLE based on the similarity of the fundus lesions. In addition, a lesion-aware multi-scale attention mechanism was developed for clinical alerts, enabling TMM-Nets not only to inform patient care, but also to provide insights consistent with those of clinicians. An adversarial strategy was also developed to refine multi- to mono-modal image generation based on diagnostic results and the data distribution to enhance the data augmentation performance. Compared to the baseline model, the TMM-Nets showed 35.19% and 33.56% F1 score improvements on the test and external validation sets, respectively. In addition, the TMM-Nets can be used to develop diagnostic models for other rare diseases.", "keywords": ["machine learning techniques", "Rare diseases", "severely underrepresented", "underrepresented in basic", "benefit from machine", "TMM-Nets", "data", "Rare", "rare diseases necessitates", "diseases", "learning techniques", "multi-modal image data", "clinical research", "machine learning", "image data", "matched multi-modal data", "unmatched multi-modal image", "image data due", "transfer learning", "mono-modal image"], "paper_title": "TMM-Nets: Transferred Multi- to Mono-modal Generation for Lupus Retinopathy Diagnosis.", "last_updated": "2023/02/04"}, {"id": "0035224527", "domain": "Uveitis", "model_name": "Sevgi et al.", "publication_date": "2021/07/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35224527/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "35224527", "task": "IWqQC1koJA", "abstract": "To determine the association between diabetic retinopathy (DR) severity and quantitative retinal vascular features. Retrospective image analysis study. Eyes with DR and eyes with no posterior segment disease (normal eyes) that had undergone ultra-widefield fluorescein angiography (UWFA) with associated color fundus photography. Exclusion criteria were any previous laser photocoagulation, low image quality, intravitreal or periocular pharmacotherapy within 6 months of imaging, and any other significant retinal disease including posterior uveitis, retinal vein occlusion, and choroidal neovascularization. The centered early mid-phase UWFA frame that captured the maximum vessel area was selected using automated custom software for each eye. Panretinal and zonal vascular features were extracted using a machine learning algorithm. Eyes with DR were graded for DR severity as mild nonproliferative DR (NPDR), moderate NPDR, severe NPDR, and proliferative DR (PDR). Parameters of normal eyes were compared with age- and gender-matched patients with DR using the <i>t</i> test. Differences between severity groups were evaluated by the analysis of variance and Kruskal-Wallis tests, generalized linear mixed-effects models, and random forest regression models. Diabetic retinopathy severity and vascular features (panretinal and zonal vessel area, length and geodesic distance, panretinal area index, tortuosity measures, vascular density measures, and zero vessel density rate). Ninety-seven eyes from 60 patients with DR and 12 normal eyes from 12 patients that underwent UWFA for evaluation of fellow eye pathology had images of sufficient quality to be included in this analysis. The mean age was 60 \u00b1 10 years in DR eyes and 46 \u00b1 17 years in normal eyes. Panretinal vessel area, mean geodesic distance, skewness, and kurtosis of local vessel density was significantly higher in normal eyes compared with the age- and gender-matched eyes with DR (<i>P</i> < 0.05). Zero vessel density rate, skewness of vessel density, and mean mid-peripheral geodesic distance were among the most important features for distinguishing mild NPDR from advanced forms of DR and PDR versus eyes without PDR. Automated analysis of retinal vasculature demonstrated associations with DR severity and visual and subvisual vascular biomarkers. Further studies are needed to evaluate the clinical significance of these parameters for DR prognosis and therapeutic response.", "keywords": ["Eyes", "normal eyes", "vessel", "vessel density", "quantitative retinal vascular", "NPDR", "normal", "severity", "UWFA", "vessel area", "vascular", "vascular features", "density", "retinal", "quantitative retinal", "Panretinal", "PDR", "vessel density rate", "features", "area"], "paper_title": "Characterization of Ultra-Widefield Angiographic Vascular Features in Diabetic Retinopathy with Automated Severity Classification.", "last_updated": "2023/02/04"}, {"id": "0034890873", "domain": "Diabetic retinopathy", "model_name": "Singh et al.", "publication_date": "2021/11/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34890873/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34890873", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a common health concern. Unfortunately, the metabolic pathway causing DR is yet to be understood. The carotenoid level in the human body is known to protect the health of the eyes. In this work, resonance Raman spectroscopy and multivariate analysis of the spectral data of human serum are reported as next-generation spectropathologic tools to detect retinal degeneration efficiently. The proposed technique shows promise by endorsing ocular carotenoids as a critical biomarker for such pathosis. Furthermore, the multivariate analysis of the spectral data distinguishes between two different stages of the disease. The machine learning algorithm is used to estimate a significant accuracy of 94% of the proposed model for the classification. As the carotenoid level can be controlled by dietary intake, we believe that the reported results also indicate a therapeutic role of the same in DR.", "keywords": ["common health concern", "Diabetic retinopathy", "health concern", "common health", "metabolic pathway causing", "carotenoid level", "multivariate analysis", "spectral data", "resonance Raman spectroscopy", "Diabetic", "retinopathy", "concern", "health", "spectral data distinguishes", "common", "metabolic pathway", "pathway causing", "resonance Raman", "carotenoid", "endorsing ocular carotenoids"], "paper_title": "Spectropathologic endorsement of ocular carotenoids for early detection of diabetic retinopathy.", "last_updated": "2023/02/04"}, {"id": "0034635403", "domain": "Diabetic retinopathy", "model_name": "Salna et al.", "publication_date": "2021/10/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34635403/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "34635403", "task": "0GPcU42tzV", "abstract": "To study the association between achievement of guideline-defined treatment targets on HbA1c, low-density lipoproteins (LDL-C), and blood pressure with the progression of diabetic complications in patients with type 1 diabetes (T1D). The study included 355 patients at baseline and 114 patients with follow-up data after 3-5\u00a0years. Outcome variables were the progression of diabetic kidney disease, retinopathy, or cardiovascular disease (CVD). We used logistic regression and other machine learning algorithms (MLA) to model the association of achievement of treatment targets and probability of progression of complications. Achievement of the target blood pressure was associated with 96% lower odds of a new CVD event (0.04 (95% CI 0.00, 0.53), p\u00a0=\u00a00.016), and 72% lower odds of progression of any complication (0.28 (95% CI 0.09, 0.89), p\u00a0=\u00a00.027. Achievement of HbA1c target was associated with lower odds of composite complication progression by 82% (0.18 (95% CI 0.04, 0.88), p\u00a0=\u00a00.034.) None of the patients who achieved HbA1c target progressed in CVD. MLA demonstrated good accuracy for the prediction of progression of CVD (AUC 0.824), and lower accuracy for other complications. The achievement of blood pressure and HbA1c treatment targets is associated with lower odds of vascular complication of T1D in a real life study.", "keywords": ["low-density lipoproteins", "lower odds", "guideline-defined treatment targets", "progression", "treatment targets", "CVD", "progression of diabetic", "patients", "achievement", "blood pressure", "lower", "achievement of guideline-defined", "odds", "guideline-defined treatment", "study", "target blood pressure", "patients with type", "targets", "target", "complication progression"], "paper_title": "Achievement of treatment targets predicts progression of vascular complications in type 1 diabetes.", "last_updated": "2023/02/04"}, {"id": "0023919537", "domain": "Diabetic retinopathy", "model_name": "Torok et al.", "publication_date": "2013/08/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/23919537/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "23919537", "task": null, "abstract": "The aim of the project was to develop a novel method for diabetic retinopathy screening based on the examination of tear fluid biomarker changes. In order to evaluate the usability of protein biomarkers for pre-screening purposes several different approaches were used, including machine learning algorithms. All persons involved in the study had diabetes. Diabetic retinopathy (DR) was diagnosed by capturing 7-field fundus images, evaluated by two independent ophthalmologists. 165 eyes were examined (from 119 patients), 55 were diagnosed healthy and 110 images showed signs of DR. Tear samples were taken from all eyes and state-of-the-art nano-HPLC coupled ESI-MS/MS mass spectrometry protein identification was performed on all samples. Applicability of protein biomarkers was evaluated by six different optimally parameterized machine learning algorithms: Support Vector Machine, Recursive Partitioning, Random Forest, Naive Bayes, Logistic Regression, K-Nearest Neighbor. Out of the six investigated machine learning algorithms the result of Recursive Partitioning proved to be the most accurate. The performance of the system realizing the above algorithm reached 74% sensitivity and 48% specificity. Protein biomarkers selected and classified with machine learning algorithms alone are at present not recommended for screening purposes because of low specificity and sensitivity values. This tool can be potentially used to improve the results of image processing methods as a complementary tool in automatic or semiautomatic systems.", "keywords": ["machine learning algorithms", "machine learning", "learning algorithms", "tear fluid biomarker", "retinopathy screening based", "Recursive Partitioning", "protein biomarkers", "machine", "Support Vector Machine", "including machine learning", "diabetic retinopathy", "fluid biomarker", "learning", "tear fluid", "protein", "algorithms", "screening based", "diabetic retinopathy screening", "Recursive Partitioning proved", "biomarkers"], "paper_title": "Tear fluid proteomics multimarkers for diabetic retinopathy screening.", "last_updated": "2023/02/04"}, {"id": "0031000806", "domain": "Diabetic retinopathy", "model_name": "Chen et al.", "publication_date": "2019/05/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31000806/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31000806", "task": null, "abstract": "", "keywords": [], "paper_title": "How to develop machine learning models for healthcare.", "last_updated": "2023/02/04"}, {"id": "0029544783", "domain": "Diabetic retinopathy", "model_name": "Kusakunniran et al.", "publication_date": "2018/02/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29544783/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "29544783", "task": "aipbNdPTIt", "abstract": "(Background and Objective): The occurrence of hard exudates is one of the early signs of diabetic retinopathy which is one of the leading causes of the blindness. Many patients with diabetic retinopathy lose their vision because of the late detection of the disease. Thus, this paper is to propose a novel method of hard exudates segmentation in retinal images in an automatic way. (Methods): The existing methods are based on either supervised or unsupervised learning techniques. In addition, the learned segmentation models may often cause miss-detection and/or fault-detection of hard exudates, due to the lack of rich characteristics, the intra-variations, and the similarity with other components in the retinal image. Thus, in this paper, the supervised learning based on the multilayer perceptron (MLP) is only used to identify initial seeds with high confidences to be hard exudates. Then, the segmentation is finalized by unsupervised learning based on the iterative graph cut (GC) using clusters of initial seeds. Also, in order to reduce color intra-variations of hard exudates in different retinal images, the color transfer (CT) is applied to normalize their color information, in the pre-processing step. (Results): The experiments and comparisons with the other existing methods are based on the two well-known datasets, e_ophtha EX and DIARETDB1. It can be seen that the proposed method outperforms the other existing methods in the literature, with the sensitivity in the pixel-level of 0.891 for the DIARETDB1 dataset and 0.564 for the e_ophtha EX dataset. The cross datasets validation where the training process is performed on one dataset and the testing process is performed on another dataset is also evaluated in this paper, in order to illustrate the robustness of the proposed method. (Conclusions): This newly proposed method integrates the supervised learning and unsupervised learning based techniques. It achieves the improved performance, when compared with the existing methods in the literature. The robustness of the proposed method for the scenario of cross datasets could enhance its practical usage. That is, the trained model could be more practical for unseen data in the real-world situation, especially when the capturing environments of training and testing images are not the same.", "keywords": ["Background and Objective", "hard exudates", "existing methods", "proposed method", "diabetic retinopathy", "unsupervised learning based", "diabetic retinopathy lose", "learning based", "early signs", "hard exudates segmentation", "method", "Methods", "hard", "unsupervised learning", "exudates", "based", "Background", "Objective", "learning", "occurrence of hard"], "paper_title": "Hard exudates segmentation based on learned initial seeds and iterative graph cut.", "last_updated": "2023/02/04"}, {"id": "0036121302", "domain": "Diabetic retinopathy", "model_name": "Huang et al.", "publication_date": "2022/09/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36121302/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "36121302", "task": "IWqQC1koJA", "abstract": "Artificial intelligence can use real-world data to create models capable of making predictions and medical diagnosis for diabetes and its complications. The aim of this commentary article is to provide a general perspective and present recent advances on how artificial intelligence can be applied to improve the prediction and diagnosis of six significant complications of diabetes including (1) gestational diabetes, (2) hypoglycemia in the hospital, (3) diabetic retinopathy, (4) diabetic foot ulcers, (5) diabetic peripheral neuropathy, and (6) diabetic nephropathy.", "keywords": ["create models capable", "Artificial intelligence", "real-world data", "data to create", "create models", "models capable", "capable of making", "making predictions", "predictions and medical", "medical diagnosis", "diabetic foot ulcers", "diabetic peripheral neuropathy", "diabetic", "present recent advances", "intelligence", "diabetes", "diabetic retinopathy", "diabetic nephropathy", "diabetes including", "gestational diabetes"], "paper_title": "Artificial Intelligence for Predicting and Diagnosing Complications of Diabetes.", "last_updated": "2023/02/04"}, {"id": "0032699108", "domain": "Diabetic retinopathy", "model_name": "Bello-Chavolla et al.", "publication_date": "2021/06/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32699108/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "32699108", "task": null, "abstract": "Previous reports in European populations demonstrated the existence of five data-driven adult-onset diabetes subgroups. Here, we use self-normalizing neural networks (SNNN) to improve reproducibility of these data-driven diabetes subgroups in Mexican cohorts to extend its application to more diverse settings. We trained SNNN and compared it with k-means clustering to classify diabetes subgroups in a multiethnic and representative population-based National Health and Nutrition Examination Survey (NHANES) datasets with all available measures (training sample: NHANES-III, n=1132; validation sample: NHANES 1999-2006, n=626). SNNN models were then applied to four Mexican cohorts (SIGMA-UIEM, n=1521; Metabolic Syndrome cohort, n=6144; ENSANUT 2016, n=614\u2009and CAIPaDi, n=1608) to characterize diabetes subgroups in Mexicans according to treatment response, risk for chronic complications and risk factors for the incidence of each subgroup. SNNN yielded four reproducible clinical profiles (obesity related, insulin deficient, insulin resistant, age related) in NHANES and Mexican cohorts even without C-peptide measurements. We observed in a population-based survey a high prevalence of the insulin-deficient form (41.25%, 95%\u2009CI 41.02% to 41.48%), followed by obesity-related (33.60%, 95%\u2009CI 33.40% to 33.79%), age-related (14.72%, 95%\u2009CI 14.63% to 14.82%) and severe insulin-resistant groups. A significant association was found between the SLC16A11 diabetes risk variant and the obesity-related subgroup (OR 1.42, 95%\u2009CI 1.10 to 1.83, p=0.008). Among incident cases, we observed a greater incidence of mild obesity-related diabetes (n=149, 45.0%). In a diabetes outpatient clinic cohort, we observed increased 1-year risk (HR 1.59, 95%\u2009CI 1.01 to 2.51) and 2-year risk (HR 1.94, 95%\u2009CI 1.13 to 3.31) for incident retinopathy in the insulin-deficient group and decreased 2-year diabetic retinopathy risk for the obesity-related subgroup (HR 0.49, 95%\u2009CI 0.27 to 0.89). Diabetes subgroup phenotypes are reproducible using SNNN; our algorithm is available as web-based tool. Application of these models allowed for better characterization of diabetes subgroups and risk factors in Mexicans that could have clinical applications.", "keywords": ["European populations demonstrated", "reports in European", "European populations", "diabetes subgroups", "Mexican cohorts", "data-driven adult-onset diabetes", "adult-onset diabetes subgroups", "Nutrition Examination Survey", "Previous reports", "diabetes", "populations demonstrated", "demonstrated the existence", "SNNN", "data-driven diabetes subgroups", "Metabolic Syndrome cohort", "population-based National Health", "subgroups", "risk", "data-driven adult-onset", "Mexican"], "paper_title": "Clinical characterization of data-driven diabetes subgroups in Mexicans using a reproducible machine learning approach.", "last_updated": "2023/02/04"}, {"id": "0028803840", "domain": "Diabetic retinopathy", "model_name": "RECODe", "publication_date": "2017/08/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28803840/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28803840", "task": null, "abstract": "In view of substantial mis-estimation of risks of diabetes complications using existing equations, we sought to develop updated Risk Equations for Complications Of type 2 Diabetes (RECODe). To develop and validate these risk equations, we used data from the Action to Control Cardiovascular Risk in Diabetes study (ACCORD, n=9635; 2001-09) and validated the equations for microvascular events using data from the Diabetes Prevention Program Outcomes Study (DPPOS, n=1018; 1996-2001), and for cardiovascular events using data from the Action for Health in Diabetes (Look AHEAD, n=4760; 2001-12). Microvascular outcomes were nephropathy, retinopathy, and neuropathy. Cardiovascular outcomes were myocardial infarction, stroke, congestive heart failure, and cardiovascular mortality. We also included all-cause mortality as an outcome. We used a cross-validating machine learning method to select predictor variables from demographic characteristics, clinical variables, comorbidities, medications, and biomarkers into Cox proportional hazards models for each outcome. The new equations were compared to older risk equations by assessing model discrimination, calibration, and the net reclassification index. All equations had moderate internal and external discrimination (C-statistics 0\u00b755-0\u00b784 internally, 0\u00b757-0\u00b779 externally) and high internal and external calibration (slopes 0\u00b771-1\u00b731 between observed and estimated risk). Our equations had better discrimination and calibration than the UK Prospective Diabetes Study Outcomes Model 2 (for microvascular and cardiovascular outcomes, C-statistics 0\u00b754-0\u00b762, slopes 0\u00b706-1\u00b712) and the American College of Cardiology/American Heart Association Pooled Cohort Equations (for fatal or non-fatal myocardial infarction or stroke, C-statistics 0\u00b761-0\u00b766, slopes 0\u00b730-0\u00b739). RECODe might improve estimation of risk of complications for patients with type 2 diabetes. National Institute for Diabetes and Digestive and Kidney Disease, National Heart, Lung and Blood Institute, and National Institute on Minority Health and Health Disparities, National Institutes of Health, and US Department of Veterans Affairs.", "keywords": ["Diabetes Prevention Program", "Prevention Program Outcomes", "develop updated Risk", "Program Outcomes Study", "Control Cardiovascular Risk", "updated Risk Equations", "Diabetes Study Outcomes", "diabetes", "equations", "Risk Equations", "Prevention Program", "Diabetes study", "view of substantial", "substantial mis-estimation", "Prospective Diabetes Study", "Cardiovascular outcomes", "Study Outcomes Model", "Risk", "Outcomes", "National Institute"], "paper_title": "Development and validation of Risk Equations for Complications Of type 2 Diabetes (RECODe) using individual participant data from randomised trials.", "last_updated": "2023/02/04"}, {"id": "0024958614", "domain": "Diabetic retinopathy", "model_name": "Ganesan et al.", "publication_date": "2014/06/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24958614/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "24958614", "task": "IWqQC1koJA", "abstract": "Diabetic retinopathy (DR) is a leading cause of vision loss among diabetic patients in developed countries. Early detection of occurrence of DR can greatly help in effective treatment. Unfortunately, symptoms of DR do not show up till an advanced stage. To counter this, regular screening for DR is essential in diabetic patients. Due to lack of enough skilled medical professionals, this task can become tedious as the number of images to be screened becomes high with regular screening of diabetic patients. An automated DR screening system can help in early diagnosis without the need for a large number of medical professionals. To improve detection, several pattern recognition techniques are being developed. In our study, we used trace transforms to model a human visual system which would replicate the way a human observer views an image. To classify features extracted using this technique, we used support vector machine (SVM) with quadratic, polynomial, radial basis function kernels and probabilistic neural network (PNN). Genetic algorithm (GA) was used to fine tune classification parameters. We obtained an accuracy of 99.41 and 99.12% with PNN-GA and SVM quadratic kernels, respectively.", "keywords": ["vision loss", "diabetic patients", "Diabetic retinopathy", "developed countries", "Diabetic", "medical professionals", "patients", "regular screening", "screening", "retinopathy", "countries", "leading", "vision", "loss", "SVM", "professionals", "effective treatment", "loss among diabetic", "regular", "developed"], "paper_title": "Computer-aided diabetic retinopathy detection using trace transforms on digital fundus images.", "last_updated": "2023/02/04"}, {"id": "0028692999", "domain": "Diabetic retinopathy", "model_name": "Fan et al.", "publication_date": "2017/07/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28692999/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "28692999", "task": "IWqQC1koJA", "abstract": "Automated optic disk (OD) detection plays an important role in developing a computer aided system for eye diseases. In this paper, we propose an algorithm for the OD detection based on structured learning. A classifier model is trained based on structured learning. Then, we use the model to achieve the edge map of OD. Thresholding is performed on the edge map, thus a binary image of the OD is obtained. Finally, circle Hough transform is carried out to approximate the boundary of OD by a circle. The proposed algorithm has been evaluated on three public datasets and obtained promising results. The results (an area overlap and Dices coefficients of 0.8605 and 0.9181, respectively, an accuracy of 0.9777, and a true positive and false positive fraction of 0.9183 and 0.0102) show that the proposed method is very competitive with the state-of-the-art methods and is a reliable tool for the segmentation of OD.", "keywords": ["Automated optic disk", "computer aided system", "Automated optic", "optic disk", "eye diseases", "plays an important", "important role", "role in developing", "developing a computer", "computer aided", "aided system", "system for eye", "detection plays", "structured learning", "based on structured", "detection based", "edge map", "detection", "learning", "Automated"], "paper_title": "Optic Disk Detection in Fundus Image Based on Structured Learning.", "last_updated": "2023/02/04"}, {"id": "0024289249", "domain": "Diabetic retinopathy", "model_name": "2B", "publication_date": "2013/12/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24289249/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "24289249", "task": "IWqQC1koJA", "abstract": "A(2B) adenosine receptor antagonists may be beneficial in treating diseases like asthma, diabetes, diabetic retinopathy, and certain cancers. This has stimulated research for the development of potent ligands for this subtype, based on quantitative structure-affinity relationships. In this work, a new ensemble machine learning algorithm is proposed for classification and prediction of the ligand-binding affinity of A(2B) adenosine receptor antagonists. This algorithm is based on the training of different classifier models with multiple training sets (composed of the same compounds but represented by diverse features). The k-nearest neighbor, decision trees, neural networks, and support vector machines were used as single classifiers. To select the base classifiers for combining into the ensemble, several diversity measures were employed. The final multiclassifier prediction results were computed from the output obtained by using a combination of selected base classifiers output, by utilizing different mathematical functions including the following: majority vote, maximum and average probability. In this work, 10-fold cross- and external validation were used. The strategy led to the following results: i) the single classifiers, together with previous features selections, resulted in good overall accuracy, ii) a comparison between single classifiers, and their combinations in the multiclassifier model, showed that using our ensemble gave a better performance than the single classifier model, and iii) our multiclassifier model performed better than the most widely used multiclassifier models in the literature. The results and statistical analysis demonstrated the supremacy of our multiclassifier approach for predicting the affinity of A(2B) adenosine receptor antagonists, and it can be used to develop other QSAR models.", "keywords": ["adenosine receptor antagonists", "adenosine receptor", "diabetic retinopathy", "receptor antagonists", "diseases like asthma", "beneficial in treating", "treating diseases", "single classifiers", "classifiers", "single classifier model", "multiclassifier", "adenosine", "base classifiers", "receptor", "multiclassifier model", "single", "antagonists", "quantitative structure-affinity relationships", "models", "model"], "paper_title": "Classifier ensemble based on feature selection and diversity measures for predicting the affinity of A(2B) adenosine receptor antagonists.", "last_updated": "2023/02/04"}, {"id": "0031946914", "domain": "Diabetic retinopathy", "model_name": "Huang et al.", "publication_date": "2020/05/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31946914/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "31946914", "task": "IWqQC1koJA", "abstract": "Retinopathy screening is a non-invasive method to collect retinal images and neovascularization detection from retinal images plays a significant role on the identification and classification of diabetes retinopathy. In this paper, an automatic parallel detection framework for neovascularization with color retinal images using ensemble of extreme learning machine is proposed. The framework employs two Map-Reduce Jobs to extract features and trains Extreme Learning Machine models. Ensemble methods such as bagging, subspace partitioning and cross validating are used to increase the accuracy. The framework is evaluated with retinal images from MESSIDOR database. Experimental results show the framework can improve the detection accuracy, as well as speedup the processing time to 22 times on average.", "keywords": ["extreme learning machine", "collect retinal images", "retinal images plays", "retinal images", "Retinopathy screening", "diabetes retinopathy", "extreme learning", "learning machine", "plays a significant", "significant role", "identification and classification", "classification of diabetes", "Learning Machine models", "color retinal images", "trains Extreme Learning", "collect retinal", "images plays", "retinal", "images", "Retinopathy"], "paper_title": "Automatic Parallel Detection of Neovascularization from Retinal Images Using Ensemble of Extreme Learning Machine<sup></sup>.", "last_updated": "2023/02/04"}, {"id": "0026774796", "domain": "Diabetic retinopathy", "model_name": "Quellec et al.", "publication_date": "2015/12/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26774796/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "26774796", "task": "IWqQC1koJA", "abstract": "With the increased prevalence of retinal pathologies, automating the detection of these pathologies is becoming more and more relevant. In the past few years, many algorithms have been developed for the automated detection of a specific pathology, typically diabetic retinopathy, using eye fundus photography. No matter how good these algorithms are, we believe many clinicians would not use automatic detection tools focusing on a single pathology and ignoring any other pathology present in the patient's retinas. To solve this issue, an algorithm for characterizing the appearance of abnormal retinas, as well as the appearance of the normal ones, is presented. This algorithm does not focus on individual images: it considers examination records consisting of multiple photographs of each retina, together with contextual information about the patient. Specifically, it relies on data mining in order to learn diagnosis rules from characterizations of fundus examination records. The main novelty is that the content of examination records (images and context) is characterized at multiple levels of spatial and lexical granularity: 1) spatial flexibility is ensured by an adaptive decomposition of composite retinal images into a cascade of regions, 2) lexical granularity is ensured by an adaptive decomposition of the feature space into a cascade of visual words. This multigranular representation allows for great flexibility in automatically characterizing normality and abnormality: it is possible to generate diagnosis rules whose precision and generalization ability can be traded off depending on data availability. A variation on usual data mining algorithms, originally designed to mine static data, is proposed so that contextual and visual data at adaptive granularity levels can be mined. This framework was evaluated in e-ophtha, a dataset of 25,702 examination records from the OPHDIAT screening network, as well as in the publicly-available Messidor dataset. It was successfully applied to the detection of patients that should be referred to an ophthalmologist and also to the specific detection of several pathologies.", "keywords": ["increased prevalence", "examination records", "detection", "data", "examination", "records", "fundus examination records", "automating the detection", "automatic detection tools", "detection tools focusing", "pathology", "automated detection", "algorithms", "typically diabetic retinopathy", "data mining algorithms", "specific detection", "automatic detection", "detection tools", "data mining", "eye fundus photography"], "paper_title": "Automatic detection of referral patients due to retinal pathologies through data mining.", "last_updated": "2023/02/04"}, {"id": "0026109519", "domain": "Diabetic retinopathy", "model_name": "Ibrahim et al.", "publication_date": "2015/06/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26109519/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "26109519", "task": "IWqQC1koJA", "abstract": "Prolonged diabetes retinopathy leads to diabetes maculopathy, which causes gradual and irreversible loss of vision. It is important for physicians to have a decision system that detects the early symptoms of the disease. This can be achieved by building a classification model using machine learning algorithms. Fuzzy logic classifiers group data elements with a degree of membership in multiple classes by defining membership functions for each attribute. Various methods have been proposed to determine the partitioning of membership functions in a fuzzy logic inference system. A clustering method partitions the membership functions by grouping data that have high similarity into clusters, while an equalized universe method partitions data into predefined equal clusters. The distribution of each attribute determines its partitioning as fine or coarse. A simple grid partitioning partitions each attribute equally and is therefore not effective in handling varying distribution amongst the attributes. A data-adaptive method uses a data frequency-driven approach to partition each attribute based on the distribution of data in that attribute. A data-adaptive neuro-fuzzy inference system creates corresponding rules for both finely distributed and coarsely distributed attributes. This method produced more useful rules and a more effective classification system. We obtained an overall accuracy of 98.55%.", "keywords": ["Prolonged diabetes retinopathy", "diabetes retinopathy leads", "Prolonged diabetes", "diabetes maculopathy", "diabetes retinopathy", "loss of vision", "retinopathy leads", "gradual and irreversible", "irreversible loss", "diabetes", "leads to diabetes", "membership functions", "attribute", "membership", "data", "system", "method", "functions", "distribution", "partitions"], "paper_title": "Classification of diabetes maculopathy images using data-adaptive neuro-fuzzy inference classifier.", "last_updated": "2023/02/04"}, {"id": "0027110272", "domain": "Diabetic retinopathy", "model_name": "Akyol et al.", "publication_date": "2016/03/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/27110272/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "27110272", "task": "IWqQC1koJA", "abstract": "With the advances in the computer field, methods and techniques in automatic image processing and analysis provide the opportunity to detect automatically the change and degeneration in retinal images. Localization of the optic disc is extremely important for determining the hard exudate lesions or neovascularization, which is the later phase of diabetic retinopathy, in computer aided eye disease diagnosis systems. Whereas optic disc detection is fairly an easy process in normal retinal images, detecting this region in the retinal image which is diabetic retinopathy disease may be difficult. Sometimes information related to optic disc and hard exudate information may be the same in terms of machine learning. We presented a novel approach for efficient and accurate localization of optic disc in retinal images having noise and other lesions. This approach is comprised of five main steps which are image processing, keypoint extraction, texture analysis, visual dictionary, and classifier techniques. We tested our proposed technique on 3 public datasets and obtained quantitative results. Experimental results show that an average optic disc detection accuracy of 94.38%, 95.00%, and 90.00% is achieved, respectively, on the following public datasets: DIARETDB1, DRIVE, and ROC.", "keywords": ["optic disc", "provide the opportunity", "opportunity to detect", "detect automatically", "automatically the change", "change and degeneration", "retinal images", "automatic image processing", "optic disc detection", "computer field", "disc", "optic", "retinal", "automatic image", "diabetic retinopathy", "computer aided eye", "diabetic retinopathy disease", "image processing", "disc detection", "normal retinal images"], "paper_title": "Automatic Detection of Optic Disc in Retinal Image by Using Keypoint Detection, Texture Analysis, and Visual Dictionary Techniques.", "last_updated": "2023/02/04"}, {"id": "0025333172", "domain": "Diabetic retinopathy", "model_name": "Orlando et al.", "publication_date": "2014/11/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25333172/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "25333172", "task": "aipbNdPTIt", "abstract": "In this work, we present a novel method for blood vessel segmentation in fundus images based on a discriminatively trained, fully connected conditional random field model. Retinal image analysis is greatly aided by blood vessel segmentation as the vessel structure may be considered both a key source of signal, e.g. in the diagnosis of diabetic retinopathy, or a nuisance, e.g. in the analysis of pigment epithelium or choroid related abnormalities. Blood vessel segmentation in fundus images has been considered extensively in the literature, but remains a challenge largely due to the desired structures being thin and elongated, a setting that performs particularly poorly using standard segmentation priors such as a Potts model or total variation. In this work, we overcome this difficulty using a discriminatively trained conditional random field model with more expressive potentials. In particular, we employ recent results enabling extremely fast inference in a fully connected model. We find that this rich but computationally efficient model family, combined with principled discriminative training based on a structured output support vector machine yields a fully automated system that achieves results statistically indistinguishable from an expert human annotator. Implementation details are available at http://pages.saclay.inria.fr/ matthew.blaschko/projects/retina/.", "keywords": ["blood vessel segmentation", "vessel segmentation", "blood vessel", "conditional random field", "random field model", "fundus images based", "fundus images", "Retinal image analysis", "discriminatively trained conditional", "trained conditional random", "vessel", "random field", "connected conditional random", "segmentation", "discriminatively trained", "conditional random", "field model", "segmentation in fundus", "fully connected conditional", "model"], "paper_title": "Learning fully-connected CRFs for blood vessel segmentation in retinal images.", "last_updated": "2023/02/04"}, {"id": "0025569914", "domain": "Diabetic retinopathy", "model_name": "Harangi et al.", "publication_date": "2015/10/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25569914/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "NC0kSKEoVp", "pmid": "25569914", "task": "aipbNdPTIt", "abstract": "Diabetic retinopathy (DR) is one of the most common causing of vision loss in developed countries. In early stage of DR, some signs like exudates appear in the retinal images. An automatic screening system must be capable to detect these signs properly so that the treatment of the patients may begin in time. The appearance of exudates shows a rich variety regarding their shape and size making automatic detection more challenging. We propose a way for the automatic segmentation of exudates consisting of a candidate extraction step followed by exact contour detection and region-wise classification. More specifically, we extract possible exudate candidates using grayscale morphology and their proper shape is determined by a Markovian segmentation model considering edge information. Finally, we label the candidates as true or false ones by an optimally adjusted SVM classifier. For testing purposes, we considered the publicly available database DiaretDB1, where the proposed method outperformed several state-of-the-art exudate detectors.", "keywords": ["Diabetic retinopathy", "developed countries", "common causing", "causing of vision", "vision loss", "loss in developed", "exudates", "Diabetic", "retinopathy", "countries", "automatic", "common", "causing", "vision", "loss", "developed", "retinal images", "signs", "early stage", "exudate"], "paper_title": "Detection of exudates in fundus images using a Markovian segmentation model.", "last_updated": "2023/02/04"}, {"id": "0033359887", "domain": "Glaucoma (unspecified)", "model_name": "Dixit et al.", "publication_date": "2020/12/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33359887/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33359887", "task": "0GPcU42tzV", "abstract": "Rule-based approaches to determining glaucoma progression from visual fields (VFs) alone are discordant and have tradeoffs. To detect better when glaucoma progression is occurring, we used a longitudinal data set of merged VF and clinical data to assess the performance of a convolutional long short-term memory (LSTM) neural network. Retrospective analysis of longitudinal clinical and VF data. From 2 initial datasets of 672\u2009123 VF results from 213\u2009254 eyes and 350\u2009437 samples of clinical data, persons at the intersection of both datasets with 4 or more VF results and corresponding baseline clinical data (cup-to-disc ratio, central corneal thickness, and intraocular pressure) were included. After exclusion criteria-specifically the removal of VFs with high false-positive and false-negative rates and entries with missing data-were applied to ensure reliable data, 11\u2009242 eyes remained. Three commonly used glaucoma progression algorithms (VF index slope, mean deviation slope, and pointwise linear regression) were used to define eyes as stable or progressing. Two machine learning models, one exclusively trained on VF data and another trained on both VF and clinical data, were tested. Area under the receiver operating characteristic curve (AUC) and area under the precision-recall curve (AUPRC) calculated on a held-out test set and mean accuracies from threefold cross-validation were used to compare the performance of the machine learning models. The convolutional LSTM network demonstrated 91% to 93% accuracy with respect to the different conventional glaucoma progression algorithms given 4 consecutive VF results for each participant. The model that was trained on both VF and clinical data (AUC, 0.89-0.93) showed better diagnostic ability than a model exclusively trained on VF results (AUC, 0.79-0.82; P < 0.001). A convolutional LSTM architecture can capture local and global trends in VFs over time. It is\u00a0well suited to assessing glaucoma progression because of its ability to extract spatiotemporal features that\u00a0other algorithms cannot. Supplementing VF results with clinical data improves the model's ability to assess glaucoma progression and better reflects the way clinicians manage data when managing glaucoma.", "keywords": ["clinical data", "glaucoma progression", "data", "determining glaucoma progression", "Rule-based approaches", "visual fields", "clinical", "approaches to determining", "glaucoma", "progression", "glaucoma progression algorithms", "AUC", "results", "determining glaucoma", "convolutional LSTM", "LSTM", "trained", "progression algorithms", "baseline clinical data", "assess glaucoma progression"], "paper_title": "Assessing Glaucoma Progression Using Machine Learning Trained on Longitudinal Visual Field and Clinical Data.", "last_updated": "2023/02/04"}, {"id": "0035877093", "domain": "Glaucoma (unspecified)", "model_name": "Kang et al.", "publication_date": "2022/07/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35877093/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35877093", "task": null, "abstract": "We evaluated racial/ethnic differences in primary open-angle glaucoma (POAG) defined by machine-learning-derived regional visual field (VF) loss patterns. Participants (N = 209,036) from the Nurses' Health Study (NHS; 1980-2018), Nurses' Health Study II (NHS2; 1989-2019), and Health Professionals Follow-Up Study (HPFS; 1986-2018) who were \u226540 years of age and free of glaucoma were followed biennially. Incident POAG cases (n = 1946) with reproducible VF loss were confirmed with medical records. Total deviation information from the earliest reliable glaucomatous VF for each POAG eye (n = 2564) was extracted, and machine learning analyses were used to identify optimal solutions (\"archetypes\") for regional VF loss patterns. Each POAG eye was assigned a VF archetype based on the highest weighting coefficient. Multivariable-adjusted hazard ratios (HRs) and 95% confidence intervals (CIs) were estimated using per-eye Cox proportional hazards models. We identified 14 archetypes: four representing advanced loss patterns, nine of early loss, and one of no VF loss. Compared to non-Hispanic whites, black participants had higher risk of early VF loss archetypes (HR = 1.98; 95% CI, 1.48-2.66) and even higher risk for advanced loss archetypes (HR = 6.17; 95% CI, 3.69-10.32; P-contrast = 0.0002); no differences were observed for Asians or Hispanic whites. Hispanic white participants had significantly higher risks of POAG with paracentral defects and advanced superior loss; black participants had significantly higher risks of all advanced loss archetypes and three early loss patterns, including paracentral defects. Blacks, compared to non-Hispanic whites, had higher risks of POAG with early central and advanced VF loss. In POAG, risks of VF loss regional patterns derived from machine learning algorithms showed racial differences.", "keywords": ["Nurses' Health Study", "Health Study", "Nurses' Health", "Health Professionals Follow-Up", "primary open-angle glaucoma", "Professionals Follow-Up Study", "loss", "POAG", "regional visual field", "Health Professionals", "loss patterns", "Health", "visual field", "Study", "primary open-angle", "POAG eye", "loss archetypes", "higher risks", "advanced loss", "advanced loss archetypes"], "paper_title": "Cohort Study of Race/Ethnicity and Incident Primary Open-Angle Glaucoma Characterized by Autonomously Determined Visual Field Loss Patterns.", "last_updated": "2023/02/04"}, {"id": "0030911364", "domain": "Glaucoma (unspecified)", "model_name": "An et al.", "publication_date": "2019/02/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30911364/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30911364", "task": "IWqQC1koJA", "abstract": "This study aimed to develop a machine learning-based algorithm for glaucoma diagnosis in patients with open-angle glaucoma, based on three-dimensional optical coherence tomography (OCT) data and color fundus images. In this study, 208 glaucomatous and 149 healthy eyes were enrolled, and color fundus images and volumetric OCT data from the optic disc and macular area of these eyes were captured with a spectral-domain OCT (3D OCT-2000, Topcon). Thickness and deviation maps were created with a segmentation algorithm. Transfer learning of convolutional neural network (CNN) was used with the following types of input images: (1) fundus image of optic disc in grayscale format, (2) disc retinal nerve fiber layer (RNFL) thickness map, (3) macular ganglion cell complex (GCC) thickness map, (4) disc RNFL deviation map, and (5) macular GCC deviation map. Data augmentation and dropout were performed to train the CNN. For combining the results from each CNN model, a random forest (RF) was trained to classify the disc fundus images of healthy and glaucomatous eyes using feature vector representation of each input image, removing the second fully connected layer. The area under receiver operating characteristic curve (AUC) of a 10-fold cross validation (CV) was used to evaluate the models. The 10-fold CV AUCs of the CNNs were 0.940 for color fundus images, 0.942 for RNFL thickness maps, 0.944 for macular GCC thickness maps, 0.949 for disc RNFL deviation maps, and 0.952 for macular GCC deviation maps. The RF combining the five separate CNN models improved the 10-fold CV AUC to 0.963. Therefore, the machine learning system described here can accurately differentiate between healthy and glaucomatous subjects based on their extracted images from OCT data and color fundus images. This system should help to improve the diagnostic accuracy in glaucoma.", "keywords": ["optical coherence tomography", "color fundus images", "three-dimensional optical coherence", "macular GCC deviation", "fundus images", "OCT data", "disc RNFL deviation", "color fundus", "OCT", "macular GCC", "volumetric OCT data", "GCC deviation map", "RNFL deviation map", "GCC deviation", "deviation maps", "fundus", "images", "RNFL deviation", "coherence tomography", "GCC thickness maps"], "paper_title": "Glaucoma Diagnosis with Machine Learning Based on Optical Coherence Tomography and Color Fundus Images.", "last_updated": "2023/02/04"}, {"id": "0036246178", "domain": "Glaucoma (unspecified)", "model_name": "Zhalechian et al.", "publication_date": "2021/12/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36246178/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36246178", "task": "IWqQC1koJA", "abstract": "To assess whether the predictive accuracy of machine learning algorithms using Kalman filtering for forecasting future values of global indices on perimetry can be enhanced by adding global retinal nerve fiber layer (RNFL) data and whether model performance is influenced by the racial composition of the training and testing sets. Retrospective, longitudinal cohort study. Patients with open-angle glaucoma (OAG) or glaucoma suspects enrolled in the African Descent and Glaucoma Evaluation Study or Diagnostic Innovation in Glaucoma Study. We developed a Kalman filter (KF) with tonometry and perimetry data (KF-TP) and another KF with tonometry, perimetry, and global RNFL data (KF-TPO), comparing these models with one another and with 2 linear regression (LR) models for predicting mean deviation (MD) and pattern standard deviation values 36 months into the future for patients with OAG and glaucoma suspects. We also compared KF model performance when trained on individuals of European and African descent and tested on patients of the same versus the other race. Predictive accuracy (percentage of MD values forecasted within the 95% repeatability interval) differences among the models. Among 362 eligible patients, the mean \u00b1 standard deviation age at baseline was 71.3 \u00b1 10.4 years; 196 patients (54.1%) were women; 202 patients (55.8%) were of European descent, and 139 (38.4%) were of African descent. Among patients with OAG (n\u00a0= 296), the predictive accuracy for 36 months in the future was higher for the KF models (73.5% for KF-TP, 71.2% for KF-TPO) than for the LR models (57.5%, 58.0%). Predictive accuracy did not differ significantly between KF-TP and KF-TPO (<i>P</i>\u00a0= 0.20). If the races of the training and testing set patients were aligned (versus nonaligned), the mean absolute prediction error of future MD improved 0.39 dB for KF-TP and 0.48 dB for KF-TPO. Adding global RNFL data to existing KFs minimally improved their predictive accuracy. Although KFs attained better predictive accuracy when the races of the training and testing sets were aligned, these improvements were modest. These findings will help to guide implementation of KFs in clinical practice.", "keywords": ["nerve fiber layer", "machine learning algorithms", "retinal nerve fiber", "Glaucoma Evaluation Study", "global RNFL data", "global retinal nerve", "predictive accuracy", "African Descent", "global RNFL", "fiber layer", "RNFL data", "Kalman filtering", "Patients", "machine learning", "learning algorithms", "filtering for forecasting", "retinal nerve", "nerve fiber", "racial composition", "glaucoma"], "paper_title": "Augmenting Kalman Filter Machine Learning Models with Data from OCT to Predict Future Visual Field Loss: An Analysis Using Data from the African Descent and Glaucoma Evaluation Study and the Diagnostic Innovation in Glaucoma Study.", "last_updated": "2023/02/04"}, {"id": "0030053471", "domain": "Glaucoma (unspecified)", "model_name": "Martin et al.", "publication_date": "2018/07/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30053471/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30053471", "task": "IWqQC1koJA", "abstract": "To test the hypothesis that contact lens sensor (CLS)-based 24-hour profiles of ocular volume changes contain information complementary to intraocular pressure (IOP) to discriminate between primary open-angle glaucoma (POAG) and healthy (H) eyes. Development and evaluation of a diagnostic test with machine learning. Subjects: From 435 subjects (193 healthy and 242 POAG), 136 POAG and 136 age-matched healthy subjects were selected. Subjects with contraindications for CLS wear were excluded. This is a pooled analysis of data from 24 prospective clinical studies and a registry. All subjects underwent 24-hour CLS recording on 1 eye. Statistical and physiological CLS parameters were derived from the signal recorded. CLS parameters frequently associated with the presence of POAG were identified using a random forest modeling approach. Area under the receiver operating characteristic curve (ROC AUC) for feature sets including CLS parameters and Start IOP, as well as a feature set with CLS parameters and Start IOP combined. The CLS parameters feature set discriminated POAG from H eyes with mean ROC AUCs of 0.611, confidence interval (CI) 0.493-0.722. Larger values of a given CLS parameter were in general associated with a diagnosis of POAG. The Start IOP feature set discriminated between POAG and H eyes with a mean ROC AUC of 0.681, CI 0.603-0.765. The combined feature set was the best indicator of POAG with an ROC AUC of 0.759, CI 0.654-0.855. This ROC AUC was statistically higher than for CLS parameters or Start IOP feature sets alone (both P < .0001). CLS recordings contain information complementary to IOP that enable discrimination between H and POAG. The feature set combining CLS parameters and Start IOP provide a better indication of the presence of POAG than each of the feature sets separately. As such, the CLS may be a new biomarker for POAG.", "keywords": ["CLS parameters", "CLS", "Start IOP", "ROC AUC", "contact lens sensor", "primary open-angle glaucoma", "POAG", "Start IOP feature", "IOP", "Start", "ROC", "feature", "parameters", "feature set", "AUC", "lens sensor", "profiles of ocular", "intraocular pressure", "open-angle glaucoma", "CLS parameters feature"], "paper_title": "Use of Machine Learning on Contact Lens Sensor-Derived Parameters for the Diagnosis of Primary Open-angle Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0034586221", "domain": "Glaucoma (unspecified)", "model_name": "Brand\u00e3o-de-Resende et al.", "publication_date": "2021/11/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34586221/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34586221", "task": null, "abstract": "To use machine learning to predict the risk of intraocular pressure peaks at 6 a.m. in primary open-angle glaucoma patients and suspects. This cross-sectional observational study included 98 eyes of 98 patients who underwent a 24-hour intraocular pressure curve (including the intraocular pressure measurements at 6 a.m.). The diurnal intraocular pressure curve was defined as a series of three measurements at 8 a.m., 9 a.m., and 11 a.m. from the 24-hour intraocular pressure curve. Two new variables were introduced: slope and concavity. The slope of the curve was calculated as the difference between intraocular pressure measurements at 9 a.m. and 8 a.m. and reflected the intraocular pressure change in the first hour. The concavity of the curve was calculated as the difference between the slopes at 9 a.m. and 8 a.m. and indicated if the curve was bent upward or downward. A classification tree was used to determine a multivariate algorithm from the measurements of the diurnal intraocular pressure curve to predict the risk of elevated intraocular pressure at 6 a.m. Forty-nine (50%) eyes had intraocular pressure measurements at 6 a.m. >21 mmHg, and the median intraocular pressure peak in these eyes at 6 a.m. was 26 mmHg. The best predictors of intraocular pressure measurements >21 mmHg at 6 a.m. were the intraocular pressure measurements at 8 a.m. and concavity. The proposed model achieved a sensitivity of 100% and a specificity of 86%, resulting in an accuracy of 93%. The machine learning approach was able to predict the risk of intraocular pressure peaks at 6 a.m. with good accuracy. This new approach to the diurnal intraocular pressure curve may become a widely used tool in daily practice and the indication of a 24-hour intraocular pressure curve could be rationalized according to risk stratification.", "keywords": ["intraocular pressure", "intraocular pressure curve", "intraocular pressure measurements", "pressure curve", "pressure", "diurnal intraocular pressure", "intraocular", "pressure measurements", "intraocular pressure peaks", "primary open-angle glaucoma", "open-angle glaucoma patients", "curve", "measurements", "diurnal intraocular", "intraocular pressure change", "pressure peaks", "elevated intraocular pressure", "median intraocular pressure", "primary open-angle", "open-angle glaucoma"], "paper_title": "Use of machine learning to predict the risk of early morning intraocular pressure peaks in glaucoma patients and suspects.", "last_updated": "2023/02/04"}, {"id": "0033484732", "domain": "Glaucoma (unspecified)", "model_name": "Nezu et al.", "publication_date": "2021/01/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33484732/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33484732", "task": null, "abstract": "Various immune mediators have crucial roles in the pathogenesis of intraocular diseases. Machine learning can be used to automatically select and weigh various predictors to develop models maximizing predictive power. However, these techniques have not yet been applied extensively in studies focused on intraocular diseases. We evaluated whether 5 machine learning algorithms applied to the data of immune-mediator levels in aqueous humor can predict the actual diagnoses of 17 selected intraocular diseases and identified which immune mediators drive the predictive power of a machine learning model. Cross-sectional study. Five hundred twelve eyes with diagnoses from among 17 intraocular diseases. Aqueous humor samples were collected, and the concentrations of 28 immune mediators were determined using a cytometric bead array. Each immune mediator was ranked according to its importance using 5 machine learning algorithms. Stratified k-fold cross-validation was used in evaluation of algorithms with the dataset divided into training and test datasets. The algorithms were evaluated in terms of precision, recall, accuracy, F-score, area under the receiver operating characteristic curve, area under the precision-recall curve, and mean decrease in Gini index. Among the 5 machine learning models, random forest (RF) yielded the highest classification accuracy in multiclass differentiation of 17 intraocular diseases. The RF prediction models for vitreoretinal lymphoma, acute retinal necrosis, endophthalmitis, rhegmatogenous retinal detachment, and primary open-angle glaucoma achieved the highest classification accuracy, precision, and recall. Random forest recognized vitreoretinal lymphoma, acute retinal necrosis, endophthalmitis, rhegmatogenous retinal detachment, and primary open-angle glaucoma with the top 5 F-scores. The 3 highest-ranking relevant immune mediators were interleukin (IL)-10, interferon-\u03b3-inducible protein (IP)-10, and angiogenin for prediction of vitreoretinal lymphoma; monokine induced by interferon \u03b3, interferon \u03b3, and IP-10 for acute retinal necrosis; and IL-6, granulocyte colony-stimulating factor, and IL-8 for endophthalmitis. Random forest algorithms based on 28 immune mediators in aqueous humor successfully predicted the diagnosis of vitreoretinal lymphoma, acute retinal necrosis, and endophthalmitis. Overall, the findings of the present study contribute to increased knowledge on new biomarkers that potentially can facilitate diagnosis of intraocular diseases in the future.", "keywords": ["intraocular diseases", "Machine learning", "immune mediators", "acute retinal necrosis", "retinal necrosis", "intraocular", "crucial roles", "machine learning algorithms", "diseases", "vitreoretinal lymphoma", "acute retinal", "immune", "retinal", "learning", "mediators", "Machine", "aqueous humor", "algorithms", "lymphoma", "necrosis"], "paper_title": "Machine Learning Approach for Intraocular Disease Prediction Based on Aqueous Humor Immune Mediator Profiles.", "last_updated": "2023/02/04"}, {"id": "0029261773", "domain": "Glaucoma (unspecified)", "model_name": "Omodaka et al.", "publication_date": "2017/12/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29261773/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "29261773", "task": "IWqQC1koJA", "abstract": "This study aimed to develop a machine learning-based algorithm for objective classification of the optic disc in patients with open-angle glaucoma (OAG), using quantitative parameters obtained from ophthalmic examination instruments. This study enrolled 163 eyes of 105 OAG patients (age: 62.3 \u00b1 12.6, mean deviation of Humphrey field analyzer: -8.9 \u00b1 7.5 dB). The eyes were classified into Nicolela's 4 optic disc types by 3 glaucoma specialists. Randomly, 114 eyes were selected for training data and 49 for test data. A neural network (NN) was trained with the training data and evaluated with the test data. We used 91 types of quantitative data, including 7 patient background characteristics, 48 quantified OCT (swept-source OCT; DRI OCT Atlantis, Topcon) values, including optic disc topography and circumpapillary retinal nerve fiber layer thickness (cpRNFLT), and 36 blood flow parameters from laser speckle flowgraphy, to build the machine learning classification model. To extract the important features among 91 parameters, minimum redundancy maximum relevance and a genetic feature selection were used. The validated accuracy against test data for the NN was 87.8% (Cohen's Kappa = 0.83). The important features in the NN were horizontal disc angle, spherical equivalent, cup area, age, 6-sector superotemporal cpRNFLT, average cup depth, average nasal rim disc ratio, maximum cup depth, and superior-quadrant cpRNFLT. The proposed machine learning system has proved to be good identifiers for different disc types with high accuracy. Additionally, the calculated confidence levels reported here should be very helpful for OAG care.", "keywords": ["ophthalmic examination instruments", "machine learning-based algorithm", "examination instruments", "aimed to develop", "learning-based algorithm", "algorithm for objective", "obtained from ophthalmic", "ophthalmic examination", "study aimed", "DRI OCT Atlantis", "OAG", "OAG patients", "Humphrey field analyzer", "optic disc", "data", "test data", "disc", "quantitative parameters obtained", "open-angle glaucoma", "objective classification"], "paper_title": "Classification of optic disc shape in glaucoma using machine learning based on quantified ocular parameters.", "last_updated": "2023/02/04"}, {"id": "0016249492", "domain": "Glaucoma (unspecified)", "model_name": "Burgansky-Eliash et al.", "publication_date": "2005/12/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/16249492/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "16249492", "task": "IWqQC1koJA", "abstract": "Machine-learning classifiers are trained computerized systems with the ability to detect the relationship between multiple input parameters and a diagnosis. The present study investigated whether the use of machine-learning classifiers improves optical coherence tomography (OCT) glaucoma detection. Forty-seven patients with glaucoma (47 eyes) and 42 healthy subjects (42 eyes) were included in this cross-sectional study. Of the glaucoma patients, 27 had early disease (visual field mean deviation [MD] > or = -6 dB) and 20 had advanced glaucoma (MD < -6 dB). Machine-learning classifiers were trained to discriminate between glaucomatous and healthy eyes using parameters derived from OCT output. The classifiers were trained with all 38 parameters as well as with only 8 parameters that correlated best with the visual field MD. Five classifiers were tested: linear discriminant analysis, support vector machine, recursive partitioning and regression tree, generalized linear model, and generalized additive model. For the last two classifiers, a backward feature selection was used to find the minimal number of parameters that resulted in the best and most simple prediction. The cross-validated receiver operating characteristic (ROC) curve and accuracies were calculated. The largest area under the ROC curve (AROC) for glaucoma detection was achieved with the support vector machine using eight parameters (0.981). The sensitivity at 80% and 95% specificity was 97.9% and 92.5%, respectively. This classifier also performed best when judged by cross-validated accuracy (0.966). The best classification between early glaucoma and advanced glaucoma was obtained with the generalized additive model using only three parameters (AROC = 0.854). Automated machine classifiers of OCT data might be useful for enhancing the utility of this technology for detecting glaucomatous abnormality.", "keywords": ["multiple input parameters", "trained computerized systems", "Machine-learning classifiers", "computerized systems", "ability to detect", "detect the relationship", "relationship between multiple", "multiple input", "glaucoma", "classifiers", "parameters", "machine-learning classifiers improves", "OCT", "Machine-learning", "input parameters", "trained computerized", "glaucoma detection", "advanced glaucoma", "classifiers improves optical", "trained"], "paper_title": "Optical coherence tomography machine learning classifiers for glaucoma detection: a preliminary study.", "last_updated": "2023/02/04"}, {"id": "0036251319", "domain": "Glaucoma (unspecified)", "model_name": "Lee et al.", "publication_date": "2022/10/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36251319/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36251319", "task": "0GPcU42tzV", "abstract": "The purpose of this study was to develop a model, based on initial optic nerve head (ONH) characteristics, predictive of long-term rapid retinal nerve fiber layer (RNFL) thinning in patients with open-angle glaucoma (OAG). This study evaluated 712 eyes with OAG that had been followed up for >5 years with annual evaluation of RNFL thickness. Baseline ophthalmic features were incorporated into the machine learning models for prediction of faster RNFL thinning. The model was trained and tested using a random forest (RF) method, and was interpreted using Shapley additive explanations. Factors associated with faster rate of RNFL thinning were statistically evaluated using a decision tree. The RF model showed that greater lamina cribrosa (LC) curvature, higher intraocular pressure (IOP), visual field mean deviation converging towards -5\u00a0dB, and thinner peripapillary choroid at baseline were the four most significant features predicting faster RNFL thinning. Partial interaction between the features showed that larger LC curvature was a strong factor for faster RNFL thinning when it exceeded approximately 12.0. When the LC curvature was \u226412, higher initial IOP and thinner peripapillary choroid played a role in the rapid RNFL thinning. Based on the decision tree, higher IOP (>26.5 mm Hg), greater laminar curvature (>13.95), and thinner peripapillary choroid (\u2264117.5 \u00b5m) were the 3 most important determinants affecting the rate of RNFL thinning. Baseline ophthalmic data and ONH characteristics of patients with OAG were predictive of eyes at risk of faster progression. Combinations of important characteristics, such as IOP, LC curvature, and choroidal thickness, could stratify eyes into groups with different rates of RNFL thinning. This work lays the foundations for developing prediction models to estimate glaucoma prognosis based on initial ONH characteristics.", "keywords": ["RNFL thinning", "optic nerve head", "nerve fiber layer", "retinal nerve fiber", "faster RNFL thinning", "RNFL", "faster RNFL", "rapid retinal nerve", "initial optic nerve", "nerve head", "long-term rapid retinal", "optic nerve", "retinal nerve", "nerve fiber", "thinning", "OAG", "fiber layer", "rapid RNFL thinning", "thinner peripapillary choroid", "IOP"], "paper_title": "Predictive Modeling of Long-Term Glaucoma Progression Based on Initial Ophthalmic Data and Optic Nerve Head Characteristics.", "last_updated": "2023/02/04"}, {"id": "0025342615", "domain": "Glaucoma (unspecified)", "model_name": "Asaoka et al.", "publication_date": "2014/10/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25342615/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "25342615", "task": null, "abstract": "To compare the visual fields (VFs) of preperimetric open angle glaucoma (OAG) patients (preperimetric glaucoma VFs, PPGVFs) with the VFs of healthy eyes, and to discriminate these two groups by using the Random Forests machine-learning method. All VFs before a first diagnosis of manifest glaucoma (Anderson-Patella's criteria) were classified as PPGVFs. Series of VFs were obtained with the Humphrey Field Analyzer 30-2 program from 171 PPGVFs from 53 eyes in 51 OAG or OAG suspect patients and 108 healthy eyes of 87 normal subjects. The area under the receiver operating characteristic curve (AROC) in discriminating between PPGVFs and healthy VFs was calculated by using the Random Forests method, with 52 total deviation (TD) values, mean deviation (MD), and pattern standard deviation (PSD) as predictors. There was a significant difference in MD between healthy VFs and PPGVFs (-0.03 \u00b1 1.11 and -0.91 \u00b1 1.56 dB [mean \u00b1 standard deviation], respectively; P < 0.001, linear mixed model) and in PSD (1.56 \u00b1 0.33 and 1.97 \u00b1 0.43 dB, respectively; P < 0.001). A significant difference was observed in the TD values between healthy VFs and PPGVFs at 25 (P < 0.001) of 52 test points (linear mixed model). The AROC obtained by using the Random Forests method was 79.0% (95% confidence interval, 73.5%-84.5%). Differences exist between healthy VFs and VFs of preperimetric glaucoma eyes, which go on to develop manifest glaucoma; these two groups of VFs could be well distinguished by using the Random Forests classifier.", "keywords": ["Random Forests", "Random Forests machine-learning", "Random Forests method", "open angle glaucoma", "Humphrey Field Analyzer", "preperimetric open angle", "VFs", "Forests machine-learning method", "healthy VFs", "Random Forests classifier", "Random", "compare the visual", "open angle", "healthy", "Forests", "Forests machine-learning", "OAG", "visual fields", "glaucoma", "Forests method"], "paper_title": "Identifying \"preperimetric\" glaucoma in standard automated perimetry visual fields.", "last_updated": "2023/02/04"}, {"id": "0033983565", "domain": "Glaucoma (unspecified)", "model_name": "Jones et al.", "publication_date": "2021/05/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33983565/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33983565", "task": "0GPcU42tzV", "abstract": "In managing patients with chronic diseases, such as open angle glaucoma (OAG), the case treated in this paper, medical tests capture the disease phase (e.g. regression, stability, progression, etc.) the patient is currently in. When medical tests have low residual variability (e.g. empirical difference between the patient's true and recorded value is small) they can effectively, without the use of sophisticated methods, identify the patient's current disease phase; however, when medical tests have moderate to high residual variability this may not be the case. This paper presents a framework for handling the latter case. The framework presented integrates the outputs of interacting multiple model Kalman filtering with supervised learning classification. The purpose of this integration is to estimate the true values of patients' disease metrics by allowing for rapid and non-rapid phases; and dynamically adapting to changes in these values over time. We apply our framework to classifying whether a patient with OAG will experience rapid progression over the next two or three years from the time of classification. The performance (AUC) of our model increased by approximately 7% (increased from 0.752 to 0.819) when the Kalman filtering results were incorporated as additional features in the supervised learning model. These results suggest the combination of filters and statistical learning methods in clinical health has significant benefits. Although this paper applies our methodology to OAG, the methodology developed is applicable to other chronic conditions.", "keywords": ["medical tests capture", "open angle glaucoma", "medical tests", "current disease phase", "patient current disease", "disease phase", "angle glaucoma", "tests capture", "open angle", "managing patients", "case treated", "patient", "residual variability", "medical", "low residual variability", "high residual variability", "tests", "Kalman filtering", "disease", "case"], "paper_title": "Predicting rapid progression phases in glaucoma using a soft voting ensemble classifier exploiting Kalman filtering.", "last_updated": "2023/02/04"}, {"id": "0033529590", "domain": "Glaucoma (unspecified)", "model_name": "Nouri-Mahdavi et al.", "publication_date": "2021/01/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33529590/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33529590", "task": "0GPcU42tzV", "abstract": "To test the hypothesis that visual field (VF) progression can be predicted from baseline and longitudinal optical coherence tomography (OCT) structural measurements. Prospective cohort study. A total of 104 eyes (104 patients) with \u22653 years of follow-up and \u22655 VF examinations were enrolled. We defined VF progression based on pointwise linear regression on 24-2 VF (\u22653 locations with slope less than or equal to -1.0 dB/year and P < .01). We used elastic net logistic regression (ENR) and machine learning to predict VF progression with demographics, baseline circumpapillary retinal nerve fiber layer (RNFL), macular ganglion cell/inner plexiform layer (GCIPL) thickness, and RNFL and GCIPL change rates at central 24 superpixels and 3 eccentricities, 3.4\u00b0, 5.5\u00b0, and 6.8\u00b0, from fovea and hemimaculas. Areas-under-ROC curves (AUC) were used to compare models. Average \u00b1 SD follow-up and VF examinations were 4.5 \u00b1 0.9 years and 8.7 \u00b1 1.6, respectively. VF progression was detected in 23 eyes (22%). ENR selected rates of change of superotemporal RNFL sector and GCIPL change rates in 5 central superpixels and at 3.4\u00b0\u00a0and 5.6\u00b0 eccentricities as the best predictor subset (AUC\u00a0=\u00a00.79 \u00b1 0.12). Best machine learning predictors consisted of baseline superior hemimacular GCIPL thickness and GCIPL change rates at 3.4\u00b0 eccentricity and 3 central superpixels (AUC\u00a0=\u00a00.81 \u00b1 0.10). Models using GCIPL-only structural variables performed better than RNFL-only models. VF progression can be predicted with clinically relevant accuracy from baseline and longitudinal structural data. Further refinement of proposed models would assist clinicians with timely prediction of functional glaucoma progression and clinical decision making.", "keywords": ["optical coherence tomography", "GCIPL change rates", "longitudinal optical coherence", "GCIPL change", "visual field", "coherence tomography", "test the hypothesis", "hypothesis that visual", "optical coherence", "GCIPL", "progression", "OCT", "change rates", "AUC", "RNFL", "change", "structural measurements", "baseline", "rates", "longitudinal optical"], "paper_title": "Prediction of Visual Field Progression from OCT Structural Measures in Moderate to Advanced Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0030336130", "domain": "Glaucoma (unspecified)", "model_name": "Garcia et al.", "publication_date": "2018/10/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30336130/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30336130", "task": "0GPcU42tzV", "abstract": "To determine whether a machine learning technique called Kalman filtering (KF) can accurately forecast future values of mean deviation (MD), pattern standard deviation, and intraocular pressure for patients with normal tension glaucoma (NTG). Development and testing of a forecasting model for glaucoma progression. We parameterized and validated a KF (KF-NTG) to forecast MD, pattern standard deviation, and intraocular pressure at 24\u00a0months into the future using 263 eyes of 263 Japanese patients with NTG. We determined the proportion of patients with MD forecasts within 0.5, 1.0, and 2.5 dBs of the actual values and calculated the root mean squared error (RMSE) for each forecast. We compared KF-NTG with a previously published KF model calibrated using patients with high-tension open-angle glaucoma (KF-HTG) and to 3 conventional forecasting algorithms. The 263 patients with NTG had mean \u00b1 standard deviation age of 63.4 \u00b1 10.5 years. KF-NTG forecasted MD values 24\u00a0months ahead within 0.5, 1.0, and 2.5 dBs of the actual value for 78 eyes (32.2%), 122 eyes (50.4%), and 211 eyes (87.2%), respectively. The proportion of eyes with MD values forecasted within 2.5 dB of the actual value for the KF-NTG (87.2%) were similar to KF-HTG (86.0%) and the null model (86.4%), and much better than the 2 linear regression-based models (72.7-74.0%; P < .001). When forecasting MD, KF-NTG (RMSE\u00a0= 2.71) and KF-HTG (RMSE\u00a0= 2.68) achieved lower RMSE than the other 3 forecasting models (RMSE\u00a0= 2.81-3.90), indicating better performance. As observed previously for patients with HTG, KF can also effectively forecast disease trajectory for many patients with NTG.", "keywords": ["called Kalman filtering", "technique called Kalman", "machine learning technique", "learning technique called", "Kalman filtering", "called Kalman", "normal tension glaucoma", "pattern standard deviation", "patients with NTG", "patients", "standard deviation", "machine learning", "learning technique", "technique called", "normal tension", "pattern standard", "accurately forecast future", "NTG", "RMSE", "intraocular pressure"], "paper_title": "Using Kalman Filtering to Forecast Disease Trajectory for Patients With Normal Tension Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0016186350", "domain": "Glaucoma (unspecified)", "model_name": "Sample et al.", "publication_date": "2005/11/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/16186350/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "16186350", "task": "0GPcU42tzV", "abstract": "To determine whether a variational Bayesian independent component analysis mixture model (vB-ICA-mm), a form of unsupervised machine learning, can be used to identify and quantify areas of progression in standard automated perimetry fields. In an earlier study, it was shown that a model using vB-ICA-mm can separate normal fields from fields with six different patterns of visual field loss related to glaucomatous optic neuropathy (GON) along maximally independent axes. In the present study, an independent group of 191 patient eyes (66 with ocular hypertension (OHT), 12 with suspected glaucoma by field, 61 with suspected glaucoma by disc, and 52 with glaucoma) with five or more standard visual fields under observation for a mean of 6.24 +/- 2.65 years and 8.11 +/- 2.42 visual fields were evaluated with the vB-ICA-mm. In addition, eyes with progressive GON (PGON) were identified (n = 39). Each participant had a series of fields tested, with each field entered independently and placed along the axes of the previously developed model. This allowed change in one pattern of visual field defect (along one axis) to be assessed relative to results other areas of that same field (no change along other axes). Progression was based on a slope falling outside the 5th and the 95th percentile limits of all slopes, with at least two axes not showing such a deviation in a given individual's series of fields. Fields were also scored using Advanced Glaucoma Intervention Study (AGIS) and the Early Manifest Glaucoma Treatment Trial (EMGT) criteria. Thirty-two of 191 eyes progressed on vB-ICA-mm by this definition. Of the 32, 22 had field loss at baseline, 7 had only GON, 3 were OHTs and 12 were from the 39 eyes (31%) with PGON. The vB-ICA-mm identified a higher percentage of progressing eyes in each diagnostic category than did AGIS or and the EMGT. The vB-ICA-mm can quantitatively identify progression in eyes with glaucoma by evaluating change in one or more patterns of the visual field loss while other areas or patterns remain stable. This may enable each eye to contribute to the determination of whether change is caused by true progression or by variability.", "keywords": ["variational Bayesian independent", "Bayesian independent component", "unsupervised machine learning", "component analysis mixture", "variational Bayesian", "analysis mixture model", "independent component analysis", "Bayesian independent", "standard automated perimetry", "automated perimetry fields", "fields", "machine learning", "field", "Glaucoma Intervention Study", "component analysis", "analysis mixture", "form of unsupervised", "unsupervised machine", "automated perimetry", "glaucoma"], "paper_title": "Unsupervised machine learning with independent component analysis to identify areas of progression in glaucomatous visual fields.", "last_updated": "2023/02/04"}, {"id": "0031725846", "domain": "Glaucoma (unspecified)", "model_name": "Garcia et al.", "publication_date": "2021/06/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31725846/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31725846", "task": "0GPcU42tzV", "abstract": "Techniques that properly identify patients in whom ocular hypertension (OHTN) is likely to progress to open-angle glaucoma can assist clinicians with deciding on the frequency of monitoring and the potential benefit of early treatment. To test whether Kalman filtering (KF), a machine learning technique, can accurately forecast mean deviation (MD), pattern standard deviation, and intraocular pressure values 5 years into the future for patients with OHTN. This cohort study was a secondary analysis of data from patients with OHTN from the Ocular Hypertension Treatment Study, performed between February 1994 and March 2009. Patients underwent tonometry and perimetry every 6 months for up to 15 years. A KF (KF-OHTN) model was trained, validated, and tested to assess how well it could forecast MD, pattern standard deviation, and intraocular pressure at up to 5 years, and the forecasts were compared with results from the actual trial. Kalman filtering for OHTN was compared with a previously developed KF for patients with high-tension glaucoma (KF-HTG) and 3 traditional forecasting algorithms. Statistical analysis for the present study was performed between May 2018 and May 2019. Prediction error and root-mean-square error at 12, 24, 36, 48, and 60 months for MD, pattern standard deviation, and intraocular pressure. Among 1407 eligible patients (2806 eyes), 809 (57.5%) were female and the mean (SD) age at baseline was 57.5\u2009(9.6) years. For 2124 eyes with sufficient measurements, KF-OHTN forecast MD values 60 months into the future within 0.5 dB of the actual value for 696 eyes (32.8%), 1.0 dB for 1295 eyes (61.0%), and 2.5 dB for 1980 eyes (93.2%). Among the 5 forecasting algorithms tested, KF-OHTN achieved the lowest root-mean-square error (1.72 vs 1.85-4.28) for MD values 60 months into the future. For the subset of eyes that progressed to open-angle glaucoma, KF-OHTN and KF-HTG forecast MD values 60 months into the future within 1 dB of the actual value for 30 eyes (68.2%; 95% CI, 54.4%-82.0%) and achieved the lowest root-mean-square error among all models. These findings suggest that machine learning algorithms such as KF can accurately forecast MD, pattern standard deviation, and intraocular pressure 5 years into the future for many patients with OHTN. These algorithms may aid clinicians in managing OHTN in their patients.", "keywords": ["Ocular Hypertension Treatment", "pattern standard deviation", "properly identify patients", "Hypertension Treatment Study", "ocular hypertension", "OHTN", "standard deviation", "pattern standard", "patients", "intraocular pressure", "properly identify", "frequency of monitoring", "potential benefit", "benefit of early", "Hypertension Treatment", "early treatment", "years", "deviation", "months", "eyes"], "paper_title": "Accuracy of Kalman Filtering in Forecasting Visual Field and Intraocular Pressure Trajectory in Patients With Ocular Hypertension.", "last_updated": "2023/02/04"}, {"id": "0021173705", "domain": "Glaucoma (unspecified)", "model_name": "Horn et al.", "publication_date": "2012/03/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/21173705/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "21173705", "task": "IWqQC1koJA", "abstract": "To develop a diagnostic setup with classification rules for combined analysis of morphology [Heidelberg Retina Tomograph (HRT)] and function [frequency doubling technology (FDT) perimetry] measurements. We used 2 independent case-control studies from the Erlangen eye department as learning and test data for automated classification using random forests. One eye of 334 open angle glaucoma patients and 254 controls entered the study. All individuals underwent HRT scanning tomography of the optic disc, FDT screening, conventional perimetry, and evaluation of fundus photographs. Random forests were learned on individuals of the Erlangen glaucoma registry (102 preperimetric patients, 130 perimetric patients, 161 controls). The classification performances of random forests and built-in classifiers were examined by receiver operator characteristic analysis on an independent second cohort of individuals (47 preperimetric patients, 55 perimetric patients, 93 controls). HRT measurements had a higher diagnostic power for early glaucomas and FDT perimetry for glaucoma patients with visual field loss. A combination of all parameters using automated classification was superior to single tests in comparison to the diagnostic instrument with the higher diagnostic power in the respective group. Highest sensitivities at a fixed specificity (95%) in the patients of the present test population were: HRT=32%, FDT=19%, combined analysis=47% in preperimetric patients and HRT=76%, FDT=89%, combined analysis=96% in perimetric patients. The feasibility of machine learning for medical diagnostic assistance could be demonstrated in patients from 2 independent study populations. A predictive model using automated classification is able to combine the advantages of morphology and function, resulting in a higher diagnostic power for glaucoma detection.", "keywords": ["Heidelberg Retina Tomograph", "Heidelberg Retina", "Retina Tomograph", "frequency doubling technology", "patients", "frequency doubling", "doubling technology", "HRT", "FDT", "Heidelberg", "Tomograph", "Retina", "diagnostic", "higher diagnostic power", "classification", "Erlangen eye department", "random forests", "preperimetric patients", "perimetric patients", "higher diagnostic"], "paper_title": "Combined evaluation of frequency doubling technology perimetry and scanning laser ophthalmoscopy for glaucoma detection using automated classification.", "last_updated": "2023/02/04"}, {"id": "0016186349", "domain": "Glaucoma (unspecified)", "model_name": "Goldbaum et al.", "publication_date": "2005/11/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/16186349/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "16186349", "task": null, "abstract": "Clustering by unsupervised learning with machine learning classifiers was shown to segment clusters of patterns in standard automated perimetry (SAP) for glaucoma in previous publications. In this study, unsupervised learning by independent component analysis decomposed SAP field patterns into axes, and the information represented by these axes was evaluated. SAP fields were used that were obtained with the Humphrey Visual Field Analyzer (Carl Zeiss Meditec, Dublin, CA) from 189 normal eyes and 156 eyes with glaucomatous optic neuropathy (GON) determined by masked review with stereoscopic optic disc photographs. The variational Bayesian independent component analysis mixture model (vB-ICA-mm) partitioned the SAP fields into the most informative number of clusters. Simultaneously, the model learned an optimal number of maximally independent axes for each cluster. The most informative number of clusters in the SAP set was two. vB-ICA-mm placed 68.6% of the eyes with GON in a cluster labeled G and 98.4% of the eyes with normal optic discs in a cluster labeled N. Cluster G optimally contained six axes. Post hoc analysis of patterns generated at -1 SD and +2 SD from the cluster G mean on the six axes revealed defects similar to those identified by experts as indicative of glaucoma. SAP fields associated with an axis showed increasing severity, as they were located farther in the positive direction from the cluster G mean. vB-ICA-mm represented the SAP fields with patterns that were meaningful for glaucoma experts. This process also captured severity in the patterns uncovered. These findings should validate vB-ICA-mm as a data-mining technique for new and unfamiliar complex tests.", "keywords": ["standard automated perimetry", "SAP fields", "machine learning classifiers", "SAP", "Carl Zeiss Meditec", "Visual Field Analyzer", "decomposed SAP field", "Humphrey Visual Field", "SAP field patterns", "unsupervised learning", "automated perimetry", "previous publications", "analysis decomposed SAP", "SAP field", "classifiers was shown", "shown to segment", "standard automated", "machine learning", "learning classifiers", "cluster"], "paper_title": "Using unsupervised learning with independent component analysis to identify patterns of glaucomatous visual field defects.", "last_updated": "2023/02/04"}, {"id": "0036631494", "domain": "Glaucoma (unspecified)", "model_name": "main", "publication_date": "2023/01/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36631494/", "code_link": "https://github.com/hyunjoongkim1/systemic-factors-for-long-term-glaucoma-progression/new/main", "model_type": "random forest", "verified": false, "model_task": "Forecasting", "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "36631494", "task": "0GPcU42tzV", "abstract": "Glaucoma is a multifactorial disease where various systemic features are involved in the progression of the disease. Based on initial systemic profiles in electronic medical records, this study aimed to develop a model predicting factors of long-term rapid retinal nerve fiber layer (RNFL) thinning over 5\u00a0years in 505 patients with primary open-angle glaucoma. Eyes with faster or slower RNFL thinning were stratified using a decision tree model, and systemic and ophthalmic data were incorporated into the models based on random forest and permutation methods, with the models interpreted by Shapley additive explanation plots (SHAP). According to the decision tree, a higher lymphocyte ratio (>\u200934.65%) was the most important systemic variable discriminating faster or slower RNFL thinning. Higher mean corpuscular hemoglobin (>\u200932.05\u00a0pg) and alkaline phosphatase (>\u200988.0\u00a0IU/L) concentrations were distinguishing factors in the eyes with lymphocyte ratios\u2009>\u200934.65% and\u2009<\u200934.65%, respectively. SHAP demonstrated larger baseline RNFL thickness, greater fluctuation of intraocular pressure (IOP), and higher maximum IOP as the strongest ophthalmic factors, while higher lymphocyte ratio and higher platelet count as the strongest systemic factors associated with faster RNFL thinning. Machine learning-based modeling identified several systemic factors as well as previously acknowledged ophthalmic risk factors associated with long-term rapid RNFL thinning.", "keywords": ["RNFL thinning", "slower RNFL thinning", "RNFL", "multifactorial disease", "features are involved", "slower RNFL", "faster RNFL thinning", "rapid RNFL thinning", "systemic", "thinning", "factors", "disease", "long-term rapid RNFL", "higher lymphocyte ratio", "higher", "faster RNFL", "systemic features", "primary open-angle glaucoma", "rapid RNFL", "lymphocyte ratio"], "paper_title": "Systemic factors associated with 10-year glaucoma progression in South Korean population: a single center study based on electronic medical records.", "last_updated": "2023/02/04"}, {"id": "0024111078", "domain": "Glaucoma (unspecified)", "model_name": "Theeraworn et al.", "publication_date": "2015/08/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24111078/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "24111078", "task": null, "abstract": "At present, Van Herick's method is a standard technique used to screen a Narrow Anterior Chamber Angle (NACA) and Angle-Closure Glaucoma (ACG). It can identify a patient who suffers from NACA and ACG by considering the width of peripheral anterior chamber depth (PACD) and corneal thickness. However, the screening result of this method often varies among ophthalmologists. So, an automatic screening of NACA and ACG based on slit-lamp image analysis by using Support Vector Machine (SVM) is proposed. SVM can automatically generate the classification model, which is used to classify the result as an angle-closure likely or an angle-closure unlikely. It shows that it can improve the accuracy of the screening result. To develop the classification model, the width of PACD and corneal thickness from many positions are measured and selected to be features. A statistic analysis is also used in the PACD and corneal thickness estimation in order to reduce the error from reflection on the cornea. In this study, it is found that the generated models are evaluated by using 5-fold cross validation and give a better result than the result classified by Van Herick's method.", "keywords": ["Anterior Chamber Angle", "Narrow Anterior Chamber", "Chamber Angle", "Narrow Anterior", "anterior chamber depth", "Anterior Chamber", "peripheral anterior chamber", "screen a Narrow", "Van Herick method", "Angle-Closure Glaucoma", "NACA and ACG", "standard technique", "Support Vector Machine", "Van Herick", "NACA", "ACG", "Herick method", "Angle", "Glaucoma", "Narrow"], "paper_title": "Automatic screening of narrow anterior chamber angle and angle-closure glaucoma based on slit-lamp image analysis by using support vector machine.", "last_updated": "2023/02/04"}, {"id": "0036671598", "domain": "Glaucoma (unspecified)", "model_name": "Marouf et al.", "publication_date": "2022/12/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36671598/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36671598", "task": null, "abstract": "The eye is generally considered to be the most important sensory organ of humans. Diseases and other degenerative conditions of the eye are therefore of great concern as they affect the function of this vital organ. With proper early diagnosis by experts and with optimal use of medicines and surgical techniques, these diseases or conditions can in many cases be either cured or greatly mitigated. Experts that perform the diagnosis are in high demand and their services are expensive, hence the appropriate identification of the cause of vision problems is either postponed or not done at all such that corrective measures are either not done or done too late. An efficient model to predict eye diseases using machine learning (ML) and ranker-based feature selection (r-FS) methods is therefore proposed which will aid in obtaining a correct diagnosis. The aim of this model is to automatically predict one or more of five common eye diseases namely, Cataracts (CT), Acute Angle-Closure Glaucoma (AACG), Primary Congenital Glaucoma (PCG), Exophthalmos or Bulging Eyes (BE) and Ocular Hypertension (OH). We have used efficient data collection methods, data annotations by professional ophthalmologists, applied five different feature selection methods, two types of data splitting techniques (train-test and stratified k-fold cross validation), and applied nine ML methods for the overall prediction approach. While applying ML methods, we have chosen suitable classic ML methods, such as Decision Tree (DT), Random Forest (RF), Naive Bayes (NB), AdaBoost (AB), Logistic Regression (LR), k-Nearest Neighbour (k-NN), Bagging (Bg), Boosting (BS) and Support Vector Machine (SVM). We have performed a symptomatic analysis of the prominent symptoms of each of the five eye diseases. The results of the analysis and comparison between methods are shown separately. While comparing the methods, we have adopted traditional performance indices, such as accuracy, precision, sensitivity, F1-Score, etc. Finally, SVM outperformed other models obtaining the highest accuracy of 99.11% for 10-fold cross-validation and LR obtained 98.58% for the split ratio of 80:20.", "keywords": ["important sensory organ", "generally considered", "important sensory", "eye diseases", "Primary Congenital Glaucoma", "sensory organ", "methods", "eye", "Diseases", "predict eye diseases", "Support Vector Machine", "organ of humans", "Acute Angle-Closure Glaucoma", "common eye diseases", "Congenital Glaucoma", "vital organ", "Bulging Eyes", "organ", "diagnosis", "Primary Congenital"], "paper_title": "An Efficient Approach to Predict Eye Diseases from Symptoms Using Machine Learning and Ranker-Based Feature Selection Methods.", "last_updated": "2023/02/04"}, {"id": "0032821488", "domain": "Glaucoma (unspecified)", "model_name": "Zhang et al.", "publication_date": "2020/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32821488/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32821488", "task": "IWqQC1koJA", "abstract": "To establish and evaluate algorithms for detection of primary angle closure suspects (PACS), the risk factor for primary angle closure disease by combining multiple static and dynamic anterior segment optical coherence tomography (ASOCT) parameters. Observational, cross-sectional study. The right eyes of subjects aged \u226540 years who participated in the 5-year follow-up of the Handan Eye Study, and underwent gonioscopy and ASOCT examinations under light and dark conditions were included. All ASOCT images were analyzed by Zhongshan Angle Assessment Program. Backward logistic regression (BLR) was used for inclusion of variables in the prediction models. BLR, na\u00efve Bayes' classification (NBC), and neural network (NN) were evaluated and compared using the area under the receiver operating characteristic curve (AUC). Data from 744 subjects (405 eyes with PACS and 339 normal eyes) were analyzed. Angle recess area at 750 \u00b5m, anterior chamber volume, lens vault in light and iris cross-sectional area change/pupil diameter change were included in the prediction models. The AUCs of BLR, NBC, and NN were 0.827 (95% confidence interval [CI], 0.798-0.856), 0.826 (95% CI, 0.797-0.854), and 0.844 (95% CI, 0.817-0.871), respectively. No significant statistical differences were found between the three algorithms (<i>P</i> = 0.622). The three algorithms did not meet the requirements for population-based screening of PACS. One possible reason could be the different angle closure mechanisms in enrolled eyes. This study provides a promise for basis for future research directed toward the development of an image-based, noncontact method to screen for angle closure.", "keywords": ["optical coherence tomography", "combining multiple static", "segment optical coherence", "primary angle closure", "dynamic anterior segment", "anterior segment optical", "primary angle", "coherence tomography", "establish and evaluate", "risk factor", "disease by combining", "combining multiple", "multiple static", "static and dynamic", "segment optical", "optical coherence", "angle closure suspects", "angle closure disease", "Angle Assessment Program", "Handan Eye Study"], "paper_title": "Establishment and Comparison of Algorithms for Detection of Primary Angle Closure Suspect Based on Static and Dynamic Anterior Segment Parameters.", "last_updated": "2023/02/04"}, {"id": "0034786216", "domain": "Cataract", "model_name": "Askarian et al.", "publication_date": "2021/04/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34786216/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34786216", "task": "IWqQC1koJA", "abstract": "Cataract, which is the clouding of the crystalline lens, is the most prevalent eye disease accounting for 51% of all eye diseases in the U.S. Cataract is a progressive disease, and its early detection is critical for preventing blindness. In this paper, an efficient approach to identify cataract disease by adopting luminance features using a smartphone is proposed. Initially, eye images captured by a smartphone were cropped to extract the lens, and the images were preprocessed to remove irrelevant background and noise by utilizing median filter and watershed transformation. Then, a novel luminance transformation from pixel brightness algorithm was introduced to extract lens image features. The luminance and texture features of different types of cataract disease images could be obtained accurately in this stage. Finally, by adopting support vector machines (SVM) as the classification method, cataract eyes were identified. From all the images that we fed into our system, our method could diagnose diseased eyes with 96.6% accuracy, 93.4% specificity, and 93.75% sensitivity. The proposed method provides an affordable, rapid, easy-to-use, and versatile method for detecting cataracts by using smartphones without the use of bulky and expensive imaging devices. This methodcan be used for bedside telemedicine applications or in remote areas that have medical shortages. Previous smartphone-based cataract detection methods include texture feature analysis with 95 % accuracy, Gray Level Co-occurrence Matrix (GLCM) method with 89% accuracy, red reflex measurement method, and RGB color feature extraction method using cascade classifier with 90% accuracy. The accuracy of cataract detection in these studies is subject to changes in smartphone models and/or environmental conditions. However, our novel luminance-based method copes with different smartphone camera sensors and chroma variations, while operating independently from sensors' color characteristics and changes in distances and camera angle. Clinical and Translational Impact-This study is an early/pre-clinical research proposing a novel luminance-based method of detecting cataract using smartphones for remote/at-home monitoring and telemedicine application.", "keywords": ["eye disease accounting", "prevalent eye disease", "preventing blindness", "Cataract", "critical for preventing", "method", "cataract disease", "disease accounting", "progressive disease", "cataract disease images", "disease", "identify cataract disease", "crystalline lens", "prevalent eye", "images", "accuracy", "Gray Level Co-occurrence", "Level Co-occurrence Matrix", "smartphone", "cataract detection"], "paper_title": "Detecting Cataract Using Smartphones.", "last_updated": "2023/02/04"}, {"id": "0033231277", "domain": "Cataract", "model_name": "Langenbucher et al.", "publication_date": "2020/11/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33231277/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33231277", "task": null, "abstract": "In the last decade, artificial intelligence and machine learning algorithms have been more and more established for the screening and detection of diseases and pathologies, as well as for describing interactions between measures where classical methods are too complex or fail. The purpose of this paper is to model the measured postoperative position of an intraocular lens implant after cataract surgery, based on preoperatively assessed biometric effect sizes using techniques of machine learning. In this study, we enrolled 249 eyes of patients who underwent elective cataract surgery at Augenklinik Castrop-Rauxel. Eyes were measured preoperatively with the IOLMaster 700 (Carl Zeiss Meditec), as well as preoperatively and postoperatively with the Casia 2 OCT (Tomey). Based on preoperative effect sizes axial length, corneal thickness, internal anterior chamber depth, thickness of the crystalline lens, mean corneal radius and corneal diameter a selection of 17 machine learning algorithms were tested for prediction performance for calculation of internal anterior chamber depth (AQD_post) and axial position of equatorial plane of the lens in the pseudophakic eye (LEQ_post). The 17 machine learning algorithms (out of 4 families) varied in root mean squared/mean absolute prediction error between 0.187/0.139\u2009mm and 0.255/0.204\u2009mm (AQD_post) and 0.183/0.135\u2009mm and 0.253/0.206\u2009mm (LEQ_post), using 5-fold cross validation techniques. The Gaussian Process Regression Model using an exponential kernel showed the best performance in terms of root mean squared error for prediction of AQDpost and LEQpost. If the entire dataset is used (without splitting for training and validation data), comparison of a simple multivariate linear regression model vs. the algorithm with the best performance showed a root mean squared prediction error for AQD_post/LEQ_post with 0.188/0.187\u2009mm vs. the best performance Gaussian Process Regression Model with 0.166/0.159\u2009mm. In this paper we wanted to show the principles of supervised machine learning applied to prediction of the measured physical postoperative axial position of the intraocular lenses. Based on our limited data pool and the algorithms used in our setting, the benefit of machine learning algorithms seems to be limited compared to a standard multivariate regression model.", "keywords": ["machine learning algorithms", "machine learning", "Process Regression Model", "Gaussian Process Regression", "learning algorithms", "Regression Model", "Carl Zeiss Meditec", "artificial intelligence", "diseases and pathologies", "complex or fail", "screening and detection", "detection of diseases", "describing interactions", "interactions between measures", "measures where classical", "classical methods", "learning", "machine", "Process Regression", "Gaussian Process"], "paper_title": "Artificial Intelligence, Machine Learning and Calculation of Intraocular Lens Power.", "last_updated": "2023/02/04"}, {"id": "0033800825", "domain": "Cataract", "model_name": "keras", "publication_date": "2021/03/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33800825/", "code_link": "https://github.com/keras-team/keras", "model_type": "random forest", "verified": false, "model_task": null, "code_available": true, "disease_class": "3kPkWQNPZH", "pmid": "33800825", "task": null, "abstract": "The present study aims to describe the use of machine learning (ML) in predicting the occurrence of postoperative refraction after cataract surgery and compares the accuracy of this method to conventional intraocular lens (IOL) power calculation formulas. In total, 3331 eyes from 2010 patients were assessed. The objects were divided into training data and test data. The constants for the IOL power calculation formulas and model training for ML were optimized using training data. Then, the occurrence of postoperative refraction was predicted using conventional formulas, or ML models were calculated using the test data. We evaluated the SRK/T formula, Haigis formula, Holladay 1 formula, Hoffer Q formula, and Barrett Universal II formula (BU-II); similar to ML methods, we assessed support vector regression (SVR), random forest regression (RFR), gradient boosting regression (GBR), and neural network (NN). Among the conventional formulas, BU-II had the lowest mean and median absolute error of prediction. Therefore, we compared the accuracy of our method with that of BU-II. The absolute errors of some ML methods were lower than those of BU-II. However, no statistically significant difference was observed. Thus, the accuracy of our method was not inferior to that of BU-II.", "keywords": ["present study aims", "conventional intraocular lens", "IOL power calculation", "machine learning", "intraocular lens", "present study", "study aims", "aims to describe", "cataract surgery", "surgery and compares", "power calculation formulas", "power calculation", "formula", "BU-II", "IOL power", "predicting the occurrence", "calculation formulas", "occurrence of postoperative", "data", "compares the accuracy"], "paper_title": "Use of a Machine Learning Method in Predicting Refraction after Cataract Surgery.", "last_updated": "2023/02/04"}, {"id": "0033530727", "domain": "Cataract", "model_name": "Liu et al.", "publication_date": "2021/02/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33530727/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33530727", "task": null, "abstract": "To predict post-operative depth of focus (DoF) using machine learning techniques after cataract surgery with Tecnis Symfony implantation and determine associated impact factors. This was a retrospective cohort study among patients receiving Tecnis Symfony implantation, an extended-range-of-vision intraocular lens, during October 2016-January 2020 at Daqing Oilfield General Hospital, China. Four different predictive models were used to predict good post-operative DoF (\u2a7e2.5\u2009D): Extreme Gradient Boost (XGBoost), random forest (RF), LASSO penalized regression, and multivariable logistic regression (MLR). Apriori algorithm was employed to further explore the association between patient attributes and DoF. A total of 182 unique cases (143 patients) were included. The XGBoost model produced the best predictive accuracy compared to RF, LASSO, and MLR models. Overall performance of the best fitting XGBoost model was as follows: accuracy\u2009=\u200970.3%, AUC\u2009=\u200980.2%, sensitivity\u2009=\u200965.5%, and specificity\u2009=\u200987.5%. The Apriori algorithm identified six preoparative attributes with substantial effects on good post-operative DoF: low anterior chamber depth (ACD) (1.9 to <2.5\u2009mm), smaller pupil size (1.7 to <2.5\u2009mm), low-to-mid axial length (21 to <23\u2009mm), minimum astigmatism degree (-0.2 to 0 diopter), low IOP (9 to <12\u2009mmHg), and medium lens target refractive error (-0.5 to <-0.25 diopter). Machine Learning models were able to predict good post-operative DoF among cataract patients receiving a Tecnis Symfony ocular lens implantation. The accuracy of the model was above 70%. The Apriori algorithm identified six preoperative attributes with a strong association with post-operative DoF.", "keywords": ["Tecnis Symfony implantation", "Tecnis Symfony", "Oilfield General Hospital", "Daqing Oilfield General", "receiving Tecnis Symfony", "Tecnis Symfony ocular", "Symfony implantation", "Extreme Gradient Boost", "impact factors", "General Hospital", "Symfony", "determine associated impact", "Daqing Oilfield", "Oilfield General", "Tecnis", "good post-operative DoF", "patients receiving Tecnis", "predict good post-operative", "post-operative DoF", "Apriori algorithm"], "paper_title": "Using machine learning to predict post-operative depth of focus after cataract surgery with implantation of Tecnis Symfony.", "last_updated": "2023/02/04"}, {"id": "0026563686", "domain": "Cataract", "model_name": "Yang et al.", "publication_date": "2015/10/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26563686/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "26563686", "task": "IWqQC1koJA", "abstract": "Cataract is defined as a lenticular opacity presenting usually with poor visual acuity. It is one of the most common causes of visual impairment worldwide. Early diagnosis demands the expertise of trained healthcare professionals, which may present a barrier to early intervention due to underlying costs. To date, studies reported in the literature utilize a single learning model for retinal image classification in grading cataract severity. We present an ensemble learning based approach as a means to improving diagnostic accuracy. Three independent feature sets, i.e., wavelet-, sketch-, and texture-based features, are extracted from each fundus image. For each feature set, two base learning models, i.e., Support Vector Machine and Back Propagation Neural Network, are built. Then, the ensemble methods, majority voting and stacking, are investigated to combine the multiple base learning models for final fundus image classification. Empirical experiments are conducted for cataract detection (two-class task, i.e., cataract or non-cataractous) and cataract grading (four-class task, i.e., non-cataractous, mild, moderate or severe) tasks. The best performance of the ensemble classifier is 93.2% and 84.5% in terms of the correct classification rates for cataract detection and grading tasks, respectively. The results demonstrate that the ensemble classifier outperforms the single learning model significantly, which also illustrates the effectiveness of the proposed approach.", "keywords": ["lenticular opacity presenting", "poor visual acuity", "lenticular opacity", "opacity presenting", "visual acuity", "poor visual", "single learning model", "base learning models", "Cataract", "learning", "visual impairment worldwide", "Propagation Neural Network", "Support Vector Machine", "learning model", "Back Propagation Neural", "ensemble", "single learning", "base learning", "visual", "image classification"], "paper_title": "Exploiting ensemble learning for automatic cataract detection and grading.", "last_updated": "2023/02/04"}, {"id": "0026886962", "domain": "Cataract", "model_name": "Caixinha et al.", "publication_date": "2016/02/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26886962/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "26886962", "task": "IWqQC1koJA", "abstract": "To early detect nuclear cataract in vivo and automatically classify its severity degree, based on the ultrasound technique, using machine learning. A 20-MHz ophthalmic ultrasound probe with a focal length of 8.9\u00a0mm and an active diameter of 3\u00a0mm was used. Twenty-seven features in time and frequency domain were extracted for cataract detection and classification with support vector machine (SVM), Bayes, multilayer perceptron, and random forest classifiers. Fifty rats were used: 14 as control and 36 as study group. An animal model for nuclear cataract was developed. Twelve rats with incipient, 13 with moderate, and 11 with severe cataract were obtained. The hardness of the nucleus and the cortex regions was objectively measured in 12 rats using the NanoTest. Velocity, attenuation, and frequency downshift significantly increased with cataract formation ( ). The SVM classifier showed the higher performance for the automatic classification of cataract severity, with a precision, sensitivity, and specificity of 99.7% (relative absolute error of 0.4%). A statistically significant difference was found for the hardness of the different cataract degrees ( P = 0.016). The nucleus showed a higher hardness increase with cataract formation ( P = 0.049 ). A moderate-to-good correlation between the features and the nucleus hardness was found in 23 out of the 27 features. The developed methodology made possible detecting the nuclear cataract in-vivo in early stages, classifying automatically its severity degree and estimating its hardness. Based on this work, a medical prototype will be developed for early cataract detection, classification, and hardness estimation.", "keywords": ["cataract", "detect nuclear cataract", "ultrasound technique", "nuclear cataract", "machine learning", "hardness", "early detect nuclear", "ophthalmic ultrasound probe", "detect nuclear", "automatically classify", "early detect", "ultrasound", "cataract formation", "nuclear", "ophthalmic ultrasound", "ultrasound probe", "cataract detection", "SVM", "severity", "rats"], "paper_title": "In-Vivo Automatic Nuclear Cataract Detection and Classification in an Animal Model by Ultrasounds.", "last_updated": "2023/02/04"}, {"id": "0036444216", "domain": "Cataract", "model_name": "Ruzicki et al.", "publication_date": "2022/10/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36444216/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36444216", "task": "IWqQC1koJA", "abstract": "To develop a method for objective analysis of the reproducible steps in routine cataract surgery. Prospective study; machine learning. Deidentified faculty and trainee surgical videos. Consecutive cataract surgeries performed by a faculty or trainee surgeon in an ophthalmology residency program over 6 months were collected and labeled according to degrees of difficulty. An existing image classification network, ResNet 152, was fine-tuned for tool detection in cataract surgery to allow for automatic identification of each unique surgical instrument. Individual microscope video frame windows were subsequently encoded as a vector. The relation between vector encodings and perceived skill using k-fold user-out cross-validation was examined. Algorithms were evaluated using area under the receiver operating characteristic curve (AUC) and the classification accuracy. Accuracy of tool detection and skill assessment. In total, 391 consecutive cataract procedures with 209 routine cases were used. Our model achieved an AUC ranging from 0.933 to 0.998 for tool detection. For skill classification, AUC was 0.550 (95% confidence interval [CI], 0.547-0.553) with an accuracy of 54.3% (95% CI, 53.9%-54.7%) for a single snippet, AUC was 0.570 (0.565-0.575) with an accuracy of 57.8% (56.8%-58.7%) for a single surgery, and AUC was 0.692 (0.659-0.758) with an accuracy of 63.3% (56.8%-69.8%) for a single user given all their trials. Our research shows that machine learning can accurately and independently identify distinct cataract surgery tools in videos, which is crucial for comparing the use of the tool in a step. However, it is more challenging for machine learning to accurately differentiate overall and specific step skill to assess the level of training or expertise. The author(s) have no proprietary or commercial interest in any materials discussed in this article.", "keywords": ["develop a method", "method for objective", "objective analysis", "AUC", "accuracy", "cataract", "cataract surgery", "tool detection", "tool", "surgery", "Consecutive cataract", "skill", "reproducible steps", "machine learning", "detection", "machine", "learning", "single", "classification", "cataract surgery tools"], "paper_title": "Use of Machine Learning to Assess Cataract Surgery Skill Level With Tool Detection.", "last_updated": "2023/02/04"}, {"id": "0034891109", "domain": "Cataract", "model_name": "Burwinkel et al.", "publication_date": "2021/11/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34891109/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34891109", "task": null, "abstract": "The human cataract, a developing opacification of the human eye lens, currently constitutes the world's most frequent cause for blindness. As a result, cataract surgery has become the most frequently performed ophthalmic surgery in the world. By removing the human lens and replacing it with an artificial intraocular lens (IOL), the optical system of the eye is restored. In order to receive a good refractive result, the IOL specifications, especially the refractive power, have to be determined precisely prior to surgery. In the last years, there has been a body of work to perform this prediction by using biometric information extracted from OCT imaging data, recently also by machine learning (ML) methods. Approaches so far consider only biometric information or physical modelling, but provide no effective combination, while often also neglecting IOL geometry. Additionally, ML on small data sets without sufficient domain coverage can be challenging. To solve these issues, we propose OpticNet, a novel optical refraction network based on an unsupervised, domain-specific loss function that explicitly incorporates physical information into the network. By providing a precise and differentiable light propagation eye model, physical gradients following the eye optics are backpropagated into the network. We further propose a new transfer learning procedure, which allows the unsupervised pre-training on the optical model and fine-tuning of the network on small amounts of surgical patient data. We show that our method outperforms the current state of the art on five OCT-image based data sets, provides better domain coverage within its predictions, and achieves better physical consistency.", "keywords": ["human eye lens", "developing opacification", "constitutes the world", "human cataract", "human lens", "IOL", "eye", "human eye", "human", "eye lens", "cataract surgery", "world most frequent", "lens", "performed ophthalmic surgery", "data", "network", "world", "physical", "surgery", "cataract"], "paper_title": "Physics-aware learning and domain-specific loss design in ophthalmology.", "last_updated": "2023/02/04"}, {"id": "0035317470", "domain": "Cataract", "model_name": "Pathak et al.", "publication_date": "2022/03/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35317470/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "35317470", "task": "IWqQC1koJA", "abstract": "This paper presents a low cost, robust, portable and automated cataract detection system which can detect the presence of cataract from the colored digital eye images and grade their severity. Ophthalmologists detect cataract through visual screening using ophthalmoscope and slit lamps. Conventionally a patient has to visit an ophthalmologist for eye screening and treatment follows the course. Developing countries lack the proper health infrastructure and face huge scarcity of trained medical professionals as well as technicians. The condition is not very satisfactory with the rural and remote areas of developed nations. To bridge this barrier between the patient and the availability of resources, current work focuses on the development of portable low-cost, robust cataract screening and grading system. Similar works use fundus and retinal images which use costly imaging modules and image based detection algorithms which use much complex neural network models. Current work derives its benefit from the advancements in digital image processing techniques. A set of preprocessing has been done on the colored eye image and later texture information in form of mean intensity, uniformity, standard deviation and randomness has been calculated and mapped with the diagnostic opinion of doctor for cataract screening of over 200 patients. For different grades of cataract severity edge pixel count was calculated as per doctor's opinion and later these data are used for calculating the thresholds using hybrid k-means algorithm, for giving a decision on the presence of cataract and grade its severity. Low value of uniformity and high value of other texture parameters confirm the presence of cataract as clouding in eye lens causes the uniformity function to take lower value due to presence of coarse texture. Higher the edge pixel count value, this confirms the presence of starting of cataract as solidified regions in lens are nonuniform. Lower value corresponds to fully solidified region or matured cataract. Proposed algorithm was initially developed on MATLAB, and tested on over 300 patients in an eye camp. The system has shown more than 98% accuracy in detection and grading of cataract. Later a cloud based system was developed with 3D printed image acquisition module to manifest an automated, portable and efficient cataract detection system for Tele-Ophthalmology. The proposed system uses a very simple and efficient technique by mapping the diagnostic opinion of the doctor as well, giving very promising results which suggest its potential use in teleophthalmology applications to reduce the cost of delivering eye care services and increasing its reach effectively. Developed system is simple in design and easy to operate and suitable for mass screening of cataracts. Due to non-invasive and non-mydriatic and mountable nature of device, in person screening is not required. Hence, social distancing norms are easy to follow and device is very useful in COVID-19 like situation.", "keywords": ["cataract", "paper presents", "Ophthalmologists detect cataract", "cataract detection system", "screening", "system", "eye", "cataract screening", "presence", "robust cataract screening", "presence of cataract", "cataract detection", "detect cataract", "image", "detection", "Ophthalmologists detect", "developed", "detect", "detection system", "portable"], "paper_title": "Development of portable and robust cataract detection and grading system by analyzing multiple texture features for Tele-Ophthalmology.", "last_updated": "2023/02/04"}, {"id": "0034004006", "domain": "Cataract", "model_name": "Gatinel et al.", "publication_date": "2021/06/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34004006/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34004006", "task": null, "abstract": "To describe a formula to back-calculate the theoretical position of the principal object plane of an intraocular lens (IOL), as well as the theoretical anatomic position in a thick lens eye model. A study was conducted to ascertain the impact of variations in design and IOL power, on the refractive outcomes of cataract surgery. A schematic eye model was designed and manipulated to reflect changes in the anterior and posterior radii of an IOL, while keeping the central thickness and paraxial powers static. Modifications of the shape factor (X) of the IOL affects the thick lens estimated effective lens position (ELP). Corresponding postoperative spherical equivalent (SE) were computed for different IOL powers (-5 diopters [D], 5 D, 15 D, 25 D, and 35 D) with X ranging from -1 to +1 by 0.1. The impact of the thick lens estimated effective lens position shift on postoperative refraction was highly dependent on the optical power of the IOL and its thickness. Design modifications could theoretically induce postoperative refraction variations between approximately 0.50 and 3.0 D, for implant powers ranging from 15 D to 35 D. This work could be of interest for researchers involved in the design of IOL power calculation formulas. The importance of IOL geometry in refractive outcomes, especially for short eyes, should challenge the fact that these data are not usually published by IOL manufacturers. The back-calculation of the estimated effective lens position is central to intraocular lens calculation formulas, especially for artificial intelligence-based optical formulas, where the algorithm can be trained to predict this value.", "keywords": ["principal object plane", "theoretical anatomic position", "IOL", "effective lens position", "estimated effective lens", "back-calculate the theoretical", "theoretical anatomic", "IOL power", "thick lens estimated", "lens", "lens estimated effective", "thick lens", "lens position", "effective lens", "principal object", "object plane", "lens eye model", "IOL power calculation", "thick lens eye", "theoretical position"], "paper_title": "Determining the Theoretical Effective Lens Position of Thick Intraocular Lenses for Machine Learning-Based IOL Power Calculation and Simulation.", "last_updated": "2023/02/04"}, {"id": "0035363261", "domain": "Cataract", "model_name": "Matton et al.", "publication_date": "2022/04/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35363261/", "code_link": null, "model_type": "RNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "35363261", "task": null, "abstract": "To develop a method for accurate automated real-time identification of instruments in cataract surgery videos. Cataract surgery videos were collected at University of Michigan's Kellogg Eye Center between 2020 and 2021. Videos were annotated for the presence of instruments to aid in the development, validation, and testing of machine learning (ML) models for multiclass, multilabel instrument identification. A new cataract surgery database, BigCat, was assembled, containing 190 videos with over 3.9 million annotated frames, the largest reported cataract surgery annotation database to date. Using a dense convolutional neural network (CNN) and a recursive averaging method, we were able to achieve a test F1 score of 0.9528 and test area under the receiver operator characteristic curve of 0.9985 for surgical instrument identification. These prove to be state-of-the-art results compared to previous works, while also only using a fraction of the model parameters of the previous architectures. Accurate automated surgical instrument identification is possible with lightweight CNNs and large datasets. Increasingly complex model architecture is not necessary to retain a well-performing model. Recurrent neural network architectures add additional complexity to a model and are unnecessary to attain state-of-the-art performance. Instrument identification in the operative field can be used for further applications such as evaluating surgical trainee skill level and developing early warning detection systems for use during surgery.", "keywords": ["Michigan Kellogg Eye", "Kellogg Eye Center", "cataract surgery videos", "cataract surgery", "University of Michigan", "Michigan Kellogg", "Kellogg Eye", "Eye Center", "instrument identification", "surgery videos", "surgery", "cataract", "automated real-time identification", "videos", "accurate automated real-time", "cataract surgery database", "identification", "surgical instrument identification", "reported cataract surgery", "cataract surgery annotation"], "paper_title": "Analysis of Cataract Surgery Instrument Identification Performance of Convolutional and Recurrent Neural Network Ensembles Leveraging BigCat.", "last_updated": "2023/02/04"}, {"id": "0035919009", "domain": "Glaucoma (unspecified)", "model_name": "Ramesh et al.", "publication_date": "2022/08/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35919009/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35919009", "task": null, "abstract": "", "keywords": [], "paper_title": "Response to comments on: Modeling and mitigating human annotations to design processing systems with human-in-the-loop machine learning for glaucomatous defects: The future in artificial intelligence.", "last_updated": "2023/02/04"}, {"id": "0036531572", "domain": "Cataract", "model_name": "Kladny et al.", "publication_date": "2022/07/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36531572/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36531572", "task": null, "abstract": "To evaluate graft detachment after Descemet membrane endothelial keratoplasty (DMEK) in pseudophakic eyes and DMEK combined with cataract surgery (triple DMEK). Analysis of 3 single-center prospective cohort studies and 1 randomized controlled trial. Participants with Fuchs' endothelial corneal dystrophy. A validated neural network for image segmentation quantified graft detachment on anterior segment OCT (AS-OCT) images 3 days after DMEK and at the 2-week postoperative visit. Area and volume of graft detachment were compared between DMEK only and triple DMEK using generalized estimating equation models and adjusting for participant age and the size of the air bubble. Area and volume of DMEK graft detachment. Among 207 participants with 270 eyes included, 75 pseudophakic eyes had DMEK only and 195 eyes had triple DMEK. A total of 147 eyes had less than one third of detachment at day 3. In 139 of these eyes (95%), detachment was still less than one third at the 2-week scan, indicating that postoperative graft detachment at 2 weeks occurred mainly in eyes with early detachment. When superimposing all 3-dimensional maps from 2 weeks after surgery, the central graft was mainly attached and detachment was located at the graft margin. The mean area of graft detachment decreased from 28% in DMEK only and 38% in triple DMEK to 16% in DMEK only and 25% in triple DMEK at the 2-week postoperative visit. At 2 weeks, the mean area of detachment was 1.85-fold higher (95% confidence interval [CI], 1.34-2.56) and the mean volume was 2.41-fold higher (95% CI, 1.51-3.86) in triple DMEK compared with DMEK. A total of 46 eyes received rebubbling procedures, with 7 eyes (9%) in the DMEK group and 39 eyes (20%) in the triple DMEK group (adjusted risk ratio, 3.1; 95% CI, 1.3-7.1), indicating that rebubbling was more common in eyes undergoing triple DMEK. Automated segmentation of AS-OCT images allowed precise quantification of graft detachment over time and identified DMEK combined with cataract surgery as a risk factor. Frequency of operative follow-up might be guided by extent of detachment in the first postoperative days after DMEK.", "keywords": ["DMEK", "Descemet membrane endothelial", "triple DMEK", "Descemet membrane", "graft detachment", "detachment", "eyes", "membrane endothelial keratoplasty", "graft", "DMEK graft detachment", "triple", "evaluate graft detachment", "DMEK combined", "Descemet", "DMEK group", "DMEK graft", "triple DMEK compared", "triple DMEK group", "undergoing triple DMEK", "detachment after Descemet"], "paper_title": "Graft Detachment after Descemet Membrane Endothelial Keratoplasty with and without Cataract Surgery.", "last_updated": "2023/02/04"}, {"id": "0034571672", "domain": "Glaucoma (unspecified)", "model_name": "Ramesh et al.", "publication_date": "2021/11/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34571672/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34571672", "task": null, "abstract": "", "keywords": [], "paper_title": "Modeling and mitigating human annotations to design processing systems with human-in-the-loop machine learning for glaucomatous defects: The future in artificial intelligence.", "last_updated": "2023/02/04"}, {"id": "0033693417", "domain": "Cataract", "model_name": "Clarke et al.", "publication_date": "2020/12/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33693417/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33693417", "task": null, "abstract": "<b>Purpose:</b> Our work introduces a highly accurate, safe, and sufficiently explicable machine-learning (artificial intelligence) model of intraocular lens power (IOL) translating into better post-surgical outcomes for patients with cataracts. We also demonstrate its improved predictive accuracy over previous formulas. <b>Methods:</b> We collected retrospective eye measurement data on 5,331 eyes from 3,276 patients across multiple centers who received a lens implantation during cataract surgery. The dependent measure is the post-operative manifest spherical equivalent error from intended and the independent variables are the patient- and eye-specific characteristics. This dataset was split so that one subset was for formula construction and the other for validating our new formula. Data excluded fellow eyes, so as not to confound the prediction with bilateral eyes. <b>Results:</b> Our formula is three times more precise than reported studies with a median absolute IOL error of 0.204 diopters (D). When converted to absolute predictive refraction errors on the cornea, the median error is 0.137 D which is close to the IOL manufacturer tolerance. These estimates are validated out-of-sample and thus are expected to reflect the future performance of our prediction formula, especially since our data were collected from a wide variety of patients, clinics, and manufacturers. <b>Conclusion:</b> The increased precision of IOL power calculations has the potential to optimize patient positive refractive outcomes. Our model also provides uncertainty plots that can be used in tandem with the clinician's expertise and previous formula output, further enhancing the safety. <b>Translational relavance:</b> Our new machine learning process has the potential to significantly improve patient IOL refractive outcomes safely.", "keywords": ["sufficiently explicable machine-learning", "artificial intelligence", "highly accurate", "explicable machine-learning", "work introduces", "introduces a highly", "sufficiently explicable", "intraocular lens power", "IOL", "Purpose", "intraocular lens", "formula", "post-surgical outcomes", "patient IOL refractive", "absolute IOL error", "IOL refractive outcomes", "patient IOL", "eyes", "patients", "patient"], "paper_title": "The Bayesian Additive Regression Trees Formula for Safe Machine Learning-Based Intraocular Lens Predictions.", "last_updated": "2023/02/04"}, {"id": "0033836989", "domain": "Cataract", "model_name": "Li et al.", "publication_date": "2021/04/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33836989/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33836989", "task": null, "abstract": "To assess whether incorporating a machine learning (ML) method for accurate prediction of postoperative anterior chamber depth (ACD) improves the refraction prediction performance of existing intraocular lens (IOL) calculation formulas. A dataset of 4806 patients with cataract was gathered at the Kellogg Eye Center, University of Michigan, and split into a training set (80% of patients, 5761 eyes) and a testing set (20% of patients, 961 eyes). A previously developed ML-based method was used to predict the postoperative ACD based on preoperative biometry. This ML-based postoperative ACD was integrated into new effective lens position (ELP) predictions using regression models to rescale the ML output for each of four existing formulas (Haigis, Hoffer Q, Holladay and SRK/T). The performance of the formulas with ML-modified ELP was compared using a testing dataset. Performance was measured by the mean absolute error (MAE) in refraction prediction. When the ELP was replaced with a linear combination of the original ELP and the ML-predicted ELP, the MAEs\u00b1SD (in Diopters) in the testing set were: 0.356\u00b10.329 for Haigis, 0.352\u00b10.319 for Hoffer Q, 0.371\u00b10.336 for Holladay, and 0.361\u00b10.331 for SRK/T which were significantly lower (p<0.05) than those of the original formulas: 0.373\u00b10.328 for Haigis, 0.408\u00b10.337 for Hoffer Q, 0.384\u00b10.341 for Holladay and 0.394\u00b10.351 for SRK/T. Using a more accurately predicted postoperative ACD significantly improves the prediction accuracy of four existing IOL power formulas.", "keywords": ["anterior chamber depth", "Kellogg Eye Center", "postoperative anterior chamber", "postoperative ACD", "machine learning", "chamber depth", "University of Michigan", "assess whether incorporating", "incorporating a machine", "anterior chamber", "ML-based postoperative ACD", "existing intraocular lens", "ACD", "ELP", "postoperative ACD based", "Eye Center", "postoperative ACD significantly", "Kellogg Eye", "predicted postoperative ACD", "postoperative anterior"], "paper_title": "AI-powered effective lens position prediction improves the accuracy of existing lens formulas.", "last_updated": "2023/02/04"}, {"id": "0035648013", "domain": "Glaucoma (unspecified)", "model_name": "Ramesh et al.", "publication_date": "2022/06/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35648013/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35648013", "task": null, "abstract": "Big data has been a game changer of machine learning. But, big data is a form of centralized version of data only available and accessible to the technology giants. A way to decentralize this data and make machine learning accessible to the smaller organizations is via the blockchain technology. This peer-to-peer network creates a common database accessible to those in the network. Furthermore, blockchain helps in securing the digital data and prevents data tampering due to human interactions. This technology keeps a constant track of the document in terms of creation, editing, etc., and makes this information accessible to all. It is a chain of data being distributed across many computers, with a database containing details about each transaction. This record helps in data security and prevents data modification. This technology also helps create big data from multiple sources of small data paving way for creating a well serving artificial intelligence model. Here in this manuscript, we discuss about the usage of blockchain, its current role in machine learning and challenges faced by it.", "keywords": ["data", "game changer", "Big data", "machine learning", "machine learning accessible", "accessible", "Big", "technology", "machine", "learning", "make machine learning", "create big data", "blockchain", "prevents data", "learning accessible", "game", "changer", "create big", "changer of machine", "make machine"], "paper_title": "Under lock and key: Incorporation of blockchain technology in the field of ophthalmic artificial intelligence for big data management - A perfect match?", "last_updated": "2023/02/04"}, {"id": "0033384892", "domain": "Cataract", "model_name": "Li et al.", "publication_date": "2020/12/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33384892/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33384892", "task": null, "abstract": "To develop a method for predicting postoperative anterior chamber depth (ACD) in cataract surgery patients based on preoperative biometry, demographics, and intraocular lens (IOL) power. Patients who underwent cataract surgery and had both preoperative and postoperative biometry measurements were included. Patient demographics and IOL power were collected from the Sight Outcomes Research Collaborative (SOURCE) database. A gradient-boosting decision tree model was developed to predict the postoperative ACD. The mean absolute error (MAE) and median absolute error (MedAE) were used as evaluation metrics. The performance of the proposed method was compared with five existing formulas. In total, 847 patients were assigned randomly in a 4:1 ratio to a training/validation set (678 patients) and a testing set (169 patients). Using preoperative biometry and patient sex as predictors, the presented method achieved an MAE of 0.106 \u00b1 0.098 (SD) on the testing set, and a MedAE of 0.082. MAE was significantly lower than that of the five existing methods (<i>P</i> < 0.01). When keratometry was excluded, our method attained an MAE of 0.123 \u00b1 0.109, and a MedAE of 0.093. When IOL power was used as an additional predictor, our method achieved an MAE of 0.105 \u00b1 0.091 and a MedAE of 0.080. The presented machine learning method achieved greater accuracy than previously reported methods for the prediction of postoperative ACD. Increasing accuracy of postoperative ACD prediction with the presented algorithm has the potential to improve refractive outcomes in cataract surgery.", "keywords": ["anterior chamber depth", "predicting postoperative anterior", "postoperative anterior chamber", "cataract surgery", "surgery patients based", "postoperative ACD", "IOL power", "chamber depth", "intraocular lens", "MAE", "Outcomes Research Collaborative", "cataract surgery patients", "Sight Outcomes Research", "anterior chamber", "preoperative biometry", "underwent cataract surgery", "IOL", "ACD", "postoperative", "method"], "paper_title": "Gradient Boosting Decision Tree Algorithm for the Prediction of Postoperative Intraocular Lens Position in Cataract Surgery.", "last_updated": "2023/02/04"}, {"id": "0033173915", "domain": "Cataract", "model_name": "Li et al.", "publication_date": "2020/11/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33173915/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "33173915", "task": null, "abstract": "To assess whether incorporating a machine learning (ML) method for accurate prediction of postoperative anterior chamber depth (ACD) improves the refraction prediction performance of existing intraocular lens (IOL) calculation formulas. A dataset of 4806 cataract patients were gathered at the Kellogg Eye Center, University of Michigan, and split into a training set (80% of patients, 5761 eyes) and a testing set (20% of patients, 961 eyes). A previously developed ML-based method was used to predict the postoperative ACD based on preoperative biometry. This ML-based postoperative ACD was integrated into new effective lens position (ELP) predictions using regression models to rescale the ML output for each of four existing formulas (Haigis, Hoffer Q, Holladay, and SRK/T). The performance of the formulas with ML-modified ELP was compared using a testing dataset. Performance was measured by the mean absolute error (MAE) in refraction prediction. When the ELP was replaced with a linear combination of the original ELP and the ML-predicted ELP, the MAEs \u00b1 SD (in Diopters) in the testing set were: 0.356 \u00b1 0.329 for Haigis, 0.352 \u00b1 0.319 for Hoffer Q, 0.371 \u00b1 0.336 for Holladay, and 0.361 \u00b1 0.331 for SRK/T which were significantly lower than those of the original formulas: 0.373 \u00b1 0.328 for Haigis, 0.408 \u00b1 0.337 for Hoffer Q, 0.384 \u00b1 0.341 for Holladay, and 0.394 \u00b1 0.351 for SRK/T. Using a more accurately predicted postoperative ACD significantly improves the prediction accuracy of four existing IOL power formulas.", "keywords": ["anterior chamber depth", "Kellogg Eye Center", "postoperative anterior chamber", "postoperative ACD", "machine learning", "chamber depth", "University of Michigan", "assess whether incorporating", "incorporating a machine", "anterior chamber", "ML-based postoperative ACD", "existing intraocular lens", "ACD", "ELP", "postoperative ACD based", "Eye Center", "postoperative ACD significantly", "Kellogg Eye", "predicted postoperative ACD", "postoperative anterior"], "paper_title": "AI-Powered Effective Lens Position Prediction Improves the Accuracy of Existing Lens Formulas.", "last_updated": "2023/02/04"}, {"id": "0034571618", "domain": "Cataract", "model_name": "Gupta et al.", "publication_date": "2021/11/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34571618/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34571618", "task": null, "abstract": "To develop predictive models to identify cataract surgery patients who are more likely to benefit from refraction at a four-week postoperative exam. In this retrospective study, we used data of all 86,776 cataract surgeries performed in 2015 at a large tertiary-care eye hospital in India. The outcome variable was a binary indicator of whether the difference between corrected distance visual acuity and uncorrected visual acuity at the four-week postoperative exam was at least two lines on the Snellen chart. We examined the following statistical models: logistic regression, decision tree, pruned decision tree, random forest, weighted k-nearest neighbor, and a neural network. Predictor variables included in each model were patient sex and age, source eye (left or right), preoperative visual acuity, first-day postoperative visual acuity, intraoperative and immediate postoperative complications, and combined surgeries. We compared the predictive performance of models and assessed their clinical impact in test samples. All models demonstrated predictive accuracy better than chance based on area under the receiver operating characteristic curve. In a targeting exercise with a fixed intervention budget, we found that gains from predictive models in identifying patients who would benefit from refraction ranged from 7.8% (increase from 1500 to 1617 patients) to 74% (increase from 250 to 435 patients). The use of predictive statistical models to identify patients who are likely to benefit from refraction at follow-up can improve the economic efficiency of interventions. Simpler models like logistic regression perform almost as well as more complex machine-learning models, but are easier to implement.", "keywords": ["four-week postoperative exam", "cataract surgery patients", "identify cataract surgery", "visual acuity", "models", "cataract surgery", "four-week postoperative", "postoperative exam", "postoperative", "patients", "visual", "acuity", "hospital in India", "predictive", "benefit from refraction", "develop predictive models", "surgery patients", "postoperative visual acuity", "develop predictive", "cataract surgeries performed"], "paper_title": "Use of predictive models to identify patients who are likely to benefit from refraction at a follow-up visit after cataract surgery.", "last_updated": "2023/02/04"}, {"id": "0036937516", "domain": "Cataract", "model_name": "Zhang et al.", "publication_date": "2023/03/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36937516/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36937516", "task": null, "abstract": "Appropriate vault height of implantable collamer lens (ICL) implantation matters for it has risks of corneal endothelial cell loss, cataract formation and intraocular pressure elevation, which could lead to irreversible damage to optic nerve. Therefore, pre-operative prediction for an ideal vault height is a hotspot. However, few data exist regarding quantitative effect of ICL orientation on vault height. This study is aimed to quantitatively investigate the effect of ICL implantation orientation on vault height, and built a machine-learning (ML)-based vault prediction model taking implantation orientation into account. 473 consecutive case series treated with ICL implantation were retrospectively analyzed (408 were horizontally implanted, and 65 were vertically implanted). Multivariable logistic regression analysis was performed to determine the association between ICL orientation and achieved vault. ML was performed to develop a new vault height prediction model taking ICL orientation into account. Receiver operating characteristic curve (ROC) and net reclassification index (NRI) were obtained to assess the prediction ability. 95% of all the patients achieved 20/20 uncorrected distance visual acuity (UDVA) or better. No complications including cataract formation, dispersion or optic nerve injury were observed in any cases. Sex, sphere power, cylinder power, axis, ICL size and ICL orientation were all significant risk factors associated to vault height, and age was positively co-related. Of note, ICL size and ICL orientation were the top-ranking risk factors. Comparing to conventional horizontal implantation, vertical implantation could reduce the achieved vault by 81.187 \u03bcm (<i>p</i> < 0.001). In regarding to different ICL sizes, vertical implantation had no good to vault reduction when using ICL of 12.1 mm. However, it could reduce the vault by 59.351 \u03bcm and 160.992 \u03bcm respectively when ICL of 12.6mm and 13.2 mm were implanted (<i>p</i> = 0.0097 and <i>p</i> = 0.0124). For prediction of vault height, ML based model significantly outperformed traditional multivariable regression model. We provide quantitative evidence that vertical implantation of ICL could effectively reduce the achieved vault height, especially when large size ICL was implanted, comparing to traditional horizontal implantation. ML is extremely applicable in development of vault prediction model.", "keywords": ["ICL", "ICL orientation", "implantable collamer lens", "endothelial cell loss", "intraocular pressure elevation", "corneal endothelial cell", "vault height", "vault", "height", "implantation", "orientation", "ICL implantation", "ICL implantation orientation", "ICL size", "collamer lens", "cell loss", "pressure elevation", "implantable collamer", "corneal endothelial", "endothelial cell"], "paper_title": "A quantitative study of the effect of ICL orientation selection on post-operative vault and model-assisted vault prediction.", "last_updated": "2023/02/04"}, {"id": "0036872684", "domain": "Cataract", "model_name": "Kundu et al.", "publication_date": "2023/03/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36872684/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36872684", "task": null, "abstract": "To create a predictive model using artificial intelligence (AI) and assess if available data from patients' registration records can help in predicting definitive endpoints such as the probability of patients signing up for refractive surgery. This was a retrospective analysis. Electronic health records data of 423 patients presenting to the refractive surgery department were incorporated into models using multivariable logistic regression, decision trees classifier, and random forest (RF). Mean area under the receiver operating characteristic curve (ROC-AUC), sensitivity (Se), specificity (Sp), classification accuracy, precision, recall, and F1-score were calculated for each model to evaluate performance. The RF classifier provided the best output among the various models, and the top variables identified in this study by the RF classifier excluding income were insurance, time spent in the clinic, age, occupation, residence, source of referral, and so on. About 93% of the cases that did undergo refractive surgery were correctly predicted as having undergone refractive surgery. The AI model achieved an ROC-AUC of 0.945 with an Se of 88% and Sp of 92.5%. This study demonstrated the importance of stratification and identifying various factors using an AI model which could impact patients' decisions while selecting a refractive surgery. Eye centers can build specialized prediction profiles across disease categories and may allow for the identification of prospective obstacles in the patient's decision-making process, as well as strategies for dealing with them.", "keywords": ["predicting definitive endpoints", "refractive surgery", "artificial intelligence", "create a predictive", "predicting definitive", "definitive endpoints", "patients' registration records", "registration records", "predictive model", "refractive", "surgery", "patients signing", "refractive surgery department", "health records data", "model", "patients' registration", "undergo refractive surgery", "records data", "Electronic health records", "classifier"], "paper_title": "Role of artificial intelligence in determining factors impacting patients' refractive surgery decisions.", "last_updated": "2023/02/04"}, {"id": "0034046526", "domain": "Cataract", "model_name": "Bourdon et al.", "publication_date": "2021/05/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34046526/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "34046526", "task": null, "abstract": "To assess linear correlation between swept-source optical coherence tomography (SS-OCT) lens density variation and patients' best-corrected visual acuity (BCVA). Linear densitometry was performed on horizontal lens images from 518 eyes, obtained using SS-OCT. All densities from the anterior to the posterior side of the cataract were exported for detailed analysis. The algorithm used a classical random forest regression machine learning approach with fourfold cross-validation, meaning four batches of data from 75% of the eyes with known preoperative best-corrected visual acuity (poBCVA) were used for training a model to predict the data from the remaining 25% of the eyes. The main judgement criterion was the ability of the algorithm to identify linear correlation between measured and predicted BCVA. A significant linear correlation between poBCVA and the algorithm's prediction was found, with Pearson correlation coefficient (R)=0.558 (95% CI: 0.496 to 0.615, p<0.001). Mean BCVA prediction error was 0.0965\u00b10.059 logarithm of the minimal angle of resolution (logMAR), with 312 eyes (58%) having a BCVA prediction correct to \u00b10.1 logMAR. The best algorithm performances were achieved for 0.20 logMAR, with 79%\u00b10.1 logMAR correct prediction. Mean, anterior cortex, nucleus and posterior cortex pixel density were all not correlated with patient BCVA. Pixel density variations based on axial lens images provided by SS-OCT biometer provide reasonably accurate information for machine learning analysis to estimate patient BCVA in all types of cataracts. This study demonstrates significant linear correlation between patients' poBCVA and the algorithmic prediction, with acceptable mean prediction error.", "keywords": ["optical coherence tomography", "swept-source optical coherence", "assess linear correlation", "best-corrected visual acuity", "linear correlation", "coherence tomography", "BCVA", "swept-source optical", "optical coherence", "significant linear correlation", "BCVA prediction", "correlation", "visual acuity", "linear", "prediction", "assess linear", "best-corrected visual", "patient BCVA", "lens images", "lens"], "paper_title": "Assessing the correlation between swept-source optical coherence tomography lens density pattern analysis and best-corrected visual acuity in patients with cataracts.", "last_updated": "2023/02/04"}, {"id": "0018840793", "domain": "Cataract", "model_name": "Swindell et al.", "publication_date": "2008/11/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/18840793/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "18840793", "task": null, "abstract": "Prediction of individual life span based on characteristics evaluated at middle-age represents a challenging objective for aging research. In this study, we used machine learning algorithms to construct models that predict life span in a stock of genetically heterogeneous mice. Life-span prediction accuracy of 22 algorithms was evaluated using a cross-validation approach, in which models were trained and tested with distinct subsets of data. Using a combination of body weight and T-cell subset measures evaluated before 2 years of age, we show that the life-span quartile to which an individual mouse belongs can be predicted with an accuracy of 35.3% (+/-0.10%). This result provides a new benchmark for the development of life-span-predictive models, but improvement can be expected through identification of new predictor variables and development of computational approaches. Future work in this direction can provide tools for aging research and will shed light on associations between phenotypic traits and longevity.", "keywords": ["life span based", "individual life span", "predict life span", "life span", "based on characteristics", "middle-age represents", "represents a challenging", "challenging objective", "span based", "objective for aging", "characteristics evaluated", "Life-span prediction accuracy", "aging research", "genetically heterogeneous mice", "machine learning algorithms", "individual life", "predict life", "life", "span", "T-cell subset measures"], "paper_title": "How long will my mouse live? Machine learning approaches for prediction of mouse life span.", "last_updated": "2023/02/04"}, {"id": "0025622686", "domain": "Cataract", "model_name": "Lacombe et al.", "publication_date": "2015/09/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25622686/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "25622686", "task": null, "abstract": "Classic galactosemia is an autosomal recessive metabolic disease involving the galactose pathway, caused by the deficiency of galactose-1-phosphate uridyltransferase. Galactose accumulation induces in newborns many symptoms, such as liver disease, cataracts, and sepsis leading to death if untreated. Neonatal screening is developed and applied in many countries using several methods to detect galactose or its derived product accumulation in blood or urine. High-throughput FTIR spectroscopy was investigated as a potential tool in the current screening methods. IR spectra were obtained from blood plasma of healthy, diabetic, and galactosemic patients. The major spectral differences were in the carbohydrate region, which was first analysed in an exploratory manner using principal component analysis (PCA). PCA score plots showed a clear discrimination between diabetic and galactosemic patients and this was more marked as a function of the glucose and galactose increased concentration in these patients' plasma respectively. Then, a support vector machine leave-one-out cross-validation (SVM-LOOCV) classifier was built with the PCA scores as the input and the model was tested on median, mean and all spectra from the three population groups. This classifier was able to discriminate healthy/diabetic, healthy/galactosemic, and diabetic/galactosemic patients with sensitivity and specificity rates ranging from 80% to 94%. The total accuracy rate ranged from 87% to 96%. High-throughput FTIR spectroscopy combined with the SVM-LOOCV classification procedure appears to be a promising tool in the screening of galactosemia patients, with good sensitivity and specificity. Furthermore, this approach presents the advantages of being cost-effective, fast, and straightforward in the screening of galactosemic patients.", "keywords": ["autosomal recessive metabolic", "recessive metabolic disease", "metabolic disease involving", "autosomal recessive", "recessive metabolic", "galactosemic patients", "metabolic disease", "disease involving", "galactose pathway", "High-throughput FTIR spectroscopy", "galactosemic", "Galactose accumulation induces", "galactose", "High-throughput FTIR", "patients", "PCA", "FTIR spectroscopy", "Classic galactosemia", "screening", "diabetic"], "paper_title": "Rapid screening of classic galactosemia patients: a proof-of-concept study using high-throughput FTIR analysis of plasma.", "last_updated": "2023/02/04"}, {"id": "0036913536", "domain": "Cataract", "model_name": "Russo et al.", "publication_date": "2023/03/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36913536/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "3kPkWQNPZH", "pmid": "36913536", "task": null, "abstract": "To compare the predicted vault using machine learning with the achieved vault using the online manufacturer's nomogram in patients undergoing posterior chamber implantation with an implantable collamer lens (ICL). Centro Oculistico Bresciano, Brescia, Italy and I.R.C.C.S. - Bietti Foundation, Rome, Italy. Retrospective multicenter comparison study. This study included 561 eyes from 300 consecutive patients who underwent ICL placement surgery. All preoperative and postoperative measurements were obtained by anterior segment optical coherence tomography (AS-OCT; MS-39, C.S.O. SRL, Italy). The actual vault was quantitatively measured and compared with the predicted vault using machine learning of AS-OCT metrics. A strong correlation between model predictions and achieved vaulting was detected by random forest regression (RF; R 2 = 0.36), extra tree regression (ET; R 2 = 0.50), and extreme gradient boosting regression (R 2 = 0.39). Conversely, a high residual difference was observed between achieved vaulting values and those predicted by the multilinear regression (R 2 = 0.33) and ridge regression (R 2 = 0.33). ET and RF regressions showed significantly lower mean absolute errors and higher percentages of eyes within \u00b1250 \u00b5m of the intended ICL vault compared to the conventional nomogram (94%, 90%, and 72%, respectively; P < 0.001). ET classifiers achieved an accuracy (percentage of vault in the range of 250-750 \u00b5m) of up to 98%. Machine learning of preoperative AS-OCT metrics achieved excellent predictability of ICL vault and size, which was significantly higher than the accuracy of the online manufacturer's nomogram, providing the surgeon with a valuable aid for predicting the ICL vault.", "keywords": ["implantable collamer lens", "undergoing posterior chamber", "posterior chamber implantation", "Centro Oculistico Bresciano", "patients undergoing posterior", "collamer lens", "Oculistico Bresciano", "undergoing posterior", "posterior chamber", "chamber implantation", "implantable collamer", "ICL vault", "ICL", "vault", "Italy", "Bietti Foundation", "machine learning", "achieved", "Centro Oculistico", "patients undergoing"], "paper_title": "Predictability of the Vault after Implantable Collamer Lens Implantation Using OCT and Artificial Intelligence in Caucasian Eyes.", "last_updated": "2023/02/04"}, {"id": "0035223859", "domain": "Disc hemorrhage", "model_name": "Li et al.", "publication_date": "2022/02/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35223859/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "b7h4xyJrfF", "pmid": "35223859", "task": null, "abstract": "<b>Objectives:</b> Intervertebral disc degeneration is a progressive and chronic disease, usually manifesting as low back pain. This study aimed to screen effective biomarkers for medical practice as well as figuring out immune infiltration situations between circulation and intervertebral discs. <b>Methods:</b> Gene expression profiles of GSE124272 was included for differentially analysis, WGCNA and immune infiltration analysis from GEO database, and other GSE series were used as validation datasets. A series of validation methods were conducted to verify the robustness of hub genes, such as principal component analysis, machine learning models, and expression verification. Lastly, nomogram was established for medical practice. <b>Results:</b> 10 genes were commonly screened <i>via</i> combination of DEGs, WGCNA analysis and lipid metabolism related genes. Furthermore, 3 hub gens CYP27A1, FAR2, CYP1B1 were chosen for subsequent analysis based on validation of different methods. GSEA analysis discovered that neutrophil extracellular traps formation and NOD-like receptor signaling pathway was activated during IDD. Immune infiltration analysis demonstrated that the imbalance of neutrophils and \u03b3\u03b4T cells were significantly correlated with IDD progression. Nomogram was established based on CYP27A1, FAR2, CYP1B1 and age, the calibration plot confirmed the stability of our model. <b>Conclusion:</b> CYP27A1, FAR2, CYP1B1 were considered as hub lipid metabolism related genes (LMRGs) in the development of IDD, which were regarded as candidate diagnostic biomarkers especially in circulation. The effects are worth expected in the early diagnosis of IDD through detecting these genes in blood.", "keywords": ["low back pain", "Intervertebral disc degeneration", "chronic disease", "back pain", "progressive and chronic", "manifesting as low", "low back", "Intervertebral disc", "immune infiltration", "Objectives", "disc degeneration", "immune infiltration analysis", "analysis", "IDD", "Intervertebral", "genes", "immune infiltration situations", "infiltration", "infiltration analysis", "immune"], "paper_title": "The Roles of Blood Lipid-Metabolism Genes in Immune Infiltration Could Promote the Development of IDD.", "last_updated": "2023/02/04"}, {"id": "0030018755", "domain": "Glaucoma (unspecified)", "model_name": "An et al.", "publication_date": "2018/06/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30018755/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30018755", "task": "IWqQC1koJA", "abstract": "This study develops an objective machine-learning classification model for classifying glaucomatous optic discs and reveals the classificatory criteria to assist in clinical glaucoma management. In this study, 163 glaucoma eyes were labelled with four optic disc types by three glaucoma specialists and then randomly separated into training and test data. All the images of these eyes were captured using optical coherence tomography and laser speckle flowgraphy to quantify the ocular structure and blood-flow-related parameters. A total of 91 parameters were extracted from each eye along with the patients' background information. Machine-learning classifiers, including the neural network (NN), na\u00efve Bayes (NB), support vector machine (SVM), and gradient boosted decision trees (GBDT), were trained to build the classification models, and a hybrid feature selection method that combines minimum redundancy maximum relevance and genetic-algorithm-based feature selection was applied to find the most valid and relevant features for NN, NB, and SVM. A comparison of the performance of the three machine-learning classification models showed that the NN had the best classification performance with a validated accuracy of 87.8% using only nine ocular parameters. These selected quantified parameters enabled the trained NN to classify glaucomatous optic discs with relatively high performance without requiring color fundus images.", "keywords": ["clinical glaucoma management", "glaucomatous optic discs", "classifying glaucomatous optic", "optic disc types", "optic discs", "develops an objective", "reveals the classificatory", "classificatory criteria", "criteria to assist", "assist in clinical", "objective machine-learning classification", "glaucoma management", "study develops", "clinical glaucoma", "glaucomatous optic", "optic", "glaucoma", "objective machine-learning", "classifying glaucomatous", "classification models"], "paper_title": "Comparison of Machine-Learning Classification Models for Glaucoma Management.", "last_updated": "2023/02/04"}, {"id": "0026958253", "domain": "Glaucoma (unspecified)", "model_name": "Varnousfaderani et al.", "publication_date": "2015/11/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26958253/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "26958253", "task": null, "abstract": "Blood vessel segmentation is important for the analysis of ocular fundus images for diseases affecting vessel caliber, occlusion, leakage, inflammation, and proliferation. We introduce a novel supervised method to evaluate performance of Leung-Malik filters in delineating vessels. First, feature vectors are extracted for every pixel with respect to the response of Leung-Malik filters on green channel retinal images in different orientations and scales. A two level hierarchical learning framework is proposed to segment vessels in retinal images with confounding disease abnormalities. In the first level, three expert classifiers are trained to delineate 1) vessels, 2) background, and 3) retinal pathologies including abnormal pathologies such as lesions and anatomical structures such as optic disc. In the second level, a new classifier is trained to detect vessels and non-vessel pixels based on results of the expert classifiers. Qualitative evaluation shows the effectiveness of the proposed expert classifiers in modeling retinal pathologies. Quantitative results on two standard datasets STARE (AUC = 0.971, Acc=0.927) and DRIVE (AUC = 0.955, Acc =0.903) are comparable with other state-of-the-art vessel segmentation methods.", "keywords": ["ocular fundus images", "affecting vessel caliber", "analysis of ocular", "ocular fundus", "Blood vessel segmentation", "diseases affecting vessel", "Leung-Malik filters", "fundus images", "Blood vessel", "vessel caliber", "diseases affecting", "retinal", "expert classifiers", "AUC", "Acc", "vessels", "affecting vessel", "occlusion", "leakage", "inflammation"], "paper_title": "Vessel Delineation in Retinal Images using Leung-Malik filters and Two Levels Hierarchical Learning.", "last_updated": "2023/02/04"}, {"id": "0035450069", "domain": "Uveitis", "model_name": "Venerito et al.", "publication_date": "2022/04/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35450069/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "35450069", "task": null, "abstract": "Inferential statistical methods failed in identifying reliable biomarkers and risk factors for relapsing giant cell arteritis (GCA) after glucocorticoids (GCs) tapering. A ML approach allows to handle complex non-linear relationships between patient attributes that are hard to model with traditional statistical methods, merging them to output a forecast or a probability for a given outcome. The objective of the study was to assess whether ML algorithms can predict GCA relapse after GCs tapering. GCA patients who underwent GCs therapy and regular follow-up visits for at least 12 months, were retrospectively analyzed and used for implementing 3 ML algorithms, namely, Logistic Regression (LR), Decision Tree (DT), and Random Forest (RF). The outcome of interest was disease relapse within 3 months during GCs tapering. After a ML variable selection method, based on a XGBoost wrapper, an attribute core set was used to train and test each algorithm using 5-fold cross-validation. The performance of each algorithm in both phases was assessed in terms of accuracy and area under receiver operating characteristic curve (AUROC). The dataset consisted of 107 GCA patients (73 women, 68.2%) with mean age ( \u00b1 SD) 74.1 ( \u00b1 8.5) years at presentation. GCA flare occurred in 40/107 patients (37.4%) within 3 months after GCs tapering. As a result of ML wrapper, the attribute core set with the least number of variables used for algorithm training included presence/absence of diabetes mellitus and concomitant polymyalgia rheumatica as well as erythrocyte sedimentation rate level at GCs baseline. RF showed the best performance, being significantly superior to other algorithms in accuracy (RF 71.4% vs LR 70.4% vs DT 62.9%). Consistently, RF precision (72.1%) was significantly greater than those of LR (62.6%) and DT (50.8%). Conversely, LR was superior to RF and DT in recall (RF 60% vs LR 62.5% vs DT 47.5%). Moreover, RF AUROC (0.76) was more significant compared to LR (0.73) and DT (0.65). RF algorithm can predict GCA relapse after GCs tapering with sufficient accuracy. To date, this is one of the most accurate predictive modelings for such outcome. This ML method represents a reproducible tool, capable of supporting clinicians in GCA patient management.", "keywords": ["giant cell arteritis", "identifying reliable biomarkers", "relapsing giant cell", "Inferential statistical methods", "GCA", "statistical methods failed", "cell arteritis", "GCs tapering", "failed in identifying", "identifying reliable", "reliable biomarkers", "biomarkers and risk", "risk factors", "factors for relapsing", "relapsing giant", "giant cell", "GCs", "predict GCA relapse", "Inferential statistical", "GCA relapse"], "paper_title": "Validity of Machine Learning in Predicting Giant Cell Arteritis Flare After Glucocorticoids Tapering.", "last_updated": "2023/02/04"}, {"id": "0033476272", "domain": "Uveitis", "model_name": "Isik et al.", "publication_date": "2022/06/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33476272/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "33476272", "task": "IWqQC1koJA", "abstract": "Beh\u00e7et's Disease (BD) is a multi-system inflammatory disorder in which the etiology remains unclear. The most probable hypothesis is that genetic tendency and environmental factors play roles in the development of BD. In order to find the essential reasons, genetic changes on thousands of genes should be analyzed. Besides, there is a need for extra analysis to find out which genetic factor affects the disease. Machine learning approaches have high potential for extracting the knowledge from genomics and selecting the representative Single Nucleotide Polymorphisms (SNPs) as the most effective features for the clinical diagnosis process. In this study, we have attempted to identify representative SNPs using feature selection methods, incorporating biological information and aimed to develop a machine-learning model for diagnosing Beh\u00e7et's disease. By combining biological information and machine learning classifiers, up to 99.64 percent accuracy of disease prediction is achieved using only 13,611 out of 311,459 SNPs. In addition, we revealed the SNPs that are most distinctive by performing repeated feature selection in cross-validation experiments.", "keywords": ["etiology remains unclear", "multi-system inflammatory disorder", "remains unclear", "multi-system inflammatory", "inflammatory disorder", "etiology remains", "Single Nucleotide Polymorphisms", "Beh\u00e7et Disease", "representative Single Nucleotide", "Disease", "diagnosing Beh\u00e7et disease", "environmental factors play", "factors play roles", "Nucleotide Polymorphisms", "SNPs", "genetic", "Single Nucleotide", "biological information", "unclear", "feature selection"], "paper_title": "The Determination of Distinctive Single Nucleotide Polymorphism Sets for the Diagnosis of Beh\u00e7et's Disease.", "last_updated": "2023/02/04"}, {"id": "0032662400", "domain": "Uveitis", "model_name": "Joo et al.", "publication_date": "2020/07/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32662400/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "32662400", "task": "0GPcU42tzV", "abstract": "Prediction and determination of drug efficacy for radiographic progression is limited by the heterogeneity inherent in axial spondyloarthritis (axSpA). We investigated whether unbiased clustering analysis of phenotypic data can lead to coherent subgroups of axSpA patients with a distinct risk of radiographic progression. A group of 412 patients with axSpA was clustered in an unbiased way using a agglomerative hierarchical clustering method, based on their phenotype mapping. We used a generalised linear model, na\u00efve Bayes, Decision Trees, K-Nearest-Neighbors, and Support Vector Machines to construct a consensus classification method. Radiographic progression over 2 years was assessed using the modified Stoke Ankylosing Spondylitis Spine Score (mSASSS). axSpA patients were classified into three distinct subgroups with distinct clinical characteristics. Sex, smoking, HLA-B27, baseline mSASSS, uveitis, and peripheral arthritis were the key features that were found to stratifying the phenogroups. The three phenogroups showed distinct differences in radiographic progression rate (p<0.05) and the proportion of progressors (p<0.001). Phenogroup 2, consisting of male smokers, had the worst radiographic progression, while phenogroup 3, exclusively suffering from uveitis, showed the least radiographic progression. The axSpA phenogroup classification, including its ability to stratify risk, was successfully replicated in an independent validation group. Phenotype mapping results in a clinically relevant classification of axSpA that is applicable for risk stratification. Novel coupling between phenotypic features and radiographic progression can provide a glimpse into the mechanisms underlying divergent and shared features of axSpA.", "keywords": ["radiographic progression", "Prediction and determination", "axial spondyloarthritis", "Support Vector Machines", "determination of drug", "drug efficacy", "heterogeneity inherent", "inherent in axial", "radiographic", "progression", "Spondylitis Spine Score", "axSpA", "Stoke Ankylosing Spondylitis", "Ankylosing Spondylitis Spine", "Decision Trees", "axSpA patients", "Support Vector", "Vector Machines", "patients", "Spine Score"], "paper_title": "Novel classification of axial spondyloarthritis to predict radiographic progression using machine learning.", "last_updated": "2023/02/04"}, {"id": "0037000839", "domain": "Uveitis", "model_name": "Kennedy et al.", "publication_date": "2023/03/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/37000839/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "fz4gI1yKi8", "pmid": "37000839", "task": "IWqQC1koJA", "abstract": "Ankylosing spondylitis is the second most common cause of inflammatory arthritis. However, a successful diagnosis can take a decade to confirm from symptom onset (via x-rays). The aim of this study was to use machine learning methods to develop a profile of the characteristics of people who are likely to be given a diagnosis of AS in future. The Secure Anonymised Information Linkage databank was used. Patients with ankylosing spondylitis were identified using their routine data and matched with controls who had no record of a diagnosis of ankylosing spondylitis or axial spondyloarthritis. Data was analysed separately for men and women. The model was developed using feature/variable selection and principal component analysis to develop decision trees. The decision tree with the highest average F value was selected and validated with a test dataset. The model for men indicated that lower back pain, uveitis, and NSAID use under age 20 is associated with AS development. The model for women showed an older age of symptom presentation compared to men with back pain and multiple pain relief medications. The models showed good prediction (positive predictive value 70%-80%) in test data but in the general population where prevalence is very low (0.09% of the population in this dataset) the positive predictive value would be very low (0.33%-0.25%). Machine learning can be used to help profile and understand the characteristics of people who will develop AS, and in test datasets with artificially high prevalence, will perform well. However, when applied to a general population with low prevalence rates, such as that in primary care, the positive predictive value for even the best model would be 1.4%. Multiple models may be needed to narrow down the population over time to improve the predictive value and therefore reduce the time to diagnosis of ankylosing spondylitis.", "keywords": ["Ankylosing spondylitis", "inflammatory arthritis", "Secure Anonymised Information", "Anonymised Information Linkage", "diagnosis", "Ankylosing", "positive predictive", "Information Linkage databank", "spondylitis", "model", "predictive", "Secure Anonymised", "Anonymised Information", "Information Linkage", "population", "diagnosis of ankylosing", "develop", "data", "positive", "test"], "paper_title": "Predicting a diagnosis of ankylosing spondylitis using primary care health records-A machine learning approach.", "last_updated": "2023/02/04"}, {"id": "0036803463", "domain": "Uveitis", "model_name": "BD", "publication_date": "2023/02/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36803463/", "code_link": "https://github.com/aly202012/beh-et-s-disease-with-machine-learning", "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "fz4gI1yKi8", "pmid": "36803463", "task": "IWqQC1koJA", "abstract": "Eye lesions, occur in nearly half of patients with Beh\u00e7et's Disease (BD), can lead to irreversible damage and vision loss; however, limited studies are available on identifying risk factors for the development of vision-threatening BD (VTBD). Using an Egyptian college of rheumatology (ECR)-BD, a national cohort of BD patients, we examined the performance of machine-learning (ML) models in predicting VTBD compared to logistic regression (LR) analysis. We identified the risk factors for the development of VTBD. Patients with complete ocular data were included. VTBD was determined by the presence of any retinal disease, optic nerve involvement, or occurrence of blindness. Various ML-models were developed and examined for VTBD prediction. The Shapley additive explanation value was used for the interpretability of the predictors. A total of 1094 BD patients [71.5% were men, mean\u2009\u00b1\u2009SD age 36.1\u2009\u00b1\u200910\u00a0years] were included. 549 (50.2%) individuals had VTBD. Extreme Gradient Boosting was the best-performing ML model (AUROC 0.85, 95% CI 0.81, 0.90) compared with logistic regression (AUROC 0.64, 95%CI 0.58, 0.71). Higher disease activity, thrombocytosis, ever smoking, and daily steroid dose were the top factors associated with VTBD. Using information obtained in the clinical settings, the Extreme Gradient Boosting identified patients at higher risk of VTBD better than the conventional statistical method. Further longitudinal studies to evaluate the clinical utility of the proposed prediction model\u00a0are needed.", "keywords": ["Beh\u00e7et Disease", "identifying risk factors", "VTBD", "Eye lesions", "vision loss", "lead to irreversible", "irreversible damage", "damage and vision", "Extreme Gradient Boosting", "development of vision-threatening", "patients with Beh\u00e7et", "patients", "risk factors", "Beh\u00e7et", "Gradient Boosting", "identifying risk", "predicting VTBD compared", "Extreme Gradient", "Egyptian college", "Boosting identified patients"], "paper_title": "Development of machine learning models for detection of vision threatening Beh\u00e7et's disease (BD) using Egyptian College of Rheumatology (ECR)-BD cohort.", "last_updated": "2023/02/04"}, {"id": "0036554021", "domain": "Glaucoma (unspecified)", "model_name": "Kashyap et al.", "publication_date": "2022/12/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36554021/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36554021", "task": "IWqQC1koJA", "abstract": "Glaucoma is prominent in a variety of nations, with the United States and Europe being two of the most famous. Glaucoma now affects around 78 million people throughout the world (2020). By the year 2040, it is expected that there will be 111.8 million cases of glaucoma worldwide. In countries that are still building enough healthcare infrastructure to cope with glaucoma, the ailment is misdiagnosed nine times out of ten. To aid in the early diagnosis of glaucoma, the creation of a detection system is necessary. In this work, the researchers propose using a technology known as deep learning to identify and predict glaucoma before symptoms appear. The glaucoma dataset is used in this deep learning algorithm that has been proposed for analyzing glaucoma images. To get the required results when using deep learning principles for the job of segmenting the optic cup, pretrained transfer learning models are integrated with the U-Net architecture. For feature extraction, the DenseNet-201 deep convolution neural network (DCNN) is used. The DCNN approach is used to determine whether a person has glaucoma. The fundamental goal of this line of research is to recognize glaucoma in retinal fundus images, which will aid in assessing whether a patient has the condition. Because glaucoma can affect the model in both positive and negative ways, the model's outcome might be either positive or negative. Accuracy, precision, recall, specificity, the F-measure, and the F-score are some of the metrics used in the model evaluation process. An extra comparison study is performed as part of the process of establishing whether the suggested model is accurate. The findings are compared to convolution neural network classification methods based on deep learning. When used for training, the suggested model has an accuracy of 98.82 percent and an accuracy of 96.90 percent when used for testing. All assessments show that the new paradigm that has been proposed is more successful than the one that is currently in use.", "keywords": ["United States", "States and Europe", "Glaucoma", "variety of nations", "deep learning", "United", "States", "Europe", "learning", "deep", "model", "suggested model", "deep learning algorithm", "deep learning principles", "Accuracy", "learning models", "million", "DCNN", "transfer learning models", "million people"], "paper_title": "Glaucoma Detection and Classification Using Improved U-Net Deep Learning Model.", "last_updated": "2023/02/04"}, {"id": "0035222876", "domain": "Glaucoma (unspecified)", "model_name": "Sudhan et al.", "publication_date": "2022/02/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35222876/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35222876", "task": "aipbNdPTIt", "abstract": "Glaucoma is the second most common cause for blindness around the world and the third most common in Europe and the USA. Around 78 million people are presently living with glaucoma (2020). It is expected that 111.8 million people will have glaucoma by the year 2040. 90% of glaucoma is undetected in developing nations. It is essential to develop a glaucoma detection system for early diagnosis. In this research, early prediction of glaucoma using deep learning technique is proposed. In this proposed deep learning model, the ORIGA dataset is used for the evaluation of glaucoma images. The U-Net architecture based on deep learning algorithm is implemented for optic cup segmentation and a pretrained transfer learning model; DenseNet-201 is used for feature extraction along with deep convolution neural network (DCNN). The DCNN approach is used for the classification, where the final results will be representing whether the glaucoma infected or not. The primary objective of this research is to detect the glaucoma using the retinal fundus images, which can be useful to determine if the patient was affected by glaucoma or not. The result of this model can be positive or negative based on the outcome detected as infected by glaucoma or not. The model is evaluated using parameters such as accuracy, precision, recall, specificity, and F-measure. Also, a comparative analysis is conducted for the validation of the model proposed. The output is compared to other current deep learning models used for CNN classification, such as VGG-19, Inception ResNet, ResNet 152v2, and DenseNet-169. The proposed model achieved 98.82% accuracy in training and 96.90% in testing. Overall, the performance of the proposed model is better in all the analysis.", "keywords": ["common in Europe", "Glaucoma", "deep learning", "model", "USA", "common", "Europe", "learning", "million people", "learning model", "deep", "deep learning model", "proposed", "proposed model", "proposed deep learning", "DCNN", "million", "glaucoma images", "transfer learning model", "deep learning technique"], "paper_title": "Segmentation and Classification of Glaucoma Using U-Net with Deep Learning Model.", "last_updated": "2023/02/04"}, {"id": "0035595784", "domain": "Glaucoma (unspecified)", "model_name": "Sharma et al.", "publication_date": "2022/05/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35595784/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35595784", "task": "aipbNdPTIt", "abstract": "Detection, diagnosis, and treatment of ophthalmic diseases depend on extraction of information (features and/or their dimensions) from the images. Deep learning (DL) model are crucial for the automation of it. Here, we report on the development of a lightweight DL model, which can precisely segment/detect the required features automatically. The model utilizes dimensionality reduction of image to extract important features, and channel contraction to allow only the required high-level features necessary for reconstruction of segmented feature image. Performance of present model in detection of glaucoma from optical coherence tomography angiography (OCTA) images of retina is high (area under the receiver-operator characteristic curve AUC\u2009~\u20090.81). Bland-Altman analysis gave exceptionally low bias (~\u20090.00185), and high Pearson's correlation coefficient (p\u2009=\u20090.9969) between the parameters determined from manual and DL based segmentation. On the same dataset, bias is an order of magnitude higher (~\u20090.0694, p\u2009=\u20090.8534) for commercial software. Present model is 10 times lighter than Unet (popular for biomedical image segmentation) and have a better segmentation accuracy and model training reproducibility (based on the analysis of 3670 OCTA images). High dice similarity coefficient (D) for variety of ophthalmic images suggested it's wider scope in precise segmentation of images even from other fields. Our concept of channel narrowing is not only important for the segmentation problems, but it can also reduce number of parameters significantly in object classification models. Enhanced disease diagnostic accuracy can be achieved for the resource limited devices (such as mobile phone, Nvidia's Jetson, Raspberry pi) used in self-monitoring, and tele-screening (memory size of trained model\u2009~\u200935\u00a0MB).", "keywords": ["extraction of information", "depend on extraction", "model", "ophthalmic diseases depend", "images", "features", "segmentation", "treatment of ophthalmic", "diseases depend", "present model", "required features automatically", "OCTA images", "image", "segmented feature image", "OCTA", "required high-level features", "diagnosis", "high", "feature image", "information"], "paper_title": "A lightweight deep learning model for automatic segmentation and analysis of ophthalmic images.", "last_updated": "2023/02/04"}, {"id": "0035577876", "domain": "Glaucoma (unspecified)", "model_name": "Akter et al.", "publication_date": "2022/05/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35577876/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35577876", "task": "IWqQC1koJA", "abstract": "In this study, we aimed to facilitate the current diagnostic assessment of glaucoma by analyzing multiple features and introducing a new cross-sectional optic nerve head (ONH) feature from optical coherence tomography (OCT) images. The data (n\u2009=\u2009100 for both glaucoma and control) were collected based on structural, functional, demographic and risk factors. The features were statistically analyzed, and the most significant four features were used to train machine learning (ML) algorithms. Two ML algorithms: deep learning (DL) and logistic regression (LR) were compared in terms of the classification accuracy for automated glaucoma detection. The performance of the ML models was evaluated on unseen test data, n\u2009=\u200955. An image segmentation pilot study was then performed on cross-sectional OCT scans. The ONH cup area was extracted, analyzed, and a new DL model was trained for glaucoma prediction. The DL model was estimated using five-fold cross-validation and compared with two pre-trained models. The DL model trained from the optimal features achieved significantly higher diagnostic performance (area under the receiver operating characteristic curve (AUC) 0.98 and accuracy of 97% on validation data and 96% on test data) compared to previous studies for automated glaucoma detection. The second DL model used in the pilot study also showed promising outcomes (AUC 0.99 and accuracy of 98.6%) to detect glaucoma compared to two pre-trained models. In combination, the result of the two studies strongly suggests the four features and the cross-sectional ONH cup area trained using deep learning have a great potential for use as an initial screening tool for glaucoma which will assist clinicians in making a precise decision.", "keywords": ["optic nerve head", "optical coherence tomography", "analyzing multiple features", "cross-sectional optic nerve", "current diagnostic assessment", "nerve head", "coherence tomography", "aimed to facilitate", "facilitate the current", "analyzing multiple", "optic nerve", "optical coherence", "glaucoma", "ONH cup area", "features", "automated glaucoma detection", "multiple features", "model", "ONH cup", "ONH"], "paper_title": "Glaucoma diagnosis using multi-feature analysis and a deep learning technique.", "last_updated": "2023/02/04"}, {"id": "0035551345", "domain": "Glaucoma (unspecified)", "model_name": "Noury et al.", "publication_date": "2022/05/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35551345/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35551345", "task": "IWqQC1koJA", "abstract": "To develop a three-dimensional (3D) deep learning algorithm to detect glaucoma using spectral-domain optical coherence tomography (SD-OCT) optic nerve head (ONH) cube scans and validate its performance on ethnically diverse real-world datasets and on cropped ONH scans. In total, 2461 Cirrus SD-OCT ONH scans of 1012 eyes were obtained from the Glaucoma Clinic Imaging Database at the Byers Eye Institute, Stanford University, from March 2010 to December 2017. A 3D deep neural network was trained and tested on this unique raw OCT cube dataset to identify a multimodal definition of glaucoma excluding other concomitant retinal disease and optic neuropathies. A total of 1022 scans of 363 glaucomatous eyes (207 patients) and 542 scans of 291 normal eyes (167 patients) from Stanford were included in training, and 142 scans of 48 glaucomatous eyes (27 patients) and 61 scans of 39 normal eyes (23 patients) were included in the validation set. A total of 3371 scans (Cirrus SD-OCT) from four different countries were used for evaluation of the model: the non overlapping test dataset from Stanford (USA) consisted of 694 scans: 241 scans from 113 normal eyes of 66 patients and 453 scans of 157 glaucomatous eyes of 89 patients. The datasets from Hong Kong (total of 1625 scans; 666 OCT scans from 196 normal eyes of 99 patients and 959 scans of 277 glaucomatous eyes of 155 patients), India (total of 672 scans; 211 scans from 147 normal eyes of 98 patients and 461 scans from 171 glaucomatous eyes of 101 patients), and Nepal (total of 380 scans; 158 scans from 143 normal eyes of 89 patients and 222 scans from 174 glaucomatous eyes of 109 patients) were used for external evaluation. The performance of the model was then evaluated on manually cropped scans from Stanford using a new algorithm called DiagFind. The ONH region was cropped by identifying the appropriate zone of the image in the expected location relative to Bruch's Membrane Opening (BMO) using a commercially available imaging software. Subgroup analyses were performed in groups stratified by eyes, myopia severity of glaucoma, and on a set of glaucoma cases without field defects. Saliency maps were generated to highlight the areas the model used to make a prediction. The model's performance was compared to that of a glaucoma specialist using all available information on a subset of cases. The 3D deep learning system achieved area under the curve (AUC) values of 0.91 (95% CI, 0.90-0.92), 0.80 (95% CI, 0.78-0.82), 0.94 (95% CI, 0.93-0.96), and 0.87 (95% CI, 0.85-0.90) on Stanford, Hong Kong, India, and Nepal datasets, respectively, to detect perimetric glaucoma and AUC values of 0.99 (95% CI, 0.97-1.00), 0.96 (95% CI, 0.93-1.00), and 0.92 (95% CI, 0.89-0.95) on severe, moderate, and mild myopia cases, respectively, and an AUC of 0.77 on cropped scans. The model achieved an AUC value of 0.92 (95% CI, 0.90-0.93) versus that of the human grader with an AUC value of 0.91 on the same subset of scans (\\(P=0.99\\)). The performance of the model in terms of recall on glaucoma cases without field defects was found to be 0.76 (0.68-0.85). Saliency maps highlighted the lamina cribrosa in glaucomatous eyes versus superficial retina in normal eyes as the regions associated with classification. A 3D convolutional neural network (CNN) trained on SD-OCT ONH cubes can distinguish glaucoma from normal cases in diverse datasets obtained from four different countries. The model trained on additional random cropping data augmentation performed reasonably on manually cropped scans, indicating the importance of lamina cribrosa in glaucoma detection. A 3D CNN trained on SD-OCT ONH cubes was developed to detect glaucoma in diverse datasets obtained from four different countries and on cropped scans. The model identified lamina cribrosa as the region associated with glaucoma detection.", "keywords": ["scans", "Byers Eye Institute", "normal eyes", "glaucomatous eyes", "eyes", "patients", "SD-OCT ONH scans", "Clinic Imaging Database", "optical coherence tomography", "Cirrus SD-OCT ONH", "cropped ONH scans", "ONH scans", "cropped scans", "spectral-domain optical coherence", "SD-OCT ONH", "glaucoma", "SD-OCT ONH cubes", "ONH", "optic nerve head", "Stanford"], "paper_title": "Deep Learning for Glaucoma Detection and Identification of Novel Diagnostic Areas in Diverse Real-World Datasets.", "last_updated": "2023/02/04"}, {"id": "0031647449", "domain": "Glaucoma (unspecified)", "model_name": "Liao et al.", "publication_date": "2019/10/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31647449/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31647449", "task": "IWqQC1koJA", "abstract": "Despite the potential to revolutionise disease diagnosis by performing data-driven classification, clinical interpretability of ConvNet remains challenging. In this paper, a novel clinical interpretable ConvNet architecture is proposed not only for accurate glaucoma diagnosis but also for the more transparent interpretation by highlighting the distinct regions recognised by the network. To the best of our knowledge, this is the first work of providing the interpretable diagnosis of glaucoma with the popular deep learning model. We propose a novel scheme for aggregating features from different scales to promote the performance of glaucoma diagnosis, which we refer to as M-LAP. Moreover, by modelling the correspondence from binary diagnosis information to the spatial pixels, the proposed scheme generates glaucoma activations, which bridge the gap between global semantical diagnosis and precise location. In contrast to previous works, it can discover the distinguish local regions in fundus images as evidence for clinical interpretable glaucoma diagnosis. Experimental results, performed on the challenging ORIGA datasets, show that our method on glaucoma diagnosis outperforms state-of-the-art methods with the highest AUC (0.88). Remarkably, the extensive results, optic disc segmentation (dice of 0.9) and local disease focus localization based on the evidence map, demonstrate the effectiveness of our methods on clinical interpretability.", "keywords": ["performing data-driven classification", "glaucoma diagnosis", "ConvNet remains challenging", "revolutionise disease diagnosis", "data-driven classification", "clinical interpretable ConvNet", "interpretable glaucoma diagnosis", "potential to revolutionise", "performing data-driven", "diagnosis", "clinical interpretable glaucoma", "interpretable ConvNet architecture", "ConvNet remains", "accurate glaucoma diagnosis", "glaucoma", "clinical interpretable", "glaucoma diagnosis outperforms", "clinical", "interpretable ConvNet", "clinical interpretability"], "paper_title": "Clinical Interpretable Deep Learning Model for Glaucoma Diagnosis.", "last_updated": "2023/02/04"}, {"id": "0033820457", "domain": "Glaucoma (unspecified)", "model_name": "Cho et al.", "publication_date": "2021/04/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33820457/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33820457", "task": "IWqQC1koJA", "abstract": "<b>Purpose</b>: This study developed and evaluated a deep learning ensemble method to automatically grade the stages of glaucoma depending on its severity.<b>Materials and Methods</b>: After cross-validation of three glaucoma specialists, the final dataset comprised of 3,460 fundus photographs taken from 2,204 patients were divided into three classes: unaffected controls, early-stage glaucoma, and late-stage glaucoma. The mean deviation value of standard automated perimetry was used to classify the glaucoma cases. We modeled 56 convolutional neural networks (CNN) with different characteristics and developed an ensemble system to derive the best performance by combining several modeling results.<b>Results</b>: The proposed method with an accuracy of 88.1% and an average area under the receiver operating characteristic of 0.975 demonstrates significantly better performance to classify glaucoma stages compared to the best single CNN model that has an accuracy of 85.2% and an average area under the receiver operating characteristic of 0.950. The false negative is the least adjacent misprediction, and it is less in the proposed method than in the best single CNN model.<b>Conclusions</b>: The method of averaging multiple CNN models can better classify glaucoma stages by using fundus photographs than a single CNN model. The ensemble method would be useful as a clinical decision support system in glaucoma screening for primary care because it provides high and stable performance with a relatively small amount of data.", "keywords": ["final dataset comprised", "deep learning ensemble", "single CNN model", "single CNN", "learning ensemble method", "classify glaucoma stages", "unaffected controls", "patients were divided", "evaluated a deep", "deep learning", "automatically grade", "final dataset", "dataset comprised", "CNN model", "CNN", "glaucoma", "receiver operating characteristic", "multiple CNN models", "glaucoma specialists", "early-stage glaucoma"], "paper_title": "Deep Learning Ensemble Method for Classifying Glaucoma Stages Using Fundus Photographs and Convolutional Neural Networks.", "last_updated": "2023/02/04"}, {"id": "0035683577", "domain": "Glaucoma (unspecified)", "model_name": "Shin et al.", "publication_date": "2022/06/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35683577/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35683577", "task": "IWqQC1koJA", "abstract": "In this retrospective, comparative study, we evaluated and compared the performance of two confocal imaging modalities in detecting glaucoma based on a deep learning (DL) classifier: ultra-wide-field (UWF) fundus imaging and true-colour confocal scanning. A total of 777 eyes, including 273 normal control eyes and 504 glaucomatous eyes, were tested. A convolutional neural network was used for each true-colour confocal scan (Eidon AF\u2122, CenterVue, Padova, Italy) and UWF fundus image (Optomap\u2122, Optos PLC, Dunfermline, UK) to detect glaucoma. The diagnostic model was trained using 545 training and 232 test images. The presence of glaucoma was determined, and the accuracy and area under the receiver operating characteristic curve (AUC) metrics were assessed for diagnostic power comparison. DL-based UWF fundus imaging achieved an AUC of 0.904 (95% confidence interval (CI): 0.861\u22120.937) and accuracy of 83.62%. In contrast, DL-based true-colour confocal scanning achieved an AUC of 0.868 (95% CI: 0.824\u22120.912) and accuracy of 81.46%. Both DL-based confocal imaging modalities showed no significant differences in their ability to diagnose glaucoma (p = 0.135) and were comparable to the traditional optical coherence tomography parameter-based methods (all p > 0.005). Therefore, using a DL-based algorithm on true-colour confocal scanning and UWF fundus imaging, we confirmed that both confocal fundus imaging techniques had high value in diagnosing glaucoma.", "keywords": ["UWF fundus imaging", "UWF fundus", "detecting glaucoma based", "comparative study", "deep learning", "UWF fundus image", "fundus imaging", "evaluated and compared", "compared the performance", "true-colour confocal scanning", "true-colour confocal", "UWF", "DL-based UWF fundus", "confocal", "fundus", "imaging", "Optos PLC", "confocal imaging modalities", "confocal scanning", "AUC"], "paper_title": "Comparison between Deep-Learning-Based Ultra-Wide-Field Fundus Imaging and True-Colour Confocal Scanning for Diagnosing Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0036507529", "domain": "Glaucoma (unspecified)", "model_name": "Seo et al.", "publication_date": "2022/11/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36507529/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36507529", "task": "IWqQC1koJA", "abstract": "We aimed to investigate the performance of a deep learning model to discriminate early normal-tension glaucoma (NTG) from glaucoma suspect (GS) eyes using Bruch's membrane opening (BMO)-based optic disc photography. 501 subjects in total were included in this cross-sectional study, including 255 GS eyes and 246 eyes of early NTG patients. BMO-based optic disc photography (BMO overview) was obtained from spectral-domain optical coherence tomography (OCT). The convolutional neural networks (CNN) model built from scratch was used to classify between early NTG and GS. For diagnostic performances of the model, the accuracy and the area under the curve (AUC) of the receiver operating characteristic curve (ROC) were evaluated in the test set. The baseline demographics were age, 48.01 \u00b1 13.03 years in GS, 54.48 \u00b1 11.28 years in NTG (<i>p</i> = 0.000); mean deviation, -0.73 \u00b1 2.10 dB in GS, -2.80 \u00b1 2.40 dB in NTG (<i>p</i> = 0.000); and intraocular pressure, 14.92 \u00b1 2.62 mmHg in GS, 14.79 \u00b1 2.61 mmHg in NTG (<i>p</i> = 0.624). Our CNN model showed the mean AUC of 0.94 (0.83-1.00) and the mean accuracy of 0.91 (0.82-0.98) with 10-fold cross validation for discriminating between early NTG and GS. The performance of the CNN model using BMO-based optic disc photography was considerably good in classifying early NTG from GS. This new disc photography of BMO overview can aid in the diagnosis of early glaucoma.", "keywords": ["Bruch membrane opening", "Bruch membrane", "discriminate early normal-tension", "early NTG", "deep learning model", "optic disc photography", "NTG", "based optic disc", "early NTG patients", "membrane opening", "disc photography", "aimed to investigate", "deep learning", "optic disc", "BMO-based optic disc", "early normal-tension glaucoma", "eyes using Bruch", "BMO overview", "early", "based optic"], "paper_title": "Deep learning classification of early normal-tension glaucoma and glaucoma suspect eyes using Bruch's membrane opening-based disc photography.", "last_updated": "2023/02/04"}, {"id": "0030481205", "domain": "Glaucoma (unspecified)", "model_name": "Ahn et al.", "publication_date": "2018/11/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30481205/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30481205", "task": "IWqQC1koJA", "abstract": "To build a deep learning model to diagnose glaucoma using fundus photography. Cross sectional case study Subjects, Participants and Controls: A total of 1,542 photos (786 normal controls, 467 advanced glaucoma and 289 early glaucoma patients) were obtained by fundus photography. The whole dataset of 1,542 images were split into 754 training, 324 validation and 464 test datasets. These datasets were used to construct simple logistic classification and convolutional neural network using Tensorflow. The same datasets were used to fine tune pre-trained GoogleNet Inception v3 model. The simple logistic classification model showed a training accuracy of 82.9%, validation accuracy of 79.9% and test accuracy of 77.2%. Convolutional neural network achieved accuracy and area under the receiver operating characteristic curve (AUROC) of 92.2% and 0.98 on the training data, 88.6% and 0.95 on the validation data, and 87.9% and 0.94 on the test data. Transfer-learned GoogleNet Inception v3 model achieved accuracy and AUROC of 99.7% and 0.99 on training data, 87.7% and 0.95 on validation data, and 84.5% and 0.93 on test data. Both advanced and early glaucoma could be correctly detected via machine learning, using only fundus photographs. Our new model that is trained using convolutional neural network is more efficient for the diagnosis of early glaucoma than previously published models.", "keywords": ["fundus photography", "build a deep", "data", "convolutional neural network", "case study Subjects", "deep learning model", "accuracy", "convolutional neural", "neural network", "glaucoma", "training data", "early glaucoma", "validation data", "test data", "GoogleNet Inception", "model", "diagnose glaucoma", "fundus", "Participants and Controls", "study Subjects"], "paper_title": "A deep learning model for the detection of both advanced and early glaucoma using fundus photography.", "last_updated": "2023/02/04"}, {"id": "0036280089", "domain": "Glaucoma (unspecified)", "model_name": "Xue et al.", "publication_date": "2022/10/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36280089/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36280089", "task": "IWqQC1koJA", "abstract": "Glaucoma is the leading cause of irreversible blindness, and the early detection and timely treatment are essential for glaucoma management. However, due to the interindividual variability in the characteristics of glaucoma onset, a single feature is not yet sufficient for monitoring glaucoma progression in isolation. There is an urgent need to develop more comprehensive diagnostic methods with higher accuracy. In this study, we proposed a multi- feature deep learning (MFDL) system based on intraocular pressure (IOP), color fundus photograph (CFP) and visual field (VF) to classify the glaucoma into four severity levels. We designed a three-phase framework for glaucoma severity diagnosis from coarse to fine, which contains screening, detection and classification. We trained it on 6,131 samples from 3,324 patients and tested it on independent 240 samples from 185 patients. Our results show that MFDL achieved a higher accuracy of 0.842 (95\u00a0% CI, 0.795-0.888) than the direct four classification deep learning (DFC-DL, accuracy of 0.513 [0.449-0.576]), CFP-based single-feature deep learning (CFP-DL, accuracy of 0.483 [0.420-0.547]) and VF-based single-feature deep learning (VF-DL, accuracy of 0.725 [0.668-0.782]). Its performance was statistically significantly superior to that of 8 juniors. It also outperformed 3 seniors and 1 expert, and was comparable with 2 glaucoma experts (0.842 vs 0.854, p\u00a0=\u00a00.663; 0.842 vs 0.858, p\u00a0=\u00a00.580). With the assistance of MFDL, junior ophthalmologists achieved statistically significantly higher accuracy performance, with the increased accuracy ranged from 7.50\u00a0% to 17.9\u00a0%, and that of seniors and experts were 6.30\u00a0% to 7.50\u00a0% and 5.40\u00a0% to 7.50\u00a0%. The mean diagnosis time per patient of MFDL was 5.96\u00a0s. The proposed model can potentially assist ophthalmologists in efficient and accurate glaucoma diagnosis that could aid the clinical management of glaucoma.", "keywords": ["deep learning", "irreversible blindness", "timely treatment", "treatment are essential", "Glaucoma", "accuracy", "single-feature deep learning", "MFDL", "learning", "higher accuracy", "feature deep learning", "early detection", "detection and timely", "deep", "classification deep learning", "single-feature deep", "higher", "blindness", "monitoring glaucoma progression", "diagnosis"], "paper_title": "A multi-feature deep learning system to enhance glaucoma severity diagnosis with high accuracy and fast speed.", "last_updated": "2023/02/04"}, {"id": "0036410708", "domain": "Glaucoma (unspecified)", "model_name": "Mariottoni et al.", "publication_date": "2022/11/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36410708/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36410708", "task": "0GPcU42tzV", "abstract": "To develop and validate a deep learning (DL) model for detection of glaucoma progression using spectral-domain (SD)-OCT measurements of retinal nerve fiber layer (RNFL) thickness. Retrospective cohort study. A total of 14\u00a0034 SD-OCT scans from 816 eyes from 462 individuals. A DL convolutional neural network was trained to assess SD-OCT RNFL thickness measurements of 2 visits (a baseline and a follow-up visit) along with time between visits to predict the probability of glaucoma progression. The ground truth was defined by consensus from subjective grading by glaucoma specialists. Diagnostic performance was summarized by the area under the receiver operator characteristic curve (AUC), sensitivity, and specificity, and was compared with conventional trend-based analyses of change. Interval likelihood ratios were calculated to determine the impact of DL model results in changing the post-test probability of progression. The AUC, sensitivity, and specificity of the DL model. The DL model had an AUC of 0.938 (95% confidence interval [CI], 0.921-0.955), with sensitivity of 87.3% (95% CI, 83.6%-91.6%) and specificity of 86.4% (95% CI, 79.9%-89.6%). When matched for the same specificity, the DL model significantly outperformed trend-based analyses. Likelihood ratios for the DL model were associated with large changes in the probability of progression in the vast majority of SD-OCT tests. A DL model was able to assess the probability of glaucomatous structural progression from SD-OCT RNFL thickness measurements. The model agreed well with expert judgments and outperformed conventional trend-based analyses of change, while also providing indication of the likely locations of change. Proprietary or commercial disclosure may be found after the references.", "keywords": ["nerve fiber layer", "retinal nerve fiber", "RNFL thickness measurements", "SD-OCT RNFL thickness", "OCT measurements", "deep learning", "fiber layer", "RNFL thickness", "develop and validate", "validate a deep", "retinal nerve", "nerve fiber", "model", "SD-OCT RNFL", "OCT", "RNFL", "thickness measurements", "assess SD-OCT RNFL", "progression", "trend-based analyses"], "paper_title": "Deep Learning-Assisted Detection of Glaucoma Progression in Spectral-Domain OCT.", "last_updated": "2023/02/04"}, {"id": "0031022592", "domain": "Glaucoma (unspecified)", "model_name": "Yu et al.", "publication_date": "2019/04/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31022592/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31022592", "task": "aipbNdPTIt", "abstract": "Glaucoma is rated as the leading cause of irreversible vision loss worldwide. Early detection of glaucoma is important for providing timely treatment and minimizing the vision loss. In this paper, we developed a robust segmentation method for optic disc and cup segmentation using a modified U-Net architecture, which combines the widely adopted pre-trained ResNet-34 model as encoding layers with classical U-Net decoding layers. The model was trained on the newly available RIGA dataset, and achieved an average dice value of 97.31% for disc segmentation and 87.61% for cup segmentation, comparable to that of the experts' performance for optic disc/cup segmentation and Cup-Disc-Ratio (CDR) calculation on a reserved RIGA dataset. When tested on DRISHTI-GS and RIM-ONE dataset without re-training or fine-tuning, the model achieved comparable performance to that of the state-of-the-art in literature. We have also fine-tuned the model on two databases, which achieves an average disc dice value of 97.38% and cup dice value of 88.77% for DRISHTI-GS test set, disc dice of 96.10% and cup dice of 84.45% for RIM-ONE database, which is the state-of-the-art performance on both databases in terms of cup dice and disc dice value. The advantage of the proposed method is the combination of the pre-trained ResNet and U-Net, which avoids training the network from scratch, thereby enabling fast network training with less epochs, thus further avoids over-fitting and achieves robust performance.", "keywords": ["vision loss worldwide", "irreversible vision loss", "vision loss", "cup dice", "cup segmentation", "disc dice", "dice", "loss worldwide", "cup", "Glaucoma is rated", "irreversible vision", "disc", "RIGA dataset", "segmentation", "average disc dice", "optic disc", "model", "performance", "Glaucoma", "vision"], "paper_title": "Robust optic disc and cup segmentation with deep learning for glaucoma detection.", "last_updated": "2023/02/04"}, {"id": "0035273784", "domain": "Glaucoma (unspecified)", "model_name": "Joshi et al.", "publication_date": "2022/03/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35273784/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35273784", "task": "IWqQC1koJA", "abstract": "A difficult challenge in the realm of biomedical engineering is the detection of physiological changes occurring inside the human body, which is a difficult undertaking. At the moment, these irregularities are graded manually, which is very difficult, time-consuming, and tiresome due to the many complexities associated with the methods involved in their identification. In order to identify illnesses at an early stage, the use of computer-assisted diagnostics has acquired increased attention as a result of the requirement of a disease detection system. The major goal of this proposed work is to build a computer-aided design (CAD) system to help in the early identification of glaucoma as well as the screening and treatment of the disease. The fundus camera is the most affordable image analysis modality available, and it meets the financial needs of the general public. The extraction of structural characteristics from the segmented optic disc and the segmented optic cup may be used to characterize glaucoma and determine its severity. For this study, the primary goal is to estimate the potential of the image analysis model for the early identification and diagnosis of glaucoma, as well as for the evaluation of ocular disorders. The suggested CAD system would aid the ophthalmologist in the diagnosis of ocular illnesses by providing a second opinion as a judgment made by human specialists in a controlled environment. An ensemble-based deep learning model for the identification and diagnosis of glaucoma is in its early stages now. This method's initial module is an ensemble-based deep learning model for glaucoma diagnosis, which is the first of its kind ever developed. It was decided to use three pretrained convolutional neural networks for the categorization of glaucoma. These networks included the residual network (ResNet), the visual geometry group network (VGGNet), and the GoogLeNet. It was necessary to use five different data sets in order to determine how well the proposed algorithm performed. These data sets included the DRISHTI-GS, the Optic Nerve Segmentation Database (DRIONS-DB), and the High-Resolution Fundus (HRF). Accuracy of 91.11% for the PSGIMSR data set and the sensitivity of 85.55% and specificity of 95.20% for the suggested ensemble architecture on the PSGIMSR data set were achieved. Similarly, accuracy rates of 95.63%, 98.67%, 95.64%, and 88.96% were achieved using the DRIONS-DB, HRF, DRISHTI-GS, and combined data sets, respectively.", "keywords": ["difficult undertaking", "difficult challenge", "PSGIMSR data set", "realm of biomedical", "biomedical engineering", "physiological changes occurring", "occurring inside", "data sets", "glaucoma", "disease detection system", "early identification", "difficult", "PSGIMSR data", "data", "data set", "suggested CAD system", "early", "human body", "identification", "diagnosis"], "paper_title": "Glaucoma Detection Using Image Processing and Supervised Learning for Classification.", "last_updated": "2023/02/04"}, {"id": "0035741192", "domain": "Glaucoma (unspecified)", "model_name": "S\u00e1nchez-Morales et al.", "publication_date": "2022/06/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35741192/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35741192", "task": "IWqQC1koJA", "abstract": "Glaucoma is a group of eye conditions that damage the optic nerve, the health of which is vital for good eyesight. This damage is often caused by higher-than-normal pressure in the eye. In the past few years, the applications of artificial intelligence and data science have increased rapidly in medicine especially in imaging applications. In particular, deep learning tools have been successfully applied obtaining, in some cases, results superior to those obtained by humans. In this article, we present a soft novel ensemble model based on the <i>K</i>-NN algorithm, that combines the probability of class membership obtained by several deep learning models. In this research, three models of different nature (CNN, CapsNets and Convolutional Autoencoders) have been selected searching for diversity. The latent space of these models are combined using the local information provided by the true sample labels and the <i>K</i>-NN algorithm is applied to determine the final decision. The results obtained on two different datasets of retinal images show that the proposed ensemble model improves the diagnosis capabilities for both the individual models and the state-of-the-art results.", "keywords": ["optic nerve", "good eyesight", "vital for good", "eye conditions", "damage the optic", "group of eye", "conditions that damage", "deep learning models", "models", "deep learning", "ensemble model", "eye", "damage", "obtained", "Convolutional Autoencoders", "Glaucoma", "nerve", "eyesight", "results", "deep learning tools"], "paper_title": "Improving Glaucoma Diagnosis Assembling Deep Networks and Voting Schemes.", "last_updated": "2023/02/04"}, {"id": "0035999058", "domain": "Glaucoma (unspecified)", "model_name": "Charng et al.", "publication_date": "2022/08/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35999058/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35999058", "task": null, "abstract": "Deep learning (DL) represents a paradigm-shifting, burgeoning field of research with emerging clinical applications in optometry. Unlike traditional programming, which relies on human-set specific rules, DL works by exposing the algorithm to a large amount of annotated data and allowing the software to develop its own set of rules (i.e. learn) by adjusting the parameters inside the model (network) during a training process in order to complete the task on its own. One major limitation of traditional programming is that, with complex tasks, it may require an extensive set of rules to accurately complete the assignment. Additionally, traditional programming can be susceptible to human bias from programmer experience. With the dramatic increase in the amount and the complexity of clinical data, DL has been utilised to automate data analysis and thus to assist clinicians in patient management. This review will present the latest advances in DL, for managing posterior eye diseases as well as DL-based solutions for patients with vision loss.", "keywords": ["emerging clinical applications", "Deep learning", "represents a paradigm-shifting", "burgeoning field", "applications in optometry", "field of research", "research with emerging", "traditional programming", "Unlike traditional programming", "emerging clinical", "clinical applications", "human-set specific rules", "set of rules", "traditional", "programming", "rules", "Unlike traditional", "Deep", "learning", "represents"], "paper_title": "Deep learning: applications in retinal and optic nerve diseases.", "last_updated": "2023/02/04"}, {"id": "0033979115", "domain": "Glaucoma (unspecified)", "model_name": "Shin et al.", "publication_date": "2021/10/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33979115/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33979115", "task": "IWqQC1koJA", "abstract": "(1) To evaluate the performance of deep learning (DL) classifier in detecting glaucoma, based on wide-field swept-source optical coherence tomography (SS-OCT) images. (2) To assess the performance of DL-based fusion methods in diagnosing glaucoma using a variety of wide-field SS-OCT images and compare their diagnostic abilities with that of conventional parameter-based methods. Overall, 675 eyes, including 258 healthy eyes and 417 eyes with glaucoma were enrolled in this retrospective observational study. Each single-page wide-field report (12\u00d79\u2009mm) of wide-field SS-OCT imaging provides different types of images that reflect the state of the eyes. A DL-based automated diagnosis system was proposed to detect glaucoma and identify its stage based on such images. We applied the convolutional neural network to each type of image to detect glaucoma. In addition, 2 fusion strategies, fusion by convolution network (FCN) and fusion by fully connected network (FFC) were developed; they differ in terms of the level of fusion of features derived from convolutional neural networks. The diagnostic models were trained using 382 and 293 images in the training and test data sets, respectively. The diagnostic ability of this method was compared with conventional parameters of the thickness of the retinal nerve fiber layer and ganglion cell complex. FCN achieved an area under the receiver operating characteristic curve (AUC) of 0.987 (95% confidence interval, CI: 0.968-0.996) and an accuracy of 95.22%. In contrast, FFC achieved an AUC of 0.987 (95% CI, 0.971-0.998) and an accuracy of 95.90%. Both FCN and FFC outperformed the conventional method (P<0.001). In detecting early glaucoma, both FCN and FFC achieved significantly higher AUC and accuracy than the conventional approach (P<0.001). In addition, the classification performance of the DL-based fusion methods in identifying the 5 stages of glaucoma is presented via a confusion matrix. DL protocol based on wide-field OCT images outperformed the conventional method in terms of both AUC and accuracy. Therefore, DL-based diagnostic methods using wide-field OCT images are promising in diagnosing glaucoma in clinical practice.", "keywords": ["optical coherence tomography", "swept-source optical coherence", "wide-field swept-source optical", "wide-field OCT images", "glaucoma", "wide-field SS-OCT images", "deep learning", "coherence tomography", "images", "wide-field SS-OCT", "swept-source optical", "optical coherence", "wide-field", "performance of deep", "FCN", "fusion", "FFC", "wide-field OCT", "AUC", "wide-field swept-source"], "paper_title": "Deep Learning-based Diagnosis of Glaucoma Using Wide-field Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0034780803", "domain": "Glaucoma (unspecified)", "model_name": "Bowd et al.", "publication_date": "2021/11/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34780803/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34780803", "task": "IWqQC1koJA", "abstract": "To compare convolutional neural network (CNN) analysis of en face vessel density images to gradient boosting classifier (GBC) analysis of instrument-provided, feature-based optical coherence tomography angiography (OCTA) vessel density measurements and OCT retinal nerve fiber layer (RNFL) thickness measurements for classifying healthy and glaucomatous eyes. Comparison of diagnostic approaches. A total of 130 eyes of 80 healthy individuals and 275 eyes of 185 glaucoma patients with optic nerve head (ONH) OCTA and OCT imaging were included. Classification performance of a VGG16 CNN trained and tested on entire en face 4.5\u00a0\u00d7\u00a04.5-mm radial peripapillary capillary OCTA ONH images was compared to the performance of separate GBC models trained and tested on standard OCTA and OCT measurements. Five-fold cross-validation was used to test predictions for CNNs and GBCs. Areas under the precision recall curves (AUPRC) were calculated to control for training/test set size imbalance and were compared. Adjusted AUPRCs for GBC models were 0.89 (95% CI\u00a0=\u00a00.82, 0.92) for whole image vessel density GBC, 0.89 (0.83, 0.92) for whole image capillary density GBC, 0.91 (0.88, 0.93) for combined whole image vessel and whole image capillary density GBC, and 0.93 (0.91, 095) for RNFL thickness GBC. The adjusted AUPRC using CNN analysis of en face vessel density images was 0.97 (0.95, 0.99) resulting in significantly improved classification compared to GBC OCTA-based results and GBC OCT-based results (P \u2264 0.01 for all comparisons). Deep learning en face image analysis improves on feature-based GBC models for classifying healthy and glaucoma eyes.", "keywords": ["convolutional neural network", "gradient boosting classifier", "coherence tomography angiography", "compare convolutional neural", "optical coherence tomography", "OCT retinal nerve", "nerve fiber layer", "GBC", "retinal nerve fiber", "OCTA ONH images", "density GBC", "capillary density GBC", "capillary OCTA ONH", "feature-based optical coherence", "GBC models", "OCT retinal", "neural network", "boosting classifier", "tomography angiography", "fiber layer"], "paper_title": "Deep Learning Image Analysis of Optical Coherence Tomography Angiography Measured Vessel Density Improves Classification of Healthy and Glaucoma Eyes.", "last_updated": "2023/02/04"}, {"id": "0031811363", "domain": "Glaucoma (unspecified)", "model_name": "Zheng et al.", "publication_date": "2019/12/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31811363/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31811363", "task": "IWqQC1koJA", "abstract": "To develop a deep learning (DL) model for automated detection of glaucoma and to compare diagnostic capability against hand-craft features (HCFs) based on spectral domain optical coherence tomography (SD-OCT) peripapillary retinal nerve fiber layer (pRNFL) images. A DL model with pre-trained convolutional neural network (CNN) based was trained using a retrospective training set of 1501 pRNFL OCT images, which included 690 images from 153 glaucoma patients and 811 images from 394 normal subjects. The DL model was further tested in an independent test set of 50 images from 50 glaucoma patients and 52 images from 52 normal subjects. A customized software was used to extract and measure HCFs including pRNFL thickness in average and four different sectors. Area under the receiver operator characteristics (AROC) curves was calculated to compare the diagnostic capability between DL model and hand-crafted pRNFL parameters. In this study, the DL model achieved an AROC of 0.99 [CI: 0.97 to 1.00] which was significantly larger than the AROC values of all other HCFs (AROCs 0.661 with 95% CI 0.549 to 0.772 for temporal sector, AROCs 0.696 with 95% CI 0.549 to 0.799 for nasal sector, AROCs 0.913 with 95% CI 0.855 to 0.970 for superior sector, AROCs 0.938 with 95% CI 0.894 to 0.982 for inferior sector, and AROCs 0.895 with 95% CI 0.832 to 0.957 for average). Our study demonstrated that DL models based on pre-trained CNN are capable of identifying glaucoma with high sensitivity and specificity based on SD-OCT pRNFL images.", "keywords": ["optical coherence tomography", "peripapillary retinal nerve", "nerve fiber layer", "spectral domain optical", "domain optical coherence", "retinal nerve fiber", "deep learning", "hand-craft features", "coherence tomography", "peripapillary retinal", "fiber layer", "pRNFL OCT images", "images", "develop a deep", "automated detection", "spectral domain", "domain optical", "optical coherence", "retinal nerve", "nerve fiber"], "paper_title": "Detecting glaucoma based on spectral domain optical coherence tomography imaging of peripapillary retinal nerve fiber layer: a comparison study between hand-crafted features and deep learning model.", "last_updated": "2023/02/04"}, {"id": "0032672542", "domain": "Glaucoma (unspecified)", "model_name": "Asaoka et al.", "publication_date": "2019/04/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32672542/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32672542", "task": null, "abstract": "To validate a deep residual learning algorithm to diagnose glaucoma from fundus photography using different fundus cameras at different institutes. Cross-sectional study. A training dataset consisted of 1364 color fundus photographs with glaucomatous indications and 1768 color fundus photographs without glaucomatous features. Two testing datasets consisted of (1) 95 images of 95 glaucomatous eyes and 110 images of 110 normative eyes, and (2) 93 images of 93 glaucomatous eyes and 78 images of 78 normative eyes. A deep learning algorithm known as Residual Network (ResNet) was used to diagnose glaucoma using a training dataset. The 2 testing datasets were obtained using different fundus cameras (different manufacturers) across multiple institutes. The size of the training data was artificially increased by adding minor alterations to the original data, known as \"image augmentation.\" Diagnostic accuracy was assessed using the area under the receiver operating characteristic curve (AROC). Area under the receiver operating characteristic curve. When image augmentation was not used, the AROC was 94.8% (90.3-96.8) in the first testing dataset and 99.7% (99.4-100.0) in the second dataset. These AROC values were significantly (P < 0.05) smaller without augmentation (87.7% [82.8-92.6] in the first testing dataset and 94.5% [91.3-97.6] in the second testing dataset). The previously developed deep residual learning algorithm achieved high diagnostic performance with different fundus cameras across multiple institutes, in particular when image augmentation was used.", "keywords": ["dataset", "fundus", "testing dataset", "color fundus photographs", "deep residual learning", "residual learning algorithm", "fundus cameras", "image augmentation", "testing", "learning algorithm", "images", "color fundus", "fundus photography", "eyes", "residual learning", "training dataset", "glaucomatous", "deep residual", "AROC", "augmentation"], "paper_title": "Validation of a Deep Learning Model to Screen for Glaucoma Using Images from Different Fundus Cameras and Data Augmentation.", "last_updated": "2023/02/04"}, {"id": "0035214351", "domain": "Glaucoma (unspecified)", "model_name": "Neto et al.", "publication_date": "2022/02/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35214351/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35214351", "task": null, "abstract": "Glaucoma is a silent disease that leads to vision loss or irreversible blindness. Current deep learning methods can help glaucoma screening by extending it to larger populations using retinal images. Low-cost lenses attached to mobile devices can increase the frequency of screening and alert patients earlier for a more thorough evaluation. This work explored and compared the performance of classification and segmentation methods for glaucoma screening with retinal images acquired by both retinography and mobile devices. The goal was to verify the results of these methods and see if similar results could be achieved using images captured by mobile devices. The used classification methods were the Xception, ResNet152 V2 and the Inception ResNet V2 models. The models' activation maps were produced and analysed to support glaucoma classifier predictions. In clinical practice, glaucoma assessment is commonly based on the cup-to-disc ratio (CDR) criterion, a frequent indicator used by specialists. For this reason, additionally, the U-Net architecture was used with the Inception ResNet V2 and Inception V3 models as the backbone to segment and estimate CDR. For both tasks, the performance of the models reached close to that of state-of-the-art methods, and the classification method applied to a low-quality private dataset illustrates the advantage of using cheaper lenses.", "keywords": ["irreversible blindness", "silent disease", "disease that leads", "leads to vision", "vision loss", "loss or irreversible", "mobile devices", "glaucoma screening", "Glaucoma", "Inception ResNet", "methods", "retinal images", "Inception", "deep learning methods", "screening", "Current deep learning", "mobile", "devices", "images", "retinal images acquired"], "paper_title": "Evaluations of Deep Learning Approaches for Glaucoma Screening Using Retinal Images from Mobile Device.", "last_updated": "2023/02/04"}, {"id": "0033279356", "domain": "Glaucoma (unspecified)", "model_name": "D\u00edaz-Alem\u00e1n et al.", "publication_date": "2020/12/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33279356/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33279356", "task": "IWqQC1koJA", "abstract": "To determine and compare the diagnostic precision in glaucoma of two deep learning models using infrared images of the optic nerve, eye fundus, and the ganglion cell layer (GCL). We have selected a sample of normal and glaucoma patients. Three infrared images were registered with a spectral-domain optical coherence tomography (SD-OCT). The first corresponds to the confocal scan image of the fundus, the second is a cut-out of the first centered on the optic nerve, and the third was the SD-OCT image of the GCL. Our deep learning models are developed on the MatLab platform with the ResNet50 and VGG19 pre-trained neural networks. 498 eyes of 298 patients were collected. Of the 498 eyes, 312 are glaucoma and 186 are normal. In the test, the precision of the models was 96% (ResNet50) and 96% (VGG19) for the GCL images, 90% (ResNet50) and 90% (VGG19) for the optic nerve images and 82% (ResNet50) and 84% (VGG19) for the fundus images. The ROC area in the test was 0.96 (ResNet50) and 0.97 (VGG19) for the GCL images, 0.87 (ResNet50) and 0.88 (VGG19) for the optic nerve images, and 0.79 (ResNet50) and 0.81 (VGG19) for the fundus images. Both deep learning models, applied to the GCL images, achieve high diagnostic precision, sensitivity and specificity in the diagnosis of glaucoma.", "keywords": ["ganglion cell layer", "GCL images", "optic nerve images", "images", "optic nerve", "GCL", "deep learning models", "cell layer", "determine and compare", "ganglion cell", "infrared images", "nerve images", "fundus images", "learning models", "deep learning", "optic", "nerve", "fundus", "glaucoma", "models"], "paper_title": "Ganglion cell layer analysis with deep learning in glaucoma diagnosis.", "last_updated": "2023/02/04"}, {"id": "0032598951", "domain": "Glaucoma (unspecified)", "model_name": "Chang et al.", "publication_date": "2020/06/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32598951/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32598951", "task": null, "abstract": "To illustrate what is inside the so-called black box of deep learning models (DLMs) so that clinicians can have greater confidence in the conclusions of artificial intelligence by evaluating adversarial explanation on its ability to explain the rationale of DLM decisions for glaucoma and glaucoma-related findings. Adversarial explanation generates adversarial examples (AEs), or images that have been changed to gain or lose pathologic characteristic-specific traits, to explain the DLM's rationale. Evaluation of explanation methods for DLMs. Health screening participants (n\u00a0= 1653) at the Seoul National University Hospital Health Promotion Center, Seoul, Republic of Korea. We trained DLMs for referable glaucoma (RG), increased cup-to-disc ratio (ICDR), disc rim narrowing (DRN), and retinal nerve fiber layer defect (RNFLD) using 6430 retinal fundus images. Surveys consisting of explanations using AE and gradient-weighted class activation mapping (GradCAM), a conventional heatmap-based explanation method, were generated for 400 pathologic and healthy patient eyes. For each method, board-trained glaucoma specialists rated location explainability, the ability to pinpoint decision-relevant areas in the image, and rationale explainability, the ability to inform the user on the model's reasoning for the decision based on pathologic features. Scores were compared by paired Wilcoxon signed-rank test. Area under the receiver operating characteristic curve (AUC), sensitivities, and specificities of DLMs; visualization of clinical pathologic changes of AEs; and survey scores for locational and rationale explainability. The AUCs were 0.90, 0.99, 0.95, and 0.79 and sensitivities were 0.79, 1.00, 0.82, and 0.55 at 0.90 specificity for RG, ICDR, DRN, and RNFLD DLMs, respectively. Generated AEs showed valid clinical feature changes, and survey results for location explainability were 3.94 \u00b1 1.33 and 2.55 \u00b1 1.24 using AEs and GradCAMs, respectively, of a possible maximum score of 5 points. The scores for rationale explainability were 3.97 \u00b1 1.31 and 2.10 \u00b1 1.25 for AEs and GradCAM, respectively. Adversarial example provided significantly better explainability than GradCAM. Adversarial explanation increased the explainability over GradCAM, a conventional heatmap-based explanation method. Adversarial explanation may help medical professionals understand more clearly the rationale of DLMs when using them for clinical decisions.", "keywords": ["so-called black box", "deep learning models", "Seoul National University", "adversarial explanation", "National University Hospital", "evaluating adversarial explanation", "explanation", "glaucoma-related findings", "Health Promotion Center", "inside the so-called", "so-called black", "black box", "box of deep", "deep learning", "greater confidence", "conclusions of artificial", "artificial intelligence", "intelligence by evaluating", "University Hospital Health", "Hospital Health Promotion"], "paper_title": "Explaining the Rationale of Deep Learning Glaucoma Decisions with Adversarial Examples.", "last_updated": "2023/02/04"}, {"id": "0033012331", "domain": "Glaucoma (unspecified)", "model_name": "Thakur et al.", "publication_date": "2020/04/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33012331/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33012331", "task": null, "abstract": "To assess the accuracy of deep learning models to predict glaucoma development from fundus photographs several years before disease onset. Algorithm development for predicting glaucoma using data from a prospective longitudinal study. A total of 66\u2009721 fundus photographs from 3272 eyes of 1636 subjects who participated in the Ocular Hypertension Treatment Study (OHTS) were included. Accuracy and area under the curve (AUC). Fundus photographs and visual fields were carefully examined by 2 independent readers from the optic disc and visual field reading centers of the OHTS. When an abnormality was detected by the readers, the subject was recalled for retesting to confirm the abnormality and for further confirmation by an end point committee. By using 66\u2009721 fundus photographs, deep learning models were trained and validated using 85% of the fundus photographs and further retested (validated) on the remaining (held-out) 15% of the fundus photographs. The AUC of the deep learning model in predicting glaucoma development 4 to 7 years before disease onset was 0.77 (95% confidence interval [CI], 0.75-0.79). The accuracy of the model in predicting glaucoma development approximately 1 to 3 years before disease onset was 0.88 (95% CI, 0.86-0.91). The accuracy of the model in detecting glaucoma after onset was 0.95 (95% CI, 0.94-0.96). Deep learning models can predict glaucoma development before disease onset with reasonable accuracy. Eyes with visual field abnormality but not glaucomatous optic neuropathy had a higher tendency to be missed by deep learning algorithms.", "keywords": ["fundus photographs", "Hypertension Treatment Study", "Ocular Hypertension Treatment", "deep learning models", "deep learning", "fundus", "disease onset", "photographs", "glaucoma development", "learning models", "glaucoma", "learning", "Treatment Study", "predicting glaucoma development", "onset", "predicting glaucoma", "accuracy", "Ocular Hypertension", "Hypertension Treatment", "deep"], "paper_title": "Predicting Glaucoma before Onset Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0032818088", "domain": "Glaucoma (unspecified)", "model_name": "Christopher et al.", "publication_date": "2020/04/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32818088/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32818088", "task": "IWqQC1koJA", "abstract": "To compare performance of independently developed deep learning algorithms for detecting glaucoma from fundus photographs and to evaluate strategies for incorporating new data into models. Two fundus photograph datasets from the Diagnostic Innovations in Glaucoma Study/African Descent and Glaucoma Evaluation Study and Matsue Red Cross Hospital were used to independently develop deep learning algorithms for detection of glaucoma at the University of California, San Diego, and the University of Tokyo. We compared three versions of the University of California, San Diego, and University of Tokyo models: original (no retraining), sequential (retraining only on new data), and combined (training on combined data). Independent datasets were used to test the algorithms. The original University of California, San Diego and University of Tokyo models performed similarly (area under the receiver operating characteristic curve = 0.96 and 0.97, respectively) for detection of glaucoma in the Matsue Red Cross Hospital dataset, but not the Diagnostic Innovations in Glaucoma Study/African Descent and Glaucoma Evaluation Study data (0.79 and 0.92; <i>P</i> < .001), respectively. Model performance was higher when classifying moderate-to-severe compared with mild disease (area under the receiver operating characteristic curve = 0.98 and 0.91; <i>P</i> < .001), respectively. Models trained with the combined strategy generally had better performance across all datasets than the original strategy. Deep learning glaucoma detection can achieve high accuracy across diverse datasets with appropriate training strategies. Because model performance was influenced by the severity of disease, labeling, training strategies, and population characteristics, reporting accuracy stratified by relevant covariates is important for cross study comparisons. High sensitivity and specificity of deep learning algorithms for moderate-to-severe glaucoma across diverse populations suggest a role for artificial intelligence in the detection of glaucoma in primary care.", "keywords": ["University of California", "University of Tokyo", "San Diego", "Matsue Red Cross", "Red Cross Hospital", "Glaucoma Evaluation Study", "University", "deep learning algorithms", "glaucoma", "African Descent", "Diagnostic Innovations", "Matsue Red", "Evaluation Study", "Evaluation Study data", "Glaucoma Evaluation", "Red Cross", "Cross Hospital", "Glaucoma Study", "Tokyo models", "independently developed deep"], "paper_title": "Effects of Study Population, Labeling and Training on Glaucoma Detection Using Deep Learning Algorithms.", "last_updated": "2023/02/04"}, {"id": "0036531633", "domain": "Glaucoma (unspecified)", "model_name": "Riza Rizky et al.", "publication_date": "2022/12/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36531633/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36531633", "task": "IWqQC1koJA", "abstract": "Glaucoma is an eye disease that can cause irreversible blindness to people if not treated properly. Although deep learning models have shown that they can provide good results in identifying diseases from medical imagery, they suffer from the vulnerability of adversarial attacks, making them perform poorly. Several techniques can be applied to improve defense against such attacks. One of which is adversarial training (AT) which trains a deep learning model using the input's gradient used to generate noises to the input image and Deep k-Nearest Neighbor (DkNN) that enforces prediction's conformity based on nearest neighbor voting on each layer's representation. This work tries to improve the defense against adversarial attacks by combining AT and DkNN. The evaluation performed on several adversarial attacks show that given an optimum <i>k</i>, the combination of these two methods is able to improve most models' overall classification result on the perturbed retinal fundus image.", "keywords": ["treated properly", "irreversible blindness", "blindness to people", "eye disease", "deep learning", "adversarial attacks", "deep learning models", "attacks", "adversarial", "deep", "Deep k-Nearest Neighbor", "improve", "identifying diseases", "Glaucoma", "properly", "learning", "learning models", "eye", "irreversible", "blindness"], "paper_title": "Adversarial training and deep k-nearest neighbors improves adversarial defense of glaucoma severity detection.", "last_updated": "2023/02/04"}, {"id": "0033439453", "domain": "Glaucoma (unspecified)", "model_name": "Singh et al.", "publication_date": "2021/01/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33439453/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33439453", "task": "IWqQC1koJA", "abstract": "This paper proposes a deep image analysis-based model for glaucoma diagnosis that uses several features to detect the formation of glaucoma in retinal fundus. These features are combined with most extracted parameters like inferior, superior, nasal, and temporal region area, and cup-to-disc ratio that overall forms a deep image analysis. This proposed model is exercised to investigate the various aspects related to the prediction of glaucoma in retinal fundus images that help the ophthalmologist in making better decisions for the human eye. The proposed model is presented with the combination of four machine learning algorithms that provide the classification accuracy of 98.60% while other existing models like support vector machine (SVM), K-nearest neighbors (KNN), and Na\u00efve Bayes provide individually with accuracies of 97.61%, 90.47%, and 95.23% respectively. These results clearly demonstrate that this proposed model offers the best methodology to an early diagnosis of glaucoma in retinal fundus.", "keywords": ["deep image analysis-based", "image analysis-based model", "deep image analysis", "deep image", "retinal fundus images", "paper proposes", "detect the formation", "retinal fundus", "proposes a deep", "glaucoma in retinal", "proposed model", "image analysis-based", "analysis-based model", "glaucoma", "Na\u00efve Bayes provide", "formation of glaucoma", "retinal", "model", "forms a deep", "deep"], "paper_title": "An enhanced deep image model for glaucoma diagnosis using feature-based detection in retinal fundus.", "last_updated": "2023/02/04"}, {"id": "0035202616", "domain": "Glaucoma (unspecified)", "model_name": "Kihara et al.", "publication_date": "2022/02/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35202616/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35202616", "task": null, "abstract": "To develop and validate a deep learning (DL) system for predicting each point on visual fields (VFs) from disc and OCT imaging and derive a structure-function mapping. Retrospective, cross-sectional database study. A total of 6437 patients undergoing routine care for glaucoma in 3 clinical sites in the United Kingdom. OCT and infrared reflectance (IR) optic disc imaging were paired with the closest VF within 7 days. EfficientNet B2 was used to train 2 single-modality DL models to predict each of the 52 sensitivity points on the 24-2 VF pattern. A policy DL model was designed and trained to fuse the 2 model predictions. Pointwise mean absolute error (PMAE). A total of 5078 imaging scans to VF pairs were used as a held-out test set to measure the final performance. The improvement in PMAE with the policy model was 0.485 (0.438, 0.533) decibels (dB) compared with the IR image of the disc alone and 0.060 (0.047, 0.073) dB with to the OCT alone. The improvement with the policy fusion model was statistically significant (P < 0.0001). Occlusion masking shows that the DL models learned the correct structure-function mapping in a data-driven, feature agnostic fashion. The multimodal, policy DL model performed the best; it provided explainable maps of its confidence in fusing data from single modalities and provides a pathway for probing the structure-function relationship in glaucoma.", "keywords": ["deep learning", "system for predicting", "visual fields", "develop and validate", "validate a deep", "point on visual", "model", "United Kingdom", "OCT", "OCT imaging", "cross-sectional database study", "policy", "optic disc imaging", "disc", "imaging", "predicting each point", "structure-function mapping", "VFs", "policy model", "learning"], "paper_title": "Policy-Driven, Multimodal Deep Learning for Predicting Visual Fields from the Optic Disc and OCT Imaging.", "last_updated": "2023/02/04"}, {"id": "0030843823", "domain": "Glaucoma (unspecified)", "model_name": "Diaz-Pinto et al.", "publication_date": "2019/03/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30843823/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30843823", "task": null, "abstract": "Recent works show that generative adversarial networks (GANs) can be successfully applied to image synthesis and semi-supervised learning, where, given a small labeled database and a large unlabeled database, the goal is to train a powerful classifier. In this paper, we trained a retinal image synthesizer and a semi-supervised learning method for automatic glaucoma assessment using an adversarial model on a small glaucoma-labeled database and a large unlabeled database. Various studies have shown that glaucoma can be monitored by analyzing the optic disc and its surroundings, and for that reason, the images used in this paper were automatically cropped around the optic disc. The novelty of this paper is to propose a new retinal image synthesizer and a semi-supervised learning method for glaucoma assessment based on the deep convolutional GANs. In addition, and to the best of our knowledge, this system is trained on an unprecedented number of publicly available images (86926 images). This system, hence, is not only able to generate images synthetically but to provide labels automatically. Synthetic images were qualitatively evaluated using t-SNE plots of features associated with the images and their anatomical consistency was estimated by measuring the proportion of pixels corresponding to the anatomical structures around the optic disc. The resulting image synthesizer is able to generate realistic (cropped) retinal images, and subsequently, the glaucoma classifier is able to classify them into glaucomatous and normal with high accuracy (AUC = 0.9017). The obtained retinal image synthesizer and the glaucoma classifier could then be used to generate an unlimited number of cropped retinal images with glaucoma labels.", "keywords": ["large unlabeled database", "Recent works show", "small labeled database", "generative adversarial networks", "semi-supervised learning method", "semi-supervised learning", "unlabeled database", "retinal image synthesizer", "large unlabeled", "small glaucoma-labeled database", "image synthesizer", "images", "Recent works", "labeled database", "works show", "show that generative", "successfully applied", "train a powerful", "database", "retinal image"], "paper_title": "Retinal Image Synthesis and Semi-Supervised Learning for Glaucoma Assessment.", "last_updated": "2023/02/04"}, {"id": "0036038107", "domain": "Glaucoma (unspecified)", "model_name": "Christopher et al.", "publication_date": "2022/08/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36038107/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36038107", "task": null, "abstract": "To investigate the efficacy of a deep learning regression method to predict macula ganglion cell-inner plexiform layer (GCIPL) and optic nerve head (ONH) retinal nerve fiber layer (RNFL) thickness for use in glaucoma neuroprotection clinical trials. Cross-sectional study. Glaucoma patients with good quality macula and ONH scans enrolled in 2 longitudinal studies, the African Descent and Glaucoma Evaluation Study and the Diagnostic Innovations in Glaucoma Study. Spectralis macula posterior pole scans and ONH circle scans on 3327 pairs of GCIPL/RNFL scans from 1096 eyes (550 patients) were included. Participants were randomly distributed into a training and validation dataset (90%) and a test dataset (10%) by participant. Networks had access to GCIPL and RNFL data from one hemiretina of the probe eye and all data of the fellow eye. The models were then trained to predict the GCIPL or RNFL thickness of the remaining probe eye hemiretina. Mean absolute error (MAE) and squared Pearson correlation coefficient (r<sup>2</sup>) were used to evaluate model performance. The deep learning model was able to predict superior and inferior GCIPL thicknesses with a global r<sup>2</sup> value of 0.90 and 0.86, r<sup>2</sup> of mean of 0.90 and 0.86, and mean MAE of 3.72 \u03bcm and 4.2 \u03bcm, respectively. For superior and inferior RNFL thickness predictions, model performance was slightly lower, with a global r<sup>2</sup> of 0.75 and 0.84, r<sup>2</sup> of mean of 0.81 and 0.82, and MAE of 9.31 \u03bcm and 8.57 \u03bcm, respectively. There was only a modest decrease in model performance when predicting GCIPL and RNFL in more severe disease. Using individualized hemiretinal predictions to account for variability across patients, we estimate that a clinical trial can detect a difference equivalent to a 25% treatment effect over 24 months with an 11-fold reduction in the number of patients compared to a conventional trial. Our deep learning models were able to accurately estimate both macula GCIPL and ONH RNFL hemiretinal thickness. Using an internal control based on these model predictions may help reduce clinical trial sample size requirements and facilitate investigation of new glaucoma neuroprotection therapies. Proprietary or commercial disclosure may be found after the references.", "keywords": ["nerve fiber layer", "cell-inner plexiform layer", "optic nerve head", "retinal nerve fiber", "ganglion cell-inner plexiform", "Glaucoma Evaluation Study", "macula ganglion cell-inner", "learning regression method", "plexiform layer", "fiber layer", "nerve head", "retinal nerve", "GCIPL", "ONH RNFL hemiretinal", "RNFL", "optic nerve", "nerve fiber", "ONH RNFL", "glaucoma", "ONH"], "paper_title": "A Deep Learning Approach to Improve Retinal Structural Predictions and Aid Glaucoma Neuroprotective Clinical Trial Design.", "last_updated": "2023/02/04"}, {"id": "0035737221", "domain": "Glaucoma (unspecified)", "model_name": "Joshi et al.", "publication_date": "2022/06/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35737221/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35737221", "task": "aipbNdPTIt", "abstract": "The fundus imaging method of eye screening detects eye diseases by segmenting the optic disc (OD) and optic cup (OC). OD and OC are still challenging to segment accurately. This work proposes three-layer graph-based deep architecture with an enhanced fusion method for OD and OC segmentation. CNN encoder-decoder architecture, extended graph network, and approximation via fusion-based rule are explored for connecting local and global information. A graph-based model is developed for combining local and overall knowledge. By extending feature masking, regularization of repetitive features with fusion for combining channels has been done. The performance of the proposed network is evaluated through the analysis of different metric parameters such as dice similarity coefficient (DSC), intersection of union (IOU), accuracy, specificity, sensitivity. Experimental verification of this methodology has been done using the four benchmarks publicly available datasets DRISHTI-GS, RIM-ONE for OD, and OC segmentation. In addition, DRIONS-DB and HRF fundus imaging datasets were analyzed for optimizing the model's performance based on OD segmentation. DSC metric of methodology achieved 0.97 and 0.96 for DRISHTI-GS and RIM-ONE, respectively. Similarly, IOU measures for DRISHTI-GS and RIM-ONE datasets were 0.96 and 0.93, respectively, for OD measurement. For OC segmentation, DSC and IOU were measured as 0.93 and 0.90 respectively for DRISHTI-GS and 0.83 and 0.82 for RIM-ONE data. The proposed technique improved value of metrics with most of the existing methods in terms of DSC and IOU of the results metric of the experiments for OD and OC segmentation.", "keywords": ["screening detects eye", "eye screening detects", "detects eye diseases", "optic disc", "optic cup", "eye screening", "detects eye", "eye diseases", "screening detects", "diseases by segmenting", "segmenting the optic", "segmentation", "eye", "optic", "IOU", "DSC", "DRISHTI-GS", "RIM-ONE", "CNN encoder-decoder architecture", "fundus imaging method"], "paper_title": "Graph deep network for optic disc and optic cup segmentation for glaucoma disease using retinal imaging.", "last_updated": "2023/02/04"}, {"id": "0033901527", "domain": "Glaucoma (unspecified)", "model_name": "Christopher et al.", "publication_date": "2021/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33901527/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33901527", "task": null, "abstract": "To develop deep learning (DL) systems estimating visual function from macula-centered spectral-domain (SD) OCT images. Evaluation of a diagnostic technology. A total of 2408 10-2 visual field (VF) SD OCT pairs and 2999 24-2 VF SD OCT pairs collected from 645 healthy and glaucoma subjects (1222 eyes). Deep learning models were trained on thickness maps from Spectralis macula SD OCT to estimate 10-2 and 24-2 VF mean deviation (MD) and pattern standard deviation (PSD). Individual and combined DL models were trained using thickness data from 6 layers (retinal nerve fiber layer [RNFL], ganglion cell layer [GCL], inner plexiform layer [IPL], ganglion cell-IPL [GCIPL], ganglion cell complex [GCC] and retina). Linear regression of mean layer thicknesses were used for comparison. Deep learning models were evaluated using R<sup>2</sup> and mean absolute error (MAE) compared with 10-2 and 24-2 VF measurements. Combined DL models estimating 10-2 achieved R<sup>2</sup> of 0.82 (95% confidence interval [CI], 0.68-0.89) for MD and 0.69 (95% CI, 0.55-0.81) for PSD and MAEs of 1.9 dB (95% CI, 1.6-2.4 dB) for MD and 1.5 dB (95% CI, 1.2-1.9 dB) for PSD. This was significantly better than mean thickness estimates for 10-2 MD (0.61 [95% CI, 0.47-0.71] and 3.0 dB [95% CI, 2.5-3.5 dB]) and 10-2 PSD (0.46 [95% CI, 0.31-0.60] and 2.3 dB [95% CI, 1.8-2.7 dB]). Combined DL models estimating 24-2 achieved R<sup>2</sup> of 0.79 (95% CI, 0.72-0.84) for\u00a0MD and 0.68 (95% CI, 0.53-0.79) for PSD and MAEs of 2.1 dB (95% CI, 1.8-2.5 dB) for MD and 1.5 dB (95% CI, 1.3-1.9 dB) for PSD. This was significantly better than mean thickness estimates for 24-2 MD (0.41 [95% CI, 0.26-0.57] and 3.4 dB [95% CI, 2.7-4.5 dB]) and 24-2 PSD (0.38 [95% CI, 0.20-0.57] and 2.4 dB [95% CI, 2.0-2.8 dB]). The GCIPL (R<sup>2</sup> = 0.79) and GCC (R<sup>2</sup> = 0.75) had the highest performance estimating 10-2 and 24-2 MD, respectively. Deep learning models improved estimates of functional loss from SD OCT imaging. Accurate estimates can help clinicians to individualize VF testing to patients.", "keywords": ["Deep learning models", "PSD", "OCT", "OCT images", "develop deep learning", "OCT pairs", "deep learning", "learning models", "macula-centered spectral-domain", "systems estimating visual", "function from macula-centered", "estimating visual function", "24-2", "models", "10-2", "OCT pairs collected", "24-2 PSD", "models estimating 24-2", "visual function", "learning"], "paper_title": "Deep Learning Estimation of 10-2 and 24-2 Visual Field Metrics Based on Thickness Maps from Macula OCT.", "last_updated": "2023/02/04"}, {"id": "0035998059", "domain": "Glaucoma (unspecified)", "model_name": "Hemelings et al.", "publication_date": "2022/08/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35998059/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35998059", "task": null, "abstract": "Standard automated perimetry is the gold standard to monitor visual field (VF) loss in glaucoma management, but it is prone to intrasubject variability. We trained and validated a customized deep learning (DL) regression model with Xception backbone that estimates pointwise and overall VF sensitivity from unsegmented optical coherence tomography (OCT) scans. DL regression models have been trained with four imaging modalities (circumpapillary OCT at 3.5 mm, 4.1 mm, and 4.7 mm diameter) and scanning laser ophthalmoscopy en face images to estimate mean deviation (MD) and 52 threshold values. This retrospective study used data from patients who underwent a complete glaucoma examination, including a reliable Humphrey Field Analyzer (HFA) 24-2 SITA Standard (SS) VF exam and a SPECTRALIS OCT. For MD estimation, weighted prediction averaging of all four individuals yielded a mean absolute error (MAE) of 2.89 dB (2.50-3.30) on 186 test images, reducing the baseline by 54% (MAEdecr%). For 52 VF threshold values' estimation, the weighted ensemble model resulted in an MAE of 4.82 dB (4.45-5.22), representing an MAEdecr% of 38% from baseline when predicting the pointwise mean value. DL managed to explain 75% and 58% of the variance (R2) in MD and pointwise sensitivity estimation, respectively. Deep learning can estimate global and pointwise VF sensitivities that fall almost entirely within the 90% test-retest confidence intervals of the 24-2 SS test. Fast and consistent VF prediction from unsegmented OCT scans could become a solution for visual function estimation in patients unable to perform reliable VF exams.", "keywords": ["Standard automated perimetry", "intrasubject variability", "Humphrey Field Analyzer", "automated perimetry", "prone to intrasubject", "monitor visual field", "Standard automated", "gold standard", "OCT", "glaucoma management", "reliable Humphrey Field", "24-2 SITA Standard", "SPECTRALIS OCT", "Field Analyzer", "Humphrey Field", "Standard", "Xception backbone", "SITA Standard", "unsegmented OCT scans", "monitor visual"], "paper_title": "Pointwise Visual Field Estimation From Optical Coherence Tomography in Glaucoma Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0033649375", "domain": "Glaucoma (unspecified)", "model_name": "An et al.", "publication_date": "2021/03/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33649375/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33649375", "task": "IWqQC1koJA", "abstract": "Deep learning is being employed in disease detection and classification based on medical images for clinical decision making. It typically requires large amounts of labelled data; however, the sample size of such medical image datasets is generally small. This study proposes a novel training framework for building deep learning models of disease detection and classification with small datasets. Our approach is based on a hierarchical classification method where the healthy/disease information from the first model is effectively utilized to build subsequent models for classifying the disease into its sub-types via a transfer learning method. To improve accuracy, multiple input datasets were used, and a stacking ensembled method was employed for final classification. To demonstrate the method's performance, a labelled dataset extracted from volumetric ophthalmic optical coherence tomography data for 156 healthy and 798 glaucoma eyes was used, in which glaucoma eyes were further labelled into four sub-types. The average weighted accuracy and Cohen's kappa for three randomized test datasets were 0.839 and 0.809, respectively. Our approach outperformed the flat classification method by 9.7% using smaller training datasets. The results suggest that the framework can perform accurate classification with a small number of medical images.", "keywords": ["clinical decision making", "decision making", "clinical decision", "medical images", "classification", "medical image datasets", "disease detection", "Deep learning", "medical", "datasets", "method", "disease", "deep learning models", "learning", "small", "images", "classification method", "detection", "building deep learning", "labelled"], "paper_title": "Hierarchical deep learning models using transfer learning for disease detection and classification based on small number of medical images.", "last_updated": "2023/02/04"}, {"id": "0030682186", "domain": "Glaucoma (unspecified)", "model_name": "Ahn et al.", "publication_date": "2019/01/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30682186/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30682186", "task": "IWqQC1koJA", "abstract": "[This corrects the article DOI: 10.1371/journal.pone.0207982.].", "keywords": ["article DOI", "DOI", "corrects the article", "corrects", "article"], "paper_title": "Correction: A deep learning model for the detection of both advanced and early glaucoma using fundus photography.", "last_updated": "2023/02/04"}, {"id": "0036471039", "domain": "Glaucoma (unspecified)", "model_name": "Moon et al.", "publication_date": "2022/12/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36471039/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36471039", "task": null, "abstract": "Close monitoring of central visual field (VF) defects with 10-2 VF helps prevent blindness in glaucoma. We aimed to develop a deep learning model to predict 10-2 VF from wide-field swept-source optical coherence tomography (SS-OCT) images. Macular ganglion cell/inner plexiform layer thickness maps with either wide-field en face images (en face model) or retinal nerve fiber layer thickness maps (RNFLT model) were extracted, combined, and preprocessed. Inception-ResNet-V2 was trained to predict 10-2 VF from combined images. Estimation performance was evaluated using mean absolute error (MAE) between actual and predicted threshold values, and the two models were compared with different input data. The training dataset comprised paired 10-2 VF and SS-OCT images of 3,025 eyes of 1,612 participants and the test dataset of 337 eyes of 186 participants. Global prediction errors (MAE<sub>point-wise</sub>) were 3.10 and 3.17\u00a0dB for the en face and RNFLT models, respectively. The en face model performed better than the RNFLT model in superonasal and inferonasal sectors (P\u2009=\u20090.011 and P\u2009=\u20090.030). Prediction errors were smaller in the inferior versus superior hemifields for both models. The deep learning model effectively predicted 10-2 VF from wide-field SS-OCT images and might help clinicians efficiently individualize the frequency of 10-2 VF in clinical practice.", "keywords": ["central visual field", "Close monitoring", "visual field", "blindness in glaucoma", "monitoring of central", "central visual", "prevent blindness", "10-2", "predict 10-2", "model", "images", "RNFLT model", "layer thickness maps", "RNFLT", "face", "MAE", "layer thickness", "thickness maps", "wide-field", "SS-OCT images"], "paper_title": "Deep learning approaches to predict 10-2 visual field from wide-field swept-source optical coherence tomography en face images in glaucoma.", "last_updated": "2023/02/04"}, {"id": "0032704418", "domain": "Glaucoma (unspecified)", "model_name": "Russakoff et al.", "publication_date": "2020/02/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32704418/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32704418", "task": "IWqQC1koJA", "abstract": "The purpose of this study was to develop a 3D deep learning system from spectral domain optical coherence tomography (SD-OCT) macular cubes to differentiate between referable and nonreferable cases for glaucoma applied to real-world datasets to understand how this would affect the performance. There were 2805 Cirrus optical coherence tomography (OCT) macula volumes (Macula protocol 512 \u00d7 128) of 1095 eyes from 586 patients at a single site that were used to train a fully 3D convolutional neural network (CNN). Referable glaucoma included true glaucoma, pre-perimetric glaucoma, and high-risk suspects, based on qualitative fundus photographs, visual fields, OCT reports, and clinical examinations, including intraocular pressure (IOP) and treatment history as the binary (two class) ground truth. The curated real-world dataset did not include eyes with retinal disease or nonglaucomatous optic neuropathies. The cubes were first homogenized using layer segmentation with the Orion Software (Voxeleron) to achieve standardization. The algorithm was tested on two separate external validation sets from different glaucoma studies, comprised of Cirrus macular cube scans of 505 and 336 eyes, respectively. The area under the receiver operating characteristic (AUROC) curve for the development dataset for distinguishing referable glaucoma was 0.88 for our CNN using homogenization, 0.82 without homogenization, and 0.81 for a CNN architecture from the existing literature. For the external validation datasets, which had different glaucoma definitions, the AUCs were 0.78 and 0.95, respectively. The performance of the model across myopia severity distribution has been assessed in the dataset from the United States and was found to have an AUC of 0.85, 0.92, and 0.95 in the severe, moderate, and mild myopia, respectively. A 3D deep learning algorithm trained on macular OCT volumes without retinal disease to detect referable glaucoma performs better with retinal segmentation preprocessing and performs reasonably well across all levels of myopia. Interpretation of OCT macula volumes based on normative data color distributions is highly influenced by population demographics and characteristics, such as refractive error, as well as the size of the normative database. Referable glaucoma, in this study, was chosen to include cases that should be seen by a specialist. This study is unique because it uses multimodal patient data for the glaucoma definition, and includes all severities of myopia as well as validates the algorithm with international data to understand generalizability potential.", "keywords": ["optical coherence tomography", "spectral domain optical", "domain optical coherence", "Cirrus optical coherence", "coherence tomography", "optical coherence", "glaucoma", "Referable glaucoma", "system from spectral", "spectral domain", "domain optical", "deep learning system", "OCT", "Cirrus optical", "OCT macula volumes", "glaucoma applied", "Cirrus macular cube", "referable", "CNN", "macular OCT volumes"], "paper_title": "A 3D Deep Learning System for Detecting Referable Glaucoma Using Full OCT Macular Cube Scans.", "last_updated": "2023/02/04"}, {"id": "0034389508", "domain": "Glaucoma (unspecified)", "model_name": "Tran et al.", "publication_date": "2021/08/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34389508/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34389508", "task": null, "abstract": "To assess the accuracy and efficacy of deep learning models, specifically convolutional neural networks (CNNs), to identify glaucoma medication bottles. Algorithm development for predicting ophthalmic medication bottles using a large mobile image-based dataset. A total of 3750 mobile images of 5 ophthalmic medication bottles were included: brimonidine tartrate, dorzolamide-timolol, latanoprost, prednisolone acetate, and moxifloxacin. Seven CNN models were initially pretrained on a large-scale image database and subsequently retrained to classify 5 commonly prescribed topical ophthalmic medications using a training dataset of 2250 mobile-phone captured images. The retrained CNN models' accuracies were compared using k-fold cross-validation (k\u00a0= 10). The top 2 performing CNN models were then embedded into separate iOS apps and evaluated using 1500 mobile images not included in the training dataset. Prediction accuracy, image processing time. Of the 7 CNN architectures, MobileNet v2 yielded the highest k-fold cross-validation accuracy of 0.974 (95% confidence interval [CI], 0.966-0.980) and the shortest average image processing time at 3.45 (95% CI, 3.13-3.77) sec/image. ResNet V2 had the second highest accuracy of 0.961 (95% CI, 0.952-0.969). When the 2 app-embedded CNNs were compared, in terms of accuracy, MobileNet V2, with an image prediction accuracy of 0.86 (95% CI, 0.84-0.88), was significantly greater than ResNet V2, 0.68 (95% CI, 0.66-0.71) (Table\u00a01). Sensitivities and specificities varied between medications (Table\u00a01). There was no significant difference in\u00a0average imaging processing time, 0.32 (95% CI, 0.28-0.36) sec/image and 0.31 (95% CI, 0.29-0.33) sec/image for MobileNet V2 and ResNet V2, respectively. Information on beta-testing of the iOS app can be found here: https://lin.hs.uci.edu/research/. We have retrained MobileNet V2 to accurately identify ophthalmic medication bottles and demonstrated that this neural network can operate in a smartphone environment. This work serves as a proof-of-concept for the production of a CNN-based smartphone application to empower patients by decreasing risk for error.", "keywords": ["ophthalmic medication bottles", "medication bottles", "deep learning models", "glaucoma medication bottles", "specifically convolutional neural", "ophthalmic medication", "specifically convolutional", "efficacy of deep", "deep learning", "identify glaucoma medication", "medication", "image", "glaucoma medication", "accuracy", "bottles", "CNN", "CNN models", "ophthalmic", "predicting ophthalmic medication", "learning models"], "paper_title": "Fast and Accurate Ophthalmic Medication Bottle Identification Using Deep Learning on a Smartphone Device.", "last_updated": "2023/02/04"}, {"id": "0030485270", "domain": "Glaucoma (unspecified)", "model_name": "early-glaucoma-identification", "publication_date": "2018/11/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30485270/", "code_link": "https://github.com/serifeseda/early-glaucoma-identification", "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "30485270", "task": "IWqQC1koJA", "abstract": "To investigate the suitability of multi-scale spatial information in 30o visual fields (VF), computed from a Convolutional Neural Network (CNN) classifier, for early-glaucoma vs. control discrimination. Two data sets of VFs acquired with the OCTOPUS 101 G1 program and the Humphrey Field Analyzer 24-2 pattern were subdivided into control and early-glaucomatous groups, and converted into a new image using a novel voronoi representation to train a custom-designed CNN so to discriminate between control and early-glaucomatous eyes. Saliency maps that highlight what regions of the VF are contributing maximally to the classification decision were computed to provide classification justification. Model fitting was cross-validated and average precision (AP) score performances were computed for our method, Mean Defect (MD), square-root of Loss Variance (sLV), their combination (MD+sLV), and a Neural Network (NN) that does not use convolutional features. CNN achieved the best AP score (0.874\u00b10.095) across all test folds for one data set compared to others (MD = 0.869\u00b10.064, sLV = 0.775\u00b10.137, MD+sLV = 0.839\u00b10.085, NN = 0.843\u00b10.089) and the third best AP score (0.986 \u00b10.019) on the other one with slight difference from the other methods (MD = 0.986\u00b10.023, sLV = 0.992\u00b10.016, MD+sLV = 0.987\u00b10.017, NN = 0.985\u00b10.017). In general, CNN consistently led to high AP across different data sets. Qualitatively, computed saliency maps appeared to provide clinically relevant information on the CNN decision for individual VFs. The proposed CNN offers high classification performance for the discrimination of control and early-glaucoma VFs when compared with standard clinical decision measures. The CNN classification, aided by saliency visualization, may support clinicians in the automatic discrimination of early-glaucomatous and normal VFs.", "keywords": ["Humphrey Field Analyzer", "Convolutional Neural Network", "Neural Network", "Field Analyzer 24-2", "multi-scale spatial information", "visual fields", "sLV", "CNN", "Humphrey Field", "Field Analyzer", "investigate the suitability", "suitability of multi-scale", "multi-scale spatial", "Convolutional Neural", "Analyzer 24-2 pattern", "control", "Network", "computed", "Neural", "Analyzer 24-2"], "paper_title": "A deep learning approach to automatic detection of early glaucoma from visual fields.", "last_updated": "2023/02/04"}, {"id": "0032442866", "domain": "Glaucoma (unspecified)", "model_name": "Wang et al.", "publication_date": "2020/05/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32442866/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32442866", "task": null, "abstract": "Glaucoma is the leading cause of irreversible blindness in the world. Structure and function assessments play an important role in diagnosing glaucoma. Nowadays, Optical Coherence Tomography (OCT) imaging gains increasing popularity in measuring the structural change of eyes. However, few automated methods have been developed based on OCT images to screen glaucoma. In this paper, we are the first to unify the structure analysis and function regression to distinguish glaucoma patients from normal controls effectively. Specifically, our method works in two steps: a semi-supervised learning strategy with smoothness assumption is first applied for the surrogate assignment of missing function regression labels. Subsequently, the proposed multi-task learning network is capable of exploring the structure and function relationship between the OCT image and visual field measurement simultaneously, which contributes to classification performance improvement. It is also worth noting that the proposed method is assessed by two large-scale multi-center datasets. In other words, we first build the largest glaucoma OCT image dataset (i.e., HK dataset) involving 975,400 B-scans from 4,877 volumes to develop and evaluate the proposed method, then the model without further fine-tuning is directly applied on another independent dataset (i.e., Stanford dataset) containing 246,200 B-scans from 1,231 volumes. Extensive experiments are conducted to assess the contribution of each component within our framework. The proposed method outperforms the baseline methods and two glaucoma experts by a large margin, achieving volume-level Area Under ROC Curve (AUC) of 0.977 on HK dataset and 0.933 on Stanford dataset, respectively. The experimental results indicate the great potential of the proposed approach for the automated diagnosis system.", "keywords": ["Optical Coherence Tomography", "irreversible blindness", "glaucoma OCT image", "proposed method", "OCT image", "OCT image dataset", "OCT", "Glaucoma", "proposed", "dataset", "Optical Coherence", "Coherence Tomography", "largest glaucoma OCT", "glaucoma OCT", "Stanford dataset", "function", "method", "OCT images", "function regression", "function assessments play"], "paper_title": "Towards multi-center glaucoma OCT image screening with semi-supervised joint structure and function multi-task learning.", "last_updated": "2023/02/04"}, {"id": "0029295107", "domain": "Glaucoma (unspecified)", "model_name": "Cerentini et al.", "publication_date": "2018/06/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29295107/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "29295107", "task": null, "abstract": "This paper proposes an automatic classification method to detect glaucoma in fundus images. The method is based on training a neural network using public image databases. The network used in this paper is the GoogLeNet, adapted for this proposal. The methodology was divided into two stages, namely: (1) detection of the region of interest (ROI); (2) image classification. We first used a sliding-window approach combined with the GoogLeNet network. This network was trained using manually extracted ROIs and other fundus image structures. Afterwards, another GoogLeNet model was trained using the previous resulting images. Then those images were used to train another GoogLeNet model to automatically detect glaucoma. To prevent overfitting, data augmentation techniques were used on smaller databases. The results demonstrated that the network had a good accuracy, even with poor quality images found in some databases or generated by the data augmentation algorithm.", "keywords": ["automatic classification method", "proposes an automatic", "paper proposes", "network", "automatic classification", "GoogLeNet", "images", "method", "classification method", "image", "GoogLeNet model", "paper", "detect glaucoma", "databases", "proposes", "automatic", "classification", "fundus", "detect", "glaucoma"], "paper_title": "Automatic Identification of Glaucoma Using Deep Learning Methods.", "last_updated": "2023/02/04"}, {"id": "0032832206", "domain": "Glaucoma (unspecified)", "model_name": "Fu et al.", "publication_date": "2020/06/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32832206/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32832206", "task": "aipbNdPTIt", "abstract": "Optic disc (OD) and optic cup (OC) segmentation are fundamental for fundus image analysis. Manual annotation is time consuming, expensive, and highly subjective, whereas an automated system is invaluable to the medical community. The aim of this study is to develop a deep learning system to segment OD and OC in fundus photographs, and evaluate how the algorithm compares against manual annotations. A total of 1200 fundus photographs with 120 glaucoma cases were collected. The OD and OC annotations were labeled by seven licensed ophthalmologists, and glaucoma diagnoses were based on comprehensive evaluations of the subject medical records. A deep learning system for OD and OC segmentation was developed. The performances of segmentation and glaucoma discriminating based on the cup-to-disc ratio (CDR) of automated model were compared against the manual annotations. The algorithm achieved an OD dice of 0.938 (95% confidence interval [CI] = 0.934-0.941), OC dice of 0.801 (95% CI = 0.793-0.809), and CDR mean absolute error (MAE) of 0.077 (95% CI = 0.073 mean absolute error (MAE)0.082). For glaucoma discriminating based on CDR calculations, the algorithm obtained an area under receiver operator characteristic curve (AUC) of 0.948 (95% CI = 0.920 mean absolute error (MAE)0.973), with a sensitivity of 0.850 (95% CI = 0.794-0.923) and specificity of 0.853 (95% CI = 0.798-0.918). We demonstrated the potential of the deep learning system to assist ophthalmologists in analyzing OD and OC segmentation and discriminating glaucoma from nonglaucoma subjects based on CDR calculations. We investigate the segmentation of OD and OC by deep learning system compared against the manual annotations.", "keywords": ["fundus image analysis", "deep learning system", "Optic disc", "optic cup", "image analysis", "learning system", "deep learning", "manual annotations", "CDR", "system", "Optic", "MAE", "absolute error", "CDR calculations", "fundus image", "glaucoma discriminating based", "segmentation", "glaucoma", "Manual", "learning"], "paper_title": "A Retrospective Comparison of Deep Learning to Manual Annotations for Optic Disc and Optic Cup Segmentation in Fundus Photographs.", "last_updated": "2023/02/04"}, {"id": "0031718841", "domain": "Glaucoma (unspecified)", "model_name": "Christopher et al.", "publication_date": "2019/09/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31718841/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31718841", "task": null, "abstract": "To develop and evaluate a deep learning system for differentiating between eyes with and without glaucomatous visual field damage (GVFD) and predicting the severity of GFVD from spectral domain OCT (SD\u00a0OCT) optic nerve head images. Evaluation of a diagnostic technology. A total of 9765 visual field (VF) SD OCT pairs collected from 1194 participants with and without GVFD (1909 eyes). Deep learning models were trained to use SD OCT retinal nerve fiber layer (RNFL) thickness maps, RNFL en face images, and confocal scanning laser ophthalmoscopy (CSLO) images to identify eyes with GVFD and predict quantitative VF mean deviation (MD), pattern standard deviation (PSD), and mean VF sectoral pattern deviation (PD) from SD OCT data. Deep learning models were compared with mean RNFL thickness for identifying GVFD using area under the curve (AUC), sensitivity, and specificity. For predicting MD, PSD, and mean sectoral PD, models were evaluated using R<sup>2</sup> and mean absolute error (MAE). In the independent test dataset, the deep learning models based on RNFL en face images achieved an AUC of 0.88 for identifying eyes with GVFD and 0.82 for detecting mild GVFD significantly (P < 0.001) better than using mean RNFL thickness measurements (AUC\u00a0= 0.82 and 0.73, respectively). Deep learning models outperformed standard RNFL thickness measurements in predicting all quantitative VF metrics. In predicting MD, deep learning models based on RNFL en face images achieved an R<sup>2</sup> of 0.70 and MAE of 2.5 decibels (dB) compared with 0.45 and 3.7 dB for RNFL thickness measurements. In predicting mean VF sectoral PD, deep learning models achieved high accuracy in the inferior nasal (R<sup>2</sup>\u00a0= 0.60) and superior nasal (R<sup>2</sup>\u00a0= 0.67) sectors, moderate accuracy in inferior (R<sup>2</sup>\u00a0= 0.26) and superior (R<sup>2</sup>\u00a0= 0.35) sectors, and lower accuracy in the central (R<sup>2</sup>\u00a0=\u00a00.15) and temporal (R<sup>2</sup>\u00a0= 0.12) sectors. Deep learning models had high accuracy in identifying eyes with GFVD and predicting the\u00a0severity of functional loss from SD OCT images. Accurately predicting the severity of GFVD from SD OCT imaging can help clinicians more effectively individualize the frequency of VF testing to the individual patient.", "keywords": ["Deep learning models", "spectral domain OCT", "learning models", "deep learning", "RNFL thickness measurements", "RNFL thickness", "RNFL", "OCT", "optic nerve head", "deep learning system", "GVFD", "learning", "learning models based", "models", "visual field damage", "domain OCT", "glaucomatous visual field", "deep", "predicting", "develop and evaluate"], "paper_title": "Deep Learning Approaches Predict Glaucomatous Visual Field Damage from OCT Optic Nerve Head En Face Images and Retinal Nerve Fiber Layer Thickness Maps.", "last_updated": "2023/02/04"}, {"id": "0032094401", "domain": "Glaucoma (unspecified)", "model_name": "Lee et al.", "publication_date": "2020/02/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32094401/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32094401", "task": null, "abstract": "We developed a hybrid deep learning model (HDLM) algorithm that quantitatively predicts macular ganglion cell-inner plexiform layer (mGCIPL) thickness from red-free retinal nerve fiber layer photographs (RNFLPs). A total of 789 pairs of RNFLPs and spectral domain-optical coherence tomography (SD-OCT) scans for 431 eyes of 259 participants (183 eyes of 114 healthy controls, 68 eyes of 46 glaucoma suspects, and 180 eyes of 99 glaucoma patients) were enrolled. An HDLM was built by combining a pre-trained deep learning network and support vector machine. The correlation coefficient and mean absolute error (MAE) between the predicted and measured mGCIPL thicknesses were calculated. The measured (OCT-based) and predicted (HDLM-based) average mGCIPL thicknesses were 73.96\u2009\u00b1\u20098.81\u2009\u00b5m and 73.92\u2009\u00b1\u20097.36\u2009\u00b5m, respectively (P\u2009=\u20090.844). The predicted mGCIPL thickness showed a strong correlation and good agreement with the measured mGCIPL thickness (Correlation coefficient r\u2009=\u20090.739; P\u2009<\u20090.001; MAE\u2009=\u20094.76\u2009\u00b5m). Even when the peripapillary area (diameter: 1.5 disc diameters) was masked, the correlation (r\u2009=\u20090.713; P\u2009<\u20090.001) and agreement (MAE\u2009=\u20094.87\u2009\u00b5m) were not changed significantly (P\u2009=\u20090.378 and 0.724, respectively). The trained HDLM algorithm showed a great capability for mGCIPL thickness prediction from RNFLPs.", "keywords": ["fiber layer photographs", "cell-inner plexiform layer", "nerve fiber layer", "quantitatively predicts macular", "predicts macular ganglion", "macular ganglion cell-inner", "ganglion cell-inner plexiform", "red-free retinal nerve", "retinal nerve fiber", "deep learning model", "hybrid deep learning", "layer photographs", "plexiform layer", "fiber layer", "developed a hybrid", "quantitatively predicts", "predicts macular", "macular ganglion", "ganglion cell-inner", "cell-inner plexiform"], "paper_title": "Macular Ganglion Cell-Inner Plexiform Layer Thickness Prediction from Red-free Fundus Photography using Hybrid Deep Learning Model.", "last_updated": "2023/02/04"}, {"id": "0033323187", "domain": "Glaucoma (unspecified)", "model_name": "Ran et al.", "publication_date": "2019/08/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33323187/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33323187", "task": "IWqQC1koJA", "abstract": "Spectral-domain optical coherence tomography (SDOCT) can be used to detect glaucomatous optic neuropathy, but human expertise in interpretation of SDOCT is limited. We aimed to develop and validate a three-dimensional (3D) deep-learning system using SDOCT volumes to detect glaucomatous optic neuropathy. We retrospectively collected a dataset including 4877 SDOCT volumes of optic disc cube for training (60%), testing (20%), and primary validation (20%) from electronic medical and research records at the Chinese University of Hong Kong Eye Centre (Hong Kong, China) and the Hong Kong Eye Hospital (Hong Kong, China). Residual network was used to build the 3D deep-learning system. Three independent datasets (two from Hong Kong and one from Stanford, CA, USA), including 546, 267, and 1231 SDOCT volumes, respectively, were used for external validation of the deep-learning system. Volumes were labelled as having or not having glaucomatous optic neuropathy according to the criteria of retinal nerve fibre layer thinning on reliable SDOCT images with position-correlated visual field defect. Heatmaps were generated for qualitative assessments. 6921 SDOCT volumes from 1\u2008384\u2008200 two-dimensional cross-sectional scans were studied. The 3D deep-learning system had an area under the receiver operation characteristics curve (AUROC) of 0\u00b7969 (95% CI 0\u00b7960-0\u00b7976), sensitivity of 89% (95% CI 83-93), specificity of 96% (92-99), and accuracy of 91% (89-93) in the primary validation, outperforming a two-dimensional deep-learning system that was trained on en face fundus images (AUROC 0\u00b7921 [0\u00b7905-0\u00b7937]; p<0\u00b70001). The 3D deep-learning system performed similarly in the external validation datasets, with AUROCs of 0\u00b7893-0\u00b7897, sensitivities of 78-90%, specificities of 79-86%, and accuracies of 80-86%. The heatmaps of glaucomatous optic neuropathy showed that the learned features by the 3D deep-learning system used for detection of glaucomatous optic neuropathy were similar to those used by clinicians. The proposed 3D deep-learning system performed well in detection of glaucomatous optic neuropathy in both primary and external validations. Further prospective studies are needed to estimate the incremental cost-effectiveness of incorporation of an artificial intelligence-based model for glaucoma screening. Hong Kong Research Grants Council.", "keywords": ["Hong Kong Eye", "glaucomatous optic neuropathy", "Hong Kong", "Spectral-domain optical coherence", "optical coherence tomography", "detect glaucomatous optic", "Kong Eye Centre", "Kong Eye Hospital", "glaucomatous optic", "optic neuropathy", "SDOCT volumes", "deep-learning system", "Hong Kong Research", "Kong Eye", "SDOCT", "Kong", "Hong", "optic", "detect glaucomatous", "system"], "paper_title": "Detection of glaucomatous optic neuropathy with spectral-domain optical coherence tomography: a retrospective training and validation deep-learning analysis.", "last_updated": "2023/02/04"}, {"id": "0036442272", "domain": "Glaucoma (unspecified)", "model_name": "Song et al.", "publication_date": "2022/11/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36442272/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36442272", "task": "IWqQC1koJA", "abstract": "Glaucoma has become a major cause of vision loss. Early-stage diagnosis of glaucoma is critical for treatment planning to avoid irreversible vision damage. Meanwhile, interpreting the rapidly accumulated medical data from ophthalmic exams is cumbersome and resource-intensive. Therefore, automated methods are highly desired to assist ophthalmologists in achieving fast and accurate glaucoma diagnosis. Deep learning has achieved great successes in diagnosing glaucoma by analyzing data from different kinds of tests, such as peripapillary optical coherence tomography (OCT) and visual field (VF) testing. Nevertheless, applying these developed models to clinical practice is still challenging because of various limiting factors. OCT models present worse glaucoma diagnosis performances compared to those achieved by OCT&VF based models, whereas VF is time-consuming and highly variable, which can restrict the wide employment of OCT&VF models. To this end, we develop a novel deep learning framework that leverages the OCT&VF model to enhance the performance of the OCT model. To transfer the complementary knowledge from the structural and functional assessments to the OCT model, a cross-modal knowledge transfer method is designed by integrating a designed distillation loss and a proposed asynchronous feature regularization (AFR) module. We demonstrate the effectiveness of the proposed method for glaucoma diagnosis by utilizing a public OCT&VF dataset and evaluating it on an external OCT dataset. Our final model with only OCT inputs achieves the accuracy of 87.4% (3.1% absolute improvement) and AUC of 92.3%, which are on par with the OCT&VF joint model. Moreover, results on the external dataset sufficiently indicate the effectiveness and generalization capability of our model.", "keywords": ["OCT", "OCT model", "Glaucoma", "glaucoma diagnosis", "model", "irreversible vision damage", "diagnosis", "models", "avoid irreversible vision", "OCT models", "OCT models present", "vision", "vision damage", "OCT dataset", "irreversible vision", "external OCT", "external OCT dataset", "vision loss", "dataset", "public OCT"], "paper_title": "Asynchronous feature regularization and cross-modal distillation for OCT based glaucoma diagnosis.", "last_updated": "2023/02/04"}, {"id": "0036094722", "domain": "Glaucoma (unspecified)", "model_name": "Shalini et al.", "publication_date": "2022/09/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36094722/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36094722", "task": "aipbNdPTIt", "abstract": "Glaucoma is a major cause of blindness worldwide, and its early detection is essential for the timely management of the condition. Glaucoma-induced anomalies of the optic nerve head may cause variation in the Optic Disc (OD) size. Therefore, robust OD segmentation techniques are necessary for the screening for glaucoma. Computer-aided segmentation has become a promising diagnostic tool for the early detection of glaucoma, and there has been much interest in recent years in using neural networks for medical image segmentation. This study proposed an enhanced lightweight U-Net model with an Attention Gate (AG) to segment OD images. We also used a transfer learning strategy to extract relevant features using a pre-trained EfficientNet-B0 CNN, which preserved the receptive field size and AG, which reduced the impact of gradient vanishing and overfitting. Additionally, the neural network trained using the binary focal loss function improved segmentation accuracy. The pre-trained Attention U-Net was validated using publicly available datasets, such as DRIONS-DB, DRISHTI-GS, and MESSIDOR. The model significantly reduced parameter quantity by around 0.53 M and had inference times of 40.3 ms, 44.2 ms, and 60.6 ms, respectively.", "keywords": ["blindness worldwide", "timely management", "Optic Disc", "optic nerve head", "early detection", "optic nerve", "segmentation", "optic", "Attention Gate", "detection is essential", "Glaucoma", "pre-trained Attention U-Net", "worldwide", "condition", "major", "blindness", "essential", "timely", "management", "Attention"], "paper_title": "Deep learning approaches based improved light weight U-Net with attention module for optic disc segmentation.", "last_updated": "2023/02/04"}, {"id": "0033500462", "domain": "Glaucoma (unspecified)", "model_name": "Asano et al.", "publication_date": "2021/01/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33500462/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33500462", "task": null, "abstract": "We aimed to develop a model to predict visual field (VF) in the central 10 degrees in patients with glaucoma, by training a convolutional neural network (CNN) with optical coherence tomography (OCT) images and adjusting the values with Humphrey Field Analyzer (HFA) 24-2 test. The training dataset included 558 eyes from 312 glaucoma patients and 90 eyes from 46 normal subjects. The testing dataset included 105 eyes from 72 glaucoma patients. All eyes were analyzed by the HFA 10-2 test and OCT; eyes in the testing dataset were additionally analyzed by the HFA 24-2 test. During CNN model training, the total deviation (TD) values of the HFA 10-2 test point were predicted from the combined OCT-measured macular retinal layers' thicknesses. Then, the predicted TD values were corrected using the TD values of the innermost four points from the HFA 24-2 test. Mean absolute error derived from the CNN models ranged between 9.4 and 9.5 B. These values reduced to 5.5\u00a0dB on average, when the data were corrected using the HFA 24-2 test. In conclusion, HFA 10-2 test results can be predicted with a OCT images using a trained CNN model with adjustment using HFA 24-2 test.", "keywords": ["Humphrey Field Analyzer", "predict visual field", "HFA 24-2 test", "Field Analyzer", "Humphrey Field", "HFA 10-2 test", "convolutional neural network", "optical coherence tomography", "HFA 24-2", "HFA", "visual field", "HFA 10-2", "24-2 test", "field", "test", "neural network", "coherence tomography", "CNN model training", "glaucoma patients", "aimed to develop"], "paper_title": "Predicting the central 10 degrees visual field in glaucoma by applying a deep learning algorithm to optical coherence tomography images.", "last_updated": "2023/02/04"}, {"id": "0034153436", "domain": "Glaucoma (unspecified)", "model_name": "Liu et al.", "publication_date": "2021/06/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34153436/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34153436", "task": null, "abstract": "Glaucoma is a chronic eye disease, which causes gradual vision loss and eventually blindness. Accurate glaucoma screening at early stage is critical to mitigate its aggravation. Extracting high-quality features are critical in training of classification models. In this paper, we propose a deep ensemble network with attention mechanism that detects glaucoma using optic nerve head stereo images. The network consists of two main sub-components, a deep Convolutional Neural Network that obtains global information and an Attention-Guided Network that localizes optic disc while maintaining beneficial information from other image regions. Both images in a stereo pair are fed into these sub-components, the outputs are fused together to generate the final prediction result. Abundant image features from different views and regions are being extracted, providing compensation when one of the stereo images is of poor quality. The attention-based localization method is trained in a weakly-supervised manner and only image-level annotation is required, which avoids expensive segmentation labelling. Results from real patient images show that our approach increases recall (sensitivity) from the state-of-the-art 88.89% to 95.48%, while maintaining precision and performance stability. The marked reduction in false-negative rate can significantly enhance the chance of successful early diagnosis of glaucoma.", "keywords": ["chronic eye disease", "gradual vision loss", "eye disease", "eventually blindness", "chronic eye", "gradual vision", "vision loss", "loss and eventually", "Convolutional Neural Network", "network", "deep Convolutional Neural", "Glaucoma", "Accurate glaucoma screening", "images", "Convolutional Neural", "Neural Network", "disease", "blindness", "stereo", "stereo images"], "paper_title": "Glaucoma screening using an attention-guided stereo ensemble network.", "last_updated": "2023/02/04"}, {"id": "0032344074", "domain": "Glaucoma (unspecified)", "model_name": "Spaide et al.", "publication_date": "2020/04/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32344074/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32344074", "task": null, "abstract": "To develop an objective and automated method for measuring intraocular pressure using deep learning and fixed-force Goldmann applanation tonometry (GAT) techniques. Prospective cross-sectional study. Patients from an academic glaucoma practice. Intraocular pressure was estimated by analyzing videos recorded using a standard slit-lamp microscope and fixed-force GAT. Video frames were labeled to identify the outline of the reference tonometer and the applanation mires. A deep learning model was trained to localize and segment the tonometer and mires. Intraocular pressure values were calculated from the deep learning-predicted tonometer and mire diameters using the Imbert-Fick formula. A separate test set was collected prospectively in which standard and automated GAT measurements were collected in random order by 2 independent masked observers to assess the deep learning model as well as interobserver variability. Intraocular pressure measurements between standard and automated methods were compared. Two hundred sixty-three eyes of 135 patients were included in the training and validation videos. For the test set, 50 eyes from 25 participants were included. Each eye was measured by 2 observers, resulting in 100 videos. Within the test set, the mean difference between automated and standard GAT results was -0.9 mmHg (95% limits of agreement [LoA], -5.4 to 3.6 mmHg). Mean difference between the 2 observers using standard GAT was 0.09 mmHg (LoA,-3.8 to 4.0 mmHg). Mean difference between the 2 observers using automated GAT videos was -0.3 mmHg (LoA, -4.1 to 3.5 mmHg). The coefficients of repeatability for automated and standard GAT were 3.8 and 3.9 mmHg, respectively. The bias for even-numbered measurements was reduced when using automated GAT. Preliminary measurements using deep learning to automate GAT demonstrate results comparable with those of standard GAT. Automated GAT has the potential to improve on our current GAT measurement standards significantly by reducing bias and improving repeatability. In addition, ocular pulse amplitudes could be observed using this technique.", "keywords": ["Goldmann applanation tonometry", "GAT", "fixed-force Goldmann applanation", "automated GAT", "fixed-force Goldmann", "Goldmann applanation", "standard GAT", "mmHg", "automated", "deep learning", "intraocular pressure", "standard", "Goldmann", "develop an objective", "deep", "measuring intraocular pressure", "intraocular", "pressure", "learning", "automated GAT videos"], "paper_title": "Using Deep Learning to Automate Goldmann Applanation Tonometry Readings.", "last_updated": "2023/02/04"}, {"id": "0032459689", "domain": "Glaucoma (unspecified)", "model_name": "Hood et al.", "publication_date": "2020/12/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32459689/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32459689", "task": "0GPcU42tzV", "abstract": "Glaucoma is typically defined as a progressive optic neuropathy characterized by a specific (arcuate) pattern of visual field (VF) and anatomic changes. Therefore, we should be comparing arcuate patterns of damage seen on VFs with those seen on optical coherence tomography (OCT) maps. Instead, clinicians often use summary metrics such as VF pattern standard deviation, OCT retinal nerve fiber (RNF) global thickness, etc. There are 2 major impediments to topographically comparing patterns of damage on VF and OCT maps. First, until recently, it was not easy to make these comparisons with commercial reports. While recent reports do make it easier to compare VF and OCT maps, they have shortcomings. In particular, the 24-2 VF covers a larger retinal region than the commercial OCT scans, and, further, it is not easy to understand the topographical relationship among the different maps/plots within the current OCT reports. Here we show how a model of RNF bundles can overcome these problems. The second major impediment is the lack of a quantitative, and automated, method for comparing patterns of damage seen on VF and OCT maps. However, it is now possible to objectively and automatically quantify this agreement. Together, the RNF bundle model and the automated structure-function method should improve the power of topographical methods for detecting glaucoma and its progression. This should prove useful in clinical studies and trials, as well as for training and validating artificial intelligence/deep learning approaches for these purposes.", "keywords": ["progressive optic neuropathy", "optic neuropathy characterized", "OCT maps", "OCT", "visual field", "typically defined", "progressive optic", "optic neuropathy", "neuropathy characterized", "comparing arcuate patterns", "patterns of damage", "maps", "comparing patterns", "pattern of visual", "arcuate patterns", "patterns", "RNF", "OCT retinal nerve", "damage", "OCT reports"], "paper_title": "Improving the Detection of Glaucoma and Its Progression: A Topographical Approach.", "last_updated": "2023/02/04"}, {"id": "0034412848", "domain": "Glaucoma (unspecified)", "model_name": "Garc\u00eda et al.", "publication_date": "2021/07/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34412848/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34412848", "task": null, "abstract": "Glaucoma is one of the leading causes of blindness worldwide and Optical Coherence Tomography (OCT) is the quintessential imaging technique for its detection. Unlike most of the state-of-the-art studies focused on glaucoma detection, in this paper, we propose, for the first time, a novel framework for glaucoma grading using raw circumpapillary B-scans. In particular, we set out a new OCT-based hybrid network which combines hand-driven and deep learning algorithms. An OCT-specific descriptor is proposed to extract hand-crafted features related to the retinal nerve fibre layer (RNFL). In parallel, an innovative CNN is developed using skip-connections to include tailored residual and attention modules to refine the automatic features of the latent space. The proposed architecture is used as a backbone to conduct a novel few-shot learning based on static and dynamic prototypical networks. The k-shot paradigm is redefined giving rise to a supervised end-to-end system which provides substantial improvements discriminating between healthy, early and advanced glaucoma samples. The training and evaluation processes of the dynamic prototypical network are addressed from two fused databases acquired via Heidelberg Spectralis system. Validation and testing results reach a categorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively. Besides, the high performance reported by the proposed model for glaucoma detection deserves a special mention. The findings from the class activation maps are directly in line with the clinicians' opinion since the heatmaps pointed out the RNFL as the most relevant structure for glaucoma diagnosis.", "keywords": ["Optical Coherence Tomography", "Coherence Tomography", "quintessential imaging technique", "Optical Coherence", "worldwide and Optical", "blindness worldwide", "quintessential imaging", "imaging technique", "glaucoma detection", "raw circumpapillary B-scans", "Glaucoma", "OCT", "glaucoma detection deserves", "Tomography", "Optical", "Coherence", "glaucoma grading", "detection", "circumpapillary B-scans", "Heidelberg Spectralis system"], "paper_title": "Circumpapillary OCT-focused hybrid learning for glaucoma grading using tailored prototypical neural networks.", "last_updated": "2023/02/04"}, {"id": "0031792321", "domain": "Glaucoma (unspecified)", "model_name": "Berchuck et al.", "publication_date": "2019/12/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31792321/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31792321", "task": "0GPcU42tzV", "abstract": "In this manuscript we develop a deep learning algorithm to improve estimation of rates of progression and prediction of future patterns of visual field loss in glaucoma. A generalized variational auto-encoder (VAE) was trained to learn a low-dimensional representation of standard automated perimetry (SAP) visual fields using 29,161 fields from 3,832 patients. The VAE was trained on a 90% sample of the data, with randomization at the patient level. Using the remaining 10%, rates of progression and predictions were generated, with comparisons to SAP mean deviation (MD) rates and point-wise (PW) regression predictions, respectively. The longitudinal rate of change through the VAE latent space (e.g., with eight dimensions) detected a significantly higher proportion of progression than MD at two (25% vs. 9%) and four (35% vs 15%) years from baseline. Early on, VAE improved prediction over PW, with significantly smaller mean absolute error in predicting the 4<sup>th</sup>, 6<sup>th</sup> and 8<sup>th</sup> visits from the first three (e.g., visit eight: VAE8: 5.14\u2009dB vs. PW: 8.07\u2009dB; P\u2009<\u20090.001). A deep VAE can be used for assessing both rates and trajectories of progression in glaucoma, with the additional benefit of being a generative technique capable of predicting future patterns of visual field damage.", "keywords": ["visual field loss", "deep learning algorithm", "visual field", "manuscript we develop", "learning algorithm", "algorithm to improve", "improve estimation", "visual field damage", "field loss", "VAE", "visual", "loss in glaucoma", "patterns of visual", "progression", "develop a deep", "deep learning", "rates", "VAE improved prediction", "future patterns", "rates of progression"], "paper_title": "Estimating Rates of Progression and Predicting Future Visual Fields in Glaucoma Using a Deep Variational Autoencoder.", "last_updated": "2023/02/04"}, {"id": "0034777561", "domain": "Glaucoma (unspecified)", "model_name": "Ganesh et al.", "publication_date": "2021/11/05", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34777561/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34777561", "task": "aipbNdPTIt", "abstract": "Glaucoma is a chronic ocular disease characterized by damage to the optic nerve resulting in progressive and irreversible visual loss. Early detection and timely clinical interventions are critical in improving glaucoma-related outcomes. As a typical and complicated ocular disease, glaucoma detection presents a unique challenge due to its insidious onset and high intra- and interpatient variabilities. Recent studies have demonstrated that robust glaucoma detection systems can be realized with deep learning approaches. The optic disc (OD) is the most commonly studied retinal structure for screening and diagnosing glaucoma. This paper proposes a novel context aware deep learning framework called GD-YNet, for OD segmentation and glaucoma detection. It leverages the potential of aggregated transformations and the simplicity of the YNet architecture in context aware OD segmentation and binary classification for glaucoma detection. Trained with the RIGA and RIMOne-V2 datasets, this model achieves glaucoma detection accuracies of 99.72%, 98.02%, 99.50%, and 99.41% with the ACRIMA, Drishti-gs, REFUGE, and RIMOne-V1 datasets. Further, the proposed model can be extended to a multiclass segmentation and classification model for glaucoma staging and severity assessment.", "keywords": ["irreversible visual loss", "optic nerve resulting", "chronic ocular disease", "ocular disease characterized", "glaucoma detection", "visual loss", "characterized by damage", "nerve resulting", "resulting in progressive", "progressive and irreversible", "irreversible visual", "Glaucoma", "ocular disease", "detection", "chronic ocular", "disease characterized", "complicated ocular disease", "optic nerve", "glaucoma detection presents", "robust glaucoma detection"], "paper_title": "A Novel Context Aware Joint Segmentation and Classification Framework for Glaucoma Detection.", "last_updated": "2023/02/04"}, {"id": "0035280876", "domain": "Glaucoma (unspecified)", "model_name": "Zhu et al.", "publication_date": "2022/02/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35280876/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35280876", "task": null, "abstract": "A six-category model of common retinal diseases is proposed to help primary medical institutions in the preliminary screening of the five common retinal diseases. A total of 2,400 fundus images of normal and five common retinal diseases were provided by a cooperative hospital. Two six-category deep learning models of common retinal diseases based on the EfficientNet-B4 and ResNet50 models were trained. The results from the six-category models in this study and the results from a five-category model in our previous study based on ResNet50 were compared. A total of 1,315 fundus images were used to test the models, the clinical diagnosis results and the diagnosis results of the two six-category models were compared. The main evaluation indicators were sensitivity, specificity, F1-score, area under the curve (AUC), 95% confidence interval, kappa and accuracy, and the receiver operator characteristic curves of the two six-category models were compared in the study. The diagnostic accuracy rate of EfficientNet-B4 model was 95.59%, the kappa value was 94.61%, and there was high diagnostic consistency. The AUC of the normal diagnosis and the five retinal diseases were all above 0.95. The sensitivity, specificity, and F1-score for the diagnosis of normal fundus images were 100, 99.9, and 99.83%, respectively. The specificity and F1-score for RVO diagnosis were 95.68, 98.61, and 93.09%, respectively. The sensitivity, specificity, and F1-score for high myopia diagnosis were 96.1, 99.6, and 97.37%, respectively. The sensitivity, specificity, and F1-score for glaucoma diagnosis were 97.62, 99.07, and 94.62%, respectively. The sensitivity, specificity, and F1-score for DR diagnosis were 90.76, 99.16, and 93.3%, respectively. The sensitivity, specificity, and F1-score for MD diagnosis were 92.27, 98.5, and 91.51%, respectively. The EfficientNet-B4 model was used to design a six-category model of common retinal diseases. It can be used to diagnose the normal fundus and five common retinal diseases based on fundus images. It can help primary doctors in the screening for common retinal diseases, and give suitable suggestions and recommendations. Timely referral can improve the efficiency of diagnosis of eye diseases in rural areas and avoid delaying treatment.", "keywords": ["common retinal diseases", "retinal diseases", "common retinal", "six-category models", "retinal diseases based", "retinal", "diagnosis", "diseases", "models", "common", "six-category", "model", "fundus images", "sensitivity", "primary medical institutions", "specificity", "medical institutions", "fundus", "diseases based", "diagnosis results"], "paper_title": "Screening of Common Retinal Diseases Using Six-Category Models Based on EfficientNet.", "last_updated": "2023/02/04"}, {"id": "0033892734", "domain": "Glaucoma (unspecified)", "model_name": "Xu et al.", "publication_date": "2021/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33892734/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33892734", "task": "IWqQC1koJA", "abstract": "Glaucoma is one of the causes that leads to irreversible vision loss. Automatic glaucoma detection based on fundus images has been widely studied in recent years. However, existing methods mainly depend on a considerable amount of labeled data to train the model, which is a serious constraint for real-world glaucoma detection. In this paper, we introduce a transfer learning technique that leverages the fundus feature learned from similar ophthalmic data to facilitate diagnosing glaucoma. Specifically, a Transfer Induced Attention Network (TIA-Net) for automatic glaucoma detection is proposed, which extracts the discriminative features that fully characterize the glaucoma-related deep patterns under limited supervision. By integrating the channel-wise attention and maximum mean discrepancy, our proposed method can achieve a smooth transition between general and specific features, thus enhancing the feature transferability. To delimit the boundary between general and specific features precisely, we first investigate how many layers should be transferred during training with the source dataset network. Next, we compare our proposed model to previously mentioned methods and analyze their performance. Finally, with the advantages of the model design, we provide a transparent and interpretable transferring visualization by highlighting the key specific features in each fundus image. We evaluate the effectiveness of TIA-Net on two real clinical datasets and achieve an accuracy of 85.7%/76.6%, sensitivity of 84.9%/75.3%, specificity of 86.9%/77.2%, and AUC of 0.929 and 0.835, far better than other state-of-the-art methods. Different from previous studies applied classic CNN models to transfer features from the non-medical dataset, we leverage knowledge from the similar ophthalmic dataset and propose an attention-based deep transfer learning model for the glaucoma diagnosis task. Extensive experiments on two real clinical datasets show that our TIA-Net outperforms other state-of-the-art methods, and meanwhile, it has certain medical value and significance for the early diagnosis of other medical tasks.", "keywords": ["irreversible vision loss", "Automatic glaucoma detection", "glaucoma detection", "vision loss", "leads to irreversible", "irreversible vision", "Glaucoma", "Transfer Induced Attention", "glaucoma detection based", "Induced Attention Network", "specific features", "Automatic glaucoma", "real-world glaucoma detection", "features", "transfer", "model", "transfer learning", "detection", "methods", "Transfer Induced"], "paper_title": "Automatic glaucoma detection based on transfer induced attention network.", "last_updated": "2023/02/04"}, {"id": "0034678479", "domain": "Glaucoma (unspecified)", "model_name": "Zulfira et al.", "publication_date": "2021/10/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34678479/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34678479", "task": "aipbNdPTIt", "abstract": "The severity of glaucoma can be observed by categorising glaucoma diseases into several classes based on a classification process. The two most suitable parameters, cup-to-disc ratio (CDR) and peripapillary atrophy (PPA), which are commonly used to identify glaucoma are utilized in this study to strengthen the classification. First, an active contour snake (ACS) is employed to retrieve both optic disc (OD) and optic cup (OC) values, which are required to calculate the CDR. Moreover, Otsu segmentation and thresholding techniques are used to identify PPA, and the features are then extracted using a grey-level co-occurrence matrix (GLCM). An advanced segmentation technique, combined with an improved classifier called dynamic ensemble selection (DES), is proposed to classify glaucoma. Because DES is generally used to handle an imbalanced dataset, the proposed model is expected to detect glaucoma severity and determine the subsequent treatment accurately. The proposed model obtains a higher mean accuracy (0.96) than the deep learning-based U-Net (0.90) when evaluated using three datasets of 250 retinal fundus images (200 training, 50 testings) based on the 5-fold cross-validation scheme.", "keywords": ["categorising glaucoma diseases", "observed by categorising", "classification process", "categorising glaucoma", "glaucoma diseases", "classes based", "identify PPA", "glaucoma", "CDR", "PPA", "classification", "proposed model", "proposed", "process", "Otsu segmentation", "DES", "observed", "categorising", "diseases", "classes"], "paper_title": "Segmentation technique and dynamic ensemble selection to enhance glaucoma severity detection.", "last_updated": "2023/02/04"}, {"id": "0033510951", "domain": "Glaucoma (unspecified)", "model_name": "Shigueoka et al.", "publication_date": "2021/01/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33510951/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33510951", "task": null, "abstract": "To assess whether age can be predicted from deep learning analysis of peripapillary spectral-domain optical coherence tomography (SD-OCT) B-scans and to determine the importance of specific retinal areas on the predictions. Deep learning (DL) convolutional neural networks were developed to predict chronological age in healthy subjects using peripapillary SD-OCT B-scan images. Models were built using the whole B-scan, as well as using specific regions through image ablation. Cross-validation was used for training and testing the model. Mean absolute error (MAE) and correlations between predicted and observed age were used to evaluate model performance. A total of 7271 images from 542 eyes of 278 healthy subjects were included. DL predictions of age using the whole B-scan were strongly correlated with chronological age (MAE = 5.82 years; <i>r</i> = 0.860, <i>P</i> < 0.001). The model also accurately discriminated between the lowest and highest tertiles of age, with an area under the receiver operating characteristic curve of 0.962. In general, class activation maps tended to show a diffuse pattern of activation throughout the scan image. For specific structures of the B-scan, the layers with the strongest correlations with chronological age were the choroid and vitreous (both <i>r</i> = 0.736), whereas retinal nerve fiber layer had the lowest correlation (<i>r</i> = 0.492). A DL algorithm was able to accurately predict age from whole peripapillary SD-OCT B-scans. DL models applied to SD-OCT scans suggest that aging appears to affect several layers in the posterior eye segment.", "keywords": ["optical coherence tomography", "spectral-domain optical coherence", "peripapillary spectral-domain optical", "deep learning analysis", "deep learning", "peripapillary SD-OCT B-scan", "coherence tomography", "spectral-domain optical", "optical coherence", "determine the importance", "age", "B-scan", "learning analysis", "SD-OCT B-scan images", "peripapillary spectral-domain", "peripapillary SD-OCT", "chronological age", "peripapillary", "SD-OCT B-scan", "SD-OCT"], "paper_title": "Predicting Age From Optical Coherence Tomography Scans With Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0036877820", "domain": "Glaucoma (unspecified)", "model_name": "Chen et al.", "publication_date": "2022/12/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36877820/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36877820", "task": "0GPcU42tzV", "abstract": "We updated a clinical decision support tool integrating predicted visual field (VF) metrics from an artificial intelligence model and assessed clinician perceptions of the predicted VF metric in this usability study. To evaluate clinician perceptions of a prototyped clinical decision support (CDS) tool that integrates visual field (VF) metric predictions from artificial intelligence (AI) models. Ten ophthalmologists and optometrists from the University of California San Diego participated in 6 cases from 6 patients, consisting of 11 eyes, uploaded to a CDS tool (\"GLANCE\", designed to help clinicians \"at a glance\"). For each case, clinicians answered questions about management recommendations and attitudes towards GLANCE, particularly regarding the utility and trustworthiness of the AI-predicted VF metrics and willingness to decrease VF testing frequency. Mean counts of management recommendations and mean Likert scale scores were calculated to assess overall management trends and attitudes towards the CDS tool for each case. In addition, system usability scale scores were calculated. The mean Likert scores for trust in and utility of the predicted VF metric and clinician willingness to decrease VF testing frequency were 3.27, 3.42, and 2.64, respectively (1=strongly disagree, 5=strongly agree). When stratified by glaucoma severity, all mean Likert scores decreased as severity increased. The system usability scale score across all responders was 66.1\u00b116.0 (43rd percentile). A CDS tool can be designed to present AI model outputs in a useful, trustworthy manner that clinicians are generally willing to integrate into their clinical decision-making. Future work is needed to understand how to best develop explainable and trustworthy CDS tools integrating AI before clinical deployment.", "keywords": ["clinical decision support", "CDS tool", "California San Diego", "assessed clinician perceptions", "visual field", "decision support tool", "decision support", "artificial intelligence model", "predicted visual field", "CDS", "artificial intelligence", "integrates visual field", "clinician perceptions", "CDS tools integrating", "tool", "San Diego participated", "Likert scores", "clinical decision", "trustworthy CDS tools", "GLANCE"], "paper_title": "Usability and Clinician Acceptance of a Deep Learning-Based Clinical Decision Support Tool for Predicting Glaucomatous Visual Field Progression.", "last_updated": "2023/02/04"}, {"id": "0031989285", "domain": "Glaucoma (unspecified)", "model_name": "Li et al.", "publication_date": "2020/01/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31989285/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31989285", "task": "IWqQC1koJA", "abstract": "To develop a deep learning approach based on deep residual neural network (ResNet101) for the automated detection of glaucomatous optic neuropathy (GON) using color fundus images, understand the process by which the model makes predictions, and explore the effect of the integration of fundus images and the medical history data from patients. A total of 34,279 fundus images and the corresponding medical history data were retrospectively collected from cohorts of 2371 adult patients, and these images were labeled by 8 glaucoma experts, in which 26,585 fundus images (12,618 images with GON-confirmed eyes, 1114 images with GON-suspected eyes, and 12,853 NORMAL eye images) were included. We adopted 10-fold cross-validation strategy to train and optimize our model. This model was tested in an independent testing dataset consisting of 3481 images (1524 images from NORMAL eyes, 1442 images from GON-confirmed eyes, and 515 images from GON-suspected eyes) from 249 patients. Moreover, the performance of the best model was compared with results obtained by two experts. Accuracy, sensitivity, specificity, kappa value, and area under receiver operating characteristic (AUC) were calculated. Further, we performed qualitative evaluation of model predictions and occlusion testing. Finally, we assessed the effect of integrating medical history data in the final classification. In a multiclass comparison between GON-confirmed eyes, GON-suspected eyes and NORMAL eyes, our model achieved 0.941 (95% confidence interval [CI], 0.936-0.946) accuracy, 0.957 (95% CI, 0.953-0.961) sensitivity, and 0.929 (95% CI, 0.923-0.935) specificity. The AUC distinguishing referrals (GON-confirmed and GON-suspected eyes) from observation was 0.992 (95% CI, 0.991-0.993). Our best model had a kappa value of 0.927, while the two experts' kappa values were 0.928 and 0.925 independently. The best 2 binary classifiers distinguishing GON-confirmed/GON-suspected eyes from NORMAL eyes obtained 0.955, 0.965 accuracy, 0.977, 0.998 sensitivity, and 0.929, 0.954 specificity, while the AUC was 0.992, 0.999 respectively. Additionally, the occlusion testing showed that our model identified the neuroretinal rim region, retinal nerve fiber layer (RNFL) defect areas (superior or inferior) as the most important parts for the discrimination of GON, which evaluated fundus images in a way similar to clinicians. Finally, the results of integration of fundus images with medical history data showed a slight improvement in sensitivity and specificity with similar AUCs. This approach could discriminate GON with high accuracy, sensitivity, specificity, and AUC using color fundus photographs. It may provide a second opinion on the diagnosis of glaucoma to the specialist quickly, efficiently and at low cost, and assist doctors and the public in large-scale screening for glaucoma.", "keywords": ["deep residual neural", "residual neural network", "glaucomatous optic neuropathy", "images", "fundus images", "medical history data", "NORMAL eye images", "deep learning approach", "NORMAL eyes", "eyes", "GON-suspected eyes", "deep learning", "deep residual", "medical history", "history data", "model", "learning approach based", "neural network", "optic neuropathy", "understand the process"], "paper_title": "Deep learning-based automated detection of glaucomatous optic neuropathy on color fundus photographs.", "last_updated": "2023/02/04"}, {"id": "0033850515", "domain": "Glaucoma (unspecified)", "model_name": "Wang et al.", "publication_date": "2019/02/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33850515/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33850515", "task": "aipbNdPTIt", "abstract": "Accurate segmentation of the optic disc (OD) depicted on color fundus images may aid in the early detection and quantitative diagnosis of retinal diseases, such as glaucoma and optic atrophy. In this study, we proposed a coarse-to-fine deep learning framework on the basis of a classical convolutional neural network (CNN), known as the U-net model, to accurately identify the optic disc. This network was trained separately on color fundus images and their grayscale vessel density maps, leading to two different segmentation results from the entire image. We combined the results using an overlap strategy to identify a local image patch (disc candidate region), which was then fed into the U-net model for further segmentation. Our experiments demonstrated that the developed framework achieved an average intersection over union (IoU) and a dice similarity coefficient (DSC) of 89.1% and 93.9%, respectively, based on 2,978 test images from our collected dataset and six public datasets, as compared to 87.4% and 92.5% obtained by only using the sole U-net model. The comparison with available approaches demonstrated a reliable and relatively high performance of the proposed deep learning framework in automated OD segmentation.", "keywords": ["color fundus images", "optic disc", "U-net model", "retinal diseases", "optic atrophy", "early detection", "detection and quantitative", "quantitative diagnosis", "diagnosis of retinal", "color fundus", "depicted on color", "fundus images", "deep learning framework", "Accurate segmentation", "sole U-net model", "optic", "U-net", "disc", "convolutional neural network", "segmentation"], "paper_title": "A coarse-to-fine deep learning framework for optic disc segmentation in fundus images.", "last_updated": "2023/02/04"}, {"id": "0034932117", "domain": "Glaucoma (unspecified)", "model_name": "AxonDeep", "publication_date": "2022/01/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34932117/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34932117", "task": "aipbNdPTIt", "abstract": "Optic nerve damage is the principal feature of glaucoma and contributes to vision loss in many diseases. In animal models, nerve health has traditionally been assessed by human experts that grade damage qualitatively or manually quantify axons from sampling limited areas from histologic cross sections of nerve. Both approaches are prone to variability and are time consuming. First-generation automated approaches have begun to emerge, but all have significant shortcomings. Here, we seek improvements through use of deep-learning approaches for segmenting and quantifying axons from cross-sections of mouse optic nerve. Two deep-learning approaches were developed and evaluated: (1) a traditional supervised approach using a fully convolutional network trained with only labeled data and (2) a semisupervised approach trained with both labeled and unlabeled data using a generative-adversarial-network framework. From comparisons with an independent test set of images with manually marked axon centers and boundaries, both deep-learning approaches outperformed an existing baseline automated approach and similarly to two independent experts. Performance of the semisupervised approach was superior and implemented into AxonDeep. AxonDeep performs automated quantification and segmentation of axons from healthy-appearing nerves and those with mild to moderate degrees of damage, similar to that of experts without the variability and constraints associated with manual performance. Use of deep learning for axon quantification provides rapid, objective, and higher throughput analysis of optic nerve that would otherwise not be possible.", "keywords": ["principal feature", "feature of glaucoma", "glaucoma and contributes", "contributes to vision", "vision loss", "Optic nerve", "deep-learning approaches", "Optic nerve damage", "nerve", "approaches", "grade damage qualitatively", "damage", "approach", "nerve damage", "Optic", "axons", "deep-learning", "sampling limited areas", "histologic cross sections", "grade damage"], "paper_title": "AxonDeep: Automated Optic Nerve Axon Segmentation in Mice With Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0032672575", "domain": "Glaucoma (unspecified)", "model_name": "ncbi\" aria-label=\"github\">", "publication_date": "2019/08/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32672575/", "code_link": "https://github.com/ncbi\" aria-label=\"github\">", "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "32672575", "task": "IWqQC1koJA", "abstract": "To assess the diagnostic accuracy of multiple machine learning models using full retinal nerve fiber layer (RNFL) thickness maps in detecting glaucoma. Case-control study. A total of 93 eyes from 69 patients with glaucoma and 128 eyes from 128 age- and sex-matched healthy controls from the Los Angeles Latino Eye Study (LALES), a large population-based, longitudinal cohort study consisting of Latino participants aged \u226540 years residing in El Puente, California. The 6\u00d76-mm RNFL thickness maps centered on the optic nerve head (Cirrus 4000; Zeiss, Dublin, CA) were supplied to 4 different machine learning algorithms. These models included 2 conventional machine learning algorithms, Support Vector Machine (SVM) and K-Nearest Neighbor (KNN), and 2 convolutional neural nets, ResNet-18 and GlaucomaNet, which was a custom-made deep learning network. All models were tested with 5-fold cross validation. Area under the curve (AUC) statistics to assess diagnostic accuracy of each model compared with conventional average circumpapillary RNFL thickness. All 4 models achieved similarly high diagnostic accuracies, with AUC values ranging from 0.91 to 0.92. These values were significantly higher than those for average circumpapillary RNFL thickness, which had an AUC of 0.76 in the same patient population. Superior diagnostic performance was achieved with both conventional machine learning and convolutional neural net models compared with circumpapillary RNFL thickness. This supports the importance of the spatial structure of RNFL thickness map data in diagnosing glaucoma and further efforts to optimize our use of this data.", "keywords": ["RNFL thickness", "circumpapillary RNFL thickness", "nerve fiber layer", "Los Angeles Latino", "full retinal nerve", "retinal nerve fiber", "RNFL", "Angeles Latino Eye", "circumpapillary RNFL", "fiber layer", "machine learning", "Latino Eye Study", "multiple machine learning", "thickness", "full retinal", "RNFL thickness maps", "machine learning algorithms", "machine", "average circumpapillary RNFL", "learning"], "paper_title": "Machine Learning Models for Diagnosing Glaucoma from Retinal Nerve Fiber Layer Thickness Maps.", "last_updated": "2023/02/04"}, {"id": "0033479405", "domain": "Glaucoma (unspecified)", "model_name": "Gheisari et al.", "publication_date": "2021/01/21", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33479405/", "code_link": null, "model_type": "RNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33479405", "task": "IWqQC1koJA", "abstract": "Glaucoma, a leading cause of blindness, is a multifaceted disease with several patho-physiological features manifesting in single fundus images (e.g., optic nerve cupping) as well as fundus videos (e.g., vascular pulsatility index). Current convolutional neural networks (CNNs) developed to detect glaucoma are all based on spatial features embedded in an image. We developed a combined CNN and recurrent neural network (RNN) that not only extracts the spatial features in a fundus image but also the temporal features embedded in a fundus video (i.e., sequential images). A total of 1810 fundus images and 295 fundus videos were used to train a CNN and a combined CNN and Long Short-Term Memory RNN. The combined CNN/RNN model reached an average F-measure of 96.2% in separating glaucoma from healthy eyes. In contrast, the base CNN model reached an average F-measure of only 79.2%. This proof-of-concept study demonstrates that extracting spatial and temporal features from fundus videos using a combined CNN and RNN, can markedly enhance the accuracy of glaucoma detection.", "keywords": ["optic nerve cupping", "vascular pulsatility index", "combined CNN", "patho-physiological features manifesting", "CNN", "optic nerve", "nerve cupping", "vascular pulsatility", "pulsatility index", "single fundus images", "fundus", "multifaceted disease", "manifesting in single", "fundus videos", "features", "RNN", "features embedded", "fundus images", "combined", "patho-physiological features"], "paper_title": "A combined convolutional and recurrent neural network for enhanced glaucoma detection.", "last_updated": "2023/02/04"}, {"id": "0032415269", "domain": "Glaucoma (unspecified)", "model_name": "AxoNet", "publication_date": "2020/05/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32415269/", "code_link": "https://github.com/ethier-lab/axonet", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "32415269", "task": null, "abstract": "In this work, we develop a robust, extensible tool to automatically and accurately count retinal ganglion cell axons in optic nerve (ON) tissue images from various animal models of glaucoma. We adapted deep learning to regress pixelwise axon count density estimates, which were then integrated over the image area to determine axon counts. The tool, termed AxoNet, was trained and evaluated using a dataset containing images of ON regions randomly selected from whole cross sections of both control and damaged rat ONs and manually annotated for axon count and location. This rat-trained network was then applied to a separate dataset of non-human primate (NHP) ON images. AxoNet was compared to two existing automated axon counting tools, AxonMaster and AxonJ, using both datasets. AxoNet outperformed the existing tools on both the rat and NHP ON datasets as judged by mean absolute error, R<sup>2</sup> values when regressing automated vs. manual counts, and Bland-Altman analysis. AxoNet does not rely on hand-crafted image features for axon recognition and is robust to variations in the extent of ON tissue damage, image quality, and species of mammal. Therefore, AxoNet is not species-specific and can be extended to quantify additional ON characteristics in glaucoma and potentially other neurodegenerative diseases.", "keywords": ["retinal ganglion cell", "accurately count retinal", "count retinal ganglion", "ganglion cell axons", "optic nerve", "automatically and accurately", "retinal ganglion", "ganglion cell", "animal models", "axon count", "accurately count", "count retinal", "pixelwise axon count", "axon count density", "determine axon counts", "axon", "cell axons", "extensible tool", "count density estimates", "AxoNet"], "paper_title": "AxoNet: A deep learning-based tool to count retinal ganglion cell axons.", "last_updated": "2023/02/04"}, {"id": "0033436866", "domain": "Glaucoma (unspecified)", "model_name": "Masin et al.", "publication_date": "2021/01/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33436866/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33436866", "task": null, "abstract": "Glaucoma is a disease associated with the loss of retinal ganglion cells (RGCs), and remains one of the primary causes of blindness worldwide. Major research efforts are presently directed towards the understanding of disease pathogenesis and the development of new therapies, with the help of rodent models as an important preclinical research tool. The ultimate goal is reaching neuroprotection of the RGCs, which requires a tool to reliably quantify RGC survival. Hence, we demonstrate a novel deep learning pipeline that enables fully automated RGC quantification in the entire murine retina. This software, called RGCode (Retinal Ganglion Cell quantification based On DEep learning), provides a user-friendly interface that requires the input of RBPMS-immunostained flatmounts and returns the total RGC count, retinal area and density, together with output images showing the computed counts and isodensity maps. The counting model was trained on RBPMS-stained healthy and glaucomatous retinas, obtained from mice subjected to microbead-induced ocular hypertension and optic nerve crush injury paradigms. RGCode demonstrates excellent performance in RGC quantification as compared to manual counts. Furthermore, we convincingly show that RGCode has potential for wider application, by retraining the model with a minimal set of training data to count FluoroGold-traced RGCs.", "keywords": ["retinal ganglion cells", "blindness worldwide", "Ganglion Cell quantification", "retinal ganglion", "RGC quantification", "RGC", "ganglion cells", "preclinical research tool", "Major research efforts", "RGCs", "important preclinical research", "retinal", "automated RGC quantification", "quantify RGC survival", "Cell quantification based", "reliably quantify RGC", "disease pathogenesis", "deep learning", "disease", "total RGC count"], "paper_title": "A novel retinal ganglion cell quantification tool based on deep learning.", "last_updated": "2023/02/04"}, {"id": "0034415271", "domain": "Glaucoma (unspecified)", "model_name": "Liu et al.", "publication_date": "2022/01/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34415271/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34415271", "task": "IWqQC1koJA", "abstract": "To date, deep learning-based detection of optic disc abnormalities in color fundus photographs has mostly been limited to the field of glaucoma. However, many life-threatening systemic and neurological conditions can manifest as optic disc abnormalities. In this study, we aimed to extend the application of deep learning (DL) in optic disc analyses to detect a spectrum of nonglaucomatous optic neuropathies. Using transfer learning, we trained a ResNet-152 deep convolutional neural network (DCNN) to distinguish between normal and abnormal optic discs in color fundus photographs (CFPs). Our training data set included 944 deidentified CFPs (abnormal 364; normal 580). Our testing data set included 151 deidentified CFPs (abnormal 71; normal 80). Both the training and testing data sets contained a wide range of optic disc abnormalities, including but not limited to ischemic optic neuropathy, atrophy, compressive optic neuropathy, hereditary optic neuropathy, hypoplasia, papilledema, and toxic optic neuropathy. The standard measures of performance (sensitivity, specificity, and area under the curve of the receiver operating characteristic curve (AUC-ROC)) were used for evaluation. During the 10-fold cross-validation test, our DCNN for distinguishing between normal and abnormal optic discs achieved the following mean performance: AUC-ROC 0.99 (95 CI: 0.98-0.99), sensitivity 94% (95 CI: 91%-97%), and specificity 96% (95 CI: 93%-99%). When evaluated against the external testing data set, our model achieved the following mean performance: AUC-ROC 0.87, sensitivity 90%, and specificity 69%. In summary, we have developed a deep learning algorithm that is capable of detecting a spectrum of optic disc abnormalities in color fundus photographs, with a focus on neuro-ophthalmological etiologies. As the next step, we plan to validate our algorithm prospectively as a focused screening tool in the emergency department, which if successful could be beneficial because current practice pattern and training predict a shortage of neuro-ophthalmologists and ophthalmologists in general in the near future.", "keywords": ["optic disc abnormalities", "optic disc", "abnormal optic discs", "optic", "disc abnormalities", "deep learning-based detection", "color fundus photographs", "optic neuropathy", "field of glaucoma", "disc", "learning-based detection", "color fundus", "fundus photographs", "data set", "data set included", "abnormal optic", "testing data set", "optic disc analyses", "optic discs achieved", "testing data"], "paper_title": "Detection of Optic Disc Abnormalities in Color Fundus Photographs Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0034086043", "domain": "Glaucoma (unspecified)", "model_name": "vfbyoct-comparison", "publication_date": "2021/06/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34086043/", "code_link": "https://github.com/climyth/vfbyoct-comparison", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "34086043", "task": null, "abstract": "To develop a deep learning model to estimate the visual field (VF) from spectral-domain optical coherence tomography (SD-OCT) and swept-source OCT (SS-OCT) and to compare the performance between them. Two deep learning models based on Inception-ResNet-v2 were trained to estimate 24-2 VF from SS-OCT and SD-OCT images. The estimation performance of the two models was evaluated by using the root mean square error between the actual and estimated VF. The performance was also compared among different glaucoma severities, Garway-Heath sectorizations, and central/peripheral regions. The training dataset comprised images of 4391 eyes from 2350 subjects, and the test dataset was obtained from another 243 subjects (243 eyes). In all subjects, the global estimation errors were 5.29 \u00b1 2.68 dB (SD-OCT) and 4.51 \u00b1 2.54 dB (SS-OCT), and the estimation error of SS-OCT was significantly lower than that of SD-OCT (P < 0.001). In the analysis of sectors, SS-OCT showed better performance in all sectors except for the inferonasal sector in normal vision and early glaucoma. In advanced glaucoma, the estimation error of the central region was worsened in both OCTs, but SS-OCT was still significantly better in the peripheral region. Our deep learning model estimated the VF 24-2 better with a wide field image of SS-OCT than did with retinal nerve fiber layer and ganglion cell-inner plexiform layer images of SD-OCT. This deep learning method can help clinicians to determine the VF from OCT images. OCT manufacturers can equip this system to provide additional VF data.", "keywords": ["optical coherence tomography", "spectral-domain optical coherence", "deep learning model", "deep learning", "coherence tomography", "spectral-domain optical", "optical coherence", "SS-OCT", "learning model", "learning models based", "SD-OCT", "swept-source OCT", "learning", "learning model estimated", "performance", "deep", "estimation", "estimation error", "images", "OCT"], "paper_title": "Visual Field Inference From Optical Coherence Tomography Using Deep Learning Algorithms: A Comparison Between Devices.", "last_updated": "2023/02/04"}, {"id": "0032479407", "domain": "Glaucoma (unspecified)", "model_name": "Li et al.", "publication_date": "2021/02/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32479407/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32479407", "task": "aipbNdPTIt", "abstract": "A common shortfall of supervised deep learning for medical imaging is the lack of labeled data, which is often expensive and time consuming to collect. This article presents a new semisupervised method for medical image segmentation, where the network is optimized by a weighted combination of a common supervised loss only for the labeled inputs and a regularization loss for both the labeled and unlabeled data. To utilize the unlabeled data, our method encourages consistent predictions of the network-in-training for the same input under different perturbations. With the semisupervised segmentation tasks, we introduce a transformation-consistent strategy in the self-ensembling model to enhance the regularization effect for pixel-level predictions. To further improve the regularization effects, we extend the transformation in a more generalized form including scaling and optimize the consistency loss with a teacher model, which is an averaging of the student model weights. We extensively validated the proposed semisupervised method on three typical yet challenging medical image segmentation tasks: 1) skin lesion segmentation from dermoscopy images in the International Skin Imaging Collaboration (ISIC) 2017 data set; 2) optic disk (OD) segmentation from fundus images in the Retinal Fundus Glaucoma Challenge (REFUGE) data set; and 3) liver segmentation from volumetric CT scans in the Liver Tumor Segmentation Challenge (LiTS) data set. Compared with state-of-the-art, our method shows superior performance on the challenging 2-D/3-D medical images, demonstrating the effectiveness of our semisupervised method for medical image segmentation.", "keywords": ["supervised deep learning", "medical image segmentation", "common supervised loss", "data set", "medical image", "image segmentation", "consuming to collect", "data", "segmentation", "deep learning", "expensive and time", "time consuming", "Tumor Segmentation Challenge", "unlabeled data", "semisupervised method", "labeled data", "common shortfall", "supervised deep", "image segmentation tasks", "medical"], "paper_title": "Transformation-Consistent Self-Ensembling Model for Semisupervised Medical Image Segmentation.", "last_updated": "2023/02/04"}, {"id": "0033412285", "domain": "Glaucoma (unspecified)", "model_name": "Hemelings et al.", "publication_date": "2020/12/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33412285/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33412285", "task": "aipbNdPTIt", "abstract": "Pathological myopia (PM) is the seventh leading cause of blindness, with a reported global prevalence up to 3%. Early and automated PM detection from fundus images could aid to prevent blindness in a world population that is characterized by a rising myopia prevalence. We aim to assess the use of convolutional neural networks (CNNs) for the detection of PM and semantic segmentation of myopia-induced lesions from fundus images on a recently introduced reference data set. This investigation reports on the results of CNNs developed for the recently introduced Pathological Myopia (PALM) dataset, which consists of 1200 images. Our CNN bundles lesion segmentation and PM classification, as the two tasks are heavily intertwined. Domain knowledge is also inserted through the introduction of a new Optic Nerve Head (ONH)-based prediction enhancement for the segmentation of atrophy and fovea localization. Finally, we are the first to approach fovea localization using segmentation instead of detection or regression models. Evaluation metrics include area under the receiver operating characteristic curve (AUC) for PM detection, Euclidean distance for fovea localization, and Dice and F1 metrics for the semantic segmentation tasks (optic disc, retinal atrophy and retinal detachment). Models trained with 400 available training images achieved an AUC of 0.9867 for PM detection, and a Euclidean distance of 58.27 pixels on the fovea localization task, evaluated on a test set of 400 images. Dice and F1 metrics for semantic segmentation of lesions scored 0.9303 and 0.9869 on optic disc, 0.8001 and 0.9135 on retinal atrophy, and 0.8073 and 0.7059 on retinal detachment, respectively. We report a successful approach for a simultaneous classification of pathological myopia and segmentation of associated lesions. Our work was acknowledged with an award in the context of the \"Pathological Myopia detection from retinal images\" challenge held during the IEEE International Symposium on Biomedical Imaging (April 2019). Considering that (pathological) myopia cases are often identified as false positives and negatives in glaucoma deep learning models, we envisage that the current work could aid in future research to discriminate between glaucomatous and highly-myopic eyes, complemented by the localization and segmentation of landmarks such as fovea, optic disc and atrophy.", "keywords": ["reported global prevalence", "Pathological myopia", "segmentation", "seventh leading", "reported global", "myopia", "fovea localization", "images", "rising myopia prevalence", "global prevalence", "Pathological Myopia detection", "detection", "semantic segmentation", "Pathological", "introduced Pathological Myopia", "Optic Nerve Head", "fundus images", "fovea", "localization", "optic disc"], "paper_title": "Pathological myopia classification with simultaneous lesion segmentation using deep learning.", "last_updated": "2023/02/04"}, {"id": "0033303289", "domain": "Glaucoma (unspecified)", "model_name": "Garc\u00eda et al.", "publication_date": "2020/11/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33303289/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33303289", "task": "IWqQC1koJA", "abstract": "Glaucoma is the leading cause of blindness worldwide. Many studies based on fundus image and optical coherence tomography (OCT) imaging have been developed in the literature to help ophthalmologists through artificial-intelligence techniques. Currently, 3D spectral-domain optical coherence tomography (SD-OCT) samples have become more important since they could enclose promising information for glaucoma detection. To analyse the hidden knowledge of the 3D scans for glaucoma detection, we have proposed, for the first time, a deep-learning methodology based on leveraging the spatial dependencies of the features extracted from the B-scans. The experiments were performed on a database composed of 176 healthy and 144 glaucomatous SD-OCT volumes centred on the optic nerve head (ONH). The proposed methodology consists of two well-differentiated training stages: a slide-level feature extractor and a volume-based predictive model. The slide-level discriminator is characterised by two new, residual and attention, convolutional modules which are combined via skip-connections with other fine-tuned architectures. Regarding the second stage, we first carried out a data-volume conditioning before extracting the features from the slides of the SD-OCT volumes. Then, Long Short-Term Memory (LSTM) networks were used to combine the recurrent dependencies embedded in the latent space to provide a holistic feature vector, which was generated by the proposed sequential-weighting module (SWM). The feature extractor reports AUC values higher than 0.93 both in the primary and external test sets. Otherwise, the proposed end-to-end system based on a combination of CNN and LSTM networks achieves an AUC of 0.8847 in the prediction stage, which outperforms other state-of-the-art approaches intended for glaucoma detection. Additionally, Class Activation Maps (CAMs) were computed to highlight the most interesting regions per B-scan when discerning between healthy and glaucomatous eyes from raw SD-OCT volumes. The proposed model is able to extract the features from the B-scans of the volumes and combine the information of the latent space to perform a volume-level glaucoma prediction. Our model, which combines residual and attention blocks with a sequential weighting module to refine the LSTM outputs, surpass the results achieved from current state-of-the-art methods focused on 3D deep-learning architectures.", "keywords": ["optical coherence tomography", "glaucoma detection", "blindness worldwide", "Glaucoma", "SD-OCT volumes", "coherence tomography", "optical coherence", "proposed", "spectral-domain optical coherence", "SD-OCT", "feature extractor", "detection", "volumes", "LSTM", "features", "feature", "Class Activation Maps", "glaucomatous SD-OCT volumes", "based", "slide-level feature extractor"], "paper_title": "Glaucoma Detection from Raw SD-OCT Volumes: A Novel Approach Focused on Spatial Dependencies.", "last_updated": "2023/02/04"}, {"id": "0035862336", "domain": "Glaucoma (unspecified)", "model_name": "AADG", "publication_date": "2022/12/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35862336/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35862336", "task": "aipbNdPTIt", "abstract": "Convolutional neural networks have been widely applied to medical image segmentation and have achieved considerable performance. However, the performance may be significantly affected by the domain gap between training data (source domain) and testing data (target domain). To address this issue, we propose a data manipulation based domain generalization method, called Automated Augmentation for Domain Generalization (AADG). Our AADG framework can effectively sample data augmentation policies that generate novel domains and diversify the training set from an appropriate search space. Specifically, we introduce a novel proxy task maximizing the diversity among multiple augmented novel domains as measured by the Sinkhorn distance in a unit sphere space, making automated augmentation tractable. Adversarial training and deep reinforcement learning are employed to efficiently search the objectives. Quantitative and qualitative experiments on 11 publicly-accessible fundus image datasets (four for retinal vessel segmentation, four for optic disc and cup (OD/OC) segmentation and three for retinal lesion segmentation) are comprehensively performed. Two OCTA datasets for retinal vasculature segmentation are further involved to validate cross-modality generalization. Our proposed AADG exhibits state-of-the-art generalization performance and outperforms existing approaches by considerable margins on retinal vessel, OD/OC and lesion segmentation tasks. The learned policies are empirically validated to be model-agnostic and can transfer well to other models. The source code is available at https://github.com/CRazorback/AADG.", "keywords": ["Convolutional neural networks", "Convolutional neural", "neural networks", "widely applied", "applied to medical", "achieved considerable performance", "Automated Augmentation", "domain", "segmentation", "medical image segmentation", "AADG", "domain generalization", "data", "called Automated Augmentation", "generalization", "performance", "retinal", "Augmentation", "medical image", "achieved considerable"], "paper_title": "AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation.", "last_updated": "2023/02/04"}, {"id": "0035927396", "domain": "Glaucoma (unspecified)", "model_name": "Zhang et al.", "publication_date": "2022/08/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35927396/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35927396", "task": null, "abstract": "Glaucoma is characterized by the progressive loss of retinal ganglion cells (RGCs), although the pathogenic mechanism remains largely unknown. To study the mechanism and assess RGC degradation, mouse models are often used to simulate human glaucoma and specific markers are used to label and quantify RGCs. However, manually counting RGCs is time-consuming and prone to distortion due to subjective bias. Furthermore, semi-automated counting methods can produce significant differences due to different parameters, thereby failing objective evaluation. Here, to improve counting accuracy and efficiency, we developed an automated algorithm based on the improved YOLOv5 model, which uses five channels instead of one, with a squeeze-and-excitation block added. The complete number of RGCs in an intact mouse retina was obtained by dividing the retina into small overlapping areas and counting, and then merging the divided areas using a non-maximum suppression algorithm. The automated quantification results showed very strong correlation (mean Pearson correlation coefficient of 0.993) with manual counting. Importantly, the model achieved an average precision of 0.981. Furthermore, the graphics processing unit (GPU) calculation time for each retina was less than 1 min. The developed software has been uploaded online as a free and convenient tool for studies using mouse models of glaucoma, which should help elucidate disease pathogenesis and potential therapeutics.", "keywords": ["retinal ganglion cells", "remains largely unknown", "pathogenic mechanism remains", "mechanism remains largely", "ganglion cells", "largely unknown", "progressive loss", "loss of retinal", "retinal ganglion", "remains largely", "pathogenic mechanism", "mechanism remains", "assess RGC degradation", "RGCs", "simulate human glaucoma", "counting", "manually counting RGCs", "Glaucoma", "RGC degradation", "mechanism"], "paper_title": "Automatic counting of retinal ganglion cells in the entire mouse retina based on improved YOLOv5.", "last_updated": "2023/02/04"}, {"id": "0034884192", "domain": "Glaucoma (unspecified)", "model_name": "fundusimageclassification", "publication_date": "2021/11/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34884192/", "code_link": "https://github.com/gcowen/fundusimageclassification", "model_type": "CNN", "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "34884192", "task": null, "abstract": "With recent advancements in machine learning, especially in deep learning, the prediction of eye diseases based on fundus photography using deep convolutional neural networks (DCNNs) has attracted great attention. However, studies focusing on identifying the right disease among several candidates, which is a better approximation of clinical diagnosis in practice comparing with the case that aims to distinguish one particular eye disease from normal controls, are limited. The performance of existing algorithms for multi-class classification of fundus images is at most mediocre. Moreover, in many studies consisting of different eye diseases, labeled images are quite limited mainly due to privacy concern of patients. In this case, it is infeasible to train huge DCNNs, which usually have millions of parameters. To address these challenges, we propose to utilize a lightweight deep learning architecture called MobileNetV2 and transfer learning to distinguish four common eye diseases, including Glaucoma, Maculopathy, Pathological Myopia, and Retinitis Pigmentosa, from normal controls using a small training data. We also apply a visualization approach to highlight the loci that are most related to the disease labels to make the model more explainable. The highlighted area chosen by the algorithm itself may give some hints for further fundus image studies. Our experimental results show that our system achieves an average accuracy of 96.2%, sensitivity of 90.4%, and specificity of 97.6% on the test data via five independent runs, and outperforms two other deep learning-based algorithms both in terms of accuracy and efficiency.", "keywords": ["convolutional neural networks", "attracted great attention", "deep convolutional neural", "eye diseases based", "neural networks", "great attention", "recent advancements", "advancements in machine", "convolutional neural", "attracted great", "eye diseases", "diseases based", "fundus photography", "eye", "machine learning", "deep convolutional", "learning", "fundus image studies", "normal controls", "common eye diseases"], "paper_title": "Prediction of Different Eye Diseases Based on Fundus Photography via Deep Transfer Learning.", "last_updated": "2023/02/04"}, {"id": "0034551738", "domain": "Glaucoma (unspecified)", "model_name": "Qian et al.", "publication_date": "2021/09/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34551738/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34551738", "task": "IWqQC1koJA", "abstract": "The purpose of this study was to implement and evaluate a deep learning (DL) approach for automatically detecting shallow anterior chamber depth (ACD) from two-dimensional (2D) overview anterior segment photographs. We trained a DL model using a dataset of anterior segment photographs collected from Shanghai Aier Eye Hospital from June 2018 to December 2019. A Pentacam HR system was used to capture a 2D overview eye image and measure the ACD. Shallow ACD was defined as ACD less than 2.4\u2009mm. The DL model was evaluated by a five-fold cross-validation test in a hold-out testing dataset. We also evaluated the DL model by testing it against two glaucoma specialists. The performance of the DL model was calculated by metrics, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). A total of 3753 photographs (1720 shallow AC and 2033 deep AC images) were assigned to the training dataset, and 1302 photographs (509 shallow AC and 793 deep AC images) were held out for two internal testing datasets. In detecting shallow ACD in the internal hold-out testing dataset, the DL model achieved an AUC of 0.86 (95% CI, 0.83-0.90) with 80% sensitivity and 79% specificity. In the same testing dataset, the DL model also achieved better performance than the two glaucoma specialists (accuracy of 80% vs. accuracy of 74 and 69%). We proposed a high-performing DL model to automatically detect shallow ACD from overview anterior segment photographs. Our DL model has potential applications in detecting and monitoring shallow ACD in the real world. http://clinicaltrials.gov , NCT04340635 , retrospectively registered on 29 March 2020.", "keywords": ["anterior segment photographs", "Shanghai Aier Eye", "Aier Eye Hospital", "anterior chamber depth", "overview anterior segment", "anterior segment", "Shallow ACD", "ACD", "shallow anterior chamber", "segment photographs", "Hospital from June", "model", "Shanghai Aier", "chamber depth", "implement and evaluate", "shallow", "overview anterior", "Aier Eye", "Eye Hospital", "anterior"], "paper_title": "Detection of shallow anterior chamber depth from two-dimensional anterior segment photographs using deep learning.", "last_updated": "2023/02/04"}, {"id": "0031260494", "domain": "Glaucoma (unspecified)", "model_name": "Maetschke et al.", "publication_date": "2019/07/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31260494/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31260494", "task": "IWqQC1koJA", "abstract": "Optical coherence tomography (OCT) based measurements of retinal layer thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell with inner plexiform layer (GCIPL) are commonly employed for the diagnosis and monitoring of glaucoma. Previously, machine learning techniques have relied on segmentation-based imaging features such as the peripapillary RNFL thickness and the cup-to-disc ratio. Here, we propose a deep learning technique that classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network (CNN). We compared the accuracy of this technique with various feature-based machine learning algorithms and demonstrated the superiority of the proposed deep learning based method. Logistic regression was found to be the best performing classical machine learning technique with an AUC of 0.89. In direct comparison, the deep learning approach achieved a substantially higher AUC of 0.94 with the additional advantage of providing insight into which regions of an OCT volume are important for glaucoma detection. Computing Class Activation Maps (CAM), we found that the CNN identified neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and its surrounding areas as the regions significantly associated with the glaucoma classification. These regions anatomically correspond to the well established and commonly used clinical markers for glaucoma diagnosis such as increased cup volume, cup diameter, and neuroretinal rim thinning at the superior and inferior segments.", "keywords": ["retinal layer thickness", "Optical coherence tomography", "nerve fibre layer", "retinal nerve fibre", "retinal layer", "peripapillary RNFL thickness", "fibre layer", "plexiform layer", "Convolutional Neural Network", "layer thickness", "Optical coherence", "coherence tomography", "machine learning techniques", "deep learning technique", "RNFL thickness", "machine learning", "deep learning", "ganglion cell", "layer", "deep learning based"], "paper_title": "A feature agnostic approach for glaucoma detection in OCT volumes.", "last_updated": "2023/02/04"}, {"id": "0035372222", "domain": "Glaucoma (unspecified)", "model_name": "Bhatia et al.", "publication_date": "2022/03/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35372222/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35372222", "task": null, "abstract": "Early detection of vessels from fundus images can effectively prevent the permanent retinal damages caused by retinopathies such as glaucoma, hyperextension, and diabetes. Concerning the red color of both retinal vessels and background and the vessel's morphological variations, the current vessel detection methodologies fail to segment thin vessels and discriminate them in the regions where permanent retinopathies mainly occur. This research aims to suggest a novel approach to take the benefit of both traditional template-matching methods with recent deep learning (DL) solutions. These two methods are combined in which the response of a Cauchy matched filter is used to replace the noisy red channel of the fundus images. Consequently, a U-shaped fully connected convolutional neural network (U-net) is employed to train end-to-end segmentation of pixels into vessel and background classes. Each preprocessed image is divided into several patches to provide enough training images and speed up the training per each instance. The DRIVE public database has been analyzed to test the proposed method, and metrics such as Accuracy, Precision, Sensitivity and Specificity have been measured for evaluation. The evaluation indicates that the average extraction accuracy of the proposed model is 0.9640 on the employed dataset.", "keywords": ["retinal damages caused", "permanent retinal damages", "vessel detection methodologies", "current vessel detection", "effectively prevent", "damages caused", "Early detection", "retinal damages", "vessel morphological variations", "segment thin vessels", "retinal vessels", "vessel detection", "detection methodologies fail", "permanent retinal", "permanent retinopathies", "fundus images", "vessels", "vessel", "prevent the permanent", "caused by retinopathies"], "paper_title": "Retinal Vessel Extraction <i>via</i> Assisted Multi-Channel Feature Map and U-Net.", "last_updated": "2023/02/04"}, {"id": "0034901467", "domain": "Glaucoma (unspecified)", "model_name": "Peroni et al.", "publication_date": "2021/11/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34901467/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34901467", "task": "aipbNdPTIt", "abstract": "To develop and test a deep learning (DL) model for semantic segmentation of anatomical layers of the anterior chamber angle (ACA) in digital gonio-photographs. We used a pilot dataset of 274 ACA sector images, annotated by expert ophthalmologists to delineate five anatomical layers: iris root, ciliary body band, scleral spur, trabecular meshwork and cornea. Narrow depth-of-field and peripheral vignetting prevented clinicians from annotating part of each image with sufficient confidence, introducing a degree of subjectivity and features correlation in the ground truth. To overcome these limitations, we present a DL model, designed and trained to perform two tasks simultaneously: (1) maximise the segmentation accuracy within the annotated region of each frame and (2) identify a region of interest (ROI) based on local image informativeness. Moreover, our calibrated model provides results interpretability returning pixel-wise classification uncertainty through Monte Carlo dropout. The model was trained and validated in a 5-fold cross-validation experiment on ~90% of available data, achieving ~91% average segmentation accuracy within the annotated part of each ground truth image of the hold-out test set. An appropriate ROI was successfully identified in all test frames. The uncertainty estimation module located correctly inaccuracies and errors of segmentation outputs. The proposed model improves the only previously published work on gonio-photographs segmentation and may be a valid support for the automatic processing of these images to evaluate local tissue morphology. Uncertainty estimation is expected to facilitate acceptance of this system in clinical settings.", "keywords": ["anterior chamber angle", "ACA sector images", "anatomical layers", "deep learning", "chamber angle", "anterior chamber", "ACA sector", "ACA", "semantic segmentation", "model", "segmentation", "digital gonio-photographs", "ciliary body band", "Monte Carlo dropout", "segmentation accuracy", "anatomical", "layers", "ground truth image", "ground truth", "annotated"], "paper_title": "Semantic segmentation of gonio-photographs via adaptive ROI localisation and uncertainty estimation.", "last_updated": "2023/02/04"}, {"id": "0032247778", "domain": "Glaucoma (unspecified)", "model_name": "Yang et al.", "publication_date": "2020/04/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32247778/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32247778", "task": null, "abstract": "We sought to assess the performance of deep learning approaches for differentiating nonglaucomatous optic neuropathy with disc pallor (NGON) vs glaucomatous optic neuropathy (GON) on color fundus photographs by the use of image recognition. Development of an Artificial Intelligence Classification algorithm. This single-institution analysis included 3815 fundus images from the picture archiving and communication system of Seoul National University Bundang Hospital consisting of 2883 normal optic disc images, 446 NGON images, and 486 GON images. The presence of NGON and GON was interpreted by 2 expert neuro-ophthalmologists and had corroborated evidence on visual field testing and optical coherence tomography. Images were preprocessed in size and color enhancement before input. We applied the convolutional neural network (CNN) of ResNet-50 architecture. The area under the precision-recall curve (average precision) was evaluated for the efficacy of deep learning algorithms to assess the performance of classifying NGON and GON. The diagnostic accuracy of the ResNet-50 model to detect GON among NGON images showed a sensitivity of 93.4% and specificity of 81.8%. The area under the precision-recall curve for differentiating NGON vs GON showed an average precision value of 0.874. False positive cases were found with extensive areas of peripapillary atrophy and tilted optic discs. Artificial intelligence-based deep learning algorithms for detecting optic disc diseases showed excellent performance in differentiating NGON and GON on color fundus photographs, necessitating further research for clinical application.", "keywords": ["nonglaucomatous optic neuropathy", "Artificial Intelligence Classification", "Seoul National University", "National University Bundang", "University Bundang Hospital", "Intelligence Classification algorithm", "NGON", "optic neuropathy", "GON", "Intelligence Classification", "NGON images", "Bundang Hospital consisting", "deep learning approaches", "differentiating NGON", "images", "differentiating nonglaucomatous optic", "neuropathy", "color fundus photographs", "NGON and GON", "Seoul National"], "paper_title": "Efficacy for Differentiating Nonglaucomatous Versus Glaucomatous Optic Neuropathy Using Deep Learning Systems.", "last_updated": "2023/02/04"}, {"id": "0032647810", "domain": "Glaucoma (unspecified)", "model_name": "Sedai et al.", "publication_date": "2019/11/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32647810/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32647810", "task": "0GPcU42tzV", "abstract": "The purpose of this study was to develop a machine learning model to forecast future circumpapillary retinal nerve fiber layer (cpRNFL) thickness in eyes of healthy, glaucoma suspect, and glaucoma participants from multimodal temporal data. Retrospective analysis of a longitudinal clinical cohort. Longitudinal clinical cohort of healthy, glaucoma suspect, and glaucoma participants. The forecasting models used multimodal patient information including clinical (age and intraocular pressure), structural (cpRNFL thickness derived from scans as well as deep learning-derived OCT image features), and functional (visual field test parameters) data and the intervisit interval for prediction of cpRNFL thickness at the next visit. Four models were developed based on the number of visits used (n = 1 to 4). Longitudinal data from 1089 participants (mean observation period, 3.65\u00b11.73 years) was used with 80% of the cohort for the development of the models. The results of our models were compared with those of a commonly adopted linear regression model, which we refer to here as <i>linear trend-based estimation</i> (LTBE). The mean absolute difference and Pearson's correlation coefficient between the true and forecasted values of the cpRNFL in the healthy, glaucoma suspect, and glaucoma patients. The best forecasting model of cpRNFL was obtained using 3 visits and incorporated deep learning-derived OCT image features. The mean error was 1.10\u00b10.60 \u03bcm, 1.79\u00b11.73 \u03bcm, and 1.87\u00b11.85 \u03bcm in eyes of healthy, glaucoma suspect, and glaucoma participants, respectively. Our method significantly outperformed the LTBE model for glaucoma suspect and glaucoma participants (<i>P</i> < 0.001), which showed a mean error of 1.55\u00b11.16 \u03bcm, 2.4\u00b12.67 \u03bcm, and 3.02\u00b13.06 \u03bcm in the 3 groups, respectively. The Pearson's correlation coefficient between the forecasted value and the measured thickness was \u03c1 = 0.96 (<i>P</i> < 0.01), \u03c1 = 0.95 (<i>P</i> < 0.01), and \u03c1 = 0.96 (<i>P</i> < 0.01) for the 3 groups, respectively. The performance of the proposed forecasting model for cpRNFL is consistent across glaucoma suspect and glaucoma patients, which implies the robustness of the developed model against the disease state. These forecasted values may be useful to personalize patient care by determining the most appropriate intervisit schedule for timely interventions.", "keywords": ["nerve fiber layer", "forecast future circumpapillary", "future circumpapillary retinal", "circumpapillary retinal nerve", "retinal nerve fiber", "glaucoma suspect", "machine learning model", "glaucoma", "glaucoma participants", "longitudinal clinical cohort", "fiber layer", "develop a machine", "machine learning", "forecast future", "future circumpapillary", "circumpapillary retinal", "retinal nerve", "nerve fiber", "suspect", "multimodal temporal data"], "paper_title": "Forecasting Retinal Nerve Fiber Layer Thickness from Multimodal Temporal Data Incorporating OCT Volumes.", "last_updated": "2023/02/04"}, {"id": "0032596065", "domain": "Glaucoma (unspecified)", "model_name": "Thakur et al.", "publication_date": "2020/05/28", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32596065/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32596065", "task": null, "abstract": "<i>Goal:</i> The purpose of this study was to identify clinically relevant patterns of glaucomatous vision loss through convex representation to predict glaucoma several years prior to disease onset. <i>Methods:</i> We developed a deep archetypal analysis to identify patterns of glaucomatous vision loss, and then projected visual fields over the identified patterns. Projections provided a representation that was more accurate in detecting glaucomatous vision loss, thus, more appropriate for recognizing preclinical signs of glaucoma prior to disease development. To overcome the class imbalance in prediction, we implemented a class-balanced bagging with neural networks. <i>Results:</i> Using original visual field as features of the class-balanced bagging classification provided an area under the receiver-operating characteristic curve (AUC) of 0.55 for predicting glaucoma approximately four years prior to disease development. Using convex representation of the visual fields as input features provided an AUC of 0.61 while using deep convex representation as input features improved the AUC to 0.71. Relevance vector machine (RVM) achieved an AUC of 0.64. <i>Conclusion:</i> Deep archetypal analysis representation of visual functional features with balanced bagging classification could serve as an automated tool for predicting glaucoma. <i>Significance:</i> Glaucoma is the second leading cause of worldwide blindness. Most people with glaucoma have no early symptoms or pain, delaying diagnosis in many patients until they reach late irreversible vision loss stages. In fact, about 50% of people with glaucoma are unaware they have the disease. Deep archetypal analysis models may impact clinical practice in effectively identifying at-risk glaucoma patients well prior to disease development.", "keywords": ["glaucomatous vision loss", "clinically relevant patterns", "identify clinically relevant", "vision loss", "glaucomatous vision", "prior to disease", "disease development", "clinically relevant", "glaucoma", "Goal", "relevant patterns", "deep archetypal analysis", "detecting glaucomatous vision", "vision", "AUC", "identify clinically", "disease", "convex representation", "loss", "disease onset"], "paper_title": "Convex Representations Using Deep Archetypal Analysis for Predicting Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0035547359", "domain": "Glaucoma (unspecified)", "model_name": "Surendiran et al.", "publication_date": "2022/05/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35547359/", "code_link": null, "model_type": "RNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35547359", "task": "aipbNdPTIt", "abstract": "Glaucoma is one of the leading factors of vision loss, where the people tends to lose their vision quickly. The examination of cup-to-disc ratio is considered essential in diagnosing glaucoma. It is hence regarded that the segmentation of optic disc and cup is useful in finding the ratio. In this paper, we develop an extraction and segmentation of optic disc and cup from an input eye image using modified recurrent neural networks (mRNN). The mRNN use the combination of recurrent neural network (RNN) with fully convolutional network (FCN) that exploits the intra- and interslice contexts. The FCN extracts the contents from an input image by constructing a feature map for the intra- and interslice contexts. This is carried out to extract the relevant information, where RNN concentrates more on interslice context. The simulation is conducted to test the efficacy of the model that integrates the contextual information for optimal segmentation of optical cup and disc. The results of simulation show that the proposed method mRNN is efficient in improving the rate of segmentation than the other deep learning models like Drive, STARE, MESSIDOR, ORIGA, and DIARETDB.", "keywords": ["vision loss", "vision quickly", "leading factors", "factors of vision", "lose their vision", "vision", "optic disc", "interslice contexts", "recurrent neural networks", "segmentation", "diagnosing glaucoma", "recurrent neural", "segmentation of optic", "disc", "Glaucoma", "cup", "interslice", "mRNN", "disc and cup", "FCN"], "paper_title": "Segmentation of Optic Disc and Cup Using Modified Recurrent Neural Network.", "last_updated": "2023/02/04"}, {"id": "0033018356", "domain": "Glaucoma (unspecified)", "model_name": "Yow et al.", "publication_date": "2020/10/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33018356/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33018356", "task": "aipbNdPTIt", "abstract": "Glaucoma is a progressive optic neuropathy that leads to loss of retinal ganglion cells and thinning of retinal nerve fiber layer (RNFL). Circumpapillary RNFL thickness measurements have been used for glaucoma diagnostic and monitoring purposes. However, manual measurement of the RNFL thickness is tedious and subjective. We proposed and evaluated the performance of automated RNFL segmentation from OCT images using a state-of-the-art deep learning-based model. Circumpapillary OCT scans were extracted from volumetric OCT scans using a high-resolution swept-source OCT device. Manual annotation was performed on the extracted scans and used for training and evaluation. The results show that the accuracy and diagnostic performance is comparable to manual assessment, and the potential application of deep learning-based approach in such segmentation.", "keywords": ["retinal ganglion cells", "retinal nerve fiber", "nerve fiber layer", "progressive optic neuropathy", "loss of retinal", "retinal ganglion", "thinning of retinal", "retinal nerve", "fiber layer", "progressive optic", "optic neuropathy", "neuropathy that leads", "leads to loss", "ganglion cells", "cells and thinning", "nerve fiber", "RNFL thickness", "Circumpapillary RNFL thickness", "RNFL thickness measurements", "RNFL"], "paper_title": "Automated circumpapillary retinal nerve fiber layer segmentation in high-resolution swept-source OCT.", "last_updated": "2023/02/04"}, {"id": "0034945957", "domain": "Glaucoma (unspecified)", "model_name": "Barua et al.", "publication_date": "2021/12/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34945957/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34945957", "task": "IWqQC1koJA", "abstract": "Optical coherence tomography (OCT) images coupled with many learning techniques have been developed to diagnose retinal disorders. This work aims to develop a novel framework for extracting deep features from 18 pre-trained convolutional neural networks (CNN) and to attain high performance using OCT images. In this work, we have developed a new framework for automated detection of retinal disorders using transfer learning. This model consists of three phases: deep fused and multilevel feature extraction, using 18 pre-trained networks and tent maximal pooling, feature selection with ReliefF, and classification using the optimized classifier. The novelty of this proposed framework is the feature generation using widely used CNNs and to select the most suitable features for classification. The extracted features using our proposed intelligent feature extractor are fed to iterative ReliefF (IRF) to automatically select the best feature vector. The quadratic support vector machine (QSVM) is utilized as a classifier in this work. We have developed our model using two public OCT image datasets, and they are named database 1 (DB1) and database 2 (DB2). The proposed framework can attain 97.40% and 100% classification accuracies using the two OCT datasets, DB1 and DB2, respectively. These results illustrate the success of our model.", "keywords": ["Optical coherence tomography", "diagnose retinal disorders", "Optical coherence", "coherence tomography", "OCT", "OCT image datasets", "images coupled", "retinal disorders", "diagnose retinal", "learning techniques", "OCT images", "OCT image", "public OCT image", "framework", "feature", "work", "developed", "proposed framework", "images", "proposed"], "paper_title": "Multilevel Deep Feature Generation Framework for Automated Detection of Retinal Abnormalities Using OCT Images.", "last_updated": "2023/02/04"}, {"id": "0036434508", "domain": "Glaucoma (unspecified)", "model_name": "Lim et al.", "publication_date": "2022/11/24", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36434508/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36434508", "task": "IWqQC1koJA", "abstract": "Glaucoma is one of the major causes of blindness; it is estimated that over 110 million people will be affected by glaucoma worldwide by 2040. Research on glaucoma detection using deep learning technology has been increasing, but the diagnosis of glaucoma in a large population with high incidence of myopia remains a challenge. This study aimed to provide a decision support system for the automatic detection of glaucoma using fundus images, which can be applied for general screening, especially in areas of high incidence of myopia. A total of 1,155 fundus images were acquired from 667 individuals with a mean axial length of 25.60\u2009\u00b1\u20092.0\u00a0mm at the National Taiwan University Hospital, Hsinchu Br. These images were graded based on the findings of complete ophthalmology examinations, visual field test, and optical coherence tomography into three groups: normal (N, n\u2009=\u2009596), pre-perimetric glaucoma (PPG, n\u2009=\u200966), and glaucoma (G, n\u2009=\u2009493), and divided into a training-validation (N: 476, PPG: 55, G: 373) and test (N: 120, PPG: 11, G: 120) sets. A multimodal model with the Xception model as image feature extraction and machine learning algorithms [random forest (RF), support vector machine (SVM), dense neural network (DNN), and others] was applied. The Xception model classified the N, PPG, and G groups with 93.9% of the micro-average area under the receiver operating characteristic curve (AUROC) with tenfold cross-validation. Although normal and glaucoma sensitivity can reach 93.51% and 86.13% respectively, the PPG sensitivity was only 30.27%. The AUROC increased to 96.4% in the N\u2009+\u2009PPG and G groups. The multimodal model with the N\u2009+\u2009PPG and G groups showed that the AUROCs of RF, SVM, and DNN were 99.56%, 99.59%, and 99.10%, respectively; The N and PPG\u2009+\u2009G groups had less than 1% difference. The test set showed an overall 3%-5% less AUROC than the validation results. The multimodal model had good AUROC while detecting glaucoma in a population with high incidence of myopia. The model shows the potential for general automatic screening and telemedicine, especially in Asia. The study was approved by the Institutional Review Board of the National Taiwan University Hospital, Hsinchu Branch (no. NTUHHCB 108-025-E).", "keywords": ["Taiwan University Hospital", "National Taiwan University", "PPG", "Glaucoma", "high incidence", "incidence of myopia", "million people", "University Hospital", "National Taiwan", "Taiwan University", "glaucoma worldwide", "Xception model", "model", "AUROC", "multimodal model", "groups", "fundus images", "incidence", "high", "myopia"], "paper_title": "Use of multimodal dataset in AI for detecting glaucoma based on fundus photographs assessed with OCT: focus group study on high prevalence of myopia.", "last_updated": "2023/02/04"}, {"id": "0032845372", "domain": "Glaucoma (unspecified)", "model_name": "Park et al.", "publication_date": "2020/08/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32845372/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32845372", "task": null, "abstract": "To develop a deep learning method to predict visual field (VF) from wide-angle swept-source optical coherence tomography (SS-OCT) and compare the performance of three Google Inception architectures. Three deep learning models (with Inception-ResNet-v2, Inception-v3, and Inception-v4) were trained to predict 24-2 VF from the macular ganglion cell-inner plexiform layer and the peripapillary retinal nerve fibre layer map obtained by SS-OCT. The prediction performance of the three models was evaluated by using the root mean square error (RMSE) between the actual and predicted VF. The performance was also compared among different glaucoma severities and Garway-Heath sectorizations. The training dataset comprised images of 2220 eyes from 1120 subjects, and the test dataset was obtained from another 305 subjects (305 eyes). In all subjects, the global prediction errors (RMSEs) were 4.44\u2009\u00b1\u20092.09\u00a0dB, 4.78\u2009\u00b1\u20092.38\u00a0dB, and 4.85\u2009\u00b1\u20092.66\u00a0dB for the Inception-ResNet-v2, Inception-v3, and Inception-v4 architectures, respectively, and the prediction error of Inception-ResNet-v2 was significantly lower than the other two (P\u2009<\u20090.001). As glaucoma progressed, the prediction error of all three architectures significantly worsened to 6.59\u00a0dB, 7.33\u00a0dB, and 7.79\u00a0dB, respectively. In the analysis of sectors, the nasal sector had the lowest prediction error, followed by the superotemporal sector. Inception-ResNet-v2 achieved the best performance, and the global prediction error (RMSE) was 4.44\u00a0dB. As glaucoma progressed, the prediction error became larger. This method may help clinicians determine VF, particularly for patients who are unable to undergo a physical VF test.", "keywords": ["Google Inception architectures", "Google Inception", "optical coherence tomography", "wide-angle swept-source optical", "swept-source optical coherence", "predict visual field", "prediction error", "deep learning models", "deep learning", "Inception architectures", "visual field", "coherence tomography", "wide-angle swept-source", "swept-source optical", "optical coherence", "deep learning method", "prediction", "predict visual", "error", "global prediction errors"], "paper_title": "Prediction of visual field from swept-source optical coherence tomography using deep learning algorithms.", "last_updated": "2023/02/04"}, {"id": "0031790065", "domain": "Glaucoma (unspecified)", "model_name": "Li et al.", "publication_date": "2020/03/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31790065/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31790065", "task": null, "abstract": "To develop a software package for automated measuring of the trabecular-iris angle (TIA) using ultrasound biomicroscopy. Ultrasound biomicroscopy images were collected and the TIA was manually measured by specialists. Different models were used as the convolutional neural network for the automatic TIA measurement. The root-mean-squared error, explained variance, and mean absolute percentage error were used to evaluate the performance of these models. The interobserver reproducibility, coefficient of variation, and intraclass correlation coefficient were calculated to evaluate the consistency between the manual measured and the model predicted values. ResNet-18 had the best performance in root-mean-squared error, explained variance, and mean absolute percentage error among all 5 models. The average difference between the angles measured manually and by the model is -0.46\u00b13.97 degrees for all eyes, -1.67\u00b15.19 degrees for open angles, and 0.75\u00b11.43 degrees for narrow angles. The coefficient of variation, intraclass correlation coefficient, and reproducibility of the total TIA measurements are 6.8%, 0.95, and 6.1 degrees for all angles; 6.4%, 0.99, and 7.7 degrees for open angles; and 8.8%, 0.93, and 4 degrees for narrow angles, respectively. Preliminary results show that this fully automated anterior chamber angle measurement method can achieve high accuracy and have good consistency with the manual measurement results, this has great significance for future clinical practice.", "keywords": ["ultrasound biomicroscopy", "TIA", "Ultrasound biomicroscopy images", "develop a software", "software package", "degrees", "automatic TIA measurement", "angles", "TIA measurement", "degrees for open", "degrees for narrow", "absolute percentage error", "total TIA measurements", "automatic TIA", "trabecular-iris angle", "error", "coefficient", "automated measuring", "total TIA", "intraclass correlation coefficient"], "paper_title": "Automatic Anterior Chamber Angle Measurement for Ultrasound Biomicroscopy Using Deep Learning.", "last_updated": "2023/02/04"}, {"id": "0030629635", "domain": "Glaucoma (unspecified)", "model_name": "MacCormick et al.", "publication_date": "2019/01/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30629635/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30629635", "task": "IWqQC1koJA", "abstract": "Glaucoma is the leading cause of irreversible blindness worldwide. It is a heterogeneous group of conditions with a common optic neuropathy and associated loss of peripheral vision. Both over and under-diagnosis carry high costs in terms of healthcare spending and preventable blindness. The characteristic clinical feature of glaucoma is asymmetrical optic nerve rim narrowing, which is difficult for humans to quantify reliably. Strategies to improve and automate optic disc assessment are therefore needed to prevent sight loss. We developed a novel glaucoma detection algorithm that segments and analyses colour photographs to quantify optic nerve rim consistency around the whole disc at 15-degree intervals. This provides a profile of the cup/disc ratio, in contrast to the vertical cup/disc ratio in common use. We introduce a spatial probabilistic model, to account for the optic nerve shape, we then use this model to derive a disc deformation index and a decision rule for glaucoma. We tested our algorithm on two separate image datasets (ORIGA and RIM-ONE). The spatial algorithm accurately distinguished glaucomatous and healthy discs on internal and external validation (AUROC 99.6% and 91.0% respectively). It achieves this using a dataset 100-times smaller than that required for deep learning algorithms, is flexible to the type of cup and disc segmentation (automated or semi-automated), utilises images with missing data, and is correlated with the disc size (p = 0.02) and the rim-to-disc at the narrowest rim (p<0.001, in external validation). The spatial probabilistic algorithm is highly accurate, highly data efficient and it extends to any imaging hardware in which the boundaries of cup and disc can be segmented, thus making the algorithm particularly applicable to research into disease mechanisms, and also glaucoma screening in low resource settings.", "keywords": ["irreversible blindness worldwide", "disc", "optic nerve rim", "optic nerve", "blindness worldwide", "irreversible blindness", "optic", "Glaucoma", "algorithm", "cup", "nerve rim", "quantify optic nerve", "nerve", "common optic neuropathy", "rim", "disc ratio", "spatial", "asymmetrical optic nerve", "blindness", "worldwide"], "paper_title": "Accurate, fast, data efficient and interpretable glaucoma diagnosis with automated spatial analysis of the whole cup to disc profile.", "last_updated": "2023/02/04"}, {"id": "0035991921", "domain": "Glaucoma (unspecified)", "model_name": "Zhang et al.", "publication_date": "2022/06/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35991921/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35991921", "task": null, "abstract": "Multicolor scanning laser imaging (MCI) images have broad application potential in the diagnosis of fundus diseases such as glaucoma. However, the performance level of automatic aided diagnosis systems based on MCI images is limited by the lack of high-quality annotations of numerous images. Producing annotations for vast amounts of MCI images will be a prolonged process if we only employ experts. Therefore, we consider non-expert crowdsourcing, which is an alternative approach to produce useful annotations efficiently and low cost. In this work, we aim to explore the effectiveness of non-expert crowdsourcing on the segmentation of the optic cup (OC) and optic disc (OD), which is an upstream task for glaucoma diagnosis, using MCI images. To this end, desensitized MCI images are independently annotated by four non-expert annotators, constructing a crowdsourcing dataset. To profit from crowdsourcing, we propose a model consisting of coupled regularization network and segmentation network. The regularization network generates learnable pixel-wise confusion matrices (CMs) that reflects preferences of each annotator. During training, the CMs and segmentation network are simultaneously optimized to enable dynamic trade-offs for non-expert annotations and generate reliable predictions. Crowdsourcing learning using our method have an average Mean Intersection Over Union ( <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>M</mi></mrow> </math> ) of 91.34%, while the average <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>M</mi></mrow> </math> of model trained by expert annotations is 91.72%. In addition, comparative experiments show that in our segmentation task non-expert crowdsourcing can be on a par with the expert who annotates 90% of data. Our work suggests that crowdsourcing in the segmentation of OC and OD using MCI images has the potential to be a substitute to expert annotation, which will accelerate the construction of large datasets to facilitate the application of deep learning in clinical diagnosis using MCI images.", "keywords": ["MCI images", "Multicolor scanning laser", "scanning laser imaging", "MCI", "images", "Multicolor scanning", "laser imaging", "desensitized MCI images", "scanning laser", "fundus diseases", "crowdsourcing", "non-expert crowdsourcing", "segmentation", "non-expert", "annotations", "mrow", "diagnosis", "math", "desensitized MCI", "network"], "paper_title": "Leveraging non-expert crowdsourcing to segment the optic cup and disc of multicolor fundus images.", "last_updated": "2023/02/04"}, {"id": "0036325476", "domain": "Glaucoma (unspecified)", "model_name": "vfprediction", "publication_date": "2022/09/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36325476/", "code_link": "https://github.com/mohaes/vfprediction", "model_type": "RNN", "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "36325476", "task": null, "abstract": "Two novel deep learning methods using a convolutional neural network (CNN) and a recurrent neural network (RNN) have recently been developed to forecast future visual fields (VFs). Although the original evaluations of these models focused on overall accuracy, it was not assessed whether they can accurately identify patients with progressive glaucomatous vision loss to aid clinicians in preventing further decline. We evaluated these 2 prediction models for potential biases in overestimating or underestimating VF changes over time. Retrospective observational cohort study. All available and reliable Swedish Interactive Thresholding Algorithm Standard 24-2 VFs from Massachusetts Eye and Ear Glaucoma Service collected between 1999 and 2020 were extracted. Because of the methods' respective needs, the CNN data set included 54\u2009373 samples from 7472 patients, and the RNN data set included 24\u2009430 samples from 1809 patients. The CNN and RNN methods were reimplemented. A fivefold cross-validation procedure was performed on each model, and pointwise mean absolute error (PMAE) was used to measure prediction accuracy. Test data were stratified into categories based on the severity of VF progression to investigate the models' performances on predicting worsening cases. The models were additionally compared with a no-change model that uses the baseline VF (for the CNN) and the last-observed VF (for the RNN) for its prediction. PMAE in predictions. The overall PMAE 95% confidence intervals were 2.21 to 2.24 decibels (dB) for the CNN and 2.56 to 2.61 dB for the RNN, which were close to the original studies' reported values. However, both models exhibited large errors in identifying patients with worsening VFs and often failed to outperform the no-change model. Pointwise mean absolute error values were higher in patients with greater changes in mean sensitivity (for the CNN) and mean total deviation (for the RNN) between baseline and follow-up VFs. Although our evaluation confirms the low overall PMAEs reported in the original studies, our findings also reveal that both models severely underpredict worsening of VF loss. Because the accurate detection and projection of glaucomatous VF decline is crucial in ophthalmic clinical practice, we recommend that this consideration is explicitly taken into account when developing and evaluating future deep learning models.", "keywords": ["convolutional neural network", "recurrent neural network", "neural network", "convolutional neural", "recurrent neural", "future visual fields", "forecast future visual", "CNN", "RNN", "visual fields", "recently been developed", "developed to forecast", "Swedish Interactive Thresholding", "Interactive Thresholding Algorithm", "Thresholding Algorithm Standard", "Ear Glaucoma Service", "network", "neural", "models", "CNN data set"], "paper_title": "Visual Field Prediction: Evaluating the Clinical Relevance of Deep Learning Models.", "last_updated": "2023/02/04"}, {"id": "0033018390", "domain": "Glaucoma (unspecified)", "model_name": "Cheng et al.", "publication_date": "2020/10/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33018390/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33018390", "task": "aipbNdPTIt", "abstract": "In this paper, we proposed and validated a probability distribution guided network for segmenting optic disc (OD) and optic cup (OC) from fundus images. Uncertainty is inevitable in deep learning, as induced by different sensors, insufficient samples, and inaccurate labeling. Since the input data and the corresponding ground truth label may be inaccurate, they may actually follow some potential distribution. In this study, a variational autoencoder (VAE) based network was proposed to estimate the joint distribution of the input image and the corresponding segmentation (both the ground truth segmentation and the predicted segmentation), making the segmentation network learn not only pixel-wise information but also semantic probability distribution. Moreover, we designed a building block, namely the Dilated Inception Block (DIB), for a better generalization of the model and a more effective extraction of multi-scale features. The proposed method was compared to several existing state-of-the-art methods. Superior segmentation performance has been observed over two datasets (ORIGA and REFUGE), with the mean Dice overlap coefficients being 96.57% and 95.81% for OD and 88.46% and 88.91% for OC.", "keywords": ["segmenting optic disc", "optic disc", "optic cup", "segmenting optic", "probability distribution guided", "distribution guided network", "Dilated Inception Block", "distribution guided", "guided network", "probability distribution", "fundus images", "optic", "distribution", "ground truth", "segmentation", "semantic probability distribution", "ground truth segmentation", "network", "validated a probability", "Inception Block"], "paper_title": "Probability distribution guided optic disc and cup segmentation from fundus images.", "last_updated": "2023/02/04"}, {"id": "0032445127", "domain": "Glaucoma (unspecified)", "model_name": "Kadambi et al.", "publication_date": "2020/05/22", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32445127/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32445127", "task": "aipbNdPTIt", "abstract": "The cup-to-disc ratio (CDR), a clinical metric of the relative size of the optic cup to the optic disc, is a key indicator of glaucoma, a chronic eye disease leading to loss of vision. CDR can be measured from fundus images through the segmentation of optic disc and optic cup . Deep convolutional networks have been proposed to achieve biomedical image segmentation with less time and more accuracy, but requires large amounts of annotated training data on a target domain, which is often unavailable. Unsupervised domain adaptation framework alleviates this problem through leveraging off-the-shelf labeled data from its relevant source domains, which is realized by learning domain invariant features and improving the generalization capabilities of the segmentation model. In this paper, we propose a WGAN domain adaptation framework for detecting optic disc-and-cup boundary in fundus images. Specifically, we build a novel adversarial domain adaptation framework that is guided by Wasserstein distance, therefore with better stability and convergence than typical adversarial methods. We finally evaluate our approach on publicly available datasets. Our experiments show that the proposed approach improves Intersection-over-Union score for optic disc-and-cup segmentation, Dice score and reduces the root-mean-square error of cup-to-disc ratio, when we compare it with direct transfer learning and other state-of-the-art adversarial domain adaptation methods. With this work, we demonstrate that WGAN guided domain adaptation obtains a state-of-the-art performance for the joint optic disc-and-cup segmentation in fundus images.", "keywords": ["chronic eye disease", "eye disease leading", "domain adaptation framework", "domain adaptation", "indicator of glaucoma", "loss of vision", "clinical metric", "relative size", "key indicator", "chronic eye", "eye disease", "disease leading", "leading to loss", "domain", "adaptation framework", "optic", "CDR", "adversarial domain adaptation", "optic cup", "optic disc"], "paper_title": "WGAN domain adaptation for the joint optic disc-and-cup segmentation in fundus images.", "last_updated": "2023/02/04"}, {"id": "0033260117", "domain": "Glaucoma (unspecified)", "model_name": "Lazaridis et al.", "publication_date": "2020/11/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33260117/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33260117", "task": null, "abstract": "Albeit spectral-domain OCT (SDOCT) is now in clinical use for glaucoma management, published clinical trials relied on time-domain OCT (TDOCT) which is characterized by low signal-to-noise ratio, leading to low statistical power. For this reason, such trials require large numbers of patients observed over long intervals and become more costly. We propose a probabilistic ensemble model and a cycle-consistent perceptual loss for improving the statistical power of trials utilizing TDOCT. TDOCT are converted to synthesized SDOCT and segmented via Bayesian fusion of an ensemble of GANs. The final retinal nerve fibre layer segmentation is obtained automatically on an averaged synthesized image using label fusion. We benchmark different networks using i) GAN, ii) Wasserstein GAN (WGAN) (iii) GAN + perceptual loss and iv) WGAN + perceptual loss. For training and validation, an independent dataset is used, while testing is performed on the UK Glaucoma Treatment Study (UKGTS), i.e. a TDOCT-based trial. We quantify the statistical power of the measurements obtained with our method, as compared with those derived from the original TDOCT. The results provide new insights into the UKGTS, showing a significantly better separation between treatment arms, while improving the statistical power of TDOCT on par with visual field measurements.", "keywords": ["Albeit spectral-domain OCT", "spectral-domain OCT", "time-domain OCT", "published clinical trials", "clinical trials relied", "low statistical power", "OCT", "published clinical", "statistical power", "Albeit spectral-domain", "relied on time-domain", "TDOCT", "low statistical", "leading to low", "clinical trials", "glaucoma management", "characterized by low", "clinical", "low", "power"], "paper_title": "Improving statistical power of glaucoma clinical trials using an ensemble of cyclical generative adversarial networks.", "last_updated": "2023/02/04"}, {"id": "0035133405", "domain": "Glaucoma (unspecified)", "model_name": "Shon et al.", "publication_date": "2022/04/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35133405/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35133405", "task": null, "abstract": "To investigate the feasibility of extracting a low-dimensional latent structure of anterior segment optical coherence tomography (AS-OCT) images by use of a \u03b2-variational autoencoder (\u03b2-VAE). We retrospectively collected 2111 AS-OCT images from 2111 eyes of 1261 participants from the ongoing Asan Glaucoma Progression Study. After hyperparameter optimization, the images were analyzed with \u03b2-VAE. The mean participant age was 64.4 years, with mean values of visual field index and mean deviation of 86.4% and -5.33 dB, respectively. After experiments, a latent space size of 6 and \u03b2 value of 53 were selected for latent space analysis with \u03b2-VAE. Latent variables were successfully disentangled, showing readily interpretable distinct characteristics, such as the overall depth and area of the anterior chamber (\u03b71), pupil diameter (\u03b72), iris profile (\u03b73 and \u03b74), and corneal curvature (\u03b75). \u03b2-VAE can successfully be applied for disentangled latent space representation of AS-OCT images, revealing the high possibility of applying unsupervised learning in the medical image analysis. This study demonstrates that a deep learning-based latent space model can be applied for the analysis of AS-OCT images.", "keywords": ["optical coherence tomography", "segment optical coherence", "Asan Glaucoma Progression", "low-dimensional latent structure", "Glaucoma Progression Study", "anterior segment optical", "ongoing Asan Glaucoma", "coherence tomography", "\u03b2-variational autoencoder", "investigate the feasibility", "feasibility of extracting", "extracting a low-dimensional", "segment optical", "optical coherence", "AS-OCT images", "Asan Glaucoma", "Glaucoma Progression", "latent space", "images", "latent"], "paper_title": "Development of a \u03b2-Variational Autoencoder for Disentangled Latent Space Representation of Anterior Segment Optical Coherence Tomography Images.", "last_updated": "2023/02/04"}, {"id": "0030840736", "domain": "Glaucoma (unspecified)", "model_name": "Panda et al.", "publication_date": "2018/10/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30840736/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30840736", "task": "IWqQC1koJA", "abstract": "Glaucoma is a progressive optic neuropathy characterized by peripheral visual field loss, which is caused by degeneration of retinal nerve fibers. The peripheral vision loss due to glaucoma is asymptomatic. If not detected and treated at an early stage, it leads to complete blindness, which is irreversible in nature. The retinal nerve fiber layer defect (RNFLD) provides an earliest objective evidence of glaucoma. In this regard, we explore cost-effective redfree fundus imaging for RNFLD detection to be practically useful for computer-assisted early glaucoma risk assessment. RNFLD appears as a wedge shaped arcuate structure radiating from the optic disc. The very low contrast between RNFLD and background makes its visual detection quite challenging even by medical experts. In our study, we formulate a deep convolutional neural network (CNN) based patch classification strategy for RNFLD boundary localization. A large number of RNFLD and background image patches train the deep CNN model, which extracts sufficient discriminative information from the patches and results in accurate RNFLD boundary pixel classification. The proposed approach is found to achieve enhanced RNFLD detection performance with sensitivity of 0.8205 and false positive per image of 0.2000 on a newly created early glaucomatic fundus image database.", "keywords": ["progressive optic neuropathy", "optic neuropathy characterized", "RNFLD", "neuropathy characterized", "caused by degeneration", "peripheral visual field", "visual field loss", "peripheral vision loss", "field loss", "retinal nerve", "retinal nerve fibers", "RNFLD detection", "Glaucoma", "RNFLD boundary", "vision loss due", "progressive optic", "optic neuropathy", "visual field", "characterized by peripheral", "peripheral"], "paper_title": "Deep convolutional neural network-based patch classification for retinal nerve fiber layer defect detection in early glaucoma.", "last_updated": "2023/02/04"}, {"id": "0036328198", "domain": "Glaucoma (unspecified)", "model_name": "Kamalipour et al.", "publication_date": "2022/11/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36328198/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36328198", "task": null, "abstract": "To estimate central 10-degree visual field (VF) map from spectral-domain optical coherence tomography (SD-OCT) retinal nerve fiber layer thickness (RNFL) measurements in glaucoma with artificial intelligence. Artificial intelligence (convolutional neural networks) study. This study included 5352 SD-OCT scans and 10-2 VF pairs from 1365 eyes of 724 healthy patients, patients with suspected glaucoma, and patients with glaucoma. Convolutional neural networks (CNNs) were developed to estimate the 68 individual sensitivity thresholds of 10-2 VF map using all-sectors (CNN<sub>A</sub>) and temporal-sectors (CNN<sub>T</sub>) RNFL thickness information of the SD-OCT circle scan (768 thickness points). 10-2 indices including pointwise total deviation (TD) values, mean deviation (MD), and pattern standard deviation (PSD) were generated using the CNN-estimated sensitivity thresholds at individual test locations. Linear regression (LR) models with the same input were used for comparison. The CNN<sub>A</sub> model achieved an average pointwise mean absolute error of 4.04 dB (95% confidence interval [CI] 3.76-4.35) and correlation coefficient (r) of 0.59 (95% CI 0.52-0.64) over 10-2 map and the mean absolute error and r of 2.88 dB (95% CI 2.63-3.15) and 0.74 (95% CI 0.67-0.80) for MD, and 2.31 dB (95% CI 2.03-2.61) and 0.59 (95% CI 0.51-0.65) for PSD estimations, respectively, significantly outperforming the LR<sub>A</sub> model. The proposed CNN<sub>A</sub> model improved the estimation of 10-2 VF map based on circumpapillary SD-OCT RNFL thickness measurements. These artificial intelligence methods using SD-OCT structural data show promise to individualize the frequency of central VF assessment in patients with glaucoma and would enable the reallocation of resources from patients at lowest risk to those at highest risk of central VF damage.", "keywords": ["retinal nerve fiber", "optical coherence tomography", "spectral-domain optical coherence", "nerve fiber layer", "fiber layer thickness", "artificial intelligence", "visual field", "convolutional neural networks", "coherence tomography", "retinal nerve", "spectral-domain optical", "optical coherence", "nerve fiber", "fiber layer", "RNFL thickness", "CNN", "SD-OCT RNFL thickness", "10-2", "RNFL", "convolutional neural"], "paper_title": "Deep Learning Estimation of 10-2 Visual Field Map Based on Circumpapillary Retinal Nerve Fiber Layer Thickness Measurements.", "last_updated": "2023/02/04"}, {"id": "0033286615", "domain": "Glaucoma (unspecified)", "model_name": "Jin et al.", "publication_date": "2020/07/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33286615/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33286615", "task": "aipbNdPTIt", "abstract": "Medical image segmentation is an important part of medical image analysis. With the rapid development of convolutional neural networks in image processing, deep learning methods have achieved great success in the field of medical image processing. Deep learning is also used in the field of auxiliary diagnosis of glaucoma, and the effective segmentation of the optic disc area plays an important assistant role in the diagnosis of doctors in the clinical diagnosis of glaucoma. Previously, many U-Net-based optic disc segmentation methods have been proposed. However, the channel dependence of different levels of features is ignored. The performance of fundus image segmentation in small areas is not satisfactory. In this paper, we propose a new aggregation channel attention network to make full use of the influence of context information on semantic segmentation. Different from the existing attention mechanism, we exploit channel dependencies and integrate information of different scales into the attention mechanism. At the same time, we improved the basic classification framework based on cross entropy, combined the dice coefficient and cross entropy, and balanced the contribution of dice coefficients and cross entropy loss to the segmentation task, which enhanced the performance of the network in small area segmentation. The network retains more image features, restores the significant features more accurately, and further improves the segmentation performance of medical images. We apply it to the fundus optic disc segmentation task. We demonstrate the segmentation performance of the model on the Messidor dataset and the RIM-ONE dataset, and evaluate the proposed architecture. Experimental results show that our network architecture improves the prediction performance of the base architectures under different datasets while maintaining the computational efficiency. The results render that the proposed technologies improve the segmentation with 0.0469 overlapping error on Messidor.", "keywords": ["medical image analysis", "Medical image segmentation", "Medical image", "medical image processing", "segmentation", "optic disc segmentation", "image segmentation", "image", "image processing", "Medical", "optic disc", "image analysis", "segmentation performance", "disc segmentation", "disc segmentation methods", "fundus image segmentation", "part of medical", "performance", "important part", "deep learning"], "paper_title": "Optic Disc Segmentation Using Attention-Based U-Net and the Improved Cross-Entropy Convolutional Neural Network.", "last_updated": "2023/02/04"}, {"id": "0036769865", "domain": "Glaucoma (unspecified)", "model_name": "Bouris et al.", "publication_date": "2023/02/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36769865/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36769865", "task": null, "abstract": "This study describes the development of a convolutional neural network (CNN) for automated assessment of optic disc photograph quality. Using a code-free deep learning platform, a total of 2377 optic disc photographs were used to develop a deep CNN capable of determining optic disc photograph quality. Of these, 1002 were good-quality images, 609 were acceptable-quality, and 766 were poor-quality images. The dataset was split 80/10/10 into training, validation, and test sets and balanced for quality. A ternary classification model (good, acceptable, and poor quality) and a binary model (usable, unusable) were developed. In the ternary classification system, the model had an overall accuracy of 91% and an AUC of 0.98. The model had higher predictive accuracy for images of good (93%) and poor quality (96%) than for images of acceptable quality (91%). The binary model performed with an overall accuracy of 98% and an AUC of 0.99. When validated on 292 images not included in the original training/validation/test dataset, the model's accuracy was 85% on the three-class classification task and 97% on the binary classification task. The proposed system for automated image-quality assessment for optic disc photographs achieves high accuracy in both ternary and binary classification systems, and highlights the success achievable with a code-free platform. There is wide clinical and research potential for such a model, with potential applications ranging from integration into fundus camera software to provide immediate feedback to ophthalmic photographers, to prescreening large databases before their use in research.", "keywords": ["optic disc photograph", "disc photograph quality", "convolutional neural network", "optic disc", "disc photograph", "determining optic disc", "photograph quality", "deep CNN capable", "neural network", "quality", "disc", "study describes", "describes the development", "convolutional neural", "disc photographs achieves", "deep CNN", "model", "CNN", "optic", "CNN capable"], "paper_title": "A Neural Network for Automated Image Quality Assessment of Optic Disc Photographs.", "last_updated": "2023/02/04"}, {"id": "0031630011", "domain": "Glaucoma (unspecified)", "model_name": "Orlando et al.", "publication_date": "2019/10/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31630011/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31630011", "task": null, "abstract": "Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.", "keywords": ["working age populations", "age populations", "irreversible but preventable", "preventable blindness", "blindness in working", "working age", "Glaucoma", "Retinal Fundus Glaucoma", "Fundus Glaucoma Challenge", "Color fundus photography", "glaucoma classification", "cost-effective imaging modality", "fundus", "Fundus Glaucoma", "Challenge", "Retinal Fundus", "Glaucoma Challenge", "glaucoma classification task", "retinal disorders", "REFUGE"], "paper_title": "REFUGE\u00a0Challenge: A unified framework for evaluating automated\u00a0methods for glaucoma\u00a0assessment from fundus photographs.", "last_updated": "2023/02/04"}, {"id": "0036983572", "domain": "Glaucoma (unspecified)", "model_name": "Hosni Mahmoud et al.", "publication_date": "2023/02/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36983572/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36983572", "task": "0GPcU42tzV", "abstract": "Deep learning models are usually utilized to learn from spatial data, only a few studies are proposed to predict glaucoma time progression utilizing deep learning models. In this article, we present a bidirectional recurrent deep learning model (Bi-RM) to detect prospective progressive visual field diagnoses. A dataset of 5413 different eyes from 3321 samples is utilized as the learning phase dataset and 1272 eyes are used for testing. Five consecutive diagnoses are recorded from the dataset as input and the sixth progressive visual field diagnosis is matched with the prediction of the Bi-RM. The precision metrics of the Bi-RM are validated in association with the linear regression algorithm (LR) and term memory (TM) technique. The total prediction error of the Bi-RM is significantly less than those of LR and TM. In the class prediction, Bi-RM depicts the least prediction error in all three methods in most of the testing cases. In addition, Bi-RM is not impacted by the reliability keys and the glaucoma degree.", "keywords": ["Deep learning models", "time progression utilizing", "progression utilizing deep", "utilizing deep learning", "Deep learning", "predict glaucoma time", "glaucoma time progression", "learning models", "recurrent deep learning", "spatial data", "learn from spatial", "studies are proposed", "proposed to predict", "time progression", "progression utilizing", "utilizing deep", "progressive visual field", "bidirectional recurrent deep", "learning", "Bi-RM"], "paper_title": "Bidirectional Neural Network Model for Glaucoma Progression Prediction.", "last_updated": "2023/02/04"}, {"id": "0032730210", "domain": "Glaucoma (unspecified)", "model_name": "Wang et al.", "publication_date": "2020/08/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32730210/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32730210", "task": null, "abstract": "We propose a conceptually simple framework for fast COVID-19 screening in 3D chest CT images. The framework can efficiently predict whether or not a CT scan contains pneumonia while simultaneously identifying pneumonia types between COVID-19 and Interstitial Lung Disease (ILD) caused by other viruses. In the proposed method, two 3D-ResNets are coupled together into a single model for the two above-mentioned tasks via a novel prior-attention strategy. We extend residual learning with the proposed prior-attention mechanism and design a new so-called prior-attention residual learning (PARL) block. The model can be easily built by stacking the PARL blocks and trained end-to-end using multi-task losses. More specifically, one 3D-ResNet branch is trained as a binary classifier using lung images with and without pneumonia so that it can highlight the lesion areas within the lungs. Simultaneously, inside the PARL blocks, prior-attention maps are generated from this branch and used to guide another branch to learn more discriminative representations for the pneumonia-type classification. Experimental results demonstrate that the proposed framework can significantly improve the performance of COVID-19 screening. Compared to other methods, it achieves a state-of-the-art result. Moreover, the proposed method can be easily extended to other similar clinical applications such as computer-aided detection and diagnosis of pulmonary nodules in CT images, glaucoma lesions in Retina fundus images, etc.", "keywords": ["conceptually simple framework", "Interstitial Lung Disease", "propose a conceptually", "conceptually simple", "PARL blocks", "simple framework", "Lung Disease", "PARL", "Interstitial Lung", "proposed", "framework", "prior-attention", "proposed framework", "proposed method", "images", "framework for fast", "residual learning", "identifying pneumonia types", "pneumonia", "proposed prior-attention mechanism"], "paper_title": "Prior-Attention Residual Learning for More Discriminative COVID-19 Screening in CT Images.", "last_updated": "2023/02/04"}, {"id": "0036545262", "domain": "Glaucoma (unspecified)", "model_name": "RimNet", "publication_date": "2022/11/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36545262/", "code_link": "https://github.com/tyleradavis/glaucomaml", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "36545262", "task": null, "abstract": "Accurate neural rim measurement based on optic disc imaging is important to glaucoma severity grading and often performed by trained glaucoma specialists. We aim to improve upon existing automated tools by building a fully automated system (RimNet) for direct rim identification in glaucomatous eyes and measurement of the minimum rim-to-disc ratio (mRDR) in intact rims, the angle of absent rim width (ARW) in incomplete rims, and the rim-to-disc-area ratio (RDAR) with the goal of optic disc damage grading. Retrospective cross sectional study. One thousand and twenty-eight optic disc photographs with evidence of glaucomatous optic nerve damage from 1021 eyes of 903 patients with any form of primary glaucoma were included. The mean age was 63.7 (\u00b1 14.9) yrs. The average mean deviation of visual fields was\u00a0-8.03 (\u00b1 8.59). The images were required to be of adequate quality, have signs of glaucomatous damage, and be free of significant concurrent pathology as independently determined by glaucoma specialists. Rim and optic cup masks for each image were manually delineated by glaucoma specialists. The database was randomly split into 80/10/10 for training, validation, and testing, respectively. RimNet consists of a deep learning rim and cup segmentation model, a computer vision mRDR measurement tool for intact rims, and an ARW measurement tool for incomplete rims. The mRDR is calculated at the thinnest rim section while ARW is calculated in regions of total rim loss. The RDAR was also calculated. Evaluation on the Drishti-GS dataset provided external validation (Sivaswamy 2015). Median Absolute Error (MAE) between glaucoma specialists and RimNet for mRDR and ARW. On the test set, RimNet achieved a mRDR MAE of 0.03 (0.05), ARW MAE of 31 (89)\u00b0, and an RDAR MAE of 0.09 (0.10). On the Drishti-GS dataset, an mRDR MAE of 0.03 (0.04) and an mRDAR MAE of 0.09 (0.10) was observed. RimNet demonstrated acceptably accurate rim segmentation and mRDR and ARW measurements. The fully automated algorithm presented here would be a valuable component in an automated mRDR-based glaucoma grading system. Further improvements could be made by improving identification and segmentation performance on incomplete rims and expanding the number and variety of glaucomatous training images.", "keywords": ["optic disc imaging", "optic disc", "MAE", "ARW", "glaucoma severity grading", "rim", "imaging is important", "performed by trained", "glaucoma", "glaucoma specialists", "optic", "mRDR", "trained glaucoma specialists", "disc damage grading", "neural rim measurement", "rim measurement based", "optic disc damage", "disc imaging", "mRDR MAE", "ARW MAE"], "paper_title": "RimNet: A Deep Neural Network Pipeline for Automated Identification of the Optic Disc Rim.", "last_updated": "2023/02/04"}, {"id": "0036829724", "domain": "Glaucoma (unspecified)", "model_name": "CTS-Net", "publication_date": "2023/02/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36829724/", "code_link": null, "model_type": "transformer", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36829724", "task": "aipbNdPTIt", "abstract": "Optical Coherence Tomography (OCT) technology is essential to obtain glaucoma diagnostic data non-invasively and rapidly. Early diagnosis of glaucoma can be achieved by analyzing the thickness and shape of retinal layers. Accurate retinal layer segmentation assists ophthalmologists in improving the efficiency of disease diagnosis. Deep learning technology is one of the most effective methods for processing OCT retinal layer images, which can segment different retinal layers and effectively obtain the topological structure of the boundary. This paper proposes a neural network method for retinal layer segmentation based on the CSWin Transformer (CTS-Net), which can achieve pixel-level segmentation and obtain smooth boundaries. A Dice loss function based on boundary areas (BADice Loss) is proposed to make CTS-Net learn more features of edge regions and improve the accuracy of boundary segmentation. We applied the model to the publicly available dataset of glaucoma retina, and the test results showed that mean absolute distance (MAD), root mean square error (RMSE), and dice-similarity coefficient (DSC) metrics were 1.79 pixels, 2.15 pixels, and 92.79%, respectively, which are better than those of the compared model. In the cross-validation experiment, the ranges of MAD, RMSE, and DSC are 0.05 pixels, 0.03 pixels, and 0.33%, respectively, with a slight difference, which further verifies the generalization ability of CTS-Net.", "keywords": ["Optical Coherence Tomography", "Coherence Tomography", "diagnostic data non-invasively", "glaucoma diagnostic data", "Optical Coherence", "OCT retinal layer", "retinal layer segmentation", "obtain glaucoma diagnostic", "retinal layer", "non-invasively and rapidly", "diagnostic data", "data non-invasively", "processing OCT retinal", "Accurate retinal layer", "glaucoma diagnostic", "retinal layer images", "retinal", "layer segmentation", "OCT retinal", "Tomography"], "paper_title": "CTS-Net: A Segmentation Network for Glaucoma Optical Coherence Tomography Retinal Layer Images.", "last_updated": "2023/02/04"}, {"id": "0036866233", "domain": "Glaucoma (unspecified)", "model_name": "Ma et al.", "publication_date": "2023/01/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36866233/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36866233", "task": null, "abstract": "Artificial intelligence (AI) has been approved for biomedical research in diverse areas from bedside clinical studies to benchtop basic scientific research. For ophthalmic research, in particular glaucoma, AI applications are rapidly growing for potential clinical translation given the vast data available and the introduction of federated learning. Conversely, AI for basic science remains limited despite its useful power in providing mechanistic insight. In this perspective, we discuss recent progress, opportunities, and challenges in the application of AI in glaucoma for scientific discoveries. Specifically, we focus on the research paradigm of reverse translation, in which clinical data are first used for patient-centered hypothesis generation followed by transitioning into basic science studies for hypothesis validation. We elaborate on several distinctive areas of research opportunities for reverse translation of AI in glaucoma including disease risk and progression prediction, pathology characterization, and sub-phenotype identification. We conclude with current challenges and future opportunities for AI research in basic science for glaucoma such as inter-species diversity, AI model generalizability and explainability, as well as AI applications using advanced ocular imaging and genomic data.", "keywords": ["Artificial intelligence", "bedside clinical studies", "approved for biomedical", "benchtop basic scientific", "bedside clinical", "basic science", "research", "benchtop basic", "biomedical research", "basic", "diverse areas", "clinical", "potential clinical translation", "glaucoma", "basic scientific research", "basic science studies", "science", "reverse translation", "translation", "opportunities"], "paper_title": "Reverse translation of artificial intelligence in glaucoma: Connecting basic science with clinical applications.", "last_updated": "2023/02/04"}, {"id": "0036611452", "domain": "Glaucoma (unspecified)", "model_name": "Chan et al.", "publication_date": "2023/01/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36611452/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36611452", "task": null, "abstract": "The quality of ocular fundus photographs can affect the accuracy of the morphologic assessment of the optic nerve head (ONH), either by humans or by deep learning systems (DLS). In order to automatically identify ONH photographs of optimal quality, we have developed, trained, and tested a DLS, using an international, multicentre, multi-ethnic dataset of 5015 ocular fundus photographs from 31 centres in 20 countries participating to the Brain and Optic Nerve Study with Artificial Intelligence (BONSAI). The reference standard in image quality was established by three experts who independently classified photographs as of \"good\", \"borderline\", or \"poor\" quality. The DLS was trained on 4208 fundus photographs and tested on an independent external dataset of 807 photographs, using a multi-class model, evaluated with a one-vs-rest classification strategy. In the external-testing dataset, the DLS could identify with excellent performance \"good\" quality photographs (AUC = 0.93 (95% CI, 0.91-0.95), accuracy = 91.4% (95% CI, 90.0-92.9%), sensitivity = 93.8% (95% CI, 92.5-95.2%), specificity = 75.9% (95% CI, 69.7-82.1%) and \"poor\" quality photographs (AUC = 1.00 (95% CI, 0.99-1.00), accuracy = 99.1% (95% CI, 98.6-99.6%), sensitivity = 81.5% (95% CI, 70.6-93.8%), specificity = 99.7% (95% CI, 99.6-100.0%). \"Borderline\" quality images were also accurately classified (AUC = 0.90 (95% CI, 0.88-0.93), accuracy = 90.6% (95% CI, 89.1-92.2%), sensitivity = 65.4% (95% CI, 56.6-72.9%), specificity = 93.4% (95% CI, 92.1-94.8%). The overall accuracy to distinguish among the three classes was 90.6% (95% CI, 89.1-92.1%), suggesting that this DLS could select optimal quality fundus photographs in patients with neuro-ophthalmic and neurological disorders affecting the ONH.", "keywords": ["optic nerve head", "Optic Nerve Study", "deep learning systems", "optic nerve", "ocular fundus photographs", "fundus photographs", "nerve head", "Nerve Study", "identify ONH photographs", "learning systems", "ocular fundus", "photographs", "DLS", "morphologic assessment", "deep learning", "Artificial Intelligence", "Study with Artificial", "quality", "quality fundus photographs", "automatically identify ONH"], "paper_title": "A Deep Learning System for Automated Quality Evaluation of Optic Disc Photographs in Neuro-Ophthalmic Disorders.", "last_updated": "2023/02/04"}, {"id": "0036944385", "domain": "Glaucoma (unspecified)", "model_name": "Herbert et al.", "publication_date": "2023/03/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36944385/", "code_link": null, "model_type": "transformer", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36944385", "task": "0GPcU42tzV", "abstract": "Assess whether we can forecast future rapid visual field (VF) worsening using deep learning models (DLMs) trained on early VF, OCT, and clinical data. Retrospective cohort study. 4,536 eyes from 2,962 patients. 263 (5.80%) of eyes underwent rapid VF worsening (MD slope <-1dB/yr across all VFs). We included eyes that met the following criteria: 1) followed for glaucoma or suspect status 2) had at least five longitudinal reliable VFs (VF<sub>1,</sub> VF<sub>2,</sub> VF<sub>3,</sub> VF<sub>4,</sub> VF<sub>5</sub>) 3) had one reliable baseline Optical Coherence Tomography (OCT) scan (OCT<sub>1</sub>) and one set of baseline clinical measurements (Clinical<sub>1</sub>) at the time of VF<sub>1</sub>. We designed a DLM to forecast future rapid VF worsening. The input consisted of spatially oriented total deviation values from VF<sub>1</sub> (including or not including VF<sub>2</sub> and VF<sub>3</sub> in some models) and retinal nerve fiber layer thickness values from the baseline OCT. We passed this VF/OCT stack into a vision transformer feature extractor, the output of which was concatenated with baseline clinical data before putting it through a linear classifier to predict that eye's risk of rapid VF worsening across the five VFs. We compared the performance of models with differing inputs by computing area under receiver operating curve (AUC) in the test set. Specifically, we trained models with the following inputs: Model V: VF<sub>1</sub>; VC: VF<sub>1</sub>+ Clinical<sub>1</sub>; VO: VF<sub>1</sub>+ OCT<sub>1</sub>; VOC: VF<sub>1</sub>+ Clinical<sub>1</sub>+ OCT<sub>1</sub>; V<sub>2</sub>: VF<sub>1</sub> + VF<sub>2</sub>; V<sub>2</sub>OC: VF<sub>1</sub> + VF<sub>2</sub> + Clinical<sub>1</sub> + OCT<sub>1</sub>; V<sub>3</sub>: VF<sub>1</sub> + VF<sub>2</sub> + VF<sub>3</sub>; V<sub>3</sub>OC: VF<sub>1</sub> + VF<sub>2</sub> + VF<sub>3</sub> + Clinical<sub>1</sub> + OCT<sub>1</sub>. AUC of DLMs when forecasting rapidly worsening eyes. Model V<sub>3</sub>OC best forecasted rapid worsening with an AUC (95% CI) of 0.87 (0.77, 0.97). Remaining models in descending order of performance and their respective AUC [95% CI] were: Model V<sub>3</sub> (0.84 [0.74 to 0.95]), Model V<sub>2</sub>OC (0.81 [0.70 to 0.92]), Model V<sub>2</sub> (0.81 [0.70 to 0.82]), Model VOC (0.77 [0.65, 0.88]), Model VO [0.75 [0.64, 0.88], Model VC (0.75 [0.63, 0.87]), Model V (0.74 [0.62, 0.86]). DLMs can forecast future rapid glaucoma worsening with modest to high performance when trained using data from early in the disease course. Including baseline data from multiple modalities and subsequent visits improves performance beyond using VF data alone.", "keywords": ["deep learning models", "OCT", "rapid visual field", "Model", "clinical", "forecast future rapid", "models", "baseline clinical data", "baseline clinical", "learning models", "future rapid visual", "clinical data", "worsening", "visual field", "future rapid", "Optical Coherence Tomography", "deep learning", "rapid", "baseline Optical Coherence", "Model VOC"], "paper_title": "Forecasting Risk of Future Rapid Glaucoma Worsening Using Early Visual Field, Optical Coherence Tomography, and Clinical Data.", "last_updated": "2023/02/04"}, {"id": "0036778387", "domain": "Glaucoma (unspecified)", "model_name": "saturn", "publication_date": "2023/02/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36778387/", "code_link": "https://github.com/snap-stanford/saturn", "model_type": null, "verified": false, "model_task": null, "code_available": true, "disease_class": "6ct3D7Ut8A", "pmid": "36778387", "task": null, "abstract": "Analysis of single-cell datasets generated from diverse organisms offers unprecedented opportunities to unravel fundamental evolutionary processes of conservation and diversification of cell types. However, inter-species genomic differences limit the joint analysis of crossspecies datasets to orthologous genes. Here, we present SATURN, a deep learning method for learning universal cell embeddings that encodes genes' biological properties using protein language models. By coupling protein embeddings from language models with RNA expression, SATURN integrates datasets profiled from different species regardless of their genomic similarity. SATURN has a unique ability to detect functionally related genes co-expressed across species, redefining differential expression for cross-species analysis. We apply SATURN to three species whole-organism atlases and frog and zebrafish embryogenesis datasets. We show that cell embeddings learnt in SATURN can be effectively used to transfer annotations across species and identify both homologous and species-specific cell types, even across evolutionarily remote species. Finally, we use SATURN to reannotate the five species Cell Atlas of Human Trabecular Meshwork and Aqueous Outflow Structures and find evidence of potentially divergent functions between glaucoma associated genes in humans and other species.", "keywords": ["diverse organisms offers", "organisms offers unprecedented", "offers unprecedented opportunities", "unravel fundamental evolutionary", "fundamental evolutionary processes", "single-cell datasets generated", "generated from diverse", "diverse organisms", "organisms offers", "offers unprecedented", "unprecedented opportunities", "opportunities to unravel", "unravel fundamental", "fundamental evolutionary", "evolutionary processes", "processes of conservation", "conservation and diversification", "SATURN", "species", "cell"], "paper_title": "Towards Universal Cell Embeddings: Integrating Single-cell RNA-seq Datasets across Species with SATURN.", "last_updated": "2023/02/04"}, {"id": "0033805685", "domain": "Glaucoma (unspecified)", "model_name": "Oh et al.", "publication_date": "2021/03/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33805685/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33805685", "task": "IWqQC1koJA", "abstract": "The aim is to develop a machine learning prediction model for the diagnosis of glaucoma and an explanation system for a specific prediction. Clinical data of the patients based on a visual field test, a retinal nerve fiber layer optical coherence tomography (RNFL OCT) test, a general examination including an intraocular pressure (IOP) measurement, and fundus photography were provided for the feature selection process. Five selected features (variables) were used to develop a machine learning prediction model. The support vector machine, C5.0, random forest, and XGboost algorithms were tested for the prediction model. The performance of the prediction models was tested with 10-fold cross-validation. Statistical charts, such as gauge, radar, and Shapley Additive Explanations (SHAP), were used to explain the prediction case. All four models achieved similarly high diagnostic performance, with accuracy values ranging from 0.903 to 0.947. The XGboost model is the best model with an accuracy of 0.947, sensitivity of 0.941, specificity of 0.950, and AUC of 0.945. Three statistical charts were established to explain the prediction based on the characteristics of the XGboost model. Higher diagnostic performance was achieved with the XGboost model. These three statistical charts can help us understand why the machine learning model produces a specific prediction result. This may be the first attempt to apply \"explainable artificial intelligence\" to eye disease diagnosis.", "keywords": ["machine learning prediction", "learning prediction model", "prediction", "RNFL OCT", "machine learning", "prediction model", "model", "learning prediction", "machine learning model", "Shapley Additive Explanations", "XGboost model", "explanation system", "machine", "specific prediction", "Statistical charts", "XGboost", "learning", "develop a machine", "visual field test", "Additive Explanations"], "paper_title": "Explainable Machine Learning Model for Glaucoma Diagnosis and Its Interpretation.", "last_updated": "2023/02/04"}, {"id": "0033747815", "domain": "Glaucoma (unspecified)", "model_name": "Fernandez Escamez et al.", "publication_date": "2021/03/18", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33747815/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33747815", "task": "IWqQC1koJA", "abstract": "To develop a classifier for differentiating between healthy and early stage glaucoma eyes based on peripapillary retinal nerve fiber layer (RNFL) thicknesses measured with optical coherence tomography (OCT), using machine learning algorithms with a high interpretability. Ninety patients with early glaucoma and 85 healthy eyes were included. Early glaucoma eyes showed a visual field (VF) defect with mean deviation >-6.00 dB and characteristic glaucomatous morphology. RNFL thickness in every quadrant, clock-hour and average thickness were used to feed machine learning algorithms. Cluster analysis was conducted to detect and exclude outliers. Tree gradient boosting algorithms were used to calculate the importance of parameters on the classifier and to check the relation between their values and its impact on the classifier. Parameters with the lowest importance were excluded and a weighted decision tree analysis was applied to obtain an interpretable classifier. Area under the ROC curve (AUC), accuracy and generalization ability of the model were estimated using cross validation techniques. Average and 7 clock-hour RNFL thicknesses were the parameters with the highest importance. Correlation between parameter values and impact on classification displayed a stepped pattern for average thickness. Decision tree model revealed that average thickness lower than 82 \u00b5m was a high predictor for early glaucoma. Model scores had AUC of 0.953 (95%CI: 0.903-0998), with an accuracy of 89%. Gradient boosting methods provide accurate and highly interpretable classifiers to discriminate between early glaucoma and healthy eyes. Average and 7-hour RNFL thicknesses have the best discriminant power.", "keywords": ["nerve fiber layer", "optical coherence tomography", "peripapillary retinal nerve", "retinal nerve fiber", "stage glaucoma eyes", "glaucoma eyes based", "early stage glaucoma", "early glaucoma", "Early glaucoma eyes", "fiber layer", "coherence tomography", "glaucoma eyes", "based on peripapillary", "peripapillary retinal", "retinal nerve", "nerve fiber", "measured with optical", "optical coherence", "machine learning algorithms", "healthy eyes"], "paper_title": "High interpretable machine learning classifier for early glaucoma diagnosis.", "last_updated": "2023/02/04"}, {"id": "0036210294", "domain": "Glaucoma (unspecified)", "model_name": "Garc\u00eda-Jim\u00e9nez et al.", "publication_date": "2022/10/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36210294/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36210294", "task": "IWqQC1koJA", "abstract": "To discriminate suspect glaucomatous from control eyes using corneal densitometry based on the statistical modeling of the pixel intensity distribution of Scheimpflug images. Twenty-four participants (10 suspect glaucomatous and 14 control eyes) were included in this retrospective study. Corneal biomechanics was assessed with the commercial Scheimpflug camera Corvis ST (Oculus). Sets of 140 images acquired per measurement were exported for further analysis. After corneal segmentation, pixel intensities corresponding to different corneal depths were statistically modeled for each image, from which corneal densitometry in the form of parameters \u03b1 (brightness) and \u03b2 (homogeneity) was derived. After data pre-processing, parameters \u03b1 and \u03b2 were input to six supervised machine learning algorithms that were trained, tested, and compared. There exists a statistically significant difference in \u03b1 and \u03b2 parameters between suspect glaucomatous and control eyes (both, P < 0.05/N, Bonferroni). From the implemented supervised machine learning algorithms, the K-nearest neighbors (K-NN) algorithm reached 83.93% accuracy to discriminate between groups only using corneal densitometry parameters (\u03b1 and \u03b2). Densitometry of the anterior cornea including epithelium on its own has the potential to serve as a clinical tool for early glaucoma risk assessment.", "keywords": ["pixel intensity distribution", "suspect glaucomatous", "control eyes", "statistical modeling", "intensity distribution", "Scheimpflug camera Corvis", "discriminate suspect glaucomatous", "corneal densitometry based", "corneal", "glaucomatous", "corneal densitometry", "distribution of Scheimpflug", "Scheimpflug", "suspect", "control", "eyes", "densitometry based", "pixel intensity", "Scheimpflug images", "commercial Scheimpflug camera"], "paper_title": "Suspect glaucoma detection from corneal densitometry supported by machine learning.", "last_updated": "2023/02/04"}, {"id": "0035600610", "domain": "Glaucoma (unspecified)", "model_name": "Gajendran et al.", "publication_date": "2022/05/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35600610/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35600610", "task": "IWqQC1koJA", "abstract": "Early-stage glaucoma diagnosis has been a challenging problem in ophthalmology. The current state-of-the-art glaucoma diagnosis techniques do not completely leverage the functional measures' such as electroretinogram's immense potential; instead, focus is on structural measures like optical coherence tomography. The current study aims to take a foundational step toward the development of a novel and reliable predictive framework for early detection of glaucoma using machine-learning-based algorithm capable of leveraging medically relevant information that ERG signals contain. ERG signals from 60 eyes of DBA/2 mice were grouped for binary classification based on age. The signals were also grouped based on intraocular pressure (IOP) for multiclass classification. Statistical and wavelet-based features were engineered and extracted. Important predictors (ERG tests and features) were determined, and the performance of five machine learning-based methods were evaluated. Random forest (bagged trees) ensemble classifier provided the best performance in both binary and multiclass classification of ERG signals. An accuracy of 91.7 and 80% was achieved for binary and multiclass classification, respectively, suggesting that machine-learning-based models can detect subtle changes in ERG signals if trained using advanced features such as those based on wavelet analyses. The present study describes a novel, machine-learning-based method to analyze ERG signals providing additional information that may be used to detect early-stage glaucoma. Based on promising performance metrics obtained using the proposed machine-learning-based framework leveraging an established ERG data set, we conclude that the novel framework allows for detection of functional deficits of early/various stages of glaucoma in mice.", "keywords": ["ERG signals", "ERG", "problem in ophthalmology", "challenging problem", "glaucoma diagnosis", "glaucoma", "glaucoma diagnosis techniques", "signals", "Early-stage glaucoma diagnosis", "multiclass classification", "Early-stage glaucoma", "based", "classification", "diagnosis", "diagnosis techniques", "multiclass", "binary", "analyze ERG signals", "ERG signals providing", "ophthalmology"], "paper_title": "Novel Machine-Learning Based Framework Using Electroretinography Data for the Detection of Early-Stage Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0034574059", "domain": "Glaucoma (unspecified)", "model_name": "Wu et al.", "publication_date": "2021/09/19", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34574059/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34574059", "task": "IWqQC1koJA", "abstract": "Early detection is important in glaucoma management. By using optical coherence tomography (OCT), the subtle structural changes caused by glaucoma can be detected. Though OCT provided abundant parameters for comprehensive information, clinicians may be confused once the results conflict. Machine learning classifiers (MLCs) are good tools for considering numerous parameters and generating reliable diagnoses in glaucoma practice. Here we aim to compare different MLCs based on Spectralis OCT parameters, including circumpapillary retinal nerve fiber layer (cRNFL) thickness, Bruch's membrane opening-minimum rim width (BMO-MRW), Early Treatment Diabetes Retinopathy Study (ETDRS) macular thickness, and posterior pole asymmetry analysis (PPAA), in discriminating normal from glaucomatous eyes. Five MLCs were proposed, namely conditional inference trees (CIT), logistic model tree (LMT), C5.0 decision tree, random forest (RF), and extreme gradient boosting (XGBoost). Logistic regression (LGR) was used as a benchmark for comparison. RF was shown to be the best model. Ganglion cell layer measurements were the most important predictors in early glaucoma detection and cRNFL measurements were more important as the glaucoma severity increased. The global, temporal, inferior, superotemporal, and inferotemporal sites were relatively influential locations among all parameters. Clinicians should cautiously integrate the Spectralis OCT results into the entire clinical picture when diagnosing glaucoma.", "keywords": ["Spectralis OCT", "Diabetes Retinopathy Study", "OCT", "Treatment Diabetes Retinopathy", "Spectralis OCT parameters", "glaucoma", "Early Treatment Diabetes", "Spectralis OCT results", "glaucoma management", "parameters", "Retinopathy Study", "Early", "OCT provided abundant", "Treatment Diabetes", "Diabetes Retinopathy", "optical coherence tomography", "OCT parameters", "management", "OCT provided", "MLCs"], "paper_title": "Comparison of Different Machine Learning Classifiers for Glaucoma Diagnosis Based on Spectralis OCT.", "last_updated": "2023/02/04"}, {"id": "0035366826", "domain": "Glaucoma (unspecified)", "model_name": "Dai et al.", "publication_date": "2022/04/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35366826/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35366826", "task": "IWqQC1koJA", "abstract": "Glaucoma is a generic term of a highly different disease group of optic neuropathies, which the leading cause of irreversible vision in the world. There are few biomarkers available for clinical prediction and diagnosis, and the diagnosis of patients is mostly delayed. Differential gene expression of transcriptome sequencing data (GSE9944 and GSE2378) for normal samples and glaucoma samples from the GEO database were analyzed. Furthermore, based on different algorithms (Logistic Regression (LR), Random Forest (RF), lasso regression (LASSO)) two diagnostic models are constructed and diagnostic markers are screened. GO and KEGG analyses revealed the possible mechanism of differential genes in the pathogenesis of glaucoma. ROC curve confirmed the effectiveness. LR-RF model included 3 key genes (NAMPT, ADH1C, ENO2), and the LASSO model outputted 5 genes (IFI16, RFTN1, NAMPT, ADH1C, and ENO2), both algorithms have excellent diagnostic efficiency. ROC curve confirmed that the three biomarkers ADH1C, ENO2, and NAMPT were effective in the diagnosis of glaucoma. Next, the expression analysis of the three diagnostic biomarkers in glaucoma and control samples confirmed that NAMPT and ADH1C were up-regulated in glaucoma samples, and ENO2 was down-regulated. Correlation analysis showed that ENO2 was significantly negatively correlated with ADH1C (cor\u2009=\u2009-0.865714202) and NAMPT (cor\u2009=\u2009-0.730541227). Finally, three compounds for the treatment of glaucoma were obtained in the TCMs database: acetylsalicylic acid, 7-o-methylisomucitol and scutellarin which were applied to molecular docking with the diagnostic biomarker ENO2. In conclusion, our research shows that ENO2, NAMPT, and ADH1C can be used as diagnostic markers for glaucoma, and ENO2 can be used as a therapeutic target.", "keywords": ["NAMPT", "optic neuropathies", "generic term", "highly different disease", "disease group", "group of optic", "irreversible vision", "Glaucoma", "ROC curve confirmed", "diagnostic", "ROC curve", "Random Forest", "lasso", "Logistic Regression", "samples", "glaucoma samples", "lasso regression", "ROC", "genes", "diagnosis"], "paper_title": "Based on multiple machine learning to identify the ENO2 as diagnosis biomarkers of glaucoma.", "last_updated": "2023/02/04"}, {"id": "0034394853", "domain": "Glaucoma (unspecified)", "model_name": "Yoon et al.", "publication_date": "2021/08/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34394853/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34394853", "task": null, "abstract": "<b>Purpose</b>: The microbiome is considered an environmental factor that contributes to the progression of several neurodegenerative diseases. However, the association between microbiome and glaucoma remains unclear. This study investigated the features of the oral microbiome in patients with glaucoma and analyzed the microbiome biomarker candidates using a machine learning approach to predict the severity of glaucoma. <b>Methods:</b> The taxonomic composition of the oral microbiome was obtained using 16S rRNA gene sequencing, operational taxonomic unit analysis, and diversity analysis. The differentially expressed gene (DEG) analysis was performed to determine the taxonomic differences between the microbiomes of patients with glaucoma and the control participants. Multinomial logistic regression and association rule mining analysis using machine learning were performed to identify the microbiome biomarker related to glaucoma severity. <b>Results:</b> DEG analysis of the oral microbiome of patients with glaucoma revealed significant depletion of <i>Lactococcus</i> (<i>P =\u00a0</i>3.71e<sup>-31</sup>), whereas <i>Faecalibacterium</i> was enriched (<i>P</i> =\u00a09.19e<sup>-14</sup>). The candidate rules generated from the oral microbiome, including <i>Lactococcus</i>, showed 96% accuracy for association with glaucoma. <b>Conclusions:</b> Our findings indicate microbiome biomarkers for glaucoma severity with high accuracy. The relatively low oral <i>Lactococcus</i> in the glaucoma population suggests that microbial dysbiosis could play an important role in the pathophysiology of glaucoma.", "keywords": ["oral microbiome", "microbiome", "glaucoma", "neurodegenerative diseases", "considered an environmental", "environmental factor", "factor that contributes", "Purpose", "oral", "microbiome biomarker", "Lactococcus", "analysis", "glaucoma severity", "microbiome biomarker candidates", "patients with glaucoma", "glaucoma remains unclear", "patients", "DEG", "DEG analysis", "taxonomic"], "paper_title": "Analysis of oral microbiome in glaucoma patients using machine learning prediction models.", "last_updated": "2023/02/04"}, {"id": "0012214886", "domain": "Glaucoma (unspecified)", "model_name": "Chan et al.", "publication_date": "2002/10/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/12214886/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "12214886", "task": "IWqQC1koJA", "abstract": "Glaucoma is a progressive optic neuropathy with characteristic structural changes in the optic nerve head reflected in the visual field. The visual-field sensitivity test is commonly used in a clinical setting to evaluate glaucoma. Standard automated perimetry (SAP) is a common computerized visual-field test whose output is amenable to machine learning. We compared the performance of a number of machine learning algorithms with STATPAC indexes mean deviation, pattern standard deviation, and corrected pattern standard deviation. The machine learning algorithms studied included multilayer perceptron (MLP), support vector machine (SVM), and linear (LDA) and quadratic discriminant analysis (QDA), Parzen window, mixture of Gaussian (MOG), and mixture of generalized Gaussian (MGG). MLP and SVM are classifiers that work directly on the decision boundary and fall under the discriminative paradigm. Generative classifiers, which first model the data probability density and then perform classification via Bayes' rule, usually give deeper insight into the structure of the data space. We have applied MOG, MGG, LDA, QDA, and Parzen window to the classification of glaucoma from SAP. Performance of the various classifiers was compared by the areas under their receiver operating characteristic curves and by sensitivities (true-positive rates) at chosen specificities (true-negative rates). The machine-learning-type classifiers showed improved performance over the best indexes from STATPAC. Forward-selection and backward-elimination methodology further improved the classification rate and also has the potential to reduce testing time by diminishing the number of visual-field location measurements.", "keywords": ["progressive optic neuropathy", "optic nerve head", "nerve head reflected", "progressive optic", "optic neuropathy", "optic nerve", "visual field", "machine learning algorithms", "nerve head", "head reflected", "pattern standard deviation", "machine learning", "standard deviation", "optic", "visual-field sensitivity test", "learning algorithms", "machine", "Parzen window", "pattern standard", "Glaucoma"], "paper_title": "Comparison of machine learning and traditional classifiers in glaucoma diagnosis.", "last_updated": "2023/02/04"}, {"id": "0031848786", "domain": "Glaucoma (unspecified)", "model_name": "Lee et al.", "publication_date": "2019/12/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31848786/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31848786", "task": "0GPcU42tzV", "abstract": "To assess the performance of machine learning classifiers for prediction of progression of normal-tension glaucoma (NTG) in young myopic patients. Cross-sectional study. One hundred and fifty-five eyes of 155 myopic NTG patients (axial length [AL]\u2009\u2265\u200924.00\u00a0mm and refractive error\u2009\u2264\u2009-\u20093.0\u00a0D) between the ages of 20 and 40 were enrolled and divided into training (110) and test (45) sets. Sixty-five eyes showed glaucoma progression as defined by standard automated perimetry, while 91 eyes (nonprogressors) had been stable over the course of a follow-up period of at least 3\u00a0years. Two machine learning classifiers were built using the random forest and extremely randomized trees (extra-trees) models. Baseline clinical measurements obtained only at the initial visit were used as input features. The area under the receiver operating characteristic curve (AUC) was calculated to evaluate the accuracy of prediction. Mean age and AL did not significantly differ between the 2 groups on either the training or the test set. The extra-trees model achieved an AUC of 0.881 [95% CI 0.814-0.945], higher than that of the random forest model (0.811 [0.731-0.888]; P\u2009=\u20090.010). The extra-trees model also outperformed all the clinical measurements for prediction of NTG progression, including average macular ganglion cell-inner plexiform layer thickness (0.735 [0.639-0.831]) and average circumpapillary retinal nerve fiber layer thickness (0.691 [0.590-0.792]; both P\u2009<\u20090.001). In young myopic patients, the machine learning classifier with the extra-trees model can predict glaucomatous progression more effectively than clinical diagnostic parameters.", "keywords": ["myopic NTG patients", "young myopic patients", "assess the performance", "machine learning classifiers", "myopic patients", "machine learning", "NTG patients", "extra-trees model", "myopic NTG", "NTG", "normal-tension glaucoma", "learning classifiers", "young myopic", "extra-trees", "patients", "NTG progression", "myopic", "model", "progression", "learning"], "paper_title": "Machine learning classifiers-based prediction of normal-tension glaucoma progression in young myopic patients.", "last_updated": "2023/02/04"}, {"id": "0028542342", "domain": "Glaucoma (unspecified)", "model_name": "Kim et al.", "publication_date": "2017/05/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28542342/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "28542342", "task": "IWqQC1koJA", "abstract": "The study aimed to develop machine learning models that have strong prediction power and interpretability for diagnosis of glaucoma based on retinal nerve fiber layer (RNFL) thickness and visual field (VF). We collected various candidate features from the examination of retinal nerve fiber layer (RNFL) thickness and visual field (VF). We also developed synthesized features from original features. We then selected the best features proper for classification (diagnosis) through feature evaluation. We used 100 cases of data as a test dataset and 399 cases of data as a training and validation dataset. To develop the glaucoma prediction model, we considered four machine learning algorithms: C5.0, random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN). We repeatedly composed a learning model using the training dataset and evaluated it by using the validation dataset. Finally, we got the best learning model that produces the highest validation accuracy. We analyzed quality of the models using several measures. The random forest model shows best performance and C5.0, SVM, and KNN models show similar accuracy. In the random forest model, the classification accuracy is 0.98, sensitivity is 0.983, specificity is 0.975, and AUC is 0.979. The developed prediction models show high accuracy, sensitivity, specificity, and AUC in classifying among glaucoma and healthy eyes. It will be used for predicting glaucoma against unknown examination records. Clinicians may reference the prediction results and be able to make better decisions. We may combine multiple learning models to increase prediction accuracy. The C5.0 model includes decision rules for prediction. It can be used to explain the reasons for specific predictions.", "keywords": ["nerve fiber layer", "retinal nerve fiber", "fiber layer", "thickness and visual", "visual field", "nerve fiber", "strong prediction power", "retinal nerve", "RNFL", "study aimed", "power and interpretability", "model", "models", "learning", "prediction", "features", "random forest model", "learning models", "accuracy", "dataset"], "paper_title": "Development of machine learning models for diagnosis of glaucoma.", "last_updated": "2023/02/04"}, {"id": "0036748392", "domain": "Glaucoma (unspecified)", "model_name": "Asmarian et al.", "publication_date": "2023/02/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36748392/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36748392", "task": null, "abstract": "Patients with \u03b2-thalassemia major (\u03b2-TM) face a wide range of complications as a result of excess iron in vital organs, including the heart and liver. Our aim was to find the best predictive machine learning (ML) model for assessing heart and liver iron overload in patients with \u03b2-TM. Data from 624 \u03b2-TM patients were entered into three ML models using random forest (RF), gradient boost model (GBM), and logistic regression (LR). The data were classified and analyzed by R software. Four evaluation metrics of predictive performance were measured: sensitivity, specificity, accuracy, and area under the curve (AUC), operating characteristic curve. For heart iron overload, the LR had the highest predictive performance based on AUC: 0.68 [95% CI (95% confidence interval): 0.60, 0.75]. The GBM also had the highest specificity (69.0%) and accuracy (67.0%). Most sensitivity is also acquired with LR (75.0%). For liver iron overload, the highest performance based on AUC was observed with RF, AUC: 0.68 (95% CI: 0.59, 0.76). The RF showed the highest accuracy (66.0%) and specificity (66.0%), while the LR had the highest sensitivity (84.0%). Ferritin, duration of transfusion, and age were determined as the most effective predictors of iron overload in both heart and liver. Logistic regression LR was determined to be the strongest method to predict cardiac and RF values for liver iron overload in patients with \u03b2-TM. Older thalassemia patients with a high serum ferritin (SF) level and a longer duration of transfusion therapy were more prone to heart and liver iron overload.", "keywords": ["liver iron overload", "iron overload", "liver iron", "\u03b2-thalassemia major", "face a wide", "vital organs", "iron", "wide range", "range of complications", "result of excess", "overload", "liver", "Patients", "AUC", "\u03b2-TM", "heart iron overload", "heart and liver", "heart", "highest", "excess iron"], "paper_title": "Prediction of Heart and Liver Iron Overload in \u03b2-Thalassemia Major Patients Using Machine Learning Methods.", "last_updated": "2023/02/04"}, {"id": "0031283476", "domain": "Glaucoma (unspecified)", "model_name": "Li et al.", "publication_date": "2019/07/08", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31283476/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31283476", "task": "IWqQC1koJA", "abstract": "Glaucoma is one of the leading causes of irreversible vision loss. Many approaches have recently been proposed for automatic glaucoma detection based on fundus images. However, none of the existing approaches can efficiently remove high redundancy in fundus images for glaucoma detection, which may reduce the reliability and accuracy of glaucoma detection. To avoid this disadvantage, this paper proposes an attention-based convolutional neural network (CNN) for glaucoma detection, called AG-CNN. Specifically, we first establish a large-scale attention-based glaucoma (LAG) database, which includes 11 760 fundus images labeled as either positive glaucoma (4878) or negative glaucoma (6882). Among the 11 760 fundus images, the attention maps of 5824 images are further obtained from ophthalmologists through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet, and a glaucoma classification subnet. The attention maps are predicted in the attention prediction subnet to highlight the salient regions for glaucoma detection, under a weakly supervised training manner. In contrast to other attention-based CNN methods, the features are also visualized as the localized pathological area, which are further added in our AG-CNN structure to enhance the glaucoma detection performance. Finally, the experiment results from testing over our LAG database and another public glaucoma database show that the proposed AG-CNN approach significantly advances the state-of-the-art in glaucoma detection.", "keywords": ["irreversible vision loss", "glaucoma detection", "Glaucoma", "fundus images", "vision loss", "irreversible vision", "detection", "images", "fundus", "attention", "automatic glaucoma detection", "glaucoma detection based", "AG-CNN", "subnet", "attention prediction subnet", "attention prediction", "attention-based", "CNN", "attention maps", "LAG"], "paper_title": "A Large-Scale Database and a CNN Model for Attention-Based Glaucoma Detection.", "last_updated": "2023/02/04"}, {"id": "0030871677", "domain": "Glaucoma (unspecified)", "model_name": "Lee et al.", "publication_date": "2019/02/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/30871677/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "30871677", "task": "IWqQC1koJA", "abstract": "Visual field testing via standard automated perimetry (SAP) is a commonly used glaucoma diagnosis method. Applying machine learning techniques to the visual field test results, a valid clinical diagnosis of glaucoma solely based on the SAP data is provided. In order to reflect structural-functional patterns of glaucoma on the automated diagnostic models, we propose composite variables derived from anatomically grouped visual field clusters to improve the prediction performance. A set of machine learning-based diagnostic models are designed that implement different input data manipulation, dimensionality reduction, and classification methods. Visual field testing data of 375 healthy and 257 glaucomatous eyes were used to build the diagnostic models. Three kinds of composite variables derived from the Garway-Heath map and the glaucoma hemifield test (GHT) sector map were included in the input variables in addition to the 52 SAP visual filed locations. Dimensionality reduction was conducted to select important variables so as to alleviate high-dimensionality problems. To validate the proposed methods, we applied four classifiers-linear discriminant analysis, na\u00efve Bayes classifier, support vector machines, and artificial neural networks-and four dimensionality reduction methods-Pearson correlation coefficient-based variable selection, Markov blanket variable selection, the minimum redundancy maximum relevance algorithm, and principal component analysis- and compared their classification performances. For all tested combinations, the classification performance improved when the proposed composite variables and dimensionality reduction techniques were implemented. The combination of total deviation values, the GHT sector map, support vector machines, and Markov blanket variable selection obtains the best performance: an area under the receiver operating characteristic curve (AUC) of 0.912. A glaucoma diagnosis model giving an AUC of 0.912 was constructed by applying machine learning techniques to SAP data. The results show that dimensionality reduction not only reduces dimensions of the input space but also enhances the classification performance. The variable selection results show that the proposed composite variables from visual field clustering play a key role in the diagnosis model.", "keywords": ["Visual field", "standard automated perimetry", "Visual field testing", "dimensionality reduction", "composite variables", "SAP data", "variable selection", "SAP", "Visual", "field", "Markov blanket variable", "composite variables derived", "proposed composite variables", "variables", "diagnostic models", "visual field test", "glaucoma", "field testing", "SAP visual filed", "field testing data"], "paper_title": "Machine learning models based on the dimensionality reduction of standard automated perimetry data for glaucoma diagnosis.", "last_updated": "2023/02/04"}, {"id": "0024795333", "domain": "Glaucoma (unspecified)", "model_name": "Hirasawa et al.", "publication_date": "2014/05/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24795333/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "24795333", "task": null, "abstract": "We investigated whether it was useful to use machine learning algorithms to predict patients' vision related quality of life (VRQoL) from visual field (VF) and visual acuity (VA). VRQoL was surveyed in 164 glaucomatous patients using the Sumi questionnaire. Their VRQoL score was predicted using machine learning algorithms (Random Forest, gradient boosting, support vector machine) based on total deviation (TD) values from integrated VF (IVF), VA, age and gender. For comparison, VRQoL score was predicted using standard linear regression with mean of IVF, TD values, and VA, and also the stepwise model selection by Akaike Information Criterion. Prediction error was calculated as root mean of the squared prediction error (RMSE) associated with the leave one out cross validation. RMSEs associated with general VRQoL score were smaller for the machine learning algorithms (1.99 to 2.21) compared with the standard linear model and the stepwise model selection (2.35 to 3.20). A similar tendency was found in each individual VRQoL task score. We found that it was advantageous to use machine learning methods to predict VRQoL accurately. These statistical methods could be used to help clinicians better understand patients' VRQoL without the need for extra tests other than standard VA and VF.", "keywords": ["vision related quality", "machine learning algorithms", "visual field", "visual acuity", "patients' vision related", "quality of life", "machine learning", "Akaike Information Criterion", "vision related", "related quality", "learning algorithms", "VRQoL", "Random Forest", "machine", "learning", "visual", "VRQoL score", "score", "Sumi questionnaire", "Information Criterion"], "paper_title": "Evaluation of various machine learning methods to predict vision-related quality of life from visual field data and visual acuity in patients with glaucoma.", "last_updated": "2023/02/04"}, {"id": "0036631567", "domain": "Glaucoma (unspecified)", "model_name": "Li et al.", "publication_date": "2023/01/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36631567/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36631567", "task": "IWqQC1koJA", "abstract": "Studies using machine learning (ML) approaches have reported high diagnostic accuracies for glaucoma detection. However, none assessed model performance across ethnicities. The aim of the study is to externally validate ML models for glaucoma detection from optical coherence tomography (OCT) data. We performed a prospective, cross-sectional study, where 514 Asians (257 glaucoma/257 controls) were enrolled to construct ML models for glaucoma detection, which was then tested on 356 Asians (183 glaucoma/173 controls) and 138 Caucasians (57 glaucoma/81 controls). We used the retinal nerve fibre layer (RNFL) thickness values produced by the compensation model, which is a multiple regression model fitted on healthy subjects that corrects the RNFL profile for anatomical factors and the original OCT data (measured) to build two classifiers, respectively. Both the ML models (area under the receiver operating [AUC]\u2009=\u20090.96 and accuracy\u2009=\u200992%) outperformed the measured data (AUC\u2009=\u20090.93; P\u2009<\u20090.001) for glaucoma detection in the Asian dataset. However, in the Caucasian dataset, the ML model trained with compensated data (AUC\u2009=\u20090.93 and accuracy\u2009=\u200984%) outperformed the ML model trained with original data (AUC\u2009=\u20090.83 and accuracy\u2009=\u200979%; P\u2009<\u20090.001) and measured data (AUC\u2009=\u20090.82; P\u2009<\u20090.001) for glaucoma detection. The performance with the ML model trained on measured data showed poor reproducibility across different datasets, whereas the performance of the compensated data was maintained. Care must be taken when ML models are applied to patient cohorts of different ethnicities.", "keywords": ["reported high diagnostic", "high diagnostic accuracies", "glaucoma detection", "glaucoma", "AUC", "Studies using machine", "machine learning", "approaches have reported", "reported high", "high diagnostic", "diagnostic accuracies", "model", "data", "model trained", "detection", "measured data", "models", "accuracies for glaucoma", "measured", "original OCT data"], "paper_title": "Assessing the external validity of machine learning-based detection of glaucoma.", "last_updated": "2023/02/04"}, {"id": "0034143145", "domain": "Glaucoma (unspecified)", "model_name": "Singh et al.", "publication_date": "2021/12/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34143145/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34143145", "task": "IWqQC1koJA", "abstract": "We propose a new method for training convolutional neural networks (CNNs) and use it to classify glaucoma from fundus images. This method integrates reinforcement learning along with supervised learning and uses it for transfer learning. The training method uses hill climbing techniques via two different climber types, namely, \"random movement\" and \"random detection,\" integrated with a supervised learning model through a stochastic gradient descent with momentum model. The model was trained and tested using the Drishti-GS and RIM-ONE-r2 datasets having glaucomatous and normal fundus images. The performance for prediction was tested by transfer learning on five CNN architectures, namely, GoogLeNet, DenseNet-201, NASNet, VGG-19, and Inception-Resnet v2. A five-fold classification was used for evaluating the performance, and high sensitivities while maintaining high accuracies were achieved. Of the models tested, the DenseNet-201 architecture performed the best in terms of sensitivity and area under the curve. This method of training allows transfer learning on small datasets and can be applied for tele-ophthalmology applications including training with local datasets.", "keywords": ["convolutional neural networks", "training convolutional neural", "neural networks", "convolutional neural", "classify glaucoma", "learning", "transfer learning", "fundus images", "supervised learning", "method", "training convolutional", "supervised learning model", "training", "normal fundus images", "method integrates reinforcement", "integrates reinforcement learning", "transfer", "model", "images", "tested"], "paper_title": "Rapid classification of glaucomatous fundus images.", "last_updated": "2023/02/04"}, {"id": "0027086033", "domain": "Glaucoma (unspecified)", "model_name": "SLO", "publication_date": "2016/04/16", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/27086033/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "27086033", "task": "IWqQC1koJA", "abstract": "Glaucoma is one of the leading causes of blindness worldwide. There is no cure for glaucoma but detection at its earliest stage and subsequent treatment can aid patients to prevent blindness. Currently, optic disc and retinal imaging facilitates glaucoma detection but this method requires manual post-imaging modifications that are time-consuming and subjective to image assessment by human observers. Therefore, it is necessary to automate this process. In this work, we have first proposed a novel computer aided approach for automatic glaucoma detection based on Regional Image Features Model (RIFM) which can automatically perform classification between normal and glaucoma images on the basis of regional information. Different from all the existing methods, our approach can extract both geometric (e.g. morphometric properties) and non-geometric based properties (e.g. pixel appearance/intensity values, texture) from images and significantly increase the classification performance. Our proposed approach consists of three new major contributions including automatic localisation of optic disc, automatic segmentation of disc, and classification between normal and glaucoma based on geometric and non-geometric properties of different regions of an image. We have compared our method with existing approaches and tested it on both fundus and Scanning laser ophthalmoscopy (SLO) images. The experimental results show that our proposed approach outperforms the state-of-the-art approaches using either geometric or non-geometric properties. The overall glaucoma classification accuracy for fundus images is 94.4% and accuracy of detection of suspicion of glaucoma in SLO images is 93.9 %.", "keywords": ["Glaucoma", "blindness worldwide", "Image Features Model", "images", "detection", "Regional Image Features", "glaucoma detection", "approach", "classification", "properties", "Features Model", "blindness", "image", "proposed", "non-geometric", "disc", "based", "proposed approach", "prevent blindness", "non-geometric properties"], "paper_title": "Regional Image Features Model for Automatic Classification between Normal and Glaucoma in Fundus and Scanning Laser Ophthalmoscopy (SLO) Images.", "last_updated": "2023/02/04"}, {"id": "0035658070", "domain": "Glaucoma (unspecified)", "model_name": "Huang et al.", "publication_date": "2022/06/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35658070/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35658070", "task": null, "abstract": "The objective of this study was to develop an objective and easy-to-use glaucoma staging system based on visual fields (VFs). A total of 13,231 VFs from 8077 subjects were used to develop models and 8024 VFs from 4445 subjects were used to validate models. We developed an unsupervised machine learning model to identify clusters with similar VF values. We annotated the clusters based on their respective mean deviation (MD). We computed optimal MD thresholds that discriminate clusters with the highest accuracy based on Bayes minimum error principle. We evaluated the accuracy of the staging system and validated findings based on an independent validation dataset. The unsupervised k -means algorithm discovered 4 clusters with 6784, 4034, 1541, and 872 VFs and average MDs of 0.0\u00a0dB (\u00b11.4: SD), -4.8\u00a0dB (\u00b11.9), -12.2\u00a0dB (\u00b12.9), and -23.0\u00a0dB (\u00b13.8), respectively. The supervised Bayes minimum error classifier identified optimal MD thresholds of -2.2, -8.0, and -17.3\u00a0dB for discriminating normal eyes and eyes at the early, moderate, and advanced stages of glaucoma. The accuracy of the glaucoma staging system was 94%, based on identified MD thresholds with respect to the initial k -means clusters. We discovered that 4 severity levels based on MD thresholds of -2.2, -8.0, and -17.3\u00a0dB, provides the optimal number of severity stages based on unsupervised and supervised machine learning. This glaucoma staging system is unbiased, objective, easy-to-use, and consistent, which makes it highly suitable for use in glaucoma research and for day-to-day clinical practice.", "keywords": ["visual fields", "based", "VFs", "glaucoma staging system", "staging system", "glaucoma staging", "clusters", "based on visual", "Bayes minimum error", "glaucoma", "staging", "Bayes minimum", "thresholds", "staging system based", "system", "develop models", "develop", "subjects", "accuracy", "Bayes"], "paper_title": "An Objective and Easy-to-Use Glaucoma Functional Severity Staging System Based on Artificial Intelligence.", "last_updated": "2023/02/04"}, {"id": "0035415417", "domain": "Glaucoma (unspecified)", "model_name": "Abidi et al.", "publication_date": "2018/06/20", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35415417/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35415417", "task": null, "abstract": "Ocular imaging instruments, such as Confocal Scanning Laser Ophthalmoscopy (CSLO), captures high-quality images of the optic disc (also known as optic nerve head) that help clinicians to diagnose glaucoma. We present an integrated data analytics framework to aid clinicians in interpreting CSLO optic nerve images to diagnose and monitor the progression of glaucoma. To distinguish between healthy and glaucomatous optic discs, our framework derives shape information from CSLO images using image processing (Zernike moment method), selects salient features (hybrid feature selection), and then trains image classifiers (Multilayer Perceptron, Support Vector Machine, Bayesian Network). To monitor glaucoma progression over time, our framework uses a mathematical model of the optic disc to extract morphological features from CSLO images and applies clustering (Self-Organizing Maps) to visualize subtypes of glaucomatous optic disc damage. We contend that our data analytics framework offers an automated and objective analysis of optic nerve images that can potentially support both diagnosis and monitoring of glaucoma. We validated our framework with CSLO optic nerve images and our data analytics approach detected glaucomatous optic discs with a sensitivity of 0.86, a specificity of 0.80, an accuracy of 0.838, and an AUROC of 0.913 with a Bayesian network classifier using the optimal subset of Zernike features (six moments). Furthermore, our framework identified, using morphological features, five clusters of CSLO images, where each cluster stands for a subtype of optic nerve damage (two healthy subtypes and three glaucoma subtypes). The characteristics of each cluster-the subtype of the image-were determined by experts who examined the morphology of the images within each cluster and provided subtype characteristics to each cluster.", "keywords": ["Scanning Laser Ophthalmoscopy", "Confocal Scanning Laser", "Laser Ophthalmoscopy", "Confocal Scanning", "Scanning Laser", "CSLO optic nerve", "Ocular imaging instruments", "optic nerve images", "optic nerve", "optic nerve head", "captures high-quality images", "CSLO optic", "CSLO images", "glaucomatous optic discs", "optic", "interpreting CSLO optic", "CSLO", "optic disc", "Support Vector Machine", "glaucomatous optic"], "paper_title": "A Data Mining Framework for Glaucoma Decision Support Based on Optic Nerve Image Analysis Using Machine Learning Methods.", "last_updated": "2023/02/04"}, {"id": "0035204482", "domain": "Glaucoma (unspecified)", "model_name": "Wu et al.", "publication_date": "2022/02/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35204482/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35204482", "task": "IWqQC1koJA", "abstract": "Spectralis optical coherence tomography (OCT) provided more detailed parameters in the peripapillary and macular areas among the OCT machines, but it is not easy to understand the enormous information (114 features) generated from Spectralis OCT in glaucoma assessment. Machine learning methodology has been well-applied in glaucoma detection in recent years and has the ability to process a large amount of information at once. Here we aimed to analyze the diagnostic capability of Spectralis OCT parameters on glaucoma detection using Support Vector Machine (SVM) classification method in our population. Our results showed that applying all OCT features with the SVM method had good capability in the detection of glaucomatous eyes (area under curve (AUC) = 0.82), as well as discriminating normal eyes from early, moderate, or severe glaucomatous eyes (AUC = 0.78, 0.89, and 0.93, respectively). Apart from using all OCT features, the minimum rim width (MRW) may be good feature groups to discriminate early glaucomatous from normal eyes (AUC = 0.78). The combination of peripapillary and macular parameters, including MRW_temporal inferior (TI), MRW_global (G), ganglion cell layer (GCL)_outer temporal (T2), GCL_inner inferior (I1), peripapillary nerve fiber layer thickness (ppNFLT)_temporal superior (TS), and GCL_inner temporal (T1), provided better results (AUC = 0.84). This study showed promise in glaucoma management in the Taiwanese population. However, further validation study is needed to test the performance of our proposed model in the real world.", "keywords": ["optical coherence tomography", "Spectralis optical coherence", "Spectralis OCT parameters", "Spectralis OCT", "Support Vector Machine", "OCT", "coherence tomography", "AUC", "Spectralis optical", "optical coherence", "easy to understand", "understand the enormous", "OCT features", "OCT machines", "glaucoma assessment", "enormous information", "glaucoma detection", "Spectralis", "glaucoma", "Vector Machine"], "paper_title": "Glaucoma Detection Using Support Vector Machine Method Based on Spectralis OCT.", "last_updated": "2023/02/04"}, {"id": "0027590198", "domain": "Glaucoma (unspecified)", "model_name": "Zilly et al.", "publication_date": "2016/08/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/27590198/", "code_link": null, "model_type": "CNN", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "27590198", "task": "aipbNdPTIt", "abstract": "We present a novel method to segment retinal images using ensemble learning based convolutional neural network (CNN) architectures. An entropy sampling technique is used to select informative points thus reducing computational complexity while performing superior to uniform sampling. The sampled points are used to design a novel learning framework for convolutional filters based on boosting. Filters are learned in several layers with the output of previous layers serving as the input to the next layer. A softmax logistic classifier is subsequently trained on the output of all learned filters and applied on test images. The output of the classifier is subject to an unsupervised graph cut algorithm followed by a convex hull transformation to obtain the final segmentation. Our proposed algorithm for optic cup and disc segmentation outperforms existing methods on the public DRISHTI-GS data set on several metrics.", "keywords": ["convolutional neural network", "segment retinal images", "ensemble learning based", "based convolutional neural", "neural network", "segment retinal", "learning based convolutional", "CNN", "ensemble learning", "convolutional neural", "convolutional filters based", "retinal images", "entropy sampling technique", "select informative points", "output", "reducing computational complexity", "architectures", "learning based", "based convolutional", "network"], "paper_title": "Glaucoma detection using entropy sampling and ensemble learning for automatic optic cup and disc segmentation.", "last_updated": "2023/02/04"}, {"id": "0029218460", "domain": "Glaucoma (unspecified)", "model_name": "Haleem et al.", "publication_date": "2017/12/07", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/29218460/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "29218460", "task": "aipbNdPTIt", "abstract": "This paper proposes a novel Adaptive Region-based Edge Smoothing Model (ARESM) for automatic boundary detection of optic disc and cup to aid automatic glaucoma diagnosis. The novelty of our approach consists of two aspects: 1) automatic detection of initial optimum object boundary based on a Region Classification Model (RCM) in a pixel-level multidimensional feature space; 2) an Adaptive Edge Smoothing Update model (AESU) of contour points (e.g. misclassified or irregular points) based on iterative force field calculations with contours obtained from the RCM by minimising energy function (an approach that does not require predefined geometric templates to guide auto-segmentation). Such an approach provides robustness in capturing a range of variations and shapes. We have conducted a comprehensive comparison between our approach and the state-of-the-art existing deformable models and validated it with publicly available datasets. The experimental evaluation shows that the proposed approach significantly outperforms existing methods. The generality of the proposed approach will enable segmentation and detection of other object boundaries and provide added value in the field of medical image processing and analysis.", "keywords": ["Region-based Edge Smoothing", "Adaptive Region-based Edge", "Adaptive Edge Smoothing", "Edge Smoothing Update", "Edge Smoothing Model", "Smoothing Update model", "Edge Smoothing", "automatic glaucoma diagnosis", "Region-based Edge", "aid automatic glaucoma", "Adaptive Region-based", "Region Classification Model", "Adaptive Edge", "automatic boundary detection", "Smoothing Update", "glaucoma diagnosis", "Smoothing Model", "paper proposes", "optic disc", "disc and cup"], "paper_title": "A Novel Adaptive Deformable Model for Automated Optic Disc and Cup Segmentation to Aid Glaucoma Diagnosis.", "last_updated": "2023/02/04"}, {"id": "0033913369", "domain": "Glaucoma (unspecified)", "model_name": "Omar et al.", "publication_date": "2021/08/10", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33913369/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33913369", "task": "IWqQC1koJA", "abstract": "Glaucoma is a serious eye disease characterized by dysfunction and loss of retinal ganglion cells (RGCs) which can eventually lead to loss of vision. Robust mass screening may help to extend the symptom-free life for the affected patients. The retinal optic nerve fiber layer can be assessed using optical coherence tomography, scanning laser polarimetry (SLP), and Heidelberg Retina Tomography (HRT) scanning methods which, unfortunately, are expensive methods and hence, a novel automated glaucoma diagnosis system is needed. This paper proposes a new model for mass screening that aims to decrease the false negative rate (FNR). The model is based on applying nine different machine learning techniques in a majority voting model. The top five techniques that provide the highest accuracy will be used to build a consensus ensemble to make the final decision. The results from applying both models on a dataset with 499 records show a decrease in the accuracy rate from 90% to 83% and a decrease in false negative rate (FNR) from 8% to 0% for majority voting and consensus model, respectively. These results indicate that the proposed model can reduce FNR dramatically while maintaining a reasonable overall accuracy which makes it suitable for mass screening.", "keywords": ["eye disease characterized", "retinal ganglion cells", "loss of vision", "Heidelberg Retina Tomography", "ganglion cells", "eye disease", "disease characterized", "characterized by dysfunction", "eventually lead", "mass screening", "dysfunction and loss", "lead to loss", "loss", "retinal ganglion", "Robust mass screening", "Heidelberg Retina", "Retina Tomography", "loss of retinal", "model", "false negative rate"], "paper_title": "GLAUDIA: A predicative system for glaucoma diagnosis in mass scanning.", "last_updated": "2023/02/04"}, {"id": "0032750922", "domain": "Glaucoma (unspecified)", "model_name": "Chesley et al.", "publication_date": "2020/12/04", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32750922/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32750922", "task": "IWqQC1koJA", "abstract": "The gold standard clinical tool for evaluating visual dysfunction in cases of glaucoma and other disorders of vision remains the visual field or threshold perimetry exam. Administration of this exam has evolved over the years into a sophisticated, standardized, automated algorithm that relies heavily on specifics of disease processes particular to common retinal disorders. The purpose of this study is to evaluate the utility of a novel general estimator applied to visual field testing. A multidimensional psychometric function estimation tool was applied to visual field estimation. This tool is built on semiparametric probabilistic classification rather than multiple logistic regression. It combines the flexibility of nonparametric estimators and the efficiency of parametric estimators. Simulated visual fields were generated from human patients with a variety of diagnoses, and the errors between simulated ground truth and estimated visual fields were quantified. Error rates of the estimates were low, typically within 2\u00a0dB units of ground truth on average. The greatest threshold errors appeared to be confined to the portions of the threshold function with the highest spatial frequencies. This method can accurately estimate a variety of visual field profiles with continuous threshold estimates, potentially using a relatively small number of stimuli.", "keywords": ["gold standard clinical", "evaluating visual dysfunction", "standard clinical tool", "visual field", "threshold perimetry exam", "gold standard", "standard clinical", "dysfunction in cases", "cases of glaucoma", "vision remains", "visual", "perimetry exam", "visual field estimation", "evaluating visual", "visual dysfunction", "visual field testing", "Simulated visual fields", "field", "common retinal disorders", "estimated visual fields"], "paper_title": "Visual Field Estimation by Probabilistic Classification.", "last_updated": "2023/02/04"}, {"id": "0034459860", "domain": "Glaucoma (unspecified)", "model_name": "Doshi et al.", "publication_date": "2021/10/14", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/34459860/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "34459860", "task": null, "abstract": "Archetypal analysis, a form of unsupervised machine learning, identifies archetypal patterns within a visual field (VF) dataset such that any VF is described as a weighted sum of its archetypes (ATs) and has been used to quantify VF defects in glaucoma. We applied archetypal analysis to VFs affected by nonglaucomatous optic neuropathy caused by idiopathic intracranial hypertension (IIH). We created an AT model from 2862 VFs prospectively collected from 330 eyes in the IIH Treatment Trial (IIHTT). We compared baseline IIH AT patterns with their descriptive VF classifications from the IIHTT. The optimum IIH AT model yielded 14 ATs resembling VF patterns reported in the IIHTT. Baseline VFs contained four or fewer meaningful ATs in 147 (89%) of study eyes. AT2 (mild general VF depression pattern) demonstrated the greatest number of study eyes with meaningful AT weight at baseline (n = 114), followed by AT1 (n = 91). Other ATs captured patterns of blind spot enlargement, hemianopia, arcuate, nasal defects, and more nonspecific patterns of general VF depression. Of all ATs, AT1 (normal pattern) had the strongest correlation with mean deviation (r = 0.69, P < 0.001). For 65 of the 93 VFs with a dominant AT, this AT matched the expert classification. Archetypal analysis identifies quantifiable, archetypal VF defects that resemble those commonly seen in IIH. Archetypal analysis provides a quantitative, objective method of measuring and monitoring disease-specific regional VF defects in IIH.", "keywords": ["unsupervised machine learning", "IIH Treatment Trial", "IIH", "machine learning", "visual field", "form of unsupervised", "unsupervised machine", "weighted sum", "IIHTT", "IIH Treatment", "Treatment Trial", "Archetypal", "Archetypal analysis", "patterns", "ATs", "analysis", "defects", "VFs", "eyes", "baseline IIH"], "paper_title": "Unsupervised Machine Learning Identifies Quantifiable Patterns of Visual Field Loss in Idiopathic Intracranial Hypertension.", "last_updated": "2023/02/04"}, {"id": "0032672624", "domain": "Glaucoma (unspecified)", "model_name": "Odaibo et al.", "publication_date": "2021/02/17", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/32672624/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "32672624", "task": "IWqQC1koJA", "abstract": "", "keywords": [], "paper_title": "Re: Wang et\u00a0al.: Machine learning models for diagnosing glaucoma from retinal nerve fiber layer thickness maps (Ophthalmology Glaucoma. 2019;2:422-428).", "last_updated": "2023/02/04"}, {"id": "0035346443", "domain": "Glaucoma (unspecified)", "model_name": "Xiong et al.", "publication_date": "2022/02/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35346443/", "code_link": null, "model_type": null, "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35346443", "task": "aipbNdPTIt", "abstract": "Fundus images have been widely used in routine examinations of ophthalmic diseases. For some diseases, the pathological changes mainly occur around the optic disc area; therefore, detection and segmentation of the optic disc are critical pre-processing steps in fundus image analysis. Current machine learning based optic disc segmentation methods typically require manual segmentation of the optic disc for the supervised training. However, it is time consuming to annotate pixel-level optic disc masks and inevitably induces inter-subject variance. To address these limitations, we propose a weak label based Bayesian U-Net exploiting Hough transform based annotations to segment optic discs in fundus images. To achieve this, we build a probabilistic graphical model and explore a Bayesian approach with the state-of-the-art U-Net framework. To optimize the model, the expectation-maximization algorithm is used to estimate the optic disc mask and update the weights of the Bayesian U-Net, alternately. Our evaluation demonstrates strong performance of the proposed method compared to both fully- and weakly-supervised baselines.", "keywords": ["optic disc", "Fundus images", "fundus image analysis", "optic disc segmentation", "optic", "routine examinations", "examinations of ophthalmic", "optic disc area", "ophthalmic diseases", "based optic disc", "optic disc masks", "disc", "pixel-level optic disc", "segment optic discs", "Fundus", "Bayesian U-Net", "disc segmentation methods", "Bayesian", "based Bayesian U-Net", "learning based optic"], "paper_title": "Weak label based Bayesian U-Net for optic disc segmentation in fundus images.", "last_updated": "2023/02/04"}, {"id": "0033514397", "domain": "Glaucoma (unspecified)", "model_name": "Orlenko et al.", "publication_date": "2021/01/29", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33514397/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33514397", "task": null, "abstract": "Non-additive interactions among genes are frequently associated with a number of phenotypes, including known complex diseases such as Alzheimer's, diabetes, and cardiovascular disease. Detecting interactions requires careful selection of analytical methods, and some machine learning algorithms are unable or underpowered to detect or model feature interactions that exhibit non-additivity. The Random Forest method is often employed in these efforts due to its ability to detect and model non-additive interactions. In addition, Random Forest has the built-in ability to estimate feature importance scores, a characteristic that allows the model to be interpreted with the order and effect size of the feature association with the outcome. This characteristic is very important for epidemiological and clinical studies where results of predictive modeling could be used to define the future direction of the research efforts. An alternative way to interpret the model is with a permutation feature importance metric which employs a permutation approach to calculate a feature contribution coefficient in units of the decrease in the model's performance and with the Shapely additive explanations which employ cooperative game theory approach. Currently, it is unclear which Random Forest feature importance metric provides a superior estimation of the true informative contribution of features in genetic association analysis. To address this issue, and to improve interpretability of Random Forest predictions, we compared different methods for feature importance estimation in real and simulated datasets with non-additive interactions. As a result, we detected a discrepancy between the metrics for the real-world datasets and further established that the permutation feature importance metric provides more precise feature importance rank estimation for the simulated datasets with non-additive interactions. By analyzing both real and simulated data, we established that the permutation feature importance metric provides more precise feature importance rank estimation in the presence of non-additive interactions.", "keywords": ["Random Forest", "feature importance", "feature importance metric", "Random Forest feature", "permutation feature importance", "Random Forest method", "Forest feature importance", "feature", "Non-additive interactions", "importance metric", "importance", "complex diseases", "cardiovascular disease", "number of phenotypes", "including known complex", "precise feature importance", "feature importance rank", "genes are frequently", "Random Forest predictions", "interactions"], "paper_title": "A comparison of methods for interpreting random forest models of genetic association in the presence of non-additive interactions.", "last_updated": "2023/02/04"}, {"id": "0035808338", "domain": "Glaucoma (unspecified)", "model_name": "Berenguer-Vidal et al.", "publication_date": "2022/06/27", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35808338/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35808338", "task": null, "abstract": "<b>Purpose</b>: The aim of this study was to analyze the relevance of asymmetry features between both eyes of the same patient for glaucoma screening using optical coherence tomography. <b>Methods</b>: Spectral-domain optical coherence tomography was used to estimate the thickness of the peripapillary retinal nerve fiber layer in both eyes of the patients in the study. These measurements were collected in a dataset from healthy and glaucoma patients. Several metrics for asymmetry in the retinal nerve fiber layer thickness between the two eyes were then proposed. These metrics were evaluated using the dataset by performing a statistical analysis to assess their significance as relevant features in the diagnosis of glaucoma. Finally, the usefulness of these asymmetry features was demonstrated by designing supervised machine learning models that can be used for the early diagnosis of glaucoma. <b>Results</b>: Machine learning models were designed and optimized, specifically decision trees, based on the values of proposed asymmetry metrics. The use of these models on the dataset provided good classification of the patients (accuracy 88%, sensitivity 70%, specificity 93% and precision 75%). <b>Conclusions</b>: The obtained machine learning models based on retinal nerve fiber layer asymmetry are simple but effective methods which offer a good trade-off in classification of patients and simplicity. The fast binary classification relies on a few asymmetry values of the retinal nerve fiber layer thickness, allowing their use in the daily clinical practice for glaucoma screening.", "keywords": ["optical coherence tomography", "Spectral-domain optical coherence", "retinal nerve fiber", "nerve fiber layer", "optical coherence", "coherence tomography", "nerve fiber", "machine learning models", "retinal nerve", "fiber layer", "fiber layer thickness", "analyze the relevance", "Spectral-domain optical", "machine learning", "Purpose", "asymmetry", "glaucoma", "learning models", "fiber layer asymmetry", "patients"], "paper_title": "Decision Trees for Glaucoma Screening Based on the Asymmetry of the Retinal Nerve Fiber Layer in Optical Coherence Tomography.", "last_updated": "2023/02/04"}, {"id": "0033310194", "domain": "Glaucoma (unspecified)", "model_name": "Choi et al.", "publication_date": "2020/12/11", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/33310194/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "33310194", "task": null, "abstract": "To model the global test-retest variability of visual fields (VFs) in glaucoma. Retrospective cohort study. Test-retest VFs from 4044 eyes of 4044 participants. We selected 2 reliable VFs per eye measured with the Humphrey Field Analyzer (Swedish interactive threshold algorithm 24-2) within 30 days of each other. Each VF had fixation losses (FLs) of 33% or less, false-negative results (FNRs) of 20% or less, and false-positive results (FPRs) of 20% or less. Stepwise linear regression was applied to select the model best predicting the global test-retest variability from 3 categories of features of the first VF: (1) base parameters (age, mean deviation, pattern standard deviation, glaucoma hemifield test results, FPR, FNR, and FL); (2) total deviation (TD) at each location; and (3) computationally derived archetype VF loss patterns. The global test-retest variability was defined as root mean square deviation (RMSD) of TD values at all 52 VF locations. Archetype models to predict the global test-retest variability. The mean \u00b1 standard deviation of the root mean square deviation was 4.39 \u00b1 2.55 dB. Between the 2 VF tests, TD values were correlated more strongly in central than in peripheral VF locations (intraclass coefficient, 0.66-0.89; P < 0.001). Compared with the model using base parameters alone (adjusted R<sup>2</sup>\u00a0= 0.45), adding TD values improved prediction accuracy of the global variability (adjusted R<sup>2</sup>\u00a0= 0.53; P < 0.001; Bayesian information criterion [BIC] decrease of 527; change of >6 represents strong improvement). Lower TD sensitivity in the outermost peripheral VF locations was predictive of higher global variability. Adding archetypes to the base model improved model performance with an adjusted R<sup>2</sup> of 0.53 (P < 0.001) and lowering of BIC by 583. Greater variability was associated with concentric peripheral defect, temporal hemianopia, inferotemporal defect, near total loss, superior peripheral defect, and central scotoma (listed in order of decreasing statistical significance), and less normal VF results and superior paracentral defect. Inclusion of archetype VF loss patterns and TD values based on first VF improved the prediction of the global test-retest variability than using traditional global VF indices alone.", "keywords": ["global test-retest variability", "global test-retest", "test-retest variability", "Humphrey Field Analyzer", "global", "variability", "test-retest", "visual fields", "global variability", "deviation", "model", "Field Analyzer", "Humphrey Field", "results", "defect", "peripheral", "VFs", "Test-retest VFs", "locations", "higher global variability"], "paper_title": "Predicting Global Test-Retest Variability of Visual Fields in Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0025940856", "domain": "Glaucoma (unspecified)", "model_name": "Belghith et al.", "publication_date": "2015/04/23", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/25940856/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "25940856", "task": "0GPcU42tzV", "abstract": "Glaucoma is a chronic neurodegenerative disease characterized by loss of retinal ganglion cells, resulting in distinctive changes in the optic nerve head (ONH) and retinal nerve fiber layer. Important advances in technology for non-invasive imaging of the eye have been made providing quantitative tools to measure structural changes in ONH topography, a crucial step in diagnosing and monitoring glaucoma. Three dimensional (3D) spectral domain optical coherence tomography (SD-OCT), an optical imaging technique, is now the standard of care for diagnosing and monitoring progression of numerous eye diseases. This paper aims to detect changes in multi-temporal 3D SD-OCT ONH images using a hierarchical fully Bayesian framework and then to differentiate between changes reflecting random variations or true changes due to glaucoma progression. To this end, we propose the use of kernel-based support vector data description (SVDD) classifier. SVDD is a well-known one-class classifier that allows us to map the data into a high-dimensional feature space where a hypersphere encloses most patterns belonging to the target class. The proposed glaucoma progression detection scheme using the whole 3D SD-OCT images detected glaucoma progression in a significant number of cases showing progression by conventional methods (78%), with high specificity in normal and non-progressing eyes (93% and 94% respectively). The use of the dependency measurement in the SVDD framework increased the robustness of the proposed change-detection scheme with comparison to the classical support vector machine and SVDD methods. The validation using clinical data of the proposed approach has shown that the use of only healthy and non-progressing eyes to train the algorithm led to a high diagnostic accuracy for detecting glaucoma progression compared to other methods.", "keywords": ["retinal nerve fiber", "retinal ganglion cells", "optic nerve head", "nerve fiber layer", "chronic neurodegenerative disease", "neurodegenerative disease characterized", "retinal nerve", "nerve head", "retinal ganglion", "optic nerve", "nerve fiber", "glaucoma progression", "ganglion cells", "resulting in distinctive", "fiber layer", "SD-OCT ONH images", "chronic neurodegenerative", "characterized by loss", "Glaucoma", "ONH"], "paper_title": "Learning from healthy and stable eyes: A new approach for detection of glaucomatous progression.", "last_updated": "2023/02/04"}, {"id": "0031359230", "domain": "Glaucoma (unspecified)", "model_name": "Raghavendra et al.", "publication_date": "2019/07/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31359230/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31359230", "task": null, "abstract": "Glaucoma is a type of eye condition which may result in partial or consummate vision loss. Higher intraocular pressure is the leading cause for this condition. Screening for glaucoma and early detection can avert vision loss. Computer aided diagnosis (CAD) is an automated process with the potential to identify glaucoma early through quantitative analysis of digital fundus images. Preparing an effective model for CAD requires a large database. This study presents a CAD tool for the precise detection of glaucoma using a machine learning approach. An autoencoder is trained to determine effective and important features from fundus images. These features are used to develop classes of glaucoma for testing. The method achieved an F\u2009-\u2009measure value of 0.95 utilizing 1426 digital fundus images (589 control and 837 glaucoma). The efficacy of the system is evident, and is suggestive of its possible utility as an additional tool for verification of clinical decisions.", "keywords": ["consummate vision loss", "type of eye", "result in partial", "partial or consummate", "vision loss", "eye condition", "fundus images", "consummate vision", "avert vision loss", "digital fundus images", "Glaucoma", "CAD", "images", "loss", "condition", "fundus", "Higher intraocular pressure", "digital fundus", "vision", "type"], "paper_title": "A Two Layer Sparse Autoencoder for Glaucoma Identification with Fundus Images.", "last_updated": "2023/02/04"}, {"id": "0035629205", "domain": "Glaucoma (unspecified)", "model_name": "Huettenbrink et al.", "publication_date": "2022/05/12", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/35629205/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "35629205", "task": null, "abstract": "When scheduling surgeries for urolithiasis, the lack of information about the complexity of procedures and required instruments can lead to mismanagement, cancellations of elective surgeries and financial risk for the hospital. The aim of this study was to develop, train, and test prediction models for ureterorenoscopy. Routinely acquired Computer Tomography (CT) imaging data and patient data were used as data sources. Machine learning models were trained and tested to predict the need for laser lithotripsy and to forecast the expected duration of ureterorenoscopy on the bases of 474 patients over a period from May 2016 to December 2019. Negative predictive value for use of laser lithotripsy was 92%, and positive predictive value 91% before application of the reject option, increasing to 97% and 94% after application of the reject option. Similar results were found for duration of surgery at \u226430 min. This combined prediction is possible for 54% of patients. Factors influencing prediction of laser application and duration \u226430 min are age, sex, height, weight, Body Mass Index (BMI), stone size, stone volume, stone density, and presence of a ureteral stent. Neuronal networks for prediction help to identify patients with an operative time \u226430 min who did not require laser lithotripsy. Thus, surgical planning and resource allocation can be optimised to increase efficiency in the Operating Room (OR).", "keywords": ["scheduling surgeries", "elective surgeries", "lead to mismanagement", "cancellations of elective", "lack of information", "complexity of procedures", "procedures and required", "required instruments", "instruments can lead", "financial risk", "acquired Computer Tomography", "surgeries for urolithiasis", "surgeries and financial", "laser lithotripsy", "surgeries", "Computer Tomography", "Body Mass Index", "laser", "Routinely acquired Computer", "reject option"], "paper_title": "Neural Networks Modeling for Prediction of Required Resources for Personalized Endourologic Treatment of Urolithiasis.", "last_updated": "2023/02/04"}, {"id": "0024658239", "domain": "Glaucoma (unspecified)", "model_name": "Yousefi et al.", "publication_date": "2014/09/02", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24658239/", "code_link": null, "model_type": null, "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "24658239", "task": "0GPcU42tzV", "abstract": "Machine learning classifiers were employed to detect glaucomatous progression using longitudinal series of structural data extracted from retinal nerve fiber layer thickness measurements and visual functional data recorded from standard automated perimetry tests. Using the collected data, a longitudinal feature vector was created for each patient's eye by computing the norm 1 difference vector of the data at the baseline and at each follow-up visit. The longitudinal features from each patient's eye were then fed to the machine learning classifier to classify each eye as stable or progressed over time. This study was performed using several machine learning classifiers including Bayesian, Lazy, Meta, and Tree, composing different families. Combinations of structural and functional features were selected and ranked to determine the relative effectiveness of each feature. Finally, the outcomes of the classifiers were assessed by several performance metrics and the effectiveness of structural and functional features were analyzed.", "keywords": ["automated perimetry tests", "detect glaucomatous progression", "retinal nerve fiber", "nerve fiber layer", "fiber layer thickness", "layer thickness measurements", "standard automated perimetry", "Machine learning classifiers", "structural data extracted", "visual functional data", "functional data recorded", "perimetry tests", "Machine learning", "employed to detect", "detect glaucomatous", "glaucomatous progression", "extracted from retinal", "retinal nerve", "nerve fiber", "fiber layer"], "paper_title": "Glaucoma progression detection using structural retinal nerve fiber layer measurements and functional visual field points.", "last_updated": "2023/02/04"}, {"id": "0022786913", "domain": "Glaucoma (unspecified)", "model_name": "POP", "publication_date": "2012/09/25", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/22786913/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "22786913", "task": "0GPcU42tzV", "abstract": "We evaluated Progression of Patterns (POP) for its ability to identify progression of glaucomatous visual field (VF) defects. POP uses variational Bayesian independent component mixture model (VIM), a machine learning classifier (MLC) developed previously. VIM separated Swedish Interactive Thresholding Algorithm (SITA) VFs from a set of 2,085 normal and glaucomatous eyes into nine axes (VF patterns): seven glaucomatous. Stable glaucoma was simulated in a second set of 55 patient eyes with five VFs each, collected within four weeks. A third set of 628 eyes with 4,186 VFs (mean \u00b1 SD of 6.7 \u00b1 1.7 VFs over 4.0 \u00b1 1.4 years) was tested for progression. Tested eyes were placed into suspect and glaucoma categories at baseline, based on VFs and disk stereoscopic photographs; a subset of eyes had stereophotographic evidence of progressive glaucomatous optic neuropathy (PGON). Each sequence of fields was projected along seven VIM glaucoma axes. Linear regression (LR) slopes generated from projections onto each axis yielded a degree of confidence (DOC) that there was progression. At 95% specificity, progression cutoffs were established for POP, visual field index (VFI), and mean deviation (MD). Guided progression analysis (GPA) was also compared. POP identified a statistically similar number of eyes (P > 0.05) as progressing compared with VFI, MD, and GPA in suspects (3.8%, 2.7%, 5.6%, and 2.9%, respectively), and more eyes than GPA (P = 0.01) in glaucoma (16.0%, 15.3%, 12.0%, and 7.3%, respectively), and more eyes than GPA (P = 0.05) in PGON eyes (26.3%, 23.7%, 27.6%, and 14.5%, respectively). POP, with its display of DOC of progression and its identification of progressing VF defect pattern, adds to the information available to the clinician for detecting VF progression.", "keywords": ["Interactive Thresholding Algorithm", "Swedish Interactive Thresholding", "eyes", "ability to identify", "VIM separated Swedish", "Progression", "POP", "separated Swedish Interactive", "VIM", "variational Bayesian independent", "Bayesian independent component", "Thresholding Algorithm", "GPA", "Swedish Interactive", "Interactive Thresholding", "VIM glaucoma axes", "VFs", "glaucomatous", "evaluated Progression", "identify progression"], "paper_title": "Progression of patterns (POP): a machine classifier algorithm to identify glaucoma progression in visual fields.", "last_updated": "2023/02/04"}, {"id": "0028528295", "domain": "Glaucoma (unspecified)", "model_name": "Miri et al.", "publication_date": "2017/05/06", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/28528295/", "code_link": null, "model_type": "random forest", "verified": false, "model_task": "Segmentation", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "28528295", "task": "aipbNdPTIt", "abstract": "Bruch's membrane opening-minimum rim width (BMO-MRW) is a recently proposed structural parameter which estimates the remaining nerve fiber bundles in the retina and is superior to other conventional structural parameters for diagnosing glaucoma. Measuring this structural parameter requires identification of BMO locations within spectral domain-optical coherence tomography (SD-OCT) volumes. While most automated approaches for segmentation of the BMO either segment the 2D projection of BMO points or identify BMO points in individual B-scans, in this work, we propose a machine-learning graph-based approach for true 3D segmentation of BMO from glaucomatous SD-OCT volumes. The problem is formulated as an optimization problem for finding a 3D path within the SD-OCT volume. In particular, the SD-OCT volumes are transferred to the radial domain where the closed loop BMO points in the original volume form a path within the radial volume. The estimated location of BMO points in 3D are identified by finding the projected location of BMO points using a graph-theoretic approach and mapping the projected locations onto the Bruch's membrane (BM) surface. Dynamic programming is employed in order to find the 3D BMO locations as the minimum-cost path within the volume. In order to compute the cost function needed for finding the minimum-cost path, a random forest classifier is utilized to learn a BMO model, obtained by extracting intensity features from the volumes in the training set, and computing the required 3D cost function. The proposed method is tested on 44 glaucoma patients and evaluated using manual delineations. Results show that the proposed method successfully identifies the 3D BMO locations and has significantly smaller errors compared to the existing 3D BMO identification approaches.", "keywords": ["opening-minimum rim width", "conventional structural parameters", "BMO points", "remaining nerve fiber", "nerve fiber bundles", "BMO", "BMO locations", "structural parameter", "membrane opening-minimum rim", "structural parameter requires", "proposed structural parameter", "recently proposed structural", "identify BMO points", "Bruch membrane opening-minimum", "loop BMO points", "rim width", "conventional structural", "opening-minimum rim", "estimates the remaining", "remaining nerve"], "paper_title": "A machine-learning graph-based approach for 3D segmentation of Bruch's membrane opening from glaucomatous SD-OCT volumes.", "last_updated": "2023/02/04"}, {"id": "0024497932", "domain": "Glaucoma (unspecified)", "model_name": "FDT", "publication_date": "2014/01/30", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/24497932/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "24497932", "task": "IWqQC1koJA", "abstract": "The variational Bayesian independent component analysis-mixture model (VIM), an unsupervised machine-learning classifier, was used to automatically separate Matrix Frequency Doubling Technology (FDT) perimetry data into clusters of healthy and glaucomatous eyes, and to identify axes representing statistically independent patterns of defect in the glaucoma clusters. FDT measurements were obtained from 1,190 eyes with normal FDT results and 786 eyes with abnormal FDT results from the UCSD-based Diagnostic Innovations in Glaucoma Study (DIGS) and African Descent and Glaucoma Evaluation Study (ADAGES). For all eyes, VIM input was 52 threshold test points from the 24-2 test pattern, plus age. FDT mean deviation was -1.00 dB (S.D.\u200a=\u200a2.80 dB) and -5.57 dB (S.D.\u200a=\u200a5.09 dB) in FDT-normal eyes and FDT-abnormal eyes, respectively (p<0.001). VIM identified meaningful clusters of FDT data and positioned a set of statistically independent axes through the mean of each cluster. The optimal VIM model separated the FDT fields into 3 clusters. Cluster N contained primarily normal fields (1109/1190, specificity 93.1%) and clusters G1 and G2 combined, contained primarily abnormal fields (651/786, sensitivity 82.8%). For clusters G1 and G2 the optimal number of axes were 2 and 5, respectively. Patterns automatically generated along axes within the glaucoma clusters were similar to those known to be indicative of glaucoma. Fields located farther from the normal mean on each glaucoma axis showed increasing field defect severity. VIM successfully separated FDT fields from healthy and glaucoma eyes without a priori information about class membership, and identified familiar glaucomatous patterns of loss.", "keywords": ["Frequency Doubling Technology", "Matrix Frequency Doubling", "separate Matrix Frequency", "Doubling Technology", "Matrix Frequency", "Frequency Doubling", "variational Bayesian independent", "Bayesian independent component", "automatically separate Matrix", "unsupervised machine-learning classifier", "variational Bayesian", "separate Matrix", "Glaucoma Evaluation Study", "FDT", "independent component analysis-mixture", "UCSD-based Diagnostic Innovations", "Bayesian independent", "component analysis-mixture model", "Evaluation Study", "identify axes representing"], "paper_title": "Glaucomatous patterns in Frequency Doubling Technology (FDT) perimetry data identified by unsupervised machine learning classifiers.", "last_updated": "2023/02/04"}, {"id": "0027152250", "domain": "Glaucoma (unspecified)", "model_name": "Yousefi et al.", "publication_date": "2016/05/03", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/27152250/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "27152250", "task": "0GPcU42tzV", "abstract": "To validate Gaussian mixture-model with expectation maximization (GEM) and variational Bayesian independent component analysis mixture-models (VIM) for detecting glaucomatous progression along visual field (VF) defect patterns (GEM-progression of patterns (POP) and VIM-POP). To compare GEM-POP and VIM-POP with other methods. GEM and VIM models separated cross-sectional abnormal VFs from 859 eyes and normal VFs from 1117 eyes into abnormal and normal clusters. Clusters were decomposed into independent axes. The confidence limit (CL) of stability was established for each axis with a set of 84 stable eyes. Sensitivity for detecting progression was assessed in a sample of 83 eyes with known progressive glaucomatous optic neuropathy (PGON). Eyes were classified as progressed if any defect pattern progressed beyond the CL of stability. Performance of GEM-POP and VIM-POP was compared to point-wise linear regression (PLR), permutation analysis of PLR (PoPLR), and linear regression (LR) of mean deviation (MD), and visual field index (VFI). Sensitivity and specificity for detecting glaucomatous VFs were 89.9% and 93.8%, respectively, for GEM and 93.0% and 97.0%, respectively, for VIM. Receiver operating characteristic (ROC) curve areas for classifying progressed eyes were 0.82 for VIM-POP, 0.86 for GEM-POP, 0.81 for PoPLR, 0.69 for LR of MD, and 0.76 for LR of VFI. GEM-POP was significantly more sensitive to PGON than PoPLR and linear regression of MD and VFI in our sample, while providing localized progression information. Detection of glaucomatous progression can be improved by assessing longitudinal changes in localized patterns of glaucomatous defect identified by unsupervised machine learning.", "keywords": ["validate Gaussian mixture-model", "variational Bayesian independent", "Bayesian independent component", "Gaussian mixture-model", "validate Gaussian", "variational Bayesian", "component analysis mixture-models", "Bayesian independent", "expectation maximization", "POP", "independent component analysis", "GEM", "Gaussian", "Bayesian", "VIM", "mixture-model with expectation", "eyes", "analysis mixture-models", "glaucomatous", "independent component"], "paper_title": "Unsupervised Gaussian Mixture-Model With Expectation Maximization for Detecting Glaucomatous Progression in Standard Automated Perimetry Visual Fields.", "last_updated": "2023/02/04"}, {"id": "0026440445", "domain": "Glaucoma (unspecified)", "model_name": "Yousefi et al.", "publication_date": "2015/10/09", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/26440445/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "26440445", "task": "IWqQC1koJA", "abstract": "Detecting glaucomatous progression is an important aspect of glaucoma management. The assessment of longitudinal series of visual fields, measured using Standard Automated Perimetry (SAP), is considered the reference standard for this effort. We seek efficient techniques for determining progression from longitudinal visual fields by formulating the problem as an optimization framework, learned from a population of glaucoma data. The longitudinal data from each patient's eye were used in a convex optimization framework to find a vector that is representative of the progression direction of the sample population, as a whole. Post-hoc analysis of longitudinal visual fields across the derived vector led to optimal progression (change) detection. The proposed method was compared to recently described progression detection methods and to linear regression of instrument-defined global indices, and showed slightly higher sensitivities at the highest specificities than other methods (a clinically desirable result). The proposed approach is simpler, faster, and more efficient for detecting glaucomatous changes, compared to our previously proposed machine learning-based methods, although it provides somewhat less information. This approach has potential application in glaucoma clinics for patient monitoring and in research centers for classification of study participants.", "keywords": ["Standard Automated Perimetry", "longitudinal visual fields", "important aspect", "visual fields", "Automated Perimetry", "Standard Automated", "longitudinal visual", "glaucoma management", "progression", "longitudinal", "visual", "fields", "optimization framework", "aspect of glaucoma", "glaucoma", "Standard", "reference standard", "Detecting glaucomatous", "management", "Detecting glaucomatous progression"], "paper_title": "Detecting glaucomatous change in visual fields: Analysis with an optimization framework.", "last_updated": "2023/02/04"}, {"id": "0022003677", "domain": "Glaucoma (unspecified)", "model_name": "Xu et al.", "publication_date": "2011/11/15", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/22003677/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "22003677", "task": "IWqQC1koJA", "abstract": "We propose a machine learning framework based on sliding windows for glaucoma diagnosis. In digital fundus photographs, our method automatically localizes the optic cup, which is the primary structural image cue for clinically identifying glaucoma. This localization uses a bundle of sliding windows of different sizes to obtain cup candidates in each disc image, then extracts from each sliding window a new histogram based feature that is learned using a group sparsity constraint. An epsilon-SVR (support vector regression) model based on non-linear radial basis function (RBF) kernels is used to rank each candidate, and final decisions are made with a non-maximal suppression (NMS) method. Tested on the large ORIGA(-light) clinical dataset, the proposed method achieves a 73.2% overlap ratio with manually-labeled ground-truth and a 0.091 absolute cup-to-disc ratio (CDR) error, a simple yet widely used diagnostic measure. The high accuracy of this framework on images from low-cost and widespread digital fundus cameras indicates much promise for developing practical automated/assisted glaucoma diagnosis systems.", "keywords": ["machine learning framework", "propose a machine", "machine learning", "learning framework based", "sliding windows", "glaucoma diagnosis", "clinically identifying glaucoma", "learning framework", "sliding", "glaucoma", "assisted glaucoma diagnosis", "glaucoma diagnosis systems", "digital fundus photographs", "primary structural image", "structural image cue", "digital fundus", "based", "identifying glaucoma", "method automatically localizes", "windows"], "paper_title": "Sliding window and regression based cup detection in digital fundus images for glaucoma diagnosis.", "last_updated": "2023/02/04"}, {"id": "0031731092", "domain": "Glaucoma (unspecified)", "model_name": "Zhao et al.", "publication_date": "2019/10/31", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/31731092/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "31731092", "task": null, "abstract": "Multi-indices quantification of optic nerve head (ONH), measuring ONH appearance with multiple types of indices simultaneously from fundus images, is the most clinically significant tasks for accurate ONH assessment and ophthalmic disease diagnosis. However, no attempt has been reported due to its challenges of the large variation of fundus appearance across patients, heavy overlap and extremely weak contrast between optic nerve head areas. In this paper, we propose a multitask collaborative learning framework (MCL-Net) for multi-indices ONH quantification. The proposed MCL-Net, a two-branch neural network, first obtains expressive shared and task-specific representations with the backbone network and its two branches; then models the feature exchanges and aggregations between two branches with a well-designed feature interaction module (FIM) to promote each other collaboratively. After that, it estimates multiple types of ONH indices under a multitask ensemble module (MEM) that is capable of learning aggregation of multiple outputs automatically. Therefore, the proposed MCL-Net is consisted of the feature representation, inter-task feature interaction, dual-branch task-specific prediction, and multitask quantification ensemble, which establish an effective framework which takes full advantages of segmentation and estimation tasks for multi-indices ONH quantification. Rather than the low-level feature sharing and individual prediction, the proposed MCL-Net collaboratively learns an optimal combination of shared and task-specific representation, as well as the aggregated prediction, therefore leads to accurate quantification of ONH with multiple types of indices. Experimental results on the dataset of 650 fundus images show that MCL-Net successfully delivers accurate quantification of all the three types of ONH indices, with average mean absolute error of 0.98\u202f\u00b1\u202f0.20, 0.97\u202f\u00b1\u202f0.16, 1.19\u202f\u00b1\u202f0.18, as well as average correlation coefficient of 0.699, 0.708 and 0.691, for diameters, whole areas and regional areas, respectively. In addition, the experiments demonstrate that quantitative indices obtained by our method provide more effective glaucoma diagnosis with AUC of 0.8698. This endows our proposed MCL-Net a great potential in clinical assessment from focal to global for ophthalmic disease diagnosis.", "keywords": ["optic nerve head", "measuring ONH appearance", "multi-indices ONH quantification", "clinically significant tasks", "ONH", "optic nerve", "nerve head", "multi-indices ONH", "ONH quantification", "measuring ONH", "nerve head areas", "proposed MCL-Net", "ONH indices", "clinically significant", "multiple types", "ONH appearance", "quantification", "MCL-Net", "accurate ONH assessment", "fundus images"], "paper_title": "Multi-indices quantification of optic nerve head in fundus image via multitask collaborative learning.", "last_updated": "2023/02/04"}, {"id": "0017057807", "domain": "Glaucoma (unspecified)", "model_name": "Goldbaum et al.", "publication_date": "2006/12/26", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/17057807/", "code_link": null, "model_type": null, "verified": false, "model_task": null, "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "17057807", "task": null, "abstract": "We previously reported the use of clustering by unsupervised learning with machine learning classifiers to segment clusters of patterns in standard automated perimetry (SAP) for glaucoma. In this study, the process of unsupervised learning by independent component analysis decomposed SAP field patterns into axes, and the information represented by these axes was evaluated. SAP fields were obtained with the Humphrey Visual Field Analyzer on 189 normal eyes and 156 eyes with glaucomatous optic neuropathy (GON) determined by masked review with stereoscopic optic disc photos. The variational Bayesian independent component analysis mixture model (vB-ICA-mm) partitioned the SAP fields into the most informative number of clusters. Simultaneously, it learned an optimal number of maximally independent axes for each cluster. The most informative number of clusters was two. vB-ICA-mm placed 68.6% of the SAP fields from eyes with GON in a cluster labeled G and 98.4% of the fields from eyes with normal optic discs in a cluster labeled N. Cluster G optimally contained six axes. Post hoc analysis of patterns generated at -1 SD and +2 SD from the cluster G mean on the six axes revealed defects similar to those identified by experts as indicative of glaucoma. SAP fields associated with an axis showed increasing severity as they were located farther in the positive direction from the cluster G mean. vB-ICA-mm represented the SAP fields with patterns that were meaningful for glaucoma experts. This process also captured severity in the patterns uncovered. These findings should validate vB-ICA-mm as a data mining technique for new and unfamiliar complex tests.", "keywords": ["SAP fields", "standard automated perimetry", "machine learning classifiers", "SAP", "decomposed SAP field", "SAP field patterns", "Humphrey Visual Field", "Visual Field Analyzer", "unsupervised learning", "analysis decomposed SAP", "automated perimetry", "SAP field", "previously reported", "classifiers to segment", "standard automated", "fields", "cluster", "machine learning", "learning classifiers", "decomposed SAP"], "paper_title": "Unsupervised learning with independent component analysis can identify patterns of glaucomatous visual field defects.", "last_updated": "2023/02/04"}, {"id": "0036328200", "domain": "Glaucoma (unspecified)", "model_name": "Kamalipour et al.", "publication_date": "2022/11/01", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/36328200/", "code_link": null, "model_type": "GLM", "verified": false, "model_task": "Forecasting", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "36328200", "task": "0GPcU42tzV", "abstract": "To use longitudinal optical coherence tomography (OCT) and OCT angiography (OCTA) data to detect glaucomatous visual field (VF) progression with a supervised machine learning approach. Prospective cohort study. One hundred ten eyes of patients with suspected glaucoma (33.6%) and patients with glaucoma (66.4%) with a minimum of 5 24-2 VF tests and 3 optic nerve head and macula images over an average follow-up duration of 4.1 years were included. VF progression was defined using a composite measure including either a \"likely progression event\" on Guided Progression Analysis, a statistically significant negative slope of VF mean deviation or VF index, or a positive pointwise linear regression event. Feature-based gradient boosting classifiers were developed using different subsets of baseline and longitudinal OCT and OCTA summary parameters. The area under the receiver operating characteristic curve (AUROC) was used to compare the classification performance of different models. VF progression was detected in 28 eyes (25.5%). The model with combined baseline and longitudinal OCT and OCTA parameters at the global and hemifield levels had the best classification accuracy to detect VF progression (AUROC\u00a0=\u00a00.89). Models including combined OCT and OCTA parameters had higher classification accuracy compared with those with individual subsets of OCT or OCTA features alone. Including hemifield measurements significantly improved the models' classification accuracy compared with using global measurements alone. Including longitudinal rates of change of OCT and OCTA parameters (AUROCs\u00a0=\u00a00.80-0.89) considerably increased the classification accuracy of the models with baseline measurements alone (AUROCs\u00a0=\u00a00.60-0.63). Longitudinal OCTA measurements complement OCT-derived structural metrics for the evaluation of functional VF loss in patients with glaucoma.", "keywords": ["optical coherence tomography", "glaucomatous visual field", "machine learning approach", "supervised machine learning", "longitudinal optical coherence", "detect glaucomatous visual", "OCT", "OCTA", "coherence tomography", "visual field", "learning approach", "OCT and OCTA", "OCTA parameters", "optical coherence", "glaucomatous visual", "supervised machine", "machine learning", "OCT angiography", "Guided Progression Analysis", "progression"], "paper_title": "Combining Optical Coherence Tomography and Optical Coherence Tomography Angiography Longitudinal Data for the Detection of Visual Field Progression in Glaucoma.", "last_updated": "2023/02/04"}, {"id": "0015894180", "domain": "Glaucoma (unspecified)", "model_name": "Tucker et al.", "publication_date": "2005/09/13", "paper_link": "https://pubmed.ncbi.nlm.nih.gov/15894180/", "code_link": null, "model_type": null, "verified": false, "model_task": "Detection / diagnosis", "code_available": false, "disease_class": "6ct3D7Ut8A", "pmid": "15894180", "task": "IWqQC1koJA", "abstract": "Progressive loss of the field of vision is characteristic of a number of eye diseases such as glaucoma which is a leading cause of irreversible blindness in the world. Recently, there has been an explosion in the amount of data being stored on patients who suffer from visual deterioration including field test data, retinal image data and patient demographic data. However, there has been relatively little work in modelling the spatial and temporal relationships common to such data. In this paper we introduce a novel method for classifying visual field (VF) data that explicitly models these spatial and temporal relationships. We carry out an analysis of our proposed spatio-temporal Bayesian classifier and compare it to a number of classifiers from the machine learning and statistical communities. These are all tested on two datasets of VF and clinical data. We investigate the receiver operating characteristics curves, the resulting network structures and also make use of existing anatomical knowledge of the eye in order to validate the discovered models. Results are very encouraging showing that our classifiers are comparable to existing statistical models whilst also facilitating the understanding of underlying spatial and temporal relationships within VF data. The results reveal the potential of using such models for knowledge discovery within ophthalmic databases, such as networks reflecting the 'nasal step', an early indicator of the onset of glaucoma. The results outlined in this paper pave the way for a substantial program of study involving many other spatial and temporal datasets, including retinal image and clinical data.", "keywords": ["Progressive loss", "spatial and temporal", "irreversible blindness", "data", "temporal relationships", "field test data", "temporal", "spatial", "field", "eye diseases", "patient demographic data", "temporal relationships common", "deterioration including field", "including field test", "models", "classifying visual field", "relationships", "clinical data", "visual field", "field test"], "paper_title": "A spatio-temporal Bayesian network classifier for understanding visual field deterioration.", "last_updated": "2023/02/04"}]